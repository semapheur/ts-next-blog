---
title: 'Neural Networks'
subject: 'Machine Learning'
showToc: true
references:
  - book_calin_2020
  - book_mohri_etal_2012
  - book_murphy_2022
  - book_shalev-schwartz_ben-david_2014
  - book_ye_2022
---

A neural network can be represented by a directed graph whose nodes correspond to neurons and edges correspond to links between them. Each neuron receives as input a weighted sum of the outputs of the neurons connected to its incoming edges.

# Artificial neuron (unit)

An artificial neuron or a computing unit is typically modelled as an affine transformation followed by a non-linear function. If $\mathbf{x}\in X\subseteq \mathbb{F}^n$ is the input signal and $\mathbf{y}\in Y\subseteq \mathbb{F}^m$ is the out signal, an artifical neuron takes the form

$$
  \mathbf{y} = \sigma(\mathbf{Wx} + \mathbf{b})
$$

where
- $\mathbf{W}\in\mathbb{F}^{m\times n}$ represents linear weights
- $\mathbf{b}\in\mathbb{F}^m$ is the bias
- $\sigma: \mathbb{F}^m \to \mathbb{F}^m$

The input and output spaces can take any form, e.g.
- $\set{0,1}$: binary
- $\set{\pm 1}:$ boolean
- $[0,1]$: unit interval (representing probabilities)

## Activation functions

### Heaviside function

$$
  H(x) = \mathbf{1}_{x\geq 0} (x) := \begin{cases}
    1, \quad x\geq& 0 \\
    0, \quad x < 0
  \end{cases}
$$

An artificial neuron using the Heaviside activitation function is known as a *perceptron*.

### Rectified linear unit (ReLU)

$$
  \sigma(\lambda) = \operatorname{ReLU}(\lambda) := \max\set{0,\lambda}
$$

### Sigmoid

The (logistic) sigmoid function $\sigma:\R\to[0,1]$ is given by 

$$
  \sigma(\lambda) := \frac{1}{1 + e^{-\lambda}}
$$

The term "sigmoid" means S-shaped. The sigmoid function can be thought of as a "soft" version of the Heaviside step function.

<GraphFigure 
  expression="1/(1 + exp(-x))"
  points={100}
  xAxis={{scale: "linear", domain: [-2,2]}}
  yAxis={{scale: "linear", domain: [0,1]}}
  caption="Sigmoid function"
/>

<MathBox title='Properties of the sigmoid function' boxType='proposition'>
1. $\sigma'(\lambda) = \sigma(\lambda)(1 - \sigma(\lambda))$
2. $1 - \sigma(\lambda) = \sigma(\lambda)$
3. $\sigma^{-1} (\lambda) = \ln\left(\frac{\lambda}{1 - \lambda} \right) =: \operatorname{logit}(\lambda)$
4. $\sigma_+ (\lambda) := \ln(1 + e^\lambda) =: \operatorname{softplus}(\lambda)$
5. $\sigma'_+ (\lambda) = \sigma(\lambda)$
</MathBox>

### Hyperbolic tangent

$$
  \tanh(\lambda) := \frac{e^\lambda - e^{-\lambda}}{e^\lambda + e^{-\lambda}}
$$

<GraphFigure 
  expression="tanh(x)"
  points={100}
  xAxis={{scale: "linear", domain: [-2,2]}}
  yAxis={{scale: "linear", domain: [-1,1]}}
  caption="Hyperbolic tangent"
/>

### Swish

$$
  \operatorname{swish}_\beta (\lambda) := \lambda \sigma(\beta\lambda) := \frac{\lambda}{1 + e^{-\beta\lambda}},\; \beta > 0
$$
In case of $\beta = 1$, this function is also called the *sigmoid-weighted linear unit* (SiLU).

### Softmax

Softmax also known as the normalized exponential function: $\operatorname{softmax}:\R^n \to[0,1]^n$

$$
  \operatorname{softmax}(\mathbf{x}) := \frac{e^\mathbf{x}}{\sum_{i=1}^n e^{x_i}}
$$
Softmax has the useful property that its output is a discrete probability distribution.

### Maxpool

$\operatorname{maxpool}:\R^n \to\R^m$

$$
  \operatorname{maxpool}\left(\begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix} \right) := \begin{bmatrix}
    \max_{j\in I_1} x_j \\
    \vdots \\
    \max_{j\in I_m} x_j
  \end{bmatrix}
$$

where for each $i\in\set{1,\dots,m}$ we have a $I_i \subset\set{1,\dots,n}$ that specifies over which inputs to take the maximum for each output. Maxpooling can easily be generalized by replacing the max operation.

### Normalization

$\operatorname{normalize}:\R^n \to\R^n$:

$$
  \operatorname{normalize}(\mathbf{x}) := \frac{\mathbf{x} - \mu\mathbf{1}_n}{\sigma}
$$
where $\mu = \mathbb{E}(\mathbf{x})$ and $\sigma^2 = \operatorname{var}(\mathbf{x})$.

### Dropout

Dropout is a stochastic function that is often used during the training process but is removed once the training is finished. It works by randomly setting individuals values a signal to zero with probability $p$

$$
  \operatorname{dropout}_p (\mathbf{x})_i := \begin{cases}
    0 \quad &\text{with probability $p$} \\
    x_1 \quad &\text{with probability $p - 1$}
  \end{cases}
$$

### Heatbath

Heatbath is a scalar function that outputs $1$ or $-1$ with a probability that depends on the input
$$
  \operatorname{heatbath}(\lambda) := \begin{cases}
    1 \quad &\text{with probability $p$} \\
    -1 \quad &\text{otherwise}
  \end{cases}
$$

# Artifical neural network (ANN)

## Feedforward neural network (FFNN)

<MathBox title='Parametric affine function' boxType='definition'>
Let $\boldsymbol{\theta} \in\Theta\subseteq\R^p$ be a parameter vector of dimension $p\in\N_+$. For $m, n\in\N_+$ and $s\in\N_0$ satisfying $p \geq s + mn + m$, we define an affine function $A_{m,n}^{\boldsymbol{\theta}, s} :\R^n \to \R^m$ associated with parameters $\boldsymbol{\theta}$, starting from index $s$, as

$$
  A_{m,n}^{\boldsymbol{\theta}, s} (\mathbf{x}) = \mathbf{Wx} + \mathbf{b}
$$

where the weight matrix $\mathbf{W}\in\R^{m\times n}$ and bias vector $\mathbf{b}\in\R^m$

$$
  \mathbf{W} = \begin{bmatrix} 
    \theta_{s+1} & \cdots & \theta_{s + n} \\
    \theta_{s + n + 1} & \cdots & \theta_{s + 2n} \\
    \vdots & \ddots & \vdots \\
    \theta_{s + (m - 1)n + 1} & \cdots & \theta_{s + mn}  
  \end{bmatrix},\quad \mathbf{b} \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} + \begin{bmatrix} \theta_{s + mn + 1} \\ \theta_{s + mn + 2} \\ \vdots \\ \theta_{s + mn + m} \end{bmatrix}
$$

Explicitly, the function can be written component-wise as

$$
  A_{m,n}^{\boldsymbol{\theta}, s} (\mathbf{x}) = \begin{bmatrix} 
    \left(\sum_{k=1}^n x_k \theta_{s + k} \right) + \theta_{s + mn + 1} \\ 
    \left(\sum_{k=1}^n x_k \theta_{s + n + k} \right) + \theta_{s + mn + 2} \\
    \vdots \\
    \left(\sum_{k=1}^n x_k \theta_{s + (m - 1)n + k} \right) + \theta_{s + mn + m}
  \end{bmatrix}
$$
</MathBox>

<MathBox title='Fully-connected feedforward neural network' boxType='definition'>
Let $\boldsymbol{\theta}\in\Theta\subseteq\R^p$ be a parameter vector of dimension $p\in\N_+$. A fully connected feedforward neural network (FFNN) with $L \in N_+$ layers is characterized by its architecture $a = (\mathbf{n}, \boldsymbol{sigma})$ for $\mathbf{n} \in\N_+^{L+1}$, where
- $\mathbf{n} = (n_\ell)_{\ell=0}^L$ denotes the number of neurons in each layer $\ell$
- $\boldsymbol{\sigma} = (\sigma_\ell : \R\to\R)_{\ell=0}^L$ denotes the activation function applied componentwise to each layer $\ell$

Layer indexing follows the convention:
- $\ell = 0$: input layer
- $\ell = L$: output layer
- $\ell \in [L - 1]$: hidden layers 

In terms on neurons, the parameter dimension is given by

$$
  p = \sum_{\ell=1}^L n_\ell (n_{\ell - 1} + 1)
$$

The width and depth of the FNNN architecture are given by $\lVert N \rVert_\infty$ and $L$, respectively, where $N = \sum_{\ell=0}^L n_\ell$ is the total number of neurons. The architecture is called deep if $L > 2$ and shallow if $L = 2$.

The realization function of the FFNN, denoted $\Phi_a : \R^{n_0} \times \R^p \to\R^{n_L}$ is given by

$$
\begin{align*}
  \Phi_a (\mathbf{x}, \boldsymbol{\theta}) =& \sigma_L \circ A_{n_L, n_{L-1}}^{s_{L-1}} \circ \sigma_{L-1} \circ A_{n_{L-1}, n_{L-2}}^{s_{L-2}} \circ\cdots \\
  & \circ \sigma_1 \circ A_{n_1, n_0}^0 (\mathbf{x}, \boldsymbol{\theta})
\end{align*}
$$

where the shift indices $s_\ell$ are defined as

$$
  s_\ell = \sum_{k=1}^\ell n_k (n_{k-1} + 1),\; \ell = 1,\dots,L - 1
$$

For each hidden layer $\ell = 1,\dots, L - 1$, we define the pre-activation $\Phi^{(\ell)}: \R^{n_{\ell - 1}} \times \R^p \to \R ^{\boldsymbol{\theta}, n_{\ell - 1}}$ and activation $\bar{\Phi}^{(\ell)} : \R^{n_{\ell - 1}} \times \R^p \to \R ^{\boldsymbol{\theta}, n_{\ell - 1}}$ as

$$
  \bar{\Phi}^{(ell)} = \sigma_\ell \left(\Phi^{(\ell)}\right)
$$

Each affine functions $A_{n_\ell, n_{\ell-1}}^{s_{n_{\ell-1}}}$ for $\ell = 1,\dots,L$, is given by

$$
  A_{n_\ell, n_{\ell-1}}^{s_{\ell-1}} (\mathbf{x}, \boldsymbol{\theta}) = \mathbf{W}^{(\ell)} \mathbf{x} + \mathbf{b}^{(\ell)}
$$

where the weights $\mathbf{W}^{(\ell)} \in \R^{n_\ell \times n_{\ell-1}}$ and biases $\mathbf{b}^{(\ell)} \in \R^{n_\ell}$ from layer $\ell - 1$ to $\ell$ are given by

$$
  \mathbf{W}^{(\ell)} = \begin{bmatrix}
    \theta_{s_{\ell - 1} + 1} & \cdots & \theta_{s_{\ell - 1} + n_{\ell - 1}} \\
    \theta_{s_{\ell - 1} + n_{\ell - 1} + 1} & \cdots & \theta_{s_{\ell - 1} + 2n_{\ell - 1}} \\
    \vdots & \ddots & \vdots \\
    \theta_{s_{\ell - 1} + (n_\ell - 1)n_{\ell - 1} + 1} & \cdots & \theta_{s_{\ell - 1} + n_\ell n_{\ell - 1}} 
  \end{bmatrix},\quad \mathbf{b}^{(\ell)} = \begin{bmatrix} \theta_{s_{\ell - 1} + n_\ell n_{\ell - 1} + 1} \\ \cdots \\ \theta_{s_{\ell - 1} + n_\ell n_{\ell - 1} + n_\ell} \end{bmatrix}
$$

In terms of $\mathbf{W}^{(\ell)}$ and  $\mathbf{b}^{(\ell)}$, the parameters $\boldsymbol{\theta}$ can be written as

$$
  \boldsymbol{\theta} = \left((\mathbf{W}^{(\ell)}, \mathbf{b}^{(\ell)})\right)_{\ell=1}^L \in \prod_{\ell=1}^L (\R^{n_\ell \times n_{\ell-1}} \times \R^{n_\ell}) \cong \R^p
$$
</MathBox>

<LatexFigure width={75} src='/fig/feedforward_neural_network.svg' alt=''
  caption='Feedforward neural network'
>
```latex
\documentclass[border=5pt]{standalone}
\usepackage{tikz}
\usepackage{amsmath}
\usetikzlibrary{arrows.meta, graphs, graphdrawing, positioning, backgrounds, fit, calc}
\usegdlibrary{layered}

\begin{document}
\begin{tikzpicture}[
    neuron/.style={circle, draw, minimum size=1.2cm, inner sep=0.5pt, font=\footnotesize},
    transformation/.style={font=\footnotesize, align=center},
    layer label/.style={font=\small\bfseries, align=center},
    dots/.style={font=\large},
    input background/.style={fill=red!5, rounded corners, draw=red!40, dashed,
        inner sep=10pt},
    hidden background/.style={fill=blue!5, rounded corners, draw=blue!40, dashed,
        inner sep=10pt},
    output background/.style={fill=green!5, rounded corners, draw=green!40, dashed,
        inner sep=10pt}
  ]

  % Constants for better layout
  \def\layerdist{3.5cm}
  \def\nodedist{1.8cm}
  \def\numLayers{6}

  % Layer 1: Input
  \node[neuron] (x1) at (0,0) {$x_1$};
  \node[neuron] (x2) at (0,-\nodedist) {$x_2$};
  \node[dots] (xdots) at (0,-2*\nodedist) {$\vdots$};
  \node[neuron] (x3) at (0,-3*\nodedist) {$x_{n_0}$};

  % Layer 2: First hidden layer
  \node[neuron] (h11) at (\layerdist,0) {$\Phi_1^{(1)}$};
  \node[neuron] (h12) at (\layerdist,-\nodedist) {$\Phi_2^{(1)}$};
  \node[dots] (h1dots) at (\layerdist,-2*\nodedist) {$\vdots$};
  \node[neuron] (h13) at (\layerdist,-3*\nodedist) {$\Phi_{n_1}^{(1)}$};

  % Layer 3: First hidden layer activated
  \node[neuron] (ph11) at (2*\layerdist,0) {$\bar{\Phi}_1^{(1)}$};
  \node[neuron] (ph12) at (2*\layerdist,-\nodedist) {$\bar{\Phi}_2^{(1)}$};
  \node[dots] (ph1dots) at (2*\layerdist,-2*\nodedist) {$\vdots$};
  \node[neuron] (ph13) at (2*\layerdist,-3*\nodedist) {$\bar{\Phi}_{n_1}^{(1)}$};

  % Middle layers indication
  \node[dots] (ldots) at (2.5*\layerdist,-1.5*\nodedist) {$\cdots$};

  % Layer 4: Last hidden layer activated
  \node[neuron] (ph21) at (3*\layerdist,0) {$\bar{\Phi}_1^{(L-1)}$};
  \node[neuron] (ph22) at (3*\layerdist,-\nodedist) {$\bar{\Phi}_2^{(L-1)}$};
  \node[dots] (ph2dots) at (3*\layerdist,-2*\nodedist) {$\vdots$};
  \node[neuron] (ph23) at (3*\layerdist,-3*\nodedist) {$\bar{\Phi}_{n_{L-1}}^{(L-1)}$};

  % Layer 5: Last layer
  \node[neuron] (h21) at (4*\layerdist,0) {$\Phi_1^{(L)}$};
  \node[neuron] (h22) at (4*\layerdist,-\nodedist) {$\Phi_2^{(L)}$};
  \node[dots] (h2dots) at (4*\layerdist,-2*\nodedist) {$\vdots$};
  \node[neuron] (h23) at (4*\layerdist,-3*\nodedist) {$\Phi_{n_L}^{(L)}$};

  % Layer 6: Output
  \node[neuron] (y1) at (5*\layerdist,0) {$y_1$};
  \node[neuron] (y2) at (5*\layerdist,-\nodedist) {$y_2$};
  \node[dots] (ydots) at (5*\layerdist,-2*\nodedist) {$\vdots$};
  \node[neuron] (y3) at (5*\layerdist,-3*\nodedist) {$y_{n_L}$};

  % Layer backgrounds
  \begin{pgfonlayer}{background}
    \node[input background, fit=(x1) (x3)] (layer1) {};
    \node[hidden background, fit=(h11) (h13)] (layer2) {};
    \node[hidden background, fit=(ph11) (ph13)] (layer3) {};
    \node[hidden background, fit=(ph21) (ph23)] (layer4) {};
    \node[hidden background, fit=(h21) (h23)] (layer5) {};
    \node[output background, fit=(y1) (y3)] (layer6) {};
  \end{pgfonlayer}

  % Layer labels
  \node[layer label, above=0.3cm of layer1] {Input\\Layer};
  \node[layer label, above=0.3cm of layer2] {First Hidden\\Layer};
  \node[layer label, above=0.3cm of layer3] {Activated\\Layer 1};
  \node[layer label, above=0.3cm of layer4] {Activated\\Layer $L-1$};
  \node[layer label, above=0.3cm of layer5] {Final Hidden\\Layer $L$};
  \node[layer label, above=0.3cm of layer6] {Output\\Layer};

  % Transformations
  \node (x) at ($(layer1.south)+(0, -0.3)$) {$\mathbf{x}$};

  \node[transformation] (t1) at ($(layer1.east)!0.5!(layer2.west)+(0,-2.75*\nodedist)$)
  {$\mathbf{W}^{(1)}\mathbf{x} + \mathbf{b}^{(1)}$};

  \node (h1) at ($(layer2.south)+(0, -0.3)$) {$\boldsymbol{\Phi}^{(1)}$};

  \node[transformation] (t2) at ($(layer2.east)!0.5!(layer3.west)+(0,-2.75*\nodedist)$)
  {$\sigma_1(\boldsymbol{\Phi}^{(1)})$};

  \node (a1) at ($(layer3.south)+(0, -0.3)$) {$\bar{\boldsymbol{\Phi}}^{(1)}$};

  \node (a2) at ($(layer4.south)+(0, -0.3)$) {$\bar{\boldsymbol{\Phi}}^{(L-1)}$};

  \node[transformation] (t3) at ($(layer4.east)!0.5!(layer5.west)+(0,-2.75*\nodedist)$)
  {$\mathbf{W}^{(L)}\bar{\boldsymbol{\Phi}}^{(L-1)} + \mathbf{b}^{(L)}$};

  \node (h2) at ($(layer5.south)+(0, -0.3)$) {$\boldsymbol{\Phi}^{(L)}$};

  \node[transformation] (t4) at ($(layer5.east)!0.5!(layer6.west)+(0,-2.75*\nodedist)$)
  {$\sigma_L(\boldsymbol{\Phi}^{(L)})$};

  \node (y) at ($(layer6.south)+(0, -0.3)$) {$\mathbf{y}$};

  \draw[|->] (x) -- (t1);
  \draw[|->] (t1) -- (h1);
  \draw[|->] (h1) -- (t2);
  \draw[|->] (t2) -- (a1);

  \draw[|->] (a2) -- (t3);
  \draw[|->] (t3) -- (h2);

  \draw[|->] (h2) -- (t4);
  \draw[|->] (t4) -- (y);

  % Connect nodes with edges
  % Layer 1 to Layer 2 (complete bipartite)
  \foreach \i in {1,2,3} {
      \foreach \j in {1,2,3} {
          \draw[->, gray] (x\i) -- (h1\j);
        }
    }

  % Layer 2 to Layer 3 (matching)
  \foreach \i in {1,2,3} {
      \draw[->, gray] (h1\i) -- (ph1\i);
    }

  % Layer 3 to Layer 4 (indicated by dots)
  \draw[->, gray, dashed] (ph11) to[bend left=15] (ph21);
  \draw[->, gray, dashed] (ph13) to[bend right=15] (ph23);

  % Layer 4 to Layer 5 (complete bipartite)
  \foreach \i in {1,2,3} {
      \foreach \j in {1,2,3} {
          \draw[->, gray] (ph2\i) -- (h2\j);
        }
    }

  % Layer 5 to Layer 6 (complete bipartite)
  \foreach \i in {1,2,3} {
      \foreach \j in {1,2,3} {
          \draw[->, gray] (h2\i) -- (y\j);
        }
    }

\end{tikzpicture}
\end{document}
```
</LatexFigure>

<MathBox title='Hypothesis sets of neural networks' boxType='definition'>
Let $a = (\mathbf{n}, \boldsymbol{\sigma})$ be a neural network architecture with input dimension $n_0 = d$, output dimension $n_L = 1$, and measurable activation functions $\boldsymbol{\sigma}$. For regression taks the corresponding hypothesis set is given by

$$
  F_a = \set{\Phi_a (\cdot, \boldsymbol{\theta}) | \boldsymbol{\theta} \in\R^p}
$$

and for classification tasks by

$$
  F_a = \set{\operatorname{sgn}(\Phi_a (\cdot, \boldsymbol{\theta})) | \boldsymbol{\theta}\in\R^p}
$$

where

$$
  \operatorname{sgn}(x) := \begin{cases} 1,\quad& x\geq 0 \\ -1,\quad& x < 0
$$
</MathBox>

A feedforward neural network (FFNN) is described by a directed asyclic graph, $G = (V,E)$, and a weight function over the edges, $w = E\to\R$. Nodes of the graph correspond to neurons. Each single neuron is modeled as a real function $\sigma:\R\to\R$ called the activation function of the neuron. Each edge in the graph link the output of some neuron to the input of another neuron. The input of a neuron is obtained by taking a weighted sum of the outputs of all the neurons connected to it, where the weighting is according to $w$.

To network is further organized into layers. The set of nodes can be decomposed into a union a disjoint subsets $V = \sqcup_{t=0}^T V_t$ such that every edge in $E$ connects some node in $V_{t-1}$ to some node in $V_t$ for some $t \in [T]$. The bottom layer, $V_0$, is called the input layer and contains $n + 1$ neurons, where $n$ is the dimensionality of the input space. For every $i\in [n]$, the output of neuron $i$ is simply $x_i$. The last neuron in $V_0$ is the intercept node, which always outputs $1$. Layers $V_1,\dots, V_{T-1}$ are often called *hidden layers*. The top layer, $V_T$ is called the output layers.

We denote by $v_{t,i}$ the $i$ th neuron of the $t$th layer and by $o_{t,i} (\mathbf{x})$ the output of $v_{t,i}$, when the network is fed with the input vector $\mathbf{x}$. Thus, for $i\in[n]$ we have $o_{0,i} (\mathbf{x}) = x_i$ and for $i = n+1$ we have $o_{0, i} (\mathbf{x}) = 1$. The outputs of the neurons at layer $t + 1$ is calculated recursively in the following way. Fix some $v_{t+1, j}\in V_{t+1}$ and let $a_{t+1, j} (\mathbf{x})$ denote the input to $v_{t+1,j}$ when the network is fed with the input vector $\mathbf{x}$. Then

$$
\begin{align*}
  a_{t+1, j} (\mathbf{x}) =& \sum_{r: (v_{t,r}, v_{t+1, j})\in E} w((v_{t,r}, v_{t+1, j})) o_{t,r} (\mathbf{x})
  o_{t+1, j} (\mathbf{x}) =& \sigma(a_{t+1m j}(\mathbf{x}))
\end{align*}
$$

That is, the input to $v_{t+1,j}$ is a weighted sum of the outpus of the neurons in $V_t$ that are connected to $v_{t+1,j}$, where weighting is according to $w$, and the output of $v_{t+1}$ is simply the application of the activation function $\sigma$ on its input.

We refer to $T$ as the number of layers in the network (excluding $V_0$), or alternatively the *debth* of the network. The size of the network is denoted $|V|$ and the width of the netweek is $\max_{t\in T} |V|_t$.

### Approximation

<MathBox title='Universal approximation theorem' boxType='theorem' tag='theorem-1'>
Let $d\in \N$, let $K \subseteq \R^d$ be compact, and let $\sigma \in L_\text{loc}^\infty (\R)$ be a locally bounded activation function such that the closure of the points of discontinuity of $\sigma$ has Lebesgue measure zero (so in particular $\sigma$ is measurable). Further, let

$$
  \tilde{F} := \bigcup_{n\in\N} F_{(d,n,1),\sigma}
$$

be the corresponding set of two-layer neural network realizations with input dimension $d$, $n$ hidden neurons and one-dimensional output. Then $C(K) \subset \operatorname{cl}(\tilde{F})$, i.e. $\tilde{F}$ is dense in $C(K)$, if only if there does not exist a polynomial $p:\R\to\R$ with $p = \sigma$ almost everywhere. Here the closure is taken with respect to the topology induced by the $L^\infty (K)$-norm.
</MathBox>

Theorem $\ref{theorem-1}$ can be proven by the Hahn-Banach theorem, which implies that $\tilde{F}$ being dense in some real normed vector space $S$ is equivalent to the following condition: For all non-trivial functionals $F\in S' \setminus\set{0}$ from the topological dual space of $S$ there exists parameters $\mathbf{w}\in\R^d$ and $b\in\R$ such that

$$
  F(\sigma(\braket{\mathbf{w}, \cdot} + b)) \neq 0
$$

In case of $S = C(K)$ we have by the Riesz-Markov-Kakutani representation theorem that $S'$ is the space of signed Borel measures on $K$. Therefore, Theorem $\ref{theorem-1}$ holds, if $\sigma$ is such that, for a signed Borel meausre $\mu$

$$
\begin{equation*}
  \int_K \sigm(\braket{\mathbf{w}, \mathbf{x}} + b) \d\mu(x) = 0
\tag{\label{equation-2}}
\end{equation*}
$$

for all $\mathbf{w}\in\R^d$ and $b\in\R$ implies that $\mu = 0$. An activation function $\sigma$ satisfying this condition is called *discriminatory*. It is not hard to see that any sigmoidal $\sigma$ is discriminatory. Indeed, assume that $\sigma$ satisfies $\eqref{equation-2}$. Since for every $\mathbf{x}\in\R^d$ it holds that $\sigma(a\mathbf{x} + b) \to \mathbf{1}_{(0,\infty)} (\mathbf{x}) + \sigma(b) \mathbf{1}_{\set{0}}(\mathbf{x})$ for $a\to\infty$, we conclude by superposition and passing to the limit that for all $c_1, c_2 \in\R$ and $\mathbf{w}\in\R^d$, $b\in\R$

$$
  \int_K \mathbf{1}_{[c_1, c_2]} (\braket{\mathbf{w}, \mathbf{x}} + b) \d\mu(\mathbf{x}) = 0
$$

Representing the exponential function $\mathbf{x} \mapsto e^{-2\pi i\mathbf{x}}$ as the limit of sums of elementary functions yields that

$$
  \int_K \exp(-2\pi i (\braket{\mathbf{w}, \mathbf{x}} + )) \d\mu(x) = 0,\; \forall \mathbf{w}\in\R^d, b\in\R
$$

Hence, the Fourier transform of $\mu$ vanishes which implies that $\mu = 0$.

<MathBox title='Interpolation' boxType='proposition'>
Let $d, m \in \N$, let $x^{(i)} \in\R^d$ for $i\in [m]$, with $x^{(i)} \neq x^{(j)}$ for $i \neq j$, let $\sigma\in C(\R)$, and assume that $\sigma$ is not a polynomial. Then, there exists parameters $\theta^{(1)} \in \R^{m\times d} \times \R^m$ with the following property: For every $k\in\N$ and every sequence of label $y^{(i)} \in\R^k$ for $i\in [m]$, there exist parameters $\theta^{(2)} = (W^{(2)}, 0) \in \R^{k\times m} \times \R^k$ for the second layer of the neural network architecture $a = ((d, m, k), \sigma)$ such that

$$
  \Phi_a (x^{(i)}, (\theta^{(1)}, \theta^{(2)})) = y^{(i)} ,\; i\in[m]
$$

<details>
<summary>Proof sketch</summary>

Note that Theorem $\ref{theorem-1}$ also holds for functions $g \in C(K, R^m)$ with multi-dimensional outpu by approximating each one-dimensional component $\mathbf{x} \mapsto (g(\mathbf{x}))_i$ and stacking the resulting networks. Second, we can add an additional row containing only zeroes to the weight matrix $\mathbf{W}^{(1)}$ of the approximating neural network as well as an additional entry to the bias vector $\mathbf{b}^{(1)}$. The effect of this is that we obtain an additional neuron with constant output. Since $\sigma \neq 0$ we can choose $\mathbf{b}^{(1)}$ such that the output of this neuron is not zero. Thus, we can include the bias vector $\mathbf{b}^{(2)}$ of the second layer into the weight matrix $\mathbf{W}^{(2)}$. Now choose $g\in C(\R^m, \R^m)$ to be a function satisfying $g(x^{(i)}) = e^{(i)}$, for $i\in[m]$, where $e^{(i)} \in\R^m$ denotes the $i$th standard basis vector. By the discussion before there exists a neural network architecture $\tilde{a} = ((d, n, m), \sigma)$ and parameters $\tilde{\boldsymbol{\theta}} = ((\tilde{\mathbf{W}}^{(1)}, \tilde{\mathbf{b}}^{(1)}), (\tilde{\mathbf{W}}^{(2)}, \mathbf{0}))$ such that

$$
\begin{equation*}
  \Norm{\Phi_{\tilde{a}} (\cdot, \tilde{\boldsymbol{\theta}}) - g}_{L^\infty (K)} < \frac{1}{m}
\tag{\label{equation-3}}
\end{equation*}
$$

where $K$ is a compact set with $\mathbf{x}^{(i)} \in K$ for $i\in[m]$. Let us abbreviate the output of the activations in the first layer evaluated at the input features by

$$
  \tilde{\mathbf{A}} := \left[\sigma\left(\tilde{W}^{(1)} (\mathbf{x}^{(1)}) + \tilde{\mathbf{b}}^{(1)} \right),\dots,\sigma\left(\tilde{\mathbf{W}}^{(1)} (\mathbf{x}^{(m)}) + \tilde{\mathbf{b}}^{(1)}\right)\right] \in\R^{n\times m}
$$

The equivalence of the max and operator norm and $\eqref{equation-3}$ establish that

$$
\begin{align*}
  \Norm{\tilde{\mathbf{W}}^{(2)} \tilde{\mathbf{A}} - \mathbf{I}_m}_\text{op} \leq& m \max_{i,j \in [m]} \left|(\tilde{\mathbf{W}}^{(2)} \tilde{\mathbf{A}} - \mathbf{I}_m)_{i,j} \right| \\
  =& m \max_{j\in [m]} \Norm{\Phi_{\tilde{a}} (\mathbf{x}^{(j)}, \tilde{\boldsymbol{\theta}}) - g(\mathbf{x}^{(j)})}_\infty < 1
\end{align*}
$$

where $\mathbf{I}_m$ denotes the $m\times m$ identity matrix. Thus, the matrix $\tilde{\mathbf{W}}^{(2)} \tilde{\mathbf{A}} \in\R^{m\times m}$ needs to have full rank and we can extract $m$ linearly independent rows from $\tilde{\mathbf{A}}$ resulting in an invertible matrix $\mathbf{A}\in\R^{m\times m}$. Now, we define the desired parameters $\boldsymbol{\theta}^{(1)}$ for the first layer by extracting the corresponding rows from $\tilde{\mathbf{W}}^{(1)}$ and $\tilde{\mathbf{b}}^{(1)}$ and the parameters $\boldsymbol{\theta}^{(2)}$ of the second layer by

$$
  \mathbf{W}^{(2)} := \begin{bmatrix} \mathbf{y}^{(1)} & \dots & \mathbf{y}^{(m)} \mathbf{A}^{-1} \in\R^{k\times m}
$$

This proves that with any discriminatory activation function we can interpolate arbitrary training data $(\mathbf{x}^{(i)}, \mathbf{y}^{(i)}) \in\R^d \times\R^k$ for $i\in [m]$, using a two layer neural networkd with $m$ hidden neurons, i.e. $O(m(d + k))$ parameters.
</details>
</MathBox>

<MathBox title='Approximation of smooth functions' boxType='theorem' tag="theorem-2">
Let $d, k \in \N$, $p\in [1,\infty]$, and let $\sigma \in C^\infty (\R)$ be a smooth activation function, which is not polynomial. Then there exists a constant $c \in (0,\infty)$ with the following property: For every $n\in\N$ there exist first-layer parameters $\theta^{(1)} \in\R^{n\times d} \times \R^n$ for the two-layer neural network architecture $a = ((d, n, 1), \sigma)$ such that for every $g\in W^{k,p} ((0,1)^d)$ we have

$$
  \inf_{\theta^{(2)} \in\R^{1\times n} \times\R} \Norm{\Phi_a (\cdot, (\theta^{(1)}, \theta^{(2)})) - g}_{L^p ((0,1)^d)} \leq cn^{-d/k} \norm{g}_{W^{k,p} ((0,1)^d)}
$$

where $\Phi_a$ is the realization function of the neural network.

<details>
<summary>Proof sketch</summary>

The proof proceeds by polynomial reduction. The idea is
1. If the activation function $\sigma$ has a derivative of some order $p$ that is nonzero at a point $\lambda\in\R$, then we can approximate the monomial $x\mapsto x^p$ on any compact interval by a fixed-size neural network whose parameters grow in magnitude as the approximation accuracy increases.
2. Specifically, for $p$-times differentiable $\sigma$ with $\sigma^{(p)} (\lambda) \neq 0$, we can recover $x^p$ from rescaled $p$th order forward differences of $\sigma$

$$
  \lim_{h\to 0} \sup_{x\in K} \left| \sum_{i=0}^p \frac{(-1)^i \binom{p}{i}}{h^p \sigma^{(p)} (\lambda)} \sigma((p/2 - i) hx + \lambda) - x^p \right| = 0
$$

where $K\subset\R$ is compact.

3. Since $\sigma$ is smooth and not a polynomial, it has a nonzero derivative of every order $p$ at som point $\lambda_p$. This allows reproduction of all univariate monomials $x \mapsto x^p$. Tensor-product constructions then extend this to multivariate polynomials.
4. Classical result from nonlinear approximation theory state that $n$-term approximation from a space containing all multivariate polynomials of degree $\leq m$ achieve the optimal rate $n^{-k/d}$ in $W^{k,p}$-norm. By reproducing polynomials, the neural network inherits this rate.
</details>
</MathBox>

Theorem $\ref{theorem-2}$ shows that two-layer neural networks with smooth, non-polynomial activations achieve the same optimal approximation rates $O(n^{-k/d})$ for $W^{k,p}$-smooth functions on $(0,1)^d$ as classical methods such as spline bases or piecewise polynomial approximation. In general, many approximation schemes can be emulated by neural networks without loss of convergence rate.

### Learning neural networks

A feedforward neural network specified by a graph $(V,E)$, activation function $\sigma:\R\to\R$ and weight function $w:E\to\R$ defines a function

$$
  h_{V,E,\sigma,w} : \R^{|V_0|-1} \to \R^{|V_T|}
$$

Any set of such functions can serve as a hypothesis class for learning. Usually, we define a hypothesis class of neural network predictors by fixing the graph $(V,E)$ as well as the activation function $\sigma$ and letting the hypothesis class be all functions of the form $h_{V,E,\sigma,w}$ for some $w: E\to\R$. The triplet $(V,E,\sigma)$ is often called the *architecture* of the network. We denote the hypothesis class by $\mathcal{H}_{V,E,\sigma} = \Set{h_{V,E,\sigma,w} | w:E\to\R}$.

<MathBox title='Neural networks implement all Boolean functions' boxType='proposition'>
For every $n\in\N$, there exists a graph $(V,E)$ of depth $2$, such that $\mathcal{H}_{V, E, \operatorname{sign}}$ contains all functions from $\set{\pm 1}^n$ to $\set{\pm 1}$.

<details>
<summary>Proof</summary>

We construct a graph with $|V_0| = n + 1$, |V_1| = 2^n + 1 and $|V_2| = 1$. Let $E$ be all possible edges between adjacent layers. Consider a Boolean function $f:\set{\pm 1}^n \to\set{\pm 1}$. We need to show that we can adjust the weights so that the network will implement $f$. Let $\mathbf{u}_1,\dots,\mathbf{u}_k$ be all vectors in $\set{\pm 1}^n$ on $f$ which outputs $1$. Note that for every $i$ and every $\mathbf{x}\in\set{\pm 1}^n$, if $\mathbf{x}\neq\mathbf{u}_i$, then $\langle\mathbf{x},\mathbf{u}_i \rangle\leq n - 2$ and if $\mathbf{x} = \mathbf{u}_i$, then $\langle\mathbf{x},\mathbf{u}_i \rangle = n$. It follows that 

$$
  g_i (\mathbf{x}) = \operatorname{sign}(\langle\mathbf{x},\mathbf{u}_i\rangle - n + 1) = 1 \iff \mathbf{x} = \mathbf{n}
$$

Consequently, we can adapt the weights between $V_0$ and $V_1$ so that for every $i\in [k]$, the neuron $v_{1,i}$ implements the function $g_i$. Finally, note that $f$ is the disjunction of the functions $g_i$ allowing us to write

$$
  f(\mathbf{x}) = \operatorname{sign}\left(\sum_{i=1}^k g_i (\mathbf{x}) + k -1 \right)
$$
</details>
</MathBox>

<MathBox title='' boxType='theorem'>
For every $n\in\N$, let $s(n)$ be the minimal integer such that there exists a graph $(V,E)$ with $|V| = s(n)$ such that the hypothesis class that $\mathcal{H}_{V, E, \operatorname{sign}}$ contains all the functions from $\set{0,1}^n$ to $\set{0,1}$. Then, $s(n)$ is exponential in $n$. Similar results holds for $\mathcal{H}_{V, E, \sigma}$ where $\sigma$ is the sigmoid function.

<details>
<summary>Proof</summary>

Suppose that for some $(V,E)$ we have that $\mathcal{H}_{V, E, \operatorname{sign}}$ contains all functions from $\set{0,1}^n$ to $\set{0,1}$. It follows that it can shatter the set of $m = 2^n$ vectors in $\set{0,1}^n$ and hence the VC dimension of $\mathcal{H}_{V, E, \operatorname{sign}}$ is $2^n$. On the other hand, the VC dimension of $\mathcal{H}_{V, E, \operatorname{sign}}$ is bounded by $O(|E| \ln(|E|))\leqO(|V|^3)$. This implies that $|V|\geq\Omega(2^{n/3})$, which concludes the proof for the case with the sign activation function. The proof for the sigmoid case is analogous.
</details>
</MathBox>

It is possible derive a similar theorem for $\mathcal{H}_{V, E, \operatorname{sign}}$ for any activation function $\sigma$, as long as we restict the weights so that it is possible to express every weight using a number of bits which is bounded by a universal constant. We can even consider hypothesis clases where different neurons can employ different activation functions, as long as the number of allowed activation functions is also finite.

<MathBox title='' boxType='corollary'>
Fix some $\epsilon\in(0,1)$. For every $n\in\N$, let $s(n)$ be the minimal integer such that there exists a graph $(V,E)$ with $|V| = s(n)$ such that the hypothesis class $\mathcal{H}_{V,E,\sigma}$ with $\sigma$ being the sigmoid function, can approximate, to within precision of $\epsilon$, every $1$-Lipschitz function $f:[-1,1]^n \to[-1,1]$. Then $s(n)$ is exponential in $n$.
</MathBox>

<MathBox title='' boxType='theorem'>
Let $T:\N\to\N$ and for every $n\in\N$, let $\mathcal{F}_n$ be the set of functions that can be implemented using a Turing maching with runtime of a most $T(n)$. Then, there exis constants $b, c\in\R_+$ such that for every $n$, there is a graph $(V_n, E_n)$ of size at most $cT(n)^2 + b$ such that $\mathcal{H}_{V_n, E_n, \operatorname{sign}}$ contains $\mathcal{F}_n$.
</MathBox>

<MathBox title='' boxType='lemma'>
Suppose that a neuron $v$, that implements the sign activation function, has $k$ incoming edges, connecting it to neurons whose outputs are in $\set{\pm 1}$. Then, by adding one more edge, linking a constant neuron to $v$, and by adjusting the weights on the edges to $v$, the output of $v$ can implement the conjunction or the disjunction of its inputs.

<details>
<summary>Proof</summary>

Note that if $f:\set{0,1}^k \to\set{\pm 1}$ is the conjunction functions $f(\mathbf{x}) = \wedge_i x_i$, then it can be written as $f(\mathbf{x}) = \operatorname{sign}\left(1 - k + \sum_{i=1}^k x_i \right)$. Similarly, the disjunction function $f(\mathbf{x}) = \vee_i x_i$ can be written as $f(\mathbf{x}) = \operatorname{sign}\left(k - 1 + \sum_{i=1}^k x_i \right)$
</details>
</MathBox>

### Sample complexity

<MathBox title='' boxType='lemma' tag='lemma-1'>
Let $a > 0$. Then

$$
  x \geq 2a \ln(a) \implies x \geq a\ln(x)
$$

It follows that a necessary condition for the inequality $x < a\ln(x)$ to hold is that $x < 2a\ln(a)$.

<details>
<summary>Proof</summary>

First note that for $a\in(0,\sqrt{e}]$ the inequality $x\geq a\ln(x)$ holds unconditionally and there the claim is trivial. Now, assume that $a > \sqrt{e}$. Consider the function $f(x) = x - a\ln(x)$. The derivative is $f'(x) = 1 - a/x$. Thus, for $x > a$ the derivative is positive and the function increases. In addition

$$
\begin{align*}
  f(2a\ln(a)) =& 2a\ln(a) - a\ln(2a\ln(a)) \\
  =& 2a\ln(a) - a\ln(a) - a\ln(2\ln(a)) \\
  =& a\ln(a) - a\ln(2\ln(a))
\end{align*}
$$

Since $a - 2\ln(a) > 0$ for all $a > 0$, the proof follows.
</details>
</MathBox>

<MathBox title='' boxType='lemma' tag='lemma-2'>
Let $a\geq 1$ and $b > 0$. Then

$$
  x \geq 4a \ln(2a) + 2b \implies x \geq a\ln(x) + b
$$

<details>
<summary>Proof</summary>

It suffices to prove that $x\geq 4a \ln(2a) + 2b$ implies that bot $x\geq 2a\ln(x)$ and $x\geq 2b$. Since we assume $a\geq 1$ we clearly have that $x\geq 2b$. In addition, since $b > 0$ we have that $x \geq 4a\ln(2a)$ which using Lemma $\ref{lemma-1}$ implies that $x \geq 2a\ln(x)$.
</details>
</MathBox>

<MathBox title='' boxType='theorem'>
The VC dimension of $\mathcal{H}_{V,E,\operatorname{sign}}$ is $O[|E| \ln(|E|)]$.

<details>
<summary>Proof</summary>

To simplify notation, let $\mathcal{H}$ denote the hypothesis class. Recall the growth function

$$
  \Pi_{\mathcal{H}}(m) = \max_{C\subset X:|C|=m} |\mathcal{H}_C|
$$

where $\mathcal{H}_C$ is the restriction of $\mathcal{H}$ to functions from $C$ to $\Set{0,1}$. We can naturally extend the definition for a set of functions from $X$ to som finite set $Y$, by letting $\mathcal{H}_C$ be the restriction of $\mathcal{H}$ to functions from $C$ to $Y$, and keeping the definition of $\Pi_\mathcal{H}(m)$ intact.

Let $V_0,\dots,V_T$ be the layers of a neural network. Fix some $t\in[T]$. By assigning different weights on the edges between $V_{t-1}$ and $V_t$, we obtain different functions from $\R^{|V_{t-1}|} \to\set{\pm 1}^{|V_t|}$. Let $\mathcal{H}_t$ be the class of all possible mappings $\R^{|V_{t-1}|}\to\set{\pm 1}^{|V_t|}$. Then $\mathcal{H}$ can be written as a composition $\mathcal{H} = \mathcal{H}_T \circ\cdots\circ\mathcal{H}_1$. Since the growth function of a composition of hypothesis classes is bounded by the product of the growth function of the individual classes, we get

$$
  \Pi_\mathcal{H} (m) \leq \prod_{t=1}^T \Pi_{\mathcal{H}_t} (m)
$$

In addition, each $\mathcal{H}_t$ can be written as a product of function classes, $\mathcal{H}_t = \mathcal{H}_{t,1} \times\cdots\times \mathcal{H}_{t,|V_t|}$, where each $\mathcal{H}_{t,j}$ is all function from layer $t - 1$ to $\set{\pm 1}$ that the $j$th neuron of layer $t$ can implement. Since the growth function of a product of hypothesis classes is bounded by the product of the growth function of the individual classes, we get

$$
  \Pi_{\mathcal{H}_t} (m) \leq \prod_{i=1}^{|V_t|} \Pi_{\mathcal{H}_{t,i}} (m)
$$

Let $d_{t,i}$ be the number of edges that are headed to the $i$th neuron of layer $t$. Since the neuron is a homogenous halfspace hypothesis and the VC dimension of homogenous halfspaces is the dimension of the input, we have by Sauer's lemma that

$$
  \Pi_{\mathcal{H}_{t,i}} (m) \leq \left( \frac{em}{d_{t,i}} \right)^{d_{t,i}} leq (em)^{d_{t,i}}
$$

Overall, we obtained that

$$
  \Pi_{\mathcal{H}} (m) \leq (em)^{\sum_{t,i}} d_{t,i} = (em)^{|E|}
$$

Now, assume that there are $m$ shattered points. Then, we must have $\Pi_\mathcal{H} (m) = 2^m$, from which we obtain

$$
  2^m \leq (em)^{|E|} \implies m \leq |E| \ln(em) / \ln(2)
$$

The claim follows from Lemma $\ref{lemma-2}$.
</details>
</MathBox>

Let $\sigma$ be a sigmoid activation function. The VC dimension of $\mathcal{H}_{V,E,\sigma}$ is lower bounded by $\Omega(|E|^2)$. That is, the VC dimension is the number of tunable parameters squared. It is also possible to upper bound the VC dimension by $O(|V|^2 |E|^2)$.

### Runtime complexity

<MathBox title='' boxType='theorem'>
Let $k\geq 3$. For every $n\in\N$, let $(V,E)$ be a layered graph with $n$ input nodes, $k+1$ nodes at the (single) hidden layer, where one of them is the constant neuron, and a single output node. Then, it is NP hard to implement the ERM rule with respect to $\mathcal{H}_{V,E,\operatorname{sign}}$.
</MathBox>

### Stochastic gradient descent and backpropagation

Stochastic gradient descent can be used search for optimal weights for a hypothesis $\mathcal{H}_{V,E,\sigma}$. Since $E$ is a finite set, we can think of the weight function as a vector $\mathbf{w}\in\R^{|E|}$. Suppose the network has $n$ input neurons and $k$ output neurons, and denote by $h_\mathbf{w} \R^n \to\R^k$ he function calculated by the network if the weight function is defined by $\mathbf{w}$. Let us denote by $\Delta (h_\mathbf{w} (\mathbf{x}), \mathbf{y})$ the loss of predicting $h_\mathbf{w} (\mathbf{x})$ when the target is $\mathbf{y}\in Y$. For concreteness, we will take $\Delta$ to be the squared loss, $\Delta (h_\mathbf{w}(\mathbf{x}),\mathbf{y}) = \frac{1}{2}\norm{ h_\mathbf{w} (\mathbf{x}) - \mathbf{y}}^2$; however, similar derivation can be botained for every differentiable function. Finally, give a distribution $\mathcal{D}$ over the example domain, $\R^n \times\R^n$, let $L_\mathcal{D}(\mathbf{w})$ be the risk of the network, i.e.

$$
  L_\mathcal{D} (\mathbf{w}) = \mathbb{E}_{(\mathbf{x},\mathbf{y})\sim\mathcal{D}} [\Delta(h_\mathbf{w}(\mathbf{x}, \mathbf{y}))]
$$

The gradient of $L_\mathcal{D}$, which does not have an analytical form, is calculated using the backpropagation algorithm.

To describe the backpropagation algorithm, let us decompose $V$ into the layers of the graph, $V = \bigcup_{t=0}^T$. For every $t$, let us write $V_t = \set{v_{t,1},\dots,v_{t, k_t}}$, where $k_t = |V_t|$. In addition, for every $t$ denote $\mathbf{W}_t \in \R^{k_{t+1} \times k_t}$ a matrix which gives a weight to every potential edge between $V_t$ and $V_{t+1}$. If the edge exists in $E$ then set $\mathbf{W}_{t;i,j}$ to be the weight, according to $\mathbf{w}$, of the edge $(v_{t,j}, v_{t+1, i})$. Otherwise, we add a "phantom" edge and set its weight to zero. When calculating the partial derivative with respect to the weight of some edge, we fix all other weights. Thus, these additional "phantom" edges have no effect on the partial derivatives with respect to existing edges. It follows that we can assume, without loss of generality, that all edges exist, i.e. $E = \bigcup_t (V_t \times V_{t+1})$.

Next we discuss how to calculate the partial derivatives with respect to the edges from $V_{t-1}$ to $V_t$, that is, with respect to the elements in $\mathbf{W}_{t-1}$. Since we fix all other weights of the network, it follows that the outputs of all the neurons in $V_{t-1}$ are fixed numbers which do not depend on the weights in $\mathbf{W}_{t-1}$. Denote the corresponding vector by $\mathbf{o}_{t-1}$. Additionally, let us denote by $\ell_t : \R^{k_t} \to\R$ the loss function of the subnetwork defined by layers $V_t,\dots,V_T$ as a function of the outputs of the neurons in $V_t$. The input to the neurons of $V_t$ can be written as $\mathbf{a}_t = \mathbf{W}_{t-1} \mathbf{o}_{t-1}$ and the output of the neurons of $V_t$ is $\mathbf{o}_t = \boldsymbol{\sigma}(\mathbf{a}_t)$. That is, for every $j$ we have $o_{t,j} = \sigma(a_{t,j})$. We obtain that the loss, as a function of $\mathbf{W}_{t-1}$, can be written as

$$
  g_t (\mathbf{W}_{t-1}) = \ell_t (\mathbf{o}_t) = \ell_t (\boldsymbol{\sigma}(\mathbf{a}_t)) = \ell_t (\boldsymbol{\sigma(\mathbf{W}_{t-1} \mathbf{o}_{t-1})})
$$

For convenience, we rewrite this as follows. Let $\mathbf{w}_{t-1} \in\R^{k_{t-1}k_t}$ be the column vector obtained by concatenating the rows of $\mathbf{W}_t$ and then taking the transpose of the resulting long vector. Define by $\mathbf{O}_{t-1}$ the $k_t \times (k_{t-1} k_t)$ matrix

$$
  \mathbf{O}_{t-1} = \begin{bmatrix}
    \mathbf{o}_{t-1}^\top & \mathbf{0} & \cdots & \mathbf{0} \\
    \mathbf{0} & \mathbf{o}_{t-1}^\top & \cdots & \mathbf{0} \\
    \vdots & \vdots & \ddots & \vdots \\
    \mathbf{0} & \mathbf{0} & \cdots & \mathbf{o}_{t-1}^\top
  \end{bmatrix}
$$

With this notation we get $\mathbf{W}_{t-1} \mathbf{o}_{t-1} = \mathbf{O}_{t-1}\mathbf{w}_{t-1}$, so we can also write

$$
  g_t (\mathbf{w}_{t-1}) = \ell_t (\boldsymbol{\sigma}(\mathbf{O}_{t-1} \mathbf{w}_{t-1}))
$$

Therefore, applying the chain rule, we obtain that

$$
  J_{\mathbf{w}_{t-1}}(g_t) = J_{\boldsymbol{\sigma}(\mathbf{O}_{t-1} \mathbf{w}_{t-1})} (\ell_t) \operatorname{diag}(\boldsymbol{\sigma}' (\mathbf{O}_{t-1}\mathbf{w}_{t-1}))\mathbf{O}_{t-1}
$$

Using our nation we have $\mathbf{o}_t = \boldsymbol{\sigma}(\mathbf{O}_{t-1}\mathbf{w}_{t-1})$ and $\mathbf{a}_t = \mathbf{O}_{t-1} \mathbf{w}_{t-1}$, which yields

$$
  J_{\mathbf{w}_{t-1}} (g_t) = J_{\mathbf{o}_t} (\ell_t) \operatorname{diag}(\boldsymbol{\sigma}'(\mathbf{a}_t))\mathbf{O}_{t-1}
$$

Let us denote $\boldsymbol{\delta}_t = \mathbf{J}_{\mathbf{o}_t} (\ell_t)$. Then, we can further rewrite the preceding as

$$
  J_{\mathbf{w}_{t-1}}(g_t) = (\delta_{t,1} \sigma'(a_{t,1})\mathbf{o}_{t-1}^\top,\dots, \delta_{t,k_t} \sigma'(a_{t, k_t}))\mathbf{o}_{t-1}^\top
$$

It remains to calculate the vector $\boldsymbol{\delta}_t = J_{\mathbf{o}_t}(\ell_t)$ for every $t$. This is the gradient of $\ell_t$ at $\mathbf{o}_t$. We calculate this in a recursive manner. First observe that for the last layer, we have that $\ell_T (\mathbf{u}) = \Delta(\mathbf{u},\mathbf{y})$, where $\Delta$ is the loss function. Since we assume $\Delta (\mathbf{u},\mathbf{y}) = \frac{1}{2}\norm{\mathbf{u} - \mathbf{y}}^2$ we obtain that $J_\mathbf{u}(\ell_T) = (\mathbf{u} - \mathbf{y})$. In particular, $\boldsymbol{\delta}_T = J_{\mathbf{o}_T}(\ell_T) = (\mathbf{o}_T - \mathbf{y})$. Next, note that

$$
  \ell_t (\mathbf{u}) = \ell_{t+1}(\boldsymbol{\sigma}(\mathbf{W}_t \mathbf{u}))
$$

Thus, by the chain rule

$$
  J_\mathbf{u} (\ell_t) = J_{\boldsymbol{\sigma}(\mathbf{W}_t \mathbf{u})} (\ell_{t+1}) \operatorname{diag}(\boldsymbol{\sigma}' (\mathbf{W}_t \mathbf{u}))\mathbf{W}_t
$$

In particular

$$
\begin{align*}
  \boldsymbol{\delta}_t =& J_{\mathbf{o}_t} (\ell_t) = J_{\boldsymbol{\sigma}(\mathbf{W}_t \mathbf{o}_t)} (\ell_{t+1}) \operatorname{diag}(\boldsymbol{\sigma}' (\mathbf{W}_t \mathbf{o}_t))\mathbf{W}_t \\
  =& J_{\mathbf{o}_{t+1}} (\ell_{t+1}) \operatorname{diag}(\boldsymbol{\sigma}' (\mathbf{a}_{t+1}))\mathbf{W}_t \\
  =& \boldsymbol{\delta}_{t+1} \operatorname{diag}(\boldsymbol{\sigma}'(\mathbf{a}_{t+1}))\mathbf{W}_t
\end{align*}
$$

<MathBox title='Stochastic gradient on feedforward neural networks' boxType='algorithm'>
**parameters:**
- number of iterations $\tau$
- step size scheduler $\eta(t)$
- regularization parameter $\lambda > 0$

**input:**
- training set $(\mathbf{x}, \mathbf{y}) \sim\mathcal{D}$
- layered graph $(V,E)$
- differentiable activation function $\sigma:\R\to\R$

**initialize:**
- choose $\mathbf{w}_0 \in\R^{|E|}$ at random (from a distribution so that $\mathbf{w}_0$ is close enought to $\mathbf{0}$)

**for** $i=1,\dots,\tau$
1. sample $(\mathbf{x}, \mathbf{y})\sim\mathcal{D}$
2. calculate gradient $\mathbf{g}_i = \operatorname{backpropagation}(\mathbf{x},\mathbf{y},\mathbf{w},(V,E),\sigma)$
4. calculate learning step $\eta_i = \eta(i)$
3. update $\mathbf{w}_{i+1} = \mathbf{w}_i - \eta_i (\mathbf{g}_i + \lambda\mathbf{w}_i$
</MathBox>

<MathBox title='Backpropagation' boxType='algorithm'>
**parameters:**
- number of iterations $\tau$
- step size scheduler $\eta(t)$
- regularization parameter $\lambda > 0$

**input:**
- training set $(\mathbf{x}, \mathbf{y}) \sim\mathcal{D}$
- layered graph $(V,E)$
- differentiable activation function $\sigma:\R\to\R$

**initialize:**
- choose $\mathbf{w}_0 \in \R^{|E|}$ at random (from a distribution so that $\mathbf{w}_0$ is close enought to $\mathbf{0}$)

**for** $i=1,\dots,\tau$
1. sample $(\mathbf{x}, \mathbf{y})\sim\mathcal{D}$
2. calculate gradient $\mathbf{g}_i = \operatorname{backpropagation}(\mathbf{x},\mathbf{y},\mathbf{w},(V,E),\sigma)$
4. calculate learning step $\eta_i = \eta(i)$
3. update $\mathbf{w}_{i+1} = \mathbf{w}_i - \eta_i (\mathbf{g}_i + \lambda\mathbf{w}_i$
</MathBox>

#### Derivation of the backpropagation algorithm

In this section we derive the backpropagation algorithm for training a feedforward neural netork (FFNN) using a quadratic cost function. The notation and methodology follow the framework established in [@book_nielsen_2015] and [@notes_smets_2024].

Suppose we have a training set $\set{(\mathbf{x}_n, \mathbf{y}_n) | \mathbf{x}\in\mathbb{F}^{n_0}, \mathbb{F}^{n_{L+1}}}_{n=1}^N$ of $N\in\N$ samples. We are interested in minimizing the mean squared error given by

$$
\begin{equation*}
  C(\mathbf{W}, \mathbf{b}) = \frac{1}{2N} \sum_{n=1}^N \lVert \hat{\mathbf{y}}_n - \mathbf{y} \rVert^2
\tag{\label{equation-1}}
\end{equation*}
$$

where $\hat{\mathbf{y}} = \mathbf{a}^{(L+1)} = \sigma(\mathbf{z}^{(L+1)})$ is the output of FFNN for the $n$th input. The cost of training example $n$, given by

$$
  C_n (\mathbf{W}, \mathbf{b}) = \frac{1}{2} \lVert \mathbf{a}^{(L+1)} - \mathbf{y}_n \rVert^2 = \frac{1}{2} \sum_{j=1}^{n_{L+1}} (a_j^{(L+1)} - y_n)^2
$$

To derive the backpropagation equations, we need to calculate the partial derivatives of $C_n$ with respect to the bias and the weights. From $\eqref{equation-}$, we obtain the partial derivatives

$$
  \frac{\partial z_j^{(\ell)}}{\partial w_{kj}^{(\ell)}} = a_k^{(\ell - 1)},\quad
  \frac{\partial z_j^{(\ell)}}{\partial b_j^{(\ell)}} = 1,\quad
  \frac{\partial a_j^{(\ell)}}{\partial z_j^{(\ell)}} = \sigma'_{\ell}(\mathbf{z}_j^{\ell})
$$

We also define the error $\delta_j^{(\ell)}$ for neuron $j$ in layer $\ell$ by 

$$
  \delta_j^{(\ell)} := \frac{\partial C_n}{\partial z_j^{(\ell)}} = \frac{\partial a_j^{(\ell)}}{\partial z_j^{(\ell)}} \frac{\partial C_n}{\partial a_j^{(\ell)}} = \sigma'_\ell (z_j^{(\ell)}) \frac{\partial C_n}{\partial a_j^{(\ell)}} 
$$

In matrix form, this can be written as

$$
  \boldsymbol{\delta}^{(\ell)} = \nabla_{\mathbf{a}^{(\ell)}} C_n \odot \sigma'_{\ell} (\mathbf{z}^{(\ell)})
$$

where $\odot$ denotes the Hadamard (elementwise) product. For the output layer $\ell = L + 1$ we find from $\eqref{equation-1}$, that $\nabla_{\mathbf{a}^{(L+1)}} C_n = \mathbf{a}^{(L)} - \mathbf{y}_n$ leading to the output error

$$
  \boldsymbol{\delta}^{(L)} = (\mathbf{a}^{(L)} - \mathbf{y}_n) \odot \sigma'_{L+1} (\mathbf{z}^{(L)})
$$

For any hidden layer $\ell < L + 1$, the partial derivative of $C_n$ with respect to an activation $a_k^{(\ell)}$ is found by summing up the errors $\delta_j^{(\ell + 1)}$ of the next layer $\ell + 1$, i.e.,

$$
  \frac{\partial C_n}{\partial a_k^{(\ell)}} = \sum_{j=1}^{n_{\ell+1}} \underbrace{\frac{\partial z_j^{(\ell + 1)}}{\partial a_k^{(\ell)}}}_{=w_{kj}^{(\ell+1)}} \underbrace{\frac{\partial a_j^{(\ell + 1)}}{\partial z_j^{(\ell+1)}} \frac{\partial C_n}{\partial a_j^{(\ell+1)}}}_{=\delta_j^{(\ell+1)}} = \sum_{j=1}^{n_{\ell+1}} w_{kj}^{(\ell + 1)} \delta_{j}^{(\ell+1)}.
$$

In matrix form this can be expressed as $\nabla_{\mathbf{a}^{(\ell)}} C_n = (\mathbf{W}^{(\ell+1)})^\top \boldsymbol{\delta}^{(\ell+1)}$. Thus, for any hidden layer $\ell < L + 1$, the error $\boldsymbol{\delta}^{(\ell)}$ can be expressed as

$$
  \boldsymbol{\delta}^{(\ell)} = (\mathbf{W}^{\ell + 1})^\top \boldsymbol{\delta}^{(\ell + 1)} \odot \boldsymbol{\sigma}'_{\ell} (\mathbf{z}^{(\ell)}).
$$

##### Gradient Computation

It remains to calculate the partial derivatives of the cost $C_n$ with respect to the bias and the weights. For any layer $\ell$, the partial derivative of $C_n$ with respect to a weight $w_{kj}^{(\ell)}$ is given by the chain rule

$$
  \frac{\partial C_n}{\partial w_{kj}^{(\ell)}} = \underbrace{\frac{\partial z_j^{(\ell)}}{\partial w_{kj}^{(\ell)}}}_{=a_k^{(\ell-1)}} \underbrace{\frac{\partial a_j^{(\ell)}}{\partial z_j^{(\ell)}} \frac{\partial C_n}{\partial a_j^{\ell}}}_{=\delta_j^{(\ell)}} = a_k^{(\ell-1)} \delta_j^{(\ell)}
$$

Likewise, the partial derivative of $C_n$ with respect a bias $b_j^{(\ell)}$ is given by

$$
  \frac{\partial C_n}{\partial b_j^{(\ell)}} = \underbrace{\frac{\partial z_j^{(\ell)}}{\partial b_j^{(\ell)}}}_{=1} \underbrace{\frac{\partial a_j^{(\ell)}}{\partial z_j^{(\ell)}} \frac{\partial C_n}{\partial a_j^{\ell}}}_{=\delta_j^{(\ell)}} = \delta_j^{(\ell)}
$$

The backpropagation algorithm can be summed up as follows:
- **Find the output error:** Calculate the output error $\boldsymbol{\delta}^{(L+1)}$ using the gradient of the cost with respect to the activations:

$$
  \boldsymbol{\delta}^{(\ell)} = \nabla_{\mathbf{a}^{(\ell)}} C_n \odot \sigma'_{\ell} (\mathbf{z}^{(\ell)}).
$$

- **Backpropagate the errors:** For each layer $\ell = L,L-1,\dots,1$ calculate the error $\boldsymbol{\delta}^{(\ell)}$ using

$$
  \boldsymbol{\delta}^{(\ell)} = (\mathbf{W}^{\ell + 1})^\top \boldsymbol{\delta}^{(\ell + 1)} \odot \boldsymbol{\sigma}'_{\ell} (\mathbf{z}^{(\ell)}).
$$

- **Update weights and biases:** For each layer $\ell = L,L-1,\dots,1$ adjust the biases $\mathbf{b}^{(\ell)}$ and weights $\mathbf{W}^{(\ell)}$ according to the updates

$$
\begin{align*}
  \hat{w}_{jk}^{(\ell)} =& w_{jk}^{(\ell)} - \eta \delta_j^{(\ell)} a_k^{(\ell - 1)} \\
  \hat{b}_j^{(\ell)} =& b_j^{(\ell)} - \eta \delta_j^{(\ell)},
\end{align*}
$$

where $\eta$ is the learning rate.

## Convolutional neural network (CNN)

<MathBox title='Convolution on a finite group' boxType='definition'>
Let $G$ be a finite group and denote by $\R^G := \set{f: G\to\R}$ the vector space of real-valued functions on $G$. For $f,g \in\R^G$ the convolution $f * g \in\R^G$ is defined by

$$
  (f * g)(i) = \sum_{j\in G} f(j) g(j^{-1} i),\; i \in G
$$
</MathBox>

<MathBox title='Discrete convolution for tensors' boxType='definition'>
Fix $T \in\N_+$. For each $t\in\set{1,\dots, T}$ let $a_t, w_t, \mathfrak{d}_t \in\N$ and assume

$$
  \mathfrak{d}_t = a_t - w_t + 1,\; a_t \geq w_t
$$

For the index sets

$$
\begin{align*}
  I_t :=& \set{1,\dots,a_t} \\
  K_t :=& \set{1,\dots,w_t} \\
  D_t :=& \set{1,\dots,\mathfrak{d}_t}
\end{align*}
$$

let

$$
  A = (A_{i_1,\dots,i_T})_{(i_1,\dots,i_T) \in I_1 \times\cdots\times I_T} \in \R^{a_1 \times\cdots\times a_T}
$$

and

$$
\begin{align*}
  W = (w_{w_1,\dots,w_T})_{(w_1,\dots,w_T) \in K_1 \times\cdots\times K_T} \in \R^{w_1 \times\cdots\times w_T}
\end{align*}
$$

be real $T$-tensors. For every multi-index $(i_1,dots,i_T) \in D_1 \times\cdots\times D_T$ the convolution $A * W \in\R^{\mathfrak{d}_1 \times\cdots\times \mathfrak{d}_T}$ is given by

$$
  (A * W)_{i_1,\dots,i_T} = \sum_{r_1 = 1}^{w_1} \cdots \sum_{r_T = 1}^{w_T} A_{i_1 - 1 + r_1, \dots, i_T - 1 + r_T} W_{r_1,\dots,r_T}
$$

Introducing $\mathbf{i} = (i_1,\dots,i_T)$ and $\mathbf{r} = (r_1, \dots, r_T)$ this can be compactly expressed as

$$
  (A * W)_\mathbf{i} = \sum_{\mathbf{r} \in K_1 \times\cdots\times K_T} A_{\mathbf{i} - 1 + \mathbf{r}} W_\mathbf{r}
$$

In short notation

$$
  (A * W)_\mathbf{i} = \braket{A[\mathbf{i}: \mathbf{i} + \mathbf{w} - 1], W}
$$

where $A[\mathbf{i}:\mathbf{i} + \mathbf{w} - 1]$ denotes the subtensor of $A$ with $t$ index ranging from $i_t$ to $i_t + w_t - 1$, and $\braket{\cdot,\cdot} :\R\times\R\to\R$ is the entrywise inner product.
</MathBox>

A convolutional neural network corresponds to multiple convolutional blocks, which are special types of layers. For a group $G$, which typically is either $[d] \simeq \Z/(d\Z)$ or $[d]^2 \simeq (\Z/(d\Z))^2$ for $d\in\N$, depending on whether we are performing one-dimensional or two-dimensional convolutions, the convolution of two vectors $\mathbf{a}, \mathbf{b} \in \R^G$ is defined as

$$
  (\mathbf{a} * \mathbf{b})_i = \sum_{j\in G} a_j b_{j^{-1} i},\; i\in G
$$

A convolutional block can be defined as follows: Let $\tilde{G}$ be a subgroup of $G$, let $p: G \to \tilde{G}$ be a pooling operator, and let $C\in\N$ denote the number of channels. Then, for a series of kernels $\kappa_i \in\R^G$ for $i\in [C]$, the output of a convolutional block is given by

$$
  \R^G \ni \mathbf{x} \mapsto \mathbf{x}' := (p(\mathbf{x} * \kappa_i))_{i=1}^C \in (\R^{\tilde{G}})^G
$$

A typical example of a pooling operator is for $G = (\Z/(2d\Z))^2$ and $\tilde{G} = (\Z/(d\Z))^2$ the $2\times 2$ subsampling operator $p: \R^G \to \R^{\tilde{G}}$ given by

$$
  \mathbf{x} \mapsto (x_{2i-1, 2j-1})_{i,j=1}^d
$$

**Discrete convolutions**

Let $T\in\N_+, a_1, \dots, a_T, w_1, \dots, w_T, \mathfrak{d}_1, \dots, \mathfrak{d}_T \in\N$ and let $A = (A_{i_1,\dots,i_T})_{(i_1,\dots,i_T) \in (\bigtimes_{t=1}^T \set{1,\dots,a_t})} \in \R^{a_1 \times \cdots \times a_T}$, $W = (W_{i_1,\dots,i_T})_{(i_1,\dots,i_T) \in (\bigtimes_{t=1}^T \set{1,\dots,w_t})} \in \R^{w_1 \times \cdots \times w_T}$ satisfy for all $t\in \set{1,\dots, T}$ that

$$
  \mathfrak{d}_t = a_t - w_t + 1
$$

Then we denote by $A * W = ((A * W)_{i_1,\dots,i_T})_{(i_1,\dots,i_T) \in (\bigtimes_{t=1}^T \set{1,\dots,\mathfrak{d}_t})} \in \R^{\mathfrak{d}_1 \times\cdots\times\mathfrak{d}_T}$ the tensor which satisfies for all $i_1 \in\set{1,\dots,\mathfrak{d}_1}, \dots, i_T \in\set{1,\dots,\mathfrak{d}_T}$ that

$$
  (A * W)_{i_1,\dots, i_T} = \sum_{r_1 = 1}^{w_1} \cdots \sum_{r_T = 1}^{w_T} A_{i_1 - 1 + r_1, \cdots, i_T - 1 + r_T} W_{r_1,\dots, r_T}
$$