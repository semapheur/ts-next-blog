---
title: 'Optimization'
subject: 'Machine Learning'
showToc: true
---

# Optimization problems

<Figure width={100} src='/fig/optimization_problems.svg' alt=''
  caption='Optimization problem types'
/>

The core problem in machine learing is parameter estimation (or model fitting) by minimizing a scalar-valued *loss/cost function* $\mathcal{L}:\theta\to\R$. This optimization problem can be stated as

$$
\begin{equation}
  \hat{\boldsymbol{\theta}} \in \argmin_{\boldsymbol{\theta}\in\Theta} \mathcal{L}(\boldsymbol{\theta}) \tag{\label{equation-1}}
\end{equation}
$$

If we want to maximize a *score/reward function* $R:\Theta\to\R$, we can equivalently minimize $\mathcal{L}(\boldsymbol{\theta}) = -R(\boldsymbol{\theta})$. The term *objective function* is generally used for the function we are seeking to optimize. An algortithm used to find an optimum of an objective function is often called a *solver*. The optimization problem is **continuous** if $\Theta\subseteq\R^p$, where $p$ is the number of parameter features.

## Local and global optimization

A point $\hat{\boldsymbol{\theta}}\in\theta$ satisfying $\eqref{equation-1}$ is called a *global optimum*. The process of finding such a point is called *global optimization*. A point $\hat{\boldsymbol{\theta}}$ is a *local minimum* if

$$
  \exists\delta > 0, \forall\boldsymbol{\theta}\in\Theta : \lVert\boldsymbol{\theta} - \hat{\boldsymbol{\theta}}\rVert \leq \delta,\; \mathcal{L}(\hat{\boldsymbol{\theta}}) \leq\mathcal{L}(\boldsymbol{\theta})
$$

A local mimimum whose neighbour contains other local minima with same objective value, is called a *flat local minimum*. A local mimimum is *strict* if its cost is strictly lower than those of neighbouring points

$$
  \exists\delta > 0,\forall\boldsymbol{\theta}\in\Theta, \boldsymbol{\theta}\neq\hat{\boldsymbol{\theta}} : \lVert\boldsymbol{\theta} - \hat{\boldsymbol{\theta}}\rVert \leq\delta, \mathcal{L}(\hat{\boldsymbol{\theta}}) < \mathcal{L}(\mathbf{\theta})
$$

If a *solver* is guaranteed to converge to a stationary point from any starting point, it called *globally convergent*. However, this does not implies that it will converge to a global optimum.

### Optimality conditions

For continuous, twice differentiable functions, we can characterize the points corresponing to local minima. Let $\mathbf{g}(\boldsymbol{\theta}) = \nabla\mathcal{L}(\boldsymbol{\theta})$ be the gradient vector, and $\mathbf{H}(\boldsymbol{\theta}) = \nabla^2 \mathcal{L}(\boldsymbol{\theta})$ be the Hessian matrix. Consider a point $\hat{\boldsymbol{\theta}}\in\R^p$ with gradient $\hat{\mathbf{g}} = \mathbf{g}(\boldsymbol{\theta})|_{\hat{\boldsymbol{\theta}}}$ and corresponding Hessian $\hat{\mathbf{H}} = \mathbf{H}(\boldsymbol{\theta})|_{\hat{\boldsymbol{\theta}}}$. The following conditions characterize every local minimum:
- Necessary condition: If $\hat{\boldsymbol{\theta}}$ is a local minimum, then we must have $\hat{\mathbf{g}} = \mathbf{0}$, and $\mathbf{H}^*$ must be positive semi-definite.
- Sufficient condition: If $\hat{\mathbf{g}} = \mathbf{0}$ and $\hat{\mathbf{H}}$ is positive definite, then $\hat{\boldsymbol{\theta}}$ is a local optimum.

A stationary point can either be a 
- mimimum with a corresponding positive definite Hessian
- maximum with a corresponding negative definite Hessian
- saddle points, whose Hessian has both positive and negative eigenvalues

Thus, a zero gradient in itself is not a sufficient conditions for optima.

## Constrained and unconstrained optimization

An optimization problem is *unconstrained* if it minimizes an objective function over the entire parameter space $\Theta$. If $\Theta$ is subject to constraints $\mathcal{C}$, the optimization problem is *constrained*. Parameter constraints are generally classified as
- *inequality constraints* of the form $g(\boldsymbol{\theta}) \leq 0$, which define open/closed subspaces of $\Theta$
- *equality constraints* of the form $h_k (\boldsymbol{\theta}) = 0$, which define hypersurfaces within $\Theta$

The subset of the parameter set that satisfies the constraints, i.e.

$$
  \mathcal{C} = \Set{\boldsymbol{\theta} | g_j (\boldsymbol{\theta}) \leq 0 \land h_k (\boldsymbol{\theta}) = 0,\; j\in\mathcal{I},\;k\in\mathcal{E}} \subseteq\Theta
$$

is called the *feasable set*. If $\mathcal{C} = \Theta$, the optimization problem is unconstrained. The constrained optimization problem can be stated as

$$
  \hat{\boldsymbol{\theta}}\in\argmin_{\boldsymbol{\theta}\in\mathcal{C}} \mathcal{L}(\boldsymbol{\theta})
$$

The addition of constraints can change the number of optima of a function. For example, an unbounded function can acquire multiple optima under constraints. Too many constraints can lead to an empty feasible set, i.e. $\mathcal{C}=\emptyset$. The task of finding any point (regardless of its cost) in the feasible set is called a *feasibility problem*.

## Convex and unconvex optimization

In *convex optimization*, we require the objective function to be a convex function defined over a convex set. The convexity implies that every minimum is global.

<MathBox title='Convex set' boxType='definition'>
A set $\mathcal{S}$ is *convex* if, for any $x, x' \in \mathcal{S}$, we have

$$
  \lambda x + (1 - \lambda)x' \in\mathcal{S},\; \forall \lambda\in[0,1]
$$
</MathBox>

<MathBox title='Convex and concave function' boxType='definition'>
A function $f:\mathcal{X}\to \mathcal{Y}$ is *convex* if its *epigraph* is a convex set. The epigraph of $f$ is the set of points above the function, i.e. $\mathcal{G}_+ = \Set{y\in Y | y \geq f(x),\forall x\in X }$. Equivalently, a function $f(x)$ is convex if it is defined on a convex set and if, for any $x_1, x_2 \in\mathcal{X}$, and for any $\lambda\in[0,1]$, we have

$$
  f(\lambda x_1 + (1 - \lambda)x_2) \leq \lambda f(x_1) + (1 - \lambda) f(x_2)
$$

A function is *strictly convex* if the inequality is strict. A function $f$ is *concave* if $-f(x)$ is convex, and *strictly convex* if $-f(x)$ is stricly convex for all $x\in\mathcal{X}$.
</MathBox>

<MathBox title='Characterization of convex functions' boxType='proposition'>
Suppose $f:\R^n \to\R$ is twice differentiable over its domain. Then $f$ is convex if and only if the Hessian $\mathbb{H} = \nabla^2 f(\mathbf{x})$ is positive semi-definite for all $\mathbf{x}\in\R^n$. Furthermore, $f$ is strictly convex if $\mathbb{H}$ is positive definite.
</MathBox>

Recall that a quadratic form is a function $f:\R^n \to\R$ defined as $f(\mathbf{x}) = \mathbf{x}^\top \mathbf{Ax}$. This is convex if $\mathbf{A}$ is positive semi-definite, and is strictly convex if $\mathbf{A}$ is positive definite. It is neither convex nor concave if $\mathbf{A}$ has eigenvalues of mixed sign.

### Strongly convex functions

A function $f:\R^n \to\R$ is *strongly convex* with parameter $m > 0$ if the following holds for all $\mathbf{x}_1, \mathbf{x}_2 \in\R^n$

$$
  (\nabla f(\mathbf{x}) - \nabla f(\mathbf{y}))^\top (\mathbf{x} - \mathbf{y}) \geq m\lVert \mathbf{x}-\mathbf{y}\rVert_2^2
$$

A strongly convex function is also strictly convex, and the opposite is not necessarily true.

If $f$ is twice differentiable, then it is stronly convex with parameter $m > 0$ if and only if $\nabla^2 f(\mathbf{x}) \succeq m\mathbf{I}_n$ for all $\mathbf{x}\in\R^n$, where $\nabla^2 f$ is the Hessian matrix and the equality $\succeq$ mean that $\nabla^2 f(\mathbf{x}) - m\mathbf{I}$ is positive semi-definite. This is equivalnet to requiring that the minimum eigenvalue of $\nabla^2 f(\mathbf{x})$ be at least $m$ for all $\mathbf{x}\in\R^n$.

## Smooth and nonsmooth optimization

In *smooth optimization*, the objective and constraints are continuously differentiable function. For smooth function, we can quantify the degree of smoothnes using the *Lipschitz constant*. For a real-valued function $f:\R\to\R$, this is defined as any constant $K\geq 0$, such that

$$
  |f(x_1) - f(x_2)| \leq K|x_1 - x_2|,\; \forall x_1, x_2 \in\R 
$$

In *nonsmooth optimization*, there is at least some point where the gradient of the objective function or the constraints is not differentiable. Some unsmooth optimization problems allows decomposing the objective into a smooth (differentiable) and a rough (not continuously differentiable) part, i.e.

$$
  \mathcal{L}(\hat{\boldsymbol{\theta}}) = \mathcal{L}_\mathrm{s} (\boldsymbol{\theta}) \mathcal{L}_\mathrm{r} (\boldsymbol{\theta})
$$

This is called a *composite objective*. In machine learning applications, $\mathcal{L}_\mathrm{s}$ is usually the training set loss, and $\mathcal{L}_\mathrm{r}$ is a regularizer, such as the $\ell_1$ norm of $\boldsymbol{\theta}$

### Subgradients

For a convex function $f:\R^n \to\R$, we say that $\mathbf{g}\in\R^n$ is a *subgradient* of $f$ at $\mathbf{x}\in\R^n$ if for all $\mathbf{z}\in\R^n$

$$
  f(\mathbf{z}) \geq f(\mathbf{x}) + \mathbf{g}^\top (\mathbf{z} - \mathbf{x})
$$

Note that a subgradient can exist even when $f$ is not differentiable at a point. A function is *subdifferentiable* at $\mathbf{x}$ is there is at least one subgradient at $\mathbf{x}$. The set of such subgradients is called the *subdifferential* of $f$ at $\mathbf{x}\in\R^n$, and is denoted $\partial f(\mathbf{x})$.

For example, the subdifferential of the absolute value function $f(x) = |x|$ is given by

$$
  \partial f(x) = \begin{cases}
    \Set{-1} \quad&, x < 0 \\
    [-1, 1] \quad&, x = 0 \\
    \Set{1} \quad&, x > 0
  \end{cases}
$$

# First-order solvers

Optimization algorithms that use first-order derivatives iteratively to find stationary points are known as stationary solvers. These solvers start from a point $\boldsymbol{\theta}_0$ and at each iteration $t$ perform an update of the form

$$
  \boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t + \eta_t \mathbf{d}_t
$$

where $\eta_t$ is the *step size* or *learning rate*, and $\mathbf{d}_t$ is a *descent direction*, such as the negative of the gradient $\mathbf{g}_t = \nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta})|_{\boldsymbol{\theta}_t}$. These update steps are continued until the method reaches a stationary point, where the gradient is zero.

## Descent direction

A direction $\mathbf{d}$ is a *descent direction* if there exists $\eta_{\mathrm{max}} > 0$ such that

$$
  \mathcal{L}(\boldsymbol{\theta} + \eta\mathbf{d}) < \mathcal{L}(\mathbf{\theta}),\; \forall 0 < \eta < \eta_{\mathrm{max}}
$$

The gradient at the current iterate $\boldsymbol{\theta}$ is given by

$$
  \mathbf{g}_t := \nabla\mathcal{L}(\boldsymbol{\theta})|_{\boldsymbol{\theta}_t} = \nabla\mathcal{L}(\boldsymbol{\theta}_t) = \mathbf{g}(\boldsymbol{\theta}_t)
$$

Since the gradient points in the direction of steepest accent, the negative gradient is the direction of steepes descent. It can be shown that any direction $\mathbf{d}$ is a descent direction if the angle $\theta$ between $\mathbf{d}$ and $-\mathbf{g}_t$ is less than $\pi/4$ ($90\deg$) and satisfies

$$
  \mathbf{d}^\top \mathbf{g}_t = \lVert\mathbf{d}\rVert\cdot\lVert\mathbf{g}_t \rVert \cos(\theta) < 0
$$

## Learning rate

In machine learning, the sequence of step sizes $\Set{\eta_t}$ is called the *learning rate schedule*.

### Constant rate

The simplest learning method is to use a constant step size $\eta_t = \eta$. However, the method may fail to converge if $\eta$ is too large, and if it is too small, the method will converge but very slowly.

In some cases, we can derive a theoretical upper bound on the maximum step size guaranteeing convergences. For quadratic objectives of the form $\mathcal{L}(\boldsymbol{\theta}) = \frac{1}{2}\boldsymbol{\theta}^\top \mathbf{A}\boldsymbol{\theta} + \mathbf{b}^\top \boldsymbol{\theta} + c$ with $\mathbf{A}\geq 0$, we can show that the steepest descent will have global convergence if and only if the step size satisfies

$$
  \eta < \frac{2}{\lambda_{\mathrm{max}}} (\mathbf{A})
$$

where $\lambda_{\mathrm{max}}(\mathbf{A})$ is the largest eigenvalue of $\mathbf{A}$. More generally, if $K\geq 0$ is the Lipschitz constant of the gradient, then $\eta < 2/K$ ensures convergence. However, the Lipschitz constant is generally unknown.

### Line-search

The optimal step size can be found by finding the value that maximally decreases the objective along the chosen direction by solving the minimalization problem

$$
\begin{eqation}
  \eta_t = \argmin_{\eta > 0} \phi_t (\eta) = \argmin_{\eta} \tag{\label{equation-2}}
\end{equation}
$$

This is known as *line search*, since we are searching along the line defined by $\mathbf{d}_t$. 

If the loss function is convex, this subproblem is also convex, because $\phi_t (\eta) = \mathcal{L}(\boldsymbol{\theta} + \eta\mathbf{d}_t)$ is a convex function of an affine function of $\eta$, for fixed $\boldsymbol{\theta}_t$ and $\mathbf{d}_t$. For example, consider the quadratic loss

$$
  \mathcal{L}(\boldsymbol{\theta}) = \frac{1}{2}\boldsymbol{\theta}^\top \mathbf{A}\boldsymbol{\theta} + \mathbf{b}^\top \boldsymbol{\theta} + c
$$

Computing the derivative of $\phi$ gives

$$
\begin{align*}
  \frac{\d \phi(\eta)}{\d\eta} =& \frac{\d}{\d\eta} \left[\frac{1}{2}(\boldsymbol{\theta} + \eta\mathbf{d})^\top \mathbf{A}(\boldsymbol{\theta} + \eta\mathbf{d}) + \mathbf{b}^\top (\boldsymbol{\theta} + \eta\mathbf{d}) + c \right] \\
  =& \mathbf{d}^\top \mathbf{A}(\boldsymbol{\theta} + \eta\mathbf{d}) + \mathbf{d}^\top \mathbf{b} \\
  =& \mathbf{d}^\top (\mathbf{A}\boldsymbol{\theta} + \mathbf{b}) + \eta\mathbf{d}^\top \mathbf{Ad}
\end{align*}
$$

Solving for $\d\pi(\eta)/\d\eta = 0$ gives

$$
  \eta = -\frac{\mathbf{d}^\top (\mathbf{A}\boldsymbol{\theta} + \mathbf{b})}{\mathbf{d}^\top \mathbf{Ad}}
$$

Using the optimal step size is known as *exact line search*. However, it is not usually necessary to be so precise. There are several methods, such as the *Armijo backtracking method*, that try to ensure sufficient reduction in the objective function without spending too much time trying to solve $\eqref{equation-2}$. In particular, we can start with the current stepsize (or some maximum value), and then reduce it by a factor $0 < c < 1$ at each step until we satisfy the following condition, known as the *Armijo-Goldstein test*:

$$
  \mathcal{L}(\boldsymbol{\theta}_t + \eta\mathbf{d}_t) \leq \mathcal{L}(\boldsymbol{\theta}_t) + c\eta\mathbf{d}_t^\top \nabla\mathcal{L}(\boldsymbol{\theta}_t)
$$

where $c\in[0,1]$ is a constant, typically $c = 10^{-1}$. In practice, the initialization of the line-search and how to backtrack can significantly affect performance.

## Convergence rates

For certain convex problems, with a gradient with bounded Lipschitz constant, one can show that gradient descent converges at a *linear rate*. This means that there exists a number $0 < \mu < 1$ such that

$$
  |\mathcal{L}(\boldsymbol{\theta}_{t+1}) - \mathcal{L}(\boldsymbol{\theta}_*) \leq \mu|\mathcal{L}(\boldsymbol{\theta}_t) - \mathcal{L}(\boldsymbol{\theta}_*)|
$$

Here $\mu$ is called the *rate of convergence*.

For some simple problem, we can derive the convergence rate explicitly. For quadratic objective of the form $\mathcal{L}(\boldsymbol{\theta}) = \frac{1}{2}\boldsymbol{\theta}^\top \mathbf{A}\boldsymbol{\theta} + \mathbf{b}^\top \boldsymbol{\theta} + c$ with $\mathbf{A} \succ 0$ positive definite. Suppose we use steepest descent with exact line search. One can show that the convergence rate is given by

$$
  \mu = \left\frac{\lambda_\mathrm{max} - \lambda_\mathbf{min}}{\lambda_\mathrm{max} + \lambda_\mathbf{min}}\right^2
$$

where $\lambda_\mathrm{max}$ and $\lambda_\mathrm{min}$ are the largest and smallest eigenvalues of $\mathbf{A}$, respectively. We can write this as $\mu = \left(\frac{\kappa - 1}{\kappa + 1}\right)^2$, where $\kappa = \lambda_\mathrm{max} / \lambda_\mathrm{min}$ is the condition number of $\mathbf{A}$. Intuitively, the condition number measures how "skewed" the space is, in the sense of being far from a symmetrical bowl.

In the more general case of non-quadratic functions, the objective will often be locally quadratic around a local optimum. Hence, the convergence rate depends on the condition number of the Hessian $\kappa(\mathbf{H})$, at that point. We can often improve the convergence speed by optimizing a surrogate objective at each step which has a Hessian that is close to the Hessian of the objective function $\eref$.

In some cases, the path of steepest descent with an exact line-search exhibits a characteristic zig-zag behaviour, which is inefficient. This problem can be overcome using a method called *conjugate descent*.

## Momentum methods

One simple heuristic, known as the *heavy ball* or *momentum* method, is to move faster along directions that were previously good, and to slow down along direction where the gradient has suddenly changed. This can be implemented as follows

$$
\begin{align*}
  \mathbf{m}_t =& \beta\mathbf{m}_{t-1} + \mathbf{g}_{t-1} \\
  \boldsymbol{\theta}_t =& \boldsymbol{\theta}_{t-1} - \eta_t \mathbf{m}_t
\end{align*}
$$

where $\mathbf{m}_t$ is the momentum and $0 < \beta < 1$. A typical value of $\beta$ is $0.9$. For $\beta = 0$, the method reduces to gradient descent. We see that $\mathbf{m}_t$ is like an exponentially weighted moving average of the past gradients:

$$
  \mathbf{m}_t = \sum_{\tau=0}^{t-1} \beta^\tau \mathbf{g}_{t - \tau - 1}
$$

If all the past gradients are a constant, say $\mathbf{g}$, this simplifies to

$$
  \mathbf{m}_t = \mathbf{g}\sum_{\tau=0}^{t-1} \beta^\tau
$$

The scaling factor is a geometric series, whose infinite sum is given by

$$
  \sum_{i=0}^\infty \beta^i = \frac{1}{1 - \beta}
$$

Thus in the limit, we multiply the gradient by $1/(1 - \beta)$.

Since we upde the parameters using the gradient average $\mathbf{m}_{t-1}$, rather than just the most recent gradient $\mathbf{g}_{t-1}$, we see that past gradients can exhibit some influence on the present. Furthermore, when momentum is combined with stochastic gradient descent, it can simulate the effects of a larger minibatch, without the computational cost.

### Nesterov momentum

One problem with the standard momentum method is that it may not slow down enough at the bottom of a valley, causing oscillation. The *Nesterov accelerated gradient* method instead modifies the gradient descent to include an extrapolation step, as follows:

$$
\begin{align*}
  \tilde{\boldsymbol{\theta}}_{t+1} =& \boldsymbol{\theta}_t + \beta(\boldsymbol{\theta}_t - \boldsymbol{\theta}_{t-1}) \\
  \boldsymbol{\theta}_{t+1} =& \tilde{\boldsymbol{\theta}}_{t+1} - \eta_t \nabla\mathcal{L} (\tilde{\boldsymbol{\theta}}_{t+1}) 
\end{align*}
$$

This is essentially a form of one-step "look ahead", that can reduce the amount of oscillation. Nesterov accelerated gradient can also be rewritten in the same format as standard momentum. In this case, the momentum term is updated using the gradient at the predicted new location,

$$
\begin{align*}
  \mathbf{m}_{t+1} =& \beta\mathbf{m}_t - \eta_t \nabla\mathcal{L}(\boldsymbol{\theta}_t + \beta\mathbf{m}_t) \\
  \boldsymbol{\theta}_{t+1} =& \boldsymbol{\theta}_t + \mathbf{m}_{t+1}
\end{align*}
$$

This show how this method can be faster than standard momentum: the momentum vector is already roughly point in the right direction, so measuring the gradient at the new location, $\boldsymbol{\theta}_t + \beta\mathbf{m}_t$, rather than the current location $\boldsymbol{\theta}_t$, can be more accurate.

The Nesterov accelerated gradient method is provably faster than steepest descent for convex functions when $\beta$ and $\eta_t$ are chosen appropriately. It is called "accelerated" because of this improved convergence rate, which is optimal for gradient-based methods using only first-order information when the objective function is convex and has Lipschitz-continuous gradients. In practice, however, using Nesterov momentum can be slower than steepest descent, and can even be unstable is if $\beta$ or $\eta_t$ are misspecified.

# Second-order methods

## Newton's method

# Stochastic gradient descent