---
title: 'Optimization'
subject: 'Machine Learning'
showToc: true
references:
  - book_murphy_2022
  - book_shalev-schwartz_ben-david_2014
---

# Optimization problems

<Figure width={100} src='/fig/optimization_problems.svg' alt=''
  caption='Optimization problem types'
/>

The core problem in machine learing is parameter estimation (or model fitting) by minimizing a scalar-valued *loss/cost function* $\mathcal{L}:\Theta\to\R$. This optimization problem can be stated as

$$
\begin{equation}
  \hat{\boldsymbol{\theta}} \in \argmin_{\boldsymbol{\theta}\in\Theta} \mathcal{L}(\boldsymbol{\theta}) \tag{\label{equation-1}}
\end{equation}
$$

If we want to maximize a *score/reward function* $R:\Theta\to\R$, we can equivalently minimize $\mathcal{L}(\boldsymbol{\theta}) = -R(\boldsymbol{\theta})$. The term *objective function* is generally used for the function we are seeking to optimize. An algortithm used to find an optimum of an objective function is often called a *solver*. The optimization problem is **continuous** if $\Theta\subseteq\R^p$, where $p$ is the number of parameter features.

## Local and global optimization

A point $\hat{\boldsymbol{\theta}}\in\theta$ satisfying $\eqref{equation-1}$ is called a *global optimum*. The process of finding such a point is called *global optimization*. A point $\hat{\boldsymbol{\theta}}$ is a *local minimum* if

$$
  \exists\delta > 0, \forall\boldsymbol{\theta}\in\Theta : \norm{\boldsymbol{\theta} - \hat{\boldsymbol{\theta}}} \leq \delta,\; \mathcal{L}(\hat{\boldsymbol{\theta}}) \leq\mathcal{L}(\boldsymbol{\theta})
$$

A local mimimum whose neighbour contains other local minima with same objective value, is called a *flat local minimum*. A local mimimum is *strict* if its cost is strictly lower than those of neighbouring points

$$
  \exists\delta > 0,\forall\boldsymbol{\theta}\in\Theta, \boldsymbol{\theta}\neq\hat{\boldsymbol{\theta}} : \norm{\boldsymbol{\theta} - \hat{\boldsymbol{\theta}}} \leq\delta, \mathcal{L}(\hat{\boldsymbol{\theta}}) < \mathcal{L}(\mathbf{\theta})
$$

If a *solver* is guaranteed to converge to a stationary point from any starting point, it called *globally convergent*. However, this does not implies that it will converge to a global optimum.

### Optimality conditions

For continuous, twice differentiable functions, we can characterize the points corresponing to local minima. Let $\mathbf{g}(\boldsymbol{\theta}) = \nabla\mathcal{L}(\boldsymbol{\theta})$ be the gradient vector, and $\mathbf{H}(\boldsymbol{\theta}) = \nabla^2 \mathcal{L}(\boldsymbol{\theta})$ be the Hessian matrix. Consider a point $\hat{\boldsymbol{\theta}}\in\R^p$ with gradient $\hat{\mathbf{g}} = \mathbf{g}(\boldsymbol{\theta})|_{\hat{\boldsymbol{\theta}}}$ and corresponding Hessian $\hat{\mathbf{H}} = \mathbf{H}(\boldsymbol{\theta})|_{\hat{\boldsymbol{\theta}}}$. The following conditions characterize every local minimum:
- Necessary condition: If $\hat{\boldsymbol{\theta}}$ is a local minimum, then we must have $\hat{\mathbf{g}} = \mathbf{0}$, and $\mathbf{H}^*$ must be positive semi-definite.
- Sufficient condition: If $\hat{\mathbf{g}} = \mathbf{0}$ and $\hat{\mathbf{H}}$ is positive definite, then $\hat{\boldsymbol{\theta}}$ is a local optimum.

A stationary point can either be a 
- mimimum with a corresponding positive definite Hessian
- maximum with a corresponding negative definite Hessian
- saddle points, whose Hessian has both positive and negative eigenvalues

Thus, a zero gradient in itself is not a sufficient conditions for optima.

## Constrained and unconstrained optimization

An optimization problem is *unconstrained* if it minimizes an objective function over the entire parameter space $\Theta$. If $\Theta$ is subject to constraints $\mathcal{C}$, the optimization problem is *constrained*. Parameter constraints are generally classified as
- *inequality constraints* of the form $g(\boldsymbol{\theta}) \leq 0$, which define open/closed subspaces of $\Theta$
- *equality constraints* of the form $h_k (\boldsymbol{\theta}) = 0$, which define hypersurfaces within $\Theta$

The subset of the parameter set that satisfies the constraints, i.e.

$$
  \mathcal{C} = \Set{\boldsymbol{\theta} | g_j (\boldsymbol{\theta}) \leq 0 \land h_k (\boldsymbol{\theta}) = 0,\; j\in\mathcal{I},\;k\in\mathcal{E}} \subseteq\Theta
$$

is called the *feasable set*. If $\mathcal{C} = \Theta$, the optimization problem is unconstrained. The constrained optimization problem can be stated as

$$
  \hat{\boldsymbol{\theta}}\in\argmin_{\boldsymbol{\theta}\in\mathcal{C}} \mathcal{L}(\boldsymbol{\theta})
$$

The addition of constraints can change the number of optima of a function. For example, an unbounded function can acquire multiple optima under constraints. Too many constraints can lead to an empty feasible set, i.e. $\mathcal{C}=\emptyset$. The task of finding any point (regardless of its cost) in the feasible set is called a *feasibility problem*.

## Convex and unconvex optimization

In *convex optimization*, we require the objective function to be a convex function defined over a convex set. The convexity implies that every minimum is global.

<MathBox title='Convex set' boxType='definition'>
A set $\mathcal{S}$ is *convex* if, for any $x, x' \in \mathcal{S}$, we have

$$
  \lambda x + (1 - \lambda)x' \in\mathcal{S},\; \forall \lambda\in[0,1]
$$
</MathBox>

<MathBox title='Convex and concave function' boxType='definition'>
A function $f:\mathcal{X}\to \mathcal{Y}$ is *convex* if its *epigraph* is a convex set. The epigraph of $f$ is the set of points above the function, i.e. $\mathcal{G}_+ = \Set{y\in Y | y \geq f(x),\forall x\in X }$. Equivalently, a function $f(x)$ is convex if it is defined on a convex set and if, for any $x_1, x_2 \in\mathcal{X}$, and for any $\lambda\in[0,1]$, we have

$$
  f(\lambda x_1 + (1 - \lambda)x_2) \leq \lambda f(x_1) + (1 - \lambda) f(x_2)
$$

A function is *strictly convex* if the inequality is strict. A function $f$ is *concave* if $-f(x)$ is convex, and *strictly convex* if $-f(x)$ is stricly convex for all $x\in\mathcal{X}$.
</MathBox>

<MathBox title='Characterization of convex functions' boxType='proposition'>
Suppose $f:\R^n \to\R$ is twice differentiable over its domain. Then $f$ is convex if and only if the Hessian $\mathbb{H} = \nabla^2 f(\mathbf{x})$ is positive semi-definite for all $\mathbf{x}\in\R^n$. Furthermore, $f$ is strictly convex if $\mathbb{H}$ is positive definite.
</MathBox>

Recall that a quadratic form is a function $f:\R^n \to\R$ defined as $f(\mathbf{x}) = \mathbf{x}^\top \mathbf{Ax}$. This is convex if $\mathbf{A}$ is positive semi-definite, and is strictly convex if $\mathbf{A}$ is positive definite. It is neither convex nor concave if $\mathbf{A}$ has eigenvalues of mixed sign.

### Strongly convex functions

A function $f:\R^n \to\R$ is *strongly convex* with parameter $m > 0$ if the following holds for all $\mathbf{x}_1, \mathbf{x}_2 \in\R^n$

$$
  (\nabla f(\mathbf{x}) - \nabla f(\mathbf{y}))^\top (\mathbf{x} - \mathbf{y}) \geq m\norm{ \mathbf{x}-\mathbf{y}}_2^2
$$

A strongly convex function is also strictly convex, and the opposite is not necessarily true.

If $f$ is twice differentiable, then it is stronly convex with parameter $m > 0$ if and only if $\nabla^2 f(\mathbf{x}) \succeq m\mathbf{I}_n$ for all $\mathbf{x}\in\R^n$, where $\nabla^2 f$ is the Hessian matrix and the equality $\succeq$ mean that $\nabla^2 f(\mathbf{x}) - m\mathbf{I}$ is positive semi-definite. This is equivalnet to requiring that the minimum eigenvalue of $\nabla^2 f(\mathbf{x})$ be at least $m$ for all $\mathbf{x}\in\R^n$.

## Smooth and nonsmooth optimization

In *smooth optimization*, the objective and constraints are continuously differentiable function. For smooth function, we can quantify the degree of smoothnes using the *Lipschitz constant*. For a real-valued function $f:\R\to\R$, this is defined as any constant $K\geq 0$, such that

$$
  |f(x_1) - f(x_2)| \leq K|x_1 - x_2|,\; \forall x_1, x_2 \in\R 
$$

In *nonsmooth optimization*, there is at least some point where the gradient of the objective function or the constraints is not differentiable. Some unsmooth optimization problems allows decomposing the objective into a smooth (differentiable) and a rough (not continuously differentiable) part, i.e.

$$
  \mathcal{L}(\hat{\boldsymbol{\theta}}) = \mathcal{L}_\mathrm{s} (\boldsymbol{\theta}) \mathcal{L}_\mathrm{r} (\boldsymbol{\theta})
$$

This is called a *composite objective*. In machine learning applications, $\mathcal{L}_\mathrm{s}$ is usually the training set loss, and $\mathcal{L}_\mathrm{r}$ is a regularizer, such as the $\ell_1$ norm of $\boldsymbol{\theta}$

### Subgradients

For a convex function $f:\R^n \to\R$, we say that $\mathbf{g}\in\R^n$ is a *subgradient* of $f$ at $\mathbf{x}\in\R^n$ if for all $\mathbf{z}\in\R^n$

$$
  f(\mathbf{z}) \geq f(\mathbf{x}) + \mathbf{g}^\top (\mathbf{z} - \mathbf{x})
$$

Note that a subgradient can exist even when $f$ is not differentiable at a point. A function is *subdifferentiable* at $\mathbf{x}$ is there is at least one subgradient at $\mathbf{x}$. The set of such subgradients is called the *subdifferential* of $f$ at $\mathbf{x}\in\R^n$, and is denoted $\partial f(\mathbf{x})$.

For example, the subdifferential of the absolute value function $f(x) = |x|$ is given by

$$
  \partial f(x) = \begin{cases}
    \Set{-1} \quad&, x < 0 \\
    [-1, 1] \quad&, x = 0 \\
    \Set{1} \quad&, x > 0
  \end{cases}
$$

# First-order solvers

Optimization algorithms that use first-order derivatives iteratively to find stationary points are known as stationary solvers. Such methods neglect the curvature of the objective, and can thus be slow to converge. First-order solvers start from a point $\boldsymbol{\theta}_0$ and at each iteration $t$ perform an update of the form

$$
  \boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t + \eta_t \mathbf{d}_t
$$

where $\eta_t$ is the *step size* or *learning rate*, and $\mathbf{d}_t$ is a *descent direction*, such as the negative of the gradient $\mathbf{g}_t = \nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta})|_{\boldsymbol{\theta}_t}$. These update steps are continued until the method reaches a stationary point, where the gradient is zero.

## Descent direction

A direction $\mathbf{d}$ is a *descent direction* if there exists $\eta_{\mathrm{max}} > 0$ such that

$$
  \mathcal{L}(\boldsymbol{\theta} + \eta\mathbf{d}) < \mathcal{L}(\mathbf{\theta}),\; \forall 0 < \eta < \eta_{\mathrm{max}}
$$

The gradient at the current iterate $\boldsymbol{\theta}$ is given by

$$
  \mathbf{g}_t := \nabla\mathcal{L}(\boldsymbol{\theta})|_{\boldsymbol{\theta}_t} = \nabla\mathcal{L}(\boldsymbol{\theta}_t) = \mathbf{g}(\boldsymbol{\theta}_t)
$$

Since the gradient points in the direction of steepest accent, the negative gradient is the direction of steepest descent. It can be shown that any direction $\mathbf{d}$ is a descent direction if the angle $\theta$ between $\mathbf{d}$ and $-\mathbf{g}_t$ is less than $\pi/4$ ($90\deg$) and satisfies

$$
  \mathbf{d}^\top \mathbf{g}_t = \norm{\mathbf{d}}\cdot\norm{\mathbf{g}_t } \cos(\theta) < 0
$$

## Learning rate

In machine learning, the sequence of step sizes $\Set{\eta_t}$ is called the *learning rate schedule*.

### Constant rate

The simplest learning method is to use a constant step size $\eta_t = \eta$. However, the method may fail to converge if $\eta$ is too large, and if it is too small, the method will converge but very slowly.

In some cases, we can derive a theoretical upper bound on the maximum step size guaranteeing convergences. For quadratic objectives of the form $\mathcal{L}(\boldsymbol{\theta}) = \frac{1}{2}\boldsymbol{\theta}^\top \mathbf{A}\boldsymbol{\theta} + \mathbf{b}^\top \boldsymbol{\theta} + c$ with $\mathbf{A}\geq 0$, we can show that the steepest descent will have global convergence if and only if the step size satisfies

$$
  \eta < \frac{2}{\lambda_{\mathrm{max}}(\mathbf{A})}
$$

where $\lambda_{\mathrm{max}}(\mathbf{A})$ is the largest eigenvalue of $\mathbf{A}$. More generally, if $K\geq 0$ is the Lipschitz constant of the gradient, then $\eta < 2/K$ ensures convergence. However, the Lipschitz constant is generally unknown.

### Line-search

The optimal step size can be found by finding the value that maximally decreases the objective along the chosen direction by solving the minimalization problem

$$
\begin{equation}
  \eta_t = \argmin_{\eta > 0} \phi_t (\eta) = \argmin_{\eta} \mathcal{L}(\boldsymbol{\theta}_t + \eta\mathbf{d}_t) \tag{\label{equation-2}}
\end{equation}
$$

This is known as *line search*, since we are searching along the line defined by $\mathbf{d}_t$. 

If the loss function is convex, this subproblem is also convex, because $\phi_t (\eta) = \mathcal{L}(\boldsymbol{\theta} + \eta\mathbf{d}_t)$ is a convex function of an affine function of $\eta$, for fixed $\boldsymbol{\theta}_t$ and $\mathbf{d}_t$. For example, consider the quadratic loss

$$
  \mathcal{L}(\boldsymbol{\theta}) = \frac{1}{2}\boldsymbol{\theta}^\top \mathbf{A}\boldsymbol{\theta} + \mathbf{b}^\top \boldsymbol{\theta} + c
$$

Computing the derivative of $\phi$ gives

$$
\begin{align*}
  \frac{\d \phi(\eta)}{\d\eta} =& \frac{\d}{\d\eta} \left[\frac{1}{2}(\boldsymbol{\theta} + \eta\mathbf{d})^\top \mathbf{A}(\boldsymbol{\theta} + \eta\mathbf{d}) + \mathbf{b}^\top (\boldsymbol{\theta} + \eta\mathbf{d}) + c \right] \\
  =& \mathbf{d}^\top \mathbf{A}(\boldsymbol{\theta} + \eta\mathbf{d}) + \mathbf{d}^\top \mathbf{b} \\
  =& \mathbf{d}^\top (\mathbf{A}\boldsymbol{\theta} + \mathbf{b}) + \eta\mathbf{d}^\top \mathbf{Ad}
\end{align*}
$$

Solving for $\d\pi(\eta)/\d\eta = 0$ gives

$$
  \eta = -\frac{\mathbf{d}^\top (\mathbf{A}\boldsymbol{\theta} + \mathbf{b})}{\mathbf{d}^\top \mathbf{Ad}}
$$

Using the optimal step size is known as *exact line search*. However, it is not usually necessary to be so precise. There are several methods, such as the *Armijo backtracking method*, that try to ensure sufficient reduction in the objective function without spending too much time trying to solve $\eqref{equation-2}$. In particular, we can start with the current stepsize (or some maximum value), and then reduce it by a factor $0 < c < 1$ at each step until we satisfy the following condition, known as the *Armijo-Goldstein test*:

$$
\begin{equation}
  \mathcal{L}(\boldsymbol{\theta}_t + \eta\mathbf{d}_t) \leq \mathcal{L}(\boldsymbol{\theta}_t) + c\eta\mathbf{d}_t^\top \nabla\mathcal{L}(\boldsymbol{\theta}_t) \tag{\label{equation-4}}
\end{equation}
$$

where $c\in[0,1]$ is a constant, typically $c = 10^{-1}$. In practice, the initialization of the line-search and how to backtrack can significantly affect performance.

## Convergence rates

For certain convex problems, with a gradient with bounded Lipschitz constant, one can show that gradient descent converges at a *linear rate*. This means that there exists a number $0 < \mu < 1$ such that

$$
  |\mathcal{L}(\boldsymbol{\theta}_{t+1}) - \mathcal{L}(\boldsymbol{\theta}_*) \leq \mu|\mathcal{L}(\boldsymbol{\theta}_t) - \mathcal{L}(\boldsymbol{\theta}_*)|
$$

Here $\mu$ is called the *rate of convergence*.

For some simple problem, we can derive the convergence rate explicitly. For quadratic objective of the form $\mathcal{L}(\boldsymbol{\theta}) = \frac{1}{2}\boldsymbol{\theta}^\top \mathbf{A}\boldsymbol{\theta} + \mathbf{b}^\top \boldsymbol{\theta} + c$ with $\mathbf{A} \succ 0$ positive definite. Suppose we use steepest descent with exact line search. One can show that the convergence rate is given by

$$
  \mu = \left(\frac{\lambda_\mathrm{max} - \lambda_\mathbf{min}}{\lambda_\mathrm{max} + \lambda_\mathbf{min}}\right)^2
$$

where $\lambda_\mathrm{max}$ and $\lambda_\mathrm{min}$ are the largest and smallest eigenvalues of $\mathbf{A}$, respectively. We can write this as $\mu = \left(\frac{\kappa - 1}{\kappa + 1}\right)^2$, where $\kappa = \lambda_\mathrm{max} / \lambda_\mathrm{min}$ is the condition number of $\mathbf{A}$. Intuitively, the condition number measures how "skewed" the space is, in the sense of being far from a symmetrical bowl.

In the more general case of non-quadratic functions, the objective will often be locally quadratic around a local optimum. Hence, the convergence rate depends on the condition number of the Hessian $\kappa(\mathbf{H})$, at that point. We can often improve the convergence speed by optimizing a surrogate objective at each step which has a Hessian that is close to the Hessian of the objective function.

In some cases, the path of steepest descent with an exact line-search exhibits a characteristic zig-zag behaviour, which is inefficient. This problem can be overcome using a method called *conjugate descent*.

## Momentum methods

One simple heuristic, known as the *heavy ball* or *momentum* method, is to move faster along directions that were previously good, and to slow down along direction where the gradient has suddenly changed. This can be implemented as follows

$$
\begin{equation}
\begin{split}
  \mathbf{m}_t =& \gamma\mathbf{m}_{t-1} + \mathbf{g}_{t-1} \\
  \boldsymbol{\theta}_t =& \boldsymbol{\theta}_{t-1} - \eta_t \mathbf{m}_t
\end{split}
\tag{\label{equation-6}}
\end{equation}
$$

where $\mathbf{m}_t$ is the momentum and $0 < \gamma < 1$. A typical value of $\gamma$ is $0.9$. For $\gamma = 0$, the method reduces to gradient descent. We see that $\mathbf{m}_t$ is like an exponentially weighted moving average of the past gradients:

$$
  \mathbf{m}_t = \sum_{\tau=0}^{t-1} \gamma^\tau \mathbf{g}_{t - \tau - 1}
$$

If all the past gradients are a constant, say $\mathbf{g}$, this simplifies to

$$
  \mathbf{m}_t = \mathbf{g}\sum_{\tau=0}^{t-1} \gamma^\tau
$$

The scaling factor is a geometric series, whose infinite sum is given by

$$
  \sum_{i=0}^\infty \gamma^i = \frac{1}{1 - \gamma}
$$

Thus in the limit, we multiply the gradient by $1/(1 - \gamma)$.

Since we upde the parameters using the gradient average $\mathbf{m}_{t-1}$, rather than just the most recent gradient $\mathbf{g}_{t-1}$, we see that past gradients can exhibit some influence on the present. Furthermore, when momentum is combined with stochastic gradient descent, it can simulate the effects of a larger minibatch, without the computational cost.

### Analogy to motion with linear drag

We can get an intuition of momentum gradient descent by comparing it to motion with linear drag. Consider a particle with mass $m$ moving in a viscous medium with drag coefficient $\mu$. If $\boldsymbol{\theta}$ is the position of the particle, its motion is described by the equation

$$
  m\frac{d^2 \boldsymbol{\theta}}{\d t^2} + \mu \frac{\d\boldsymbol{\theta}}{\d t} = -\nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta})
$$

where $\mathcal{L}$ is the potential energy of the particle. Discretizing the equation we get

$$
  m \frac{\boldsymbol{\theta}_{t + \Delta t} - 2\boldsymbol{\theta}_t + \boldsymbol{\theta}_{t - \Delta t}}{(\Delta t)^2} + \mu \frac{\boldsymbol{\theta}_{t + \Delta t} - \boldsymbol{\theta}}{\Delta t} = -\nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta})
$$

Rearranging the equation we obtain

$$
\begin{align*}
  \Delta\boldsymbol{\theta}_{t + \Delta t} =& \frac{m}{m + \mu\Delta t} \Delta \boldsymbol{\theta} - \frac{(\Delta t)^2}{m + \mu\Delta t} \nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta})  \\
  =& \gamma \Delta\boldsymbol{\theta}_t - \eta \nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta})
\end{align*}
$$

This equation is identical with the momentum gradient descent update $\eqref{equation-5}$ where the learning rate $\eta$ and momentum parameter $\gamma$ are related to the mass of the particle and the viscous drag by

$$
  \eta = \frac{(\Delta t)^2}{m + \mu\Delta t},\quad \gamma = \frac{m}{m + \mu\Delta t}
$$

This shows that the momentum parameter is proportional to the mass of the particle and provides intertia. In the large viscosity/small learning rate limit, the memory time scales as $(1 - \gamma)^{-1} \approx m/(\mu\Delta t)$.

### Nesterov momentum

One problem with the standard momentum method is that it may not slow down enough at the bottom of a valley, causing oscillation. The *Nesterov accelerated gradient* method instead modifies the gradient descent to include an extrapolation step, as follows:

$$
\begin{align*}
  \tilde{\boldsymbol{\theta}}_{t+1} =& \boldsymbol{\theta}_t + \beta(\boldsymbol{\theta}_t - \boldsymbol{\theta}_{t-1}) \\
  \boldsymbol{\theta}_{t+1} =& \tilde{\boldsymbol{\theta}}_{t+1} - \eta_t \nabla\mathcal{L} (\tilde{\boldsymbol{\theta}}_{t+1}) 
\end{align*}
$$

This is essentially a form of one-step "look ahead", that can reduce the amount of oscillation. Nesterov accelerated gradient can also be rewritten in the same format as standard momentum. In this case, the momentum term is updated using the gradient at the predicted new location,

$$
\begin{align*}
  \mathbf{m}_{t+1} =& \beta\mathbf{m}_t - \eta_t \nabla\mathcal{L}(\boldsymbol{\theta}_t + \beta\mathbf{m}_t) \\
  \boldsymbol{\theta}_{t+1} =& \boldsymbol{\theta}_t + \mathbf{m}_{t+1}
\end{align*}
$$

This show how this method can be faster than standard momentum: the momentum vector is already roughly point in the right direction, so measuring the gradient at the new location, $\boldsymbol{\theta}_t + \beta\mathbf{m}_t$, rather than the current location $\boldsymbol{\theta}_t$, can be more accurate.

The Nesterov accelerated gradient method is provably faster than steepest descent for convex functions when $\beta$ and $\eta_t$ are chosen appropriately. It is called "accelerated" because of this improved convergence rate, which is optimal for gradient-based methods using only first-order information when the objective function is convex and has Lipschitz-continuous gradients. In practice, however, using Nesterov momentum can be slower than steepest descent, and can even be unstable is if $\beta$ or $\eta_t$ are misspecified.

# Second-order methods

Second-order solvers consider curvature of the objective in various ways, which may yield faster convergence.

## Newton's method

Newton's method is a second-order solver consisting of updates of the form

$$
  \boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta_t \mathbf{H}_t^{-1} \mathbf{g}_t
$$

where

$$
  \mathbf{H}_t := \nabla^2 \mathcal{L}(\boldsymbol{\theta})|_{\boldsymbol{\theta}_t} = \nabla^2 \mathcal{L}(\boldsymbol{\theta}_t) = \mathbf{H}(\boldsymbol{\theta})
$$

is assumed to be positive-definite to ensure the update is well-defined. If the quadratic approximation is a good one, we should pick $\mathbf{d}_t = -\mathbf{H}_t^{-1} \mathbf{g}_t$ as our descent direction. However, we can also use line seach to find the best stepsize; this tends to be more robust as using $\eta_1 = 1$ may not always converge globally.

<MathBox title="Newton's method" boxType='algorithm'>
1. Initialize $\boldsymbol{\theta}_0$
2. **for** $t = 0,1,\dots$ *until convergence* **do**
    1. Evaluate $\mathbf{g}_t = \nabla\mathcal{L}(\boldsymbol{\theta}_t)$
    2. Evaluate $\mathbf{H}_t = \nabla^2 \mathcal{L}(\boldsymbol{\theta}_t)$
    3. Solve $\mathbf{H}_t \mathbf{d}_t = -\mathbf{g}_t$ for $\mathbf{d}_t$
    4. Use line search to find stepsize $\eta_t$ along $\mathbf{d}_t$
    5. $\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t + \eta_t \mathbf{d}_t$

<details>
<summary>Details</summary>

Newton's method can be derived by minimizing the second order Taylor series approximation of $\mathbb{L}(\boldsymbol{\theta})$ around $\boldsymbol{\theta}_t$, which is given by

$$
\begin{equation}
  \mathcal{L}(\boldsymbol{\theta}) \approx \mathcal{L}(\boldsymbol{\theta}_t) + \mathbf{g}_t^\top (\boldsymbol{\theta} - \boldsymbol{\theta}_t) + \frac{1}{2}(\boldsymbol{\theta} - \boldsymbol{\theta}_t)^\top \boldsymbol{H}_t (\boldsymbol{\theta} - \boldsymbol{\theta}_t) \tag{\label{equation-3}}
\end{equation}
$$

The minimum of $\eqref{equation-3}$ is at

$$
  \boldsymbol{\theta} = \boldsymbol{\theta}_t - \mathbf{H}_t^{-1} \mathbf{g}_t
$$
</details>
</MathBox>

### Quasi-Newton methods

*Quasi-Newton methods*, or *variable metric methods*, iteratively build up an approximation to the Hessian using information gleaned from the gradient vector at each step. The most common method is called *BFGS* (named after its simultaneous inventors, Broyden, Fletcher, Goldfarb and Shanno), which updates the approximation to the Hessian $\mathbf{B}_t \approx \mathbf{H}_t$ as follows

$$
\begin{align*}
  \mathbf{B}_{t+1} =& \mathbf{B}_t + \frac{\mathbf{y}_t \mathbf{y}_t^\top}{\mathbf{y}_t^\top \mathbf{s}_t} - \frac{(\mathbf{B}_t \mathbf{s}_t)(\mathbf{B}_t \mathbf{s}_t)^\top}{\mathbf{s}_t^\top \mathbf{B}_t \mathbf{s}_t} \\
  \mathbf{s}_t =& \boldsymbol{\theta}_t - \boldsymbol{\theta}_{t-1} \\
  \mathbf{y}_t =& \mathbf{g}_t - \mathbf{g}_{t-1}
\end{align*}
$$

This a rank-two update to the matrix. If $\mathbf{B}_0$ is positive-definite, and the step size $\eta$ is chosen via line search satisfying both the Armijo-Goldstein condition $\eqref{equation-4}$ and the following condition

$$
  \nabla\mathcal{L}(\boldsymbol{\theta}_t + \eta\mathbf{d}_t) \geq c_2 \eta \mathbf{d}_t^\top \nabla\mathcal{L}(\boldsymbol{\theta}_t)
$$

then $\mathbf{B}_{t+1}$ will remain positive definite. The constant $c_2$ is choosen within $(c,1)$ where $c$ is the tunable parameter in $\eqref{equation-4}$. The two step size condition are together know as the *Wolfe conditions*. We typically start with a diagonal approximation $\mathbf{B}_0 = \mathbf{I}$. Thus, BFGS can be though of as a "diagonal plus low-rank" approximation to the Hessian.

Alternatively, BFGS can iteratively update an approximation to the inverse Hessian, $\mathbf{C}_t \approx \mathbf{H}_t^{-1}$, as follows:

$$
  \mathbf{C}_{t+1} = \left(\mathbf{I} - \frac{\mathbf{s}_t \mathbf{y}_t^\top}{\mathbf{y}_t^\top \mathbf{s}_t} \right) \mathbf{C}_t \left(\mathbf{I} - \frac{\mathbf{y}_t \mathbf{s}_t^\top}{\mathbf{y}_t^\top \mathbf{s}_t} \right) + \frac{\mathbf{s}_t \mathbf{s}_t^\top}{\mathbf{y}_t^\top \mathbf{s}_t}
$$

Since storing the Hessian approximation still takes $\mathcal{O}(p^2)$ space, for very large problems, one can use *limited memory BGFS*, or *L-BFGS*, where we control the rank of the approximation by only using the $M$ most recent $(\mathbf{s}_t, \mathbf{y}_t)$ pairs while ignoring older information. Rather than storing $\mathbf{B}_t$ explicitly, we just store these vectors in memory, and then approximate $\mathbf{H}_t^{-1} \mathbf{g}_t$ by performing a sequence of inner products with the stored $\mathbf{s}_t$ and $\mathbf{y}_t$ vectors. The storage requirements are therefore $\mathcal{O}(Mp)$. Typically choosing $M$ to be between $5-20$ suffices for good performance.

### Trust region methods

If the objective function is nonconvex, then the Hessian $\mathbf{H}_t$ may not be positive definite, so $\mathbf{d}_t = -\mathbf{H}_t^{-1} \mathbf{g}_t$ may not be a descent direction. Newton's method becomes troublesome if the quadratic approximation becomes invalid. However, there is usually a local region around the current iterate where we can safely approximate the objective by a quadratic. Let us call this region $\mathcal{R}_t$, and let us call $M(\boldsymbol{\delta})$ the approximation to the objective, where $\boldsymbol{\delta} = \boldsymbol{\theta} - \boldsymbol{\theta}_t$. Then at each step we can solve

$$
  \boldsymbol{\delta}^* = \argmin_{\boldsymbol{\delta}\in\mathcal{R}_t} M_t (\boldsymbol{\delta})
$$

This is called *trust-region optimization*, and be seen as the "opposite" of line search, in the sense that we pick a distance we want to travel, determined by $\mathcal{R}_t$, and then solve for the optimal direction.

We usually assume that $M_t (\boldsymbol{\delta})$ is a quadratic approximation

$$
  M_t (\boldsymbol{\delta}) = \mathcal{L}(\boldsymbol{\theta}_t) + \mathbf{g}_t^\top \boldsymbol{\delta} + \frac{1}{2}\boldsymbol{\delta}^\top \mathbf{H}_t \boldsymbol{\delta}
$$

where $\mathbf{g}_t = \nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta})|_{\boldsymbol{\theta}_t}$, and $\mathbf{H}_t = \nabla_{\boldsymbol{\theta}}^2 \mathcal{L}(\boldsymbol{\theta})|_{\boldsymbol{\theta}_t}$ is the Hessian. Furthermore, it is common to assume that $\mathcal{R}_t$ is a ball of radius $r$, i.e. $\mathcal{R}_t = \Set{\boldsymbol{\delta} : \norm{\boldsymbol{\delta}}_2 \leq r}$. Using this, we can convert the constrained problem into an unconstrained one as follows:

$$
  \boldsymbol{\delta}^* = \argmin_{\boldsymbol{\delta}} M(\boldsymbol{\delta})
$$

for some Lagrangian multiplier $\lambda > 0$, which depends on the radius $r$. We can solve this usin

$$
  \boldsymbol{\delta} = -(\mathbf{H} + \lambda\mathbf{I})^{-1} \mathbf{g}
$$

This is called *Tikhonov damping* or *Tikhonov regularization*. Note that adding a sufficiently large $\lambda\mathbf{I}$ to $\mathbf{H}$ ensures the resulting matrix is always positive definite. As $\lambda\to 0$, this trust method reduces to Newton's method, but for $\lambda$ large enough, it will make all the negative eigenvalues positive.

## Gradient descent

Gradient descent is a first-order solver that iteratively moves along the negative gradient $\mathbf{g}_t = \nabla_\boldsymbol{\theta} \mathcal{L}(\boldsymbol{\theta})$. Each update takes the form

$$
\begin{equation}
  \boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta_t \nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta}) \tag{\label{equation-7}}
\end{equation}
$$

To motivate this method, note that the gradient of $\mathcal{L}$ at $\boldsymbol{\theta}$ yields the first order Taylor approximation

$$
  \mathcal{L}(\boldsymbol{\theta}_0) \approx \mathcal{L}(\boldsymbol{\theta}) + \langle \boldsymbol{\theta}_0 - \boldsymbol{\theta}, \nabla_{\boldsymbol{\theta} \mathcal{L}(\boldsymbol{\theta})}\rangle
$$

When $\boldsymbol{L}$ is convex, this approximation gives a lower for bound for $\mathcal{L}$, i.e.

$$
  \mathcal{L}(\boldsymbol{\theta}_0) \geq \mathcal{L}(\boldsymbol{\theta}) + \langle\boldsymbol{\theta}_0 - \boldsymbol{\theta}, \nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta})\rangle
$$

Thus, for $\boldsymbol{\theta}$ close to $\boldsymbol{\theta}_t$ we have that $\mathcal{L}(\boldsymbol{\theta}) \approx \mathcal{L}(\boldsymbol{\theta}_t) + \langle \boldsymbol{\theta} - \boldsymbol{\theta}_t, \nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta}_t)\rangle$, which we can minimize. However, this approximation might become loose when $\boldsymbol{\theta}$ is far away from $\boldsymbol{\theta}_t$. We therefore would like to minimize jointly the distance between $\boldsymbol{\theta}$ and $\boldsymbol{\theta}_t$ and the approximation of $\mathcal{L}$ around $\boldsymbol{\theta}_t$. If the parameter $\eta > 0$ controls the tradeoff between the two terms, we obtain the update rule

$$
  \boldsymbol{\theta}_{t+1} = \argmin_{\boldsymbol{\theta}} \frac{1}{2}\norm{ \boldsymbol{\theta} - \boldsymbol{\theta}_t }^2 + \eta\left(\mathcal{L}(\boldsymbol{\theta}_t) + \langle \boldsymbol{\theta} - \boldsymbol{\theta}_t, \nabla\mathcal{L}(\boldsymbol{\theta}_t)\rangle \right)
$$

Derivating with respect to $\boldsymbol{\theta}$ and solving for zero gives the update rule $\eqref{equation-7}$.

## Convex-Lipschitz functions

<MathBox title='' boxType='lemma' tag='lemma-1'>
Let $\mathbf{v}_1,\dots,\boldsymbol{v}_\tau$ be an arbitrary sequence in $\R^n$. Any algorithm with an initialization $\mathbf{w}_0 = 0$ and an update rule of the form

$$
  \mathbf{w}_{t+1} = \mathbf{w}_t - \eta\mathbf{v}_t
$$

satisfies

$$
\begin{equation}
  \sum_{t=1}^\tau \langle\mathbf{w}_t - \mathbf{w}^*, \mathbf{v}_t \rangle \leq \frac{1}{2\eta}\norm{\mathbf{w}^* }^2 + \frac{\eta}{2} \sum_{t=1}^\tau \norm{\mathbf{v}_t }^2 \tag{\label{equation-9}}
\end{equation}
$$

In particular, for every $B, \rho > 0$, if we have that $\norm{\mathbf{v}_t }\leq\rho$ for all $t$ and if we set $\eta = \sqrt{B^2 / (\rho^2 t)}$, then for every $\mathbf{w}^*$ with $\norm{\mathbf{w}^* } \leq B$ we have

$$
\begin{equation}
  \frac{1}{\tau}\sum_{t=1}^\tau \langle \mathbf{w}_t - \mathbf{w}^*, \mathbf{v}_t \rangle\leq\frac{B\rho}{\sqrt{\tau}} \tag{\label{equation-10}}
\end{equation}
$$

<details>
<summary>Proof</summary>

Completing the square, we obtain

$$
\begin{align*}
  \langle\mathbf{w}_t - \mathbf{w}^*, \mathbf{v}_t \rangle =& \frac{1}{\eta}\langle \mathbf{w}_t - \mathbf{w}^*, \eta\mathbf{v}_t \rangle \\
  =& \frac{1}{2\eta} \left(-\norm{ \mathbf{w}_t - \mathbf{w}^* - \eta\mathbf{w}_t }^2 + \norm{\mathbf{w}_t - \mathbf{w}^* }^2 + \eta^2 \norm{\mathbf{v}_t }^2 \right) \\
  =& \frac{1}{2\eta} \left(-\norm{\mathbf{w}_{t+1} - \mathbf{w}^* }^2 + \norm{\mathbf{w}_t - \mathbf{w}^* }^2 \right) + \frac{\eta}{2}\norm{\mathbf{v}_t }^2
\end{align*}
$$

where the last equality follows from the definition of the update rule. Summing the equality over $t$, we have

$$
\begin{equation}
\begin{split}
  \sum_{t=1}^\tau \langle\mathbf{w}_t - \mathbf{w}^*, \mathbf{v}_t \rangle =& \frac{1}{2\eta} \sum_{t=1}^\tau \left(-\norm{ \mathbf{w}_{t+1} - \mathbf{w}^* }^2 + \norm{ \mathbf{w}_t - \mathbf{w}^* }^2 \right) \\
  &+ \frac{2}{\eta} \sum_{t=1}^T \norm{\mathbf{v}_t }^2
\end{split}\tag{\label{equation-8}}
\end{equation}
$$

The first sum on the right-hand side is a telescopic sum that collapses to

$$
  \norm{ \mathbf{w}_0 - \mathbf{w}^* }^2 - \norm{ \mathbf{w}_{\tau + 1} - \mathbf{w}^* }^2
$$

Substuting this into $\eqref{equation-8}$ gives

$$
\begin{align*}
  \sum_{t=1}^\tau \langle\mathbf{w}_t - \mathbf{w}^*, \mathbf{v}_t \rangle =& \frac{1}{2\eta} (\norm{\mathbf{w}_0 - \mathbf{w}^* }^2 - \norm{ \mathbf{w}_{\tau+1} - \lVert \mathbf{w}_{\tau+1} - \mathbf{w}^* }^2) \\
  &+ \frac{\eta}{2} \sum_{t=1}^\tau \norm{\mathbf{v}_t }^2 \\
  \leq& \frac{1}{2\eta}\norm{ \mathbf{w}_0 - \mathbf{w}^* } + \frac{\eta}{2}\sum_{t=1}^\tau \norm{\mathbf{v}_t }^2 \\
  =& \frac{1}{2\eta} \norm{\mathbf{w}^* }^2 + \frac{\eta}{2} \sum_{t=1}^\tau \sum_{t=1}^\tau \norm{\mathbf{v}_t }^2
\end{align*}
$$

This proves $\eqref{equation-9}$. The second inequality follows by upper bounding $\norm{\mathbf{w}^* }$ by $B$, $\norm{\mathbf{v}_t }$ by $\rho$, dividing by $\tau$, and plugging in the value of $\eta$.
</details>
</MathBox>

Consider a loss function $\mathcal{L}$ that is convex-Lipschitz and let $\boldsymbol{\theta}^* \in\R^p$ be any vector with upper bound $\norm{\boldsymbol{\theta}^* }\leq B$. We would like to obtain an upper bound on the suboptimality of our solution with respect to $\boldsymbol{\theta^*}$, i.e. $\mathcal{L}(\bar{\boldsymbol{\theta}}) - \mathcal{L}(\boldsymbol{\theta}^*)$ where $\bar{\boldsymbol{\theta}} = \frac{1}{\tau} \sum_{t=1}^\tau \boldsymbol{\theta}_t$. From the definition of $\bar{\boldsymbol{\theta}}$ and Jensen's inequality, we have that

$$
\begin{align*}
  \mathcal{L}(\bar{\boldsymbol{\theta}}) - \mathcal{L}(\boldsymbol{\theta}^*) =& \boldsymbol{L}\left(\frac{1}{\tau}\sum_{t=1}^\tau \boldsymbol{\theta}_t \right) - \mathcal{L}(\mathbf{\theta}^*) \\
  \leq& \frac{1}{\tau} \left(\sum_{t=1}^\tau \mathcal{L}(\boldsymbol{\theta}_t)\right) - \mathcal{L}(\boldsymbol{\theta}) \\
  = \frac{1}{\tau} \sum_{t=1}^\tau \left(\mathcal{L}(\boldsymbol{\theta}_t) - \mathcal{L}(\boldsymbol{\theta}^*) \right) \tag{\label{equation-10}}
\end{align*}
$$

From the convexity of $\mathcal{L}$, we have for every $t$ that

$$
\begin{equation}
  \mathcal{L}(\boldsymbol{\theta}_t) - \mathcal{L}(\boldsymbol{\theta}^*) \leq \langle \boldsymbol{\theta}_t - \boldsymbol{\theta}^*, \nabla_\boldsymbol{\theta} \mathcal{L}(\boldsymbol{\theta}_t) \tag{\label{equation-11}}
\end{equation}
$$

Combining $\eqref{equation-10}$ and $\eqref{equation-11}$, we obtain

$$
  \mathcal{L}(\bar{\boldsymbol{\theta}}) - \mathcal{L}(\boldsymbol{\theta}^*) \leq \frac{1}{\tau} \langle \boldsymbol{\theta}_t - \boldsymbol{\theta}^*, \nabla\mathcal{L}(\boldsymbol{\theta}_t) \rangle
$$

Since $\mathcal{L}$ is $\rho$-Lipschitz, it follows that $\norm{\nabla\mathcal{L}(\boldsymbol{\theta}_t)} \leq\rho$. Applying $\ref{lemma-1}$, we obtain

$$
  \mathcal{L}(\bar{\boldsymbol{\theta}}) -\mathcal{L}(\boldsymbol{\theta}^*) \leq \frac{B\rho}{\sqrt{\tau}}
$$

Furthermore, for every $\epsilon > 0$, to achieve $\mathcal{L}(\bar{\boldsymbol{\theta}}) - \mathcal{L}(\boldsymbol{\theta}^*) \leq\epsilon$, it suffices to run the gradient descent algorithm for a number of iterations satisfying

$$
  \frac{\tau}{B^2 \rho^2}{\epsilon^2}
$$

## Subgradients

The gradient descent algorithm can be generalized to nondifferentiable functions by using a subgradient of $\mathcal{L}(\boldsymbol{\theta})$ at $\boldsymbol{\theta}_t$, instead of the gradient. The analysis of the convergence rate remains unchanged in this case and $\eqref{equation-11}$ also holds for subgradients.

<MathBox title='' boxType='lemma'>
Let $S\subseteq\R^n$ be an open convex set. A function $f:S\to\R$ is convex if and only if for every $\mathbf{w}\in S$, there exists $\mathbf{v}$ such that

$$
  f(\mathbf{u}) \geq f(\mathbf{w}) + \langle \mathbf{u} - \mathbf{v}, \mathbf{v} \rangle,\; \forall \mathbf{u}\in S
$$
</MathBox>

<MathBox title='Subgradient' boxType='definition' tag='definition-1'>
Let $S\subseteq\R^n$ be an open convex set and $f:S\to\R$ a convex function. A vector $\mathbf{v}\in\R^n$ that satifies 

$$
  f(\mathbf{u}) \geq f(\mathbf{w}) + \langle \mathbf{u} - \mathbf{v}, \mathbf{v} \rangle,\; \forall \mathbf{u}\in S
$$

is called a subgradient of $f$ at $\mathbf{w}$. The set of subgradient of $f$ at $\mathbf{w}$ is called the differential set, denoted $\partial f(\mathbf{w})$.
</MathBox>

<MathBox title='' boxType='proposition'>
Let $g(\mathbf{w}) = \max_{i\in [r]} g_i (\mathbf{w})$ for $r$ convex functions $g_1,\dots,g_r$. Given some $\mathbf{w}$, let $j\in\argmax_i g_i(\mathbf{w})$. Then $\nabla g_j (\mathbf{w}) \in \partial g(\mathbf{w})$.

<details>
<summary>Proof</summary>

Since $g_j$ is convex we have that for all $\mathbf{u}$

$$
  g_j (\mathbf{u}) \geq g_j (\mathbf{w}) + \langle \mathbf{u} - \mathbf{w}, \nabla g_j (\mathbf{w}) \rangle
$$

Since $g(\mathbf{w}) = g_j (\mathbf{w})$ and $g(\mathbf{u}) \geq g_j (\mathbf{u})$ we get

$$
  g(\mathbf{u}) \geq g(\mathbf{w}) + \langle\mathbf{u} - \mathbf{w}, \nabla g_j (\mathbf{w})\rangle 
$$
</details>
</MathBox>

### Subgradient for Lipschitz functions

<MathBox title='' boxType='lemma'>
Let $S\subseteq\R^n$ be a convex open set and let $f:A\to\R$ be a convex function. Then, $f$ is $\rho$-Lipschitz over $A$ if and only if for all $\mathbf{w}\in A$ and $\mathbf{v}\in \partial f(\mathbf{w})$ we have that $\norm{\mathbf{v}}\leq\rho$.


<details>
<summary>Proof</summary>

Assume that for all $\mathbf{v}\in\partial f(\mathbf{w})$ we have that $\norm{\mathbf{v}}\leq\rho$. Since $\mathbf{v}\in\partial f(\mathbf{w})$ we have

$$
  f(\mathbf{w}) - f(\mathbf{u}) \leq \langle\mathbf{v}, \mathbf{w} - \mathbf{u}\rangle
$$

Bounding the right-hand side using the Cauchy-Schwartz inequality we obtain

$$
  f(\mathbf{w}) - f(\mathbf{u}) \leq \langle\mathbf{v},\mathbf{w} - \mathbf{u}\rangle \leq \norm{\mathbf{v}}\cdot\norm{\mathbf{w} - \mathbf{v}} \leq \rho\norm{\mathbf{w} - \mathbf{u}}
$$

An analogous argument can show that $f(\mathbf{u}) - f(\mathbf{w}) \leq\rho\norm{\mathbf{w} - \mathbf{u}}$. Hence $f$ is $\rho$-Lipschitz.

Now assume that $f$ is $\rho$-Lipschitz. Choose some $\mathbf{w}\in A$ and $\mathbf{v}\in\partial f(\mathbf{w})$. Since $A$ is open, there exists $\epsilon > 0$ such that $\mathbf{u} = \mathbf{w} + \epsilon\mathbf{v}/\norm{\mathbf{v}}$ belongs to $A$. Thus, $\langle\mathbf{u} - \mathbf{w}, \mathbf{v}\rangle = \epsilon\norm{\mathbf{v}}$ and $\norm{\mathbf{u} - \mathbf{w}} = \epsilon$. From the definition of the subgradient (Definition $\ref{definition-1}$) we get

$$
  f(\mathbf{u}) - f(\mathbf{w}) \geq \langle\mathbf{v}, \mathbf{u} - \mathbf{w} \rangle = \epsilon\norm{\mathbf{v}}
$$

On the other hand, from the Lipschitzness of $f$ we have

$$
  \rho\epsilon = \rho\norm{\mathbf{u} - \mathbf{w}} \geq f(\mathbf{u}) - f(\mathbf{w})
$$

Combining these two inequalities we conclude that $\norm{\mathbf{v}}\leq\rho$.
</details>
</MathBox>

# Stochastic gradient descent

In *stochastic optimization* we seek to minimize the average value of a function

$$
  \mathcal{L}(\boldsymbol{\theta}) = \mathbb{E}_{q(\mathbf{z})} [\mathcal{L}(\boldsymbol{\theta},\mathbf{z})]
$$

where $\mathbf{z}$ is a random input to the objective. This could be a noise term, coming from the environment, or it could be a training example drawn randomly from the training set. At each iteration, we assume we observe $\mathcal{L}_t (\mathbf{\theta}) = \mathcal{L}(\boldsymbol{\theta}, \mathbf{z}_t)$, where $\mathbf{z}_t \sim q$. We also assume a way to compute an unbiased estimate of the gradient of $\mathcal{L}$. If the distribution $q(\mathbf{z})$ is independent of the parameters we are optimizing, we can use $\mathbf{g}_t = \lambda_{\boldsymbol{\theta}} \mathcal{L}_t (\boldsymbol{\theta})$. In this case, the resulting algorithm can be written as follows:

$$
  \boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta_t \nabla\mathcal{L}(\boldsymbol{\theta}_t, \boldsymbol{z}_t) = \boldsymbol{\theta}_t - \eta_t \mathbf{g}_t
$$

This method is called *stochastic gradient descent* (SGD). As long as the gradient estimate is unbiased, then this method will converge to a stationary point, provide we decay the step size $\eta_t$ at a certain rate.

<MathBox title='Stochastic gradient descent' boxType='algorithm'>
**parameters:**
- iterations $\tau \in\N_+$
- learning rate $\eta > 0$

**initialization:**
- choose $\mathbf{w}_1 = \mathbf{0}$

**for** $t=1,\dots,\tau$
1. choose gradient $\mathbf{g}_t$ at random from a distribution such that $\mathbb{E}[\mathbf{g}_t | \mathbf{w}_t] \in\partial f(\mathbf{w}_t)$
2. update $\mathbf{w}_{t+1} = \mathbf{w}_t - \eta\mathbf{g}_t$

**output:**
- $\bar{w} = \frac{1}{\tau} \sum_{t=1}^\tau$
</MathBox>

## Application to finite sum problems

SGD is particularly useful in machine learning models based on minimizing empirical risk, which is given by the loss function

$$
  \mathcal{L}(\boldsymbol{\theta}_t) = \frac{1}{n} \sum_{i=1}^n \ell(\mathbf{y}_i , f(\mathbf{x}_i; \boldsymbol{\theta}_t)) = \frac{1}{2} \sum_{i=1}^n \mathcal{L}_i (\boldsymbol{\theta}_t)
$$

This is called a *finite sum problem*. The gradient of this objective has the form

$$
\begin{equation}
  \mathbf{g}_t = \frac{1}{n} \sum_{i=1}^n \nabla_{\boldsymbol{\theta}} \mathcal{L}_i (\boldsymbol{\theta}_t) = \frac{1}{n} \sum_{i=1}^n \nabla_{\boldsymbol{\theta}} \ell(\mathbf{y}_i, f(\mathbf{x}_i; \boldsymbol{\theta}_t)) \tag{\label{equation-5}}
\end{equation}
$$

This requires summing over all $n$ training examples, which can be slow if $n$ is large. On the other hand, we can approximate this by sampling a *minibatch* of $b \ll n$ samples to get

$$
  \mathbf{g}_t \approx \frac{1}{|\mathcal{B}_t|} \sum_{i\in\mathcal{B}_t} \nabla_{\boldsymbol{\theta}} \mathcal{L}_i (\boldsymbol{\theta}_t) = \frac{1}{|\mathcal{B}_t|} \sum_{i\in\mathcal{B}_t} \nabla_{\boldsymbol{\theta}} \ell(\mathbf{y}_i, f(\mathbf{x}_i;\boldsymbol{\theta}_t))
$$

where $\mathcal{B}_t$ is a set of randomly chosen examples to use at iteration $t$. This is an unbiased approximation to the empirical average $\eqref{equation-5}$. Hence, we can safely use this with SGD.

In practive we usually sample $\mathcal{B}_t$ without replacement. Once we reach the end of a dataset after a single training *epoch*, we can perform a random shuffling of the examples, to ensure that each minibatch on the epoch is different from the last.

Although the theoretical rate of convergence of SGD is slower than batch GD (in particular, SGD has a sublinear convergence rate), in practice SGD is often faster, since the per-step time is much lower. To see why SGD can make faster progress than full batch GD, suppose we have a dataset consisting of a single example duplicated $K$ times. Batch training will be (at least) $K$ times slower than $SGD$, since it will waste time computing the gradient for the repeated examples. Even if there are no duplicates, batch training can be wasteful, since early on in training the parameters are not well estimated, so it is not worth carefully evaluating the gradient.

<MathBox title='SGD for fitting linear regression' boxType='example' tag=''>
Recall the objective for a linear regression problem has the form

$$
  \mathcal{L}(\boldsymbol{\theta}) = \frac{1}{n} \sum_{i=1}^n (\mathbf{x}_i^\top \boldsymbol{\theta} - y_i)^2 = \frac{1}{n}\norm{\mathbf{X}\boldsymbol{\theta} - \mathbf{y}}_2^2
$$

with gradient

$$
  \mathbf{g}_t = \frac{2}{n} \sum_{i=1}^n (\boldsymbol{\theta}_t^\top \mathbf{x}_i - y_i)\mathbf{x}_i = \frac{2}{n}\mathbf{X}^\top (\mathbf{X}\beta - \mathbf{y}) 
$$

and Hessian

$$
  \mathbf{H}(\mathbf{X}) = \frac{2}{n}\mathbf{X}^\top \mathbf{X}
$$

Since $\mathbf{X}^\top \mathbf{X}$ is always positive semi-definite, it follows that $\mathcal{L}$ is a convex function. Now consider using SGD with a minibatch size of $b = 1$. The update becomes

$$
  \boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta_t (\boldsymbol{\theta}_t^\top \mathbf{x}_i - y_i)\mathbf{x}_i
$$

where $i = i(t)$ is the index of the example chosen at iteration $t$. This solver is called the *least mean squares* (LMS) algorithm, and is also know as the *delta rule*, or the *Widrow-Hoff rule*. 
</MathBox>

## Step sizing

When using SGD, we need to be careful in how we choose the learning rate in order to achieve convergence. Typically, an overly small learning rate results in underfitting, and overly large learning rate results in instability of the model.

One heuristic for choosing a good learning rate is to start with a small learning rate and gradually increase it, evaluating performance using a small number of minibatches. The learning rate with the lowest loss is picked. To ensure stability, it is better to pick a rate that is slightly smaller then one with the lowest loss.

Rather than choosing a single constant learning rate, we can use a *learning rate schedule*, adjusting the step size over time. Theoretically, a sufficient condition for SGD to achieve convergence is if the learning rate schedule satisfies the *Robbins-Monro conditions*:

$$
  \eta_t \to 0,\; \frac{\sum_{t=1}^\infty \eta_t^2}{\sum_{t=1}^\infty \eta_t} \to 0
$$

The following three learning rate schedules are commonly used:
1. Piecewise constant: $\eta_t = \eta_i$ if $t_i \leq t \leq t_{i+1}$
2. Exponential decay: $\eta_t = \eta_0 e^{-\lambda t}$
3. Polynomial decay: $\eta_t = \eta_0 (\beta t + 1)^{-\alpha}$

In the piecewise constant schedule, $t_i$ are a set of time points at which we adjust the learning rate to a specified value. For example, we may set $\eta_i = \eta_0 \gamma^i$, which reduces the initial learning rate by a factor of $\gamma$ for each threshold that we pass. This is called *step decay*. Another approach called *time decay rate* decreases the learning for batch training within an epoch, as well as for each epoch. If $m$ denotes the number of batches and $e_i$ the current epoch, the learning rate is updated according to

$$
  \eta_i(t) = \frac{t_0}{t + t_1},\; i=1,\dots
$$

where $t = e_i m + i$ and $t_0$ and $t_1$ are fixed parameters. Sometimes the threshold times are computed adaptively, by estimating when the train or validation loss are plateaued. This is called *reduce-on-plateau*.

Exponential decay is typically too fast. A common choice is polynomial decay, with $\alpha = 0.5$ and $\beta = 1$. This corresponds to a *square-root schedule*, $\eta_t = \eta_0 \frac{1}{\sqrt{t + 1}}$.

In deep learning, another common schedule is to quickly increase the learning rate and then gradually decrease it again. This is called *learning rate warmup*, or the *one-cycle learning rate schedule*. The motivation for this is the following: initially the parameters may be in a part of the loss landscape that is poorly conditioned, so a large step size will “bounce around” too much and fail to make progress downhill. However, with a slow learning rate, the algorithm can discover flatter regions of space, where a larger step size can be used. Once there, fast progress can be made. However, to ensure convergence to a point, we must reduce the learning rate to 0.

It is also possible to increase and decrease the learning rate multiple times, in a cyclical fashion. This is called a *cyclical learning rate*. The motivation behind this approach is to escape local minima. The minimum and maximum learning rates can be found based on the initial "dry run" described above, and the half-cycle can be chosen based on how many restarts you want to do with yor training budget. A related approach, known as *stochastic gradient descent with warm restarts*, works by storing all the checkpoints visited after each cool down, and using all of them as members of a model ensemble.

An alternative to using heuristics for estimating the learning rate is to use line seach. This is tricky when using SGD, because the noisy gradient make the computation of the Armijo condition difficult. However, this can be made to work if the variance of the gradient noise goes to zero over time. This can happed if the model is sufficiently flexible that it can perfectly interpolate the training set.

## Iterate averaging

The parameter estimates produced by SGD can be very unstable over time. To reduce the variance of the estimate, we can compte the average using

$$
  \bar{\boldsymbol{\theta}}_t = \frac{1}{t} \sum_{i=1}^t \boldsymbol{\theta}_i = \frac{1}{t}\boldsymbol{\theta}_t + \frac{t - 1}{t} \bar{\boldsymbol{\theta}}_{t-1}
$$

where $\boldsymbol{\theta}_t$ are the usual SGD iterates. This is called *iterated averaging* or Polyak-Ruppert averaging.

It can be proven that the estimate $\bar{\boldsymbol{\theta}}_t$ achieves the best possible asymptotic convergence rate among SGD algorithms, matching that of variants using second-order information, such as Hessians. This averaging can also have statistical benefits. In the case of linear regression, it can be proven that this method is equivalent to $\ell_2$ regularization.

Rather than an exponential moving average of SGD, *stochaastig weight averaging* (SWA) uses an equal average in conjunction with a modified learning rate schedule. In contrast to standard Polyak-Ruppert averaging, which was motivated for faster convergence rates, SWA exploits the flatness is objective used to train deep neural networkds, to find solutions which provide better generalization.

## Variance reduction

Variance reduction methods can reduce the variance of the gradients, rather that the parameters themselves and are designed to work for finite sum problems. In some cases, this can improve the theoretical convergence rate from sublinear to linear (i.e., the same as full-batch gradient descent). Variance reduction methods are widely used for fitting machine learning model with convex objectives, such as linear models

### Stochastic variance reduced gradient (SVRG)

Stochastic variance reduced gradient (SVRG) introduces a control variate used to estimate a baseline value of the gradient based on the full batch, which we then use to compare the stochastic gradient to. At a given interval, e.g. once per epoch, we compute the full gradient at a "snapshot" of the model parameters $\tilde{\boldsymbol{\theta}}$. Thus, the corresponding "exact" gradient isi $\nabla\mathcal{L}(\tilde{\boldsymbol{\theta}})$. At step $t$, we compute the usual stochastic gradient at the current parameters, $\nabla_t \mathcal{L}(\boldsymbol{\theta}_t)$, but also at the snapshot parameters, $\nabla\mathcal{L}_t (\tilde{\boldsymbol{\theta}})$, which we use a baseline. We can then use the following improved gradient estimate

$$
  \mathbf{g}_t = \nabla\mathcal{L}_t (\boldsymbol{\theta}_t) - \nabla\mathcal{L}_t (\tilde{\boldsymbol{\theta}}) + \nabla\mathcal{L}(\tilde{\boldsymbol{\theta}})
$$

to compute $\boldsymbol{\theta}_{t+1}$. This unbiased because $\mathbb{E}\left[\nabla\mathcal{L}_t (\tilde{\boldsymbol{\theta}})\right] = \nabla\mathcal{L}(\tilde{\boldsymbol{\theta}})$. Furthermore, the update only involves two gradient computations, since we can compute $\nabla\mathcal{L}(\tilde{\boldsymbol{\theta}})$ once per epoch. At the end of the epoch, we update the snapshot parameters, $\tilde{\boldsymbol{\theta}}$, based on the most recent value of $\boldsymbol{\theta}_t$, or a running average of the iterates, and update the expected baseline. We can compute the baseline less often, but then the baseline will not be correlated with the objective and can hurt performance.

Iterations of SVRG are computationally faster compared with full-batch GD, but SVRG can still match the theoretical convergence rate of GD. There are various difficulties with using SVRG with conventional deep learning training practices. For example, the use of batch normalization, data augmentation and dropout all break the assumption of the method, since the loss will differ randomly in ways that depend not just on the parameters and the data index $i$.

### Stochastic averaged gradient accelerated (SAGA)

Unlike SVRG, the stochastic averaged gradient accelerated (SAGA) algorithm requires only one full batch gradient computation, at the start of the algorithm. This saves time at the cost of using more memory by storing $n$ gradient vectors. This enables SAGA to maintain an approximation of the global gradient by romving the old local gradient from the overall sum and replacing it with the new local gradient. This is called an *aggregated gradient* method. 

More precisely, we first initialize by computing $\mathbf{g}_n^{\text{local}} = \nabla\mathcal{L}_i (\boldsymbol{\theta}_0)$ for all $i=1,\dots,n$, and the average $\bar{\mathbf{g}} = \frac{1}{n}\sum_{i=1}^n \mathbf{g}_n^{\text{local}}$. Then, at iteration $t$, we use the gradient estimate

$$
  \mathbf{g}_t = \nabla\mathcal{L}_i (\boldsymbol{\theta}_t) - \boldsymbol{g}_i^{\text{local}} + \bar{\mathbf{g}}
$$

where $i\sim\operatorname{Uniform}\Set{1,\dots,n}$ is the example index sampled at iteration $t$. We then update $\mathbf{g}_i^{\text{local}} = \nabla \mathcal{L}_i (\boldsymbol{\theta}_t)$ and $\bar{\mathbf{g}}$ by replacing the old $\mathbf{g}_i^{\text{local}}$ by its new value.

This has an advantage over SVRG since it only has to do one full batch sweep at the start. In fact, the initial step is not necessary, since we can compute $\bar{\mathbf{g}}$ "lazily", by only incorporating gradient we have seen so far. The downside is the large extra memory cost. However, if the features are sparse, the memory cost can be reasonable.

## Preconditioned SGD

Preconditioned SGD involves the following update

$$
  \boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta_t \mathbf{M}_t^{-1} \mathbf{g}_t
$$

where $\mathbf{M}_t$ is a *precondition matrix*, or simply the *preconditioner*, typically chosen to be positive definite. Unfortunately, the noise in the gradient estimates make it difficul to reliably estimate the Hessian, which makes it difficult to use second-order methods. In addition, it is expensive to solve for the update direction with a full preconditioning matrix. Thus, most practitioners use a diagonal preconditioner $\mathbf{M}_t$. Such preconditioners do not necessarily use second-order information, but often result in speedups compared to vanilla SGD.

### Adaptive gradient (AdaGrad)

The adaptive gradient (AdaGrad) method was originally designed for optimizing convex objectives where many elements of the gradient vector are zero; these might correspond to features that are rarely present in the input, such as rare words. The update has the following form

$$
  \theta_{t+1, j} = \theta_{t,p} - \eta_t \frac{1}{\sqrt{s_{t,j} + \epsilon}} g_{t,i},\; i=1,\dots,p
$$

where $s_{t,j} = \sum_{i=1}^t g_{i,j}^2$ is the sum of the squared gradients and $\epsilon > 0$ is a small term to avoid dividing by zero. Equivalently, we can write the update in the vector form

$$
  \Delta\boldsymbol{\theta}_t = \eta_t \frac{1}{\sqrt{\mathbf{s}_t + \epsilon}}\mathbf{g}_t
$$

where the square root and division is performed elementwise. Viewed as preconditioned SGD, this is equivalent to taking $\mathbf{M}_t = \operatorname{diag}(\mathbf{s}_t + \boldsymbol{\epsilon})^{1/2}$. This is an example of an *adaptive learning rate*. The overall step size $\eta_t$ still needs to be chosen, but the results are less sensitive to it compared to vanilla GD. In particular, we usually fix $\eta_t = \eta_0$.

### Root mean square propagation (RMSProp)

A defining feature of AdaGrad is that the term in the denominator gets larger over time, so the effective learning rate drops. While it is necessary to ensure convergence, it might hurt performance as the denominator gets to large too fast.

An alternative is to use an exponentially weighted moving average (EWMA) of the past squared gradients, rather than their sum

$$
  s_{t+1, j} = \beta s_{t,j} + (1 - \beta) g_{t,j}^2
$$

In practice we usually use $\beta\sim 0.9$, which puts more weight on recent examples. In this case,

$$
  \sqrt{s_{t,j}} \approx \operatorname{RMS}(\mathbf{g}_{1:t,j}) = \sqrt{\frac{1}{t}\sum_{\tau=1}^t g_{\tau,j}^2}
$$

This method is known as root mean squared propagator (RMSProp). The overall update of RMPSProp is

$$
  \Delta\boldsymbol{\theta}_t = -\eta_t \frac{1}{\sqrt{\mathbf{s}_t + \epsilon}}\mathbf{g}_t
$$

The adaptive delta (ADADelta) is similar to RMSPRops. However, in addition to accumulating an EWMA of the gradients in $\hat{\mathbf{s}}$, it also keeps an EWMA of the updates $\boldsymbol{\delta}_t$ to obtain an update of the form

$$
  \Delta\boldsymbol{\theta}_t = -\eta_t \frac{\sqrt{\boldsymbol{\delta}_{t-1} + \epsilon}}{\sqrt{\mathbf{s}_t + \epsilon}}\mathbf{g}_t
$$

where $\boldsymbol{\delta}_t = \beta\boldsymbol{\delta}_{t-1} + (1-\beta)(\Delta\boldsymbol{\theta}_t)^2$ and $\mathbf{s}_t$ is the same as in RMSProp. This has the advantage that the "units" of the numerator and denominator cance, so we are just elementwise-multiplying the gradient by a scalar. This eliminates the need to tune the learning rate $\eta_t$, which means one can simply set $\eta_t = 1$. However, since these adaptive learning rates need not decrease with time (unless chosen explicitly to do so), these methods are not guaranteed to converge to a solution.

### Adaptive moment estimation (ADAM)

It is possible to combine RMSProp with momentum. In particular, let us compute an EWMA of the gradients (as in momentum) and squared gradients (as in RMSProp)

$$
\begin{align*}
  \mathbf{m}_t =& \beta_1 \mathbf{m}_{t-1} + (1 - \beta_1) \mathbf{g}_t \\
  \mathbf{s}_t =& \beta_2 \mathbf{s}_{t-1} + (1 - \beta_2) \mathbf{g}_t^2 
\end{align*}
$$

We then perform the following update

$$
  \Delta\boldsymbol{\theta}_t = -\eta_t \frac{1}{\sqrt{s}_t + \epsilon} \mathbf{m}_t
$$

The resulting method is called adaptive momentum estimation (ADAM). The standard values for the various constants are
- $\beta_1 = 0.9$
- $\beta_2 = 0.999$
- $\epsilon = 10^{-6}$

If we set $\beta_1 = 0$ and no bias-correction, we recover RMSProp, which does not use momentum. For the overall learning rate, it is common to use a fixed value such as $\eta_t = 0.001$. Again, as the adaptive learning rate may not decrease over time, convergence is not guaranteed.

If we initialize with $\mathbf{m}_0 = \mathbf{s}_0 = \mathbf{0}$, then initial estimates will be biased towards small values. This can be avoided with biased-corrected moments, which increase the values early in the optimization process. These estimates are given by

$$
\begin{align*}
  \hat{\mathbf{m}}_t =& \frac{\mathbf{m}_t}{1 - \beta_1^t} \\
  \hat{\mathbf{s}}_t =& \frac{\mathbf{s}_t}{1 - \beta_2^2}
\end{align*}
$$

### Issues with adaptive learning rates

When using diagonal scaling methods, the overall learning rate i determined by $\eta_0 \mathbf{M}_t^{-1}$, which changes with time. Hence these methods are often called *adaptive learning rate methods*. However, they still require setting the base learning rate $\eta_0$.

Since the EWMA methods are typically used in the stochastic setting where the gradient estimates are noisy, their learning rate adaption can result in non-convergence even on convex problems. Various solutions addressing this problem have been proposed, including
- Adaptive momentum stochastic gradient (AMSGrad)
- Partially adaptive momentum estimation method (PADAM)
- Yogi

The Yogi update modifies ADAM by replacing

$$
  \mathbf{s}_t = \beta_2 \mathbf{s}_{t-1} + (1 - \beta_2) \mathbf{g}_t^2 = \mathbf{s}_{t-1} + (1 - \beta_2)(\mathbf{g}_t^2 - \mathbf{s}_{t-1})
$$

with

$$
  \mathbf{s}_t = \mathbf{s}_{t-1} + (1 - \beta_2) \mathbf{g}_t^2 \odot \operatorname{sign}(\mathbf{g}_t^2 - \mathbf{s}_{t-1})
$$

However, more recent work has shown that vanilla Adam can be made to alway converge provided the $\beta_1$ and $\beta_2$ parameters are tuned on a per-dataset basis. In practice, it is common to fix $\beta_1 = 0.9$ and just tune $\beta_2$.

### Non-diagonal preconditioning matrices

Although adaptive methods can adapt the learning rate of each parameter, they do not solve the more fundamental problem of ill-conditioning due to correlation of the parameters, and hence do not always provide as mush of a speed boost over SGD as one may hope.

One way to get faster convergence is to use the following preconditioning matrix, known as *full-matrix ADAGrad*:

$$
  \mathbf{M}_t = [(\mathbf{G}_t \mathbf{G}_t^\top)^{1/2} + \epsilon\mathbf{I}_p]^{-1}
$$

where $\mathbf{G}_t = [\mathbf{g}_t,\dots,\mathbf{g}_1]$. Here $\mathbf{g}_i = \nabla_{\boldsymbol{\psi}} c(\boldsymbol{psi})$ is the $p$-dimensional gradient vector computed at step $i$. Unfortunately, $\mathbf{M}_t$ is a $p\times p$ matrix, which is expensive to store and invert.

The *Shampoo algorithm* makes a block diagonal approximation to $\mathbf{M}$, one per layer of the model, and then exploits Kronecker product structure to efficiently invert it. 

# References

[^ref]