---
title: 'Learning Framework'
subject: 'Machine Learning'
showToc: true
---

# Terminology

- **Examples:** Items or instances of data used for learning or evaluation.
- **Features:** The set of attributes, often represented as a tensor, associated to an example.
- **Labels:** Values or categories assigned to examples.
- **Training sample:** Examples used to train a learning algorithm.
- **Validation sample:** Examples used tune the parameters of a learning algorithm when working with labeled data.
- **Test sample:** Examples used to evaluate the performance of a learning algorithm that are separate from the training and validation samples.
- **Loss function:** A function measuring the difference, or loss, between a predicted label and a true label. If $Y$ is the set of all labels and $Y'$ is the set of all possible predictions, the loss function is a mapping $L: Y \times Y' \to \R_+$. In most cases $Y' = Y$ and the loss function is bounded. Common example of loss functions are the zero-one (or misclassification) loss defined over $\Set{-1,1} \times \Set{-1,1}$ by $L(y, y') = \mathbf{1}_{y' = \neq y}$ and the squared loss defined over $I\times I$ by $L(y, y') = (y' - y)^2$, where $I\subseteq\R$ is typically a bounded interval.
- **Hypothesis set:** A set of functions mapping features to the set of labels.

Machine learning techniques can be divided into seven approaches:
- **Supervised learning:** The learner receives a set of labeled examples as training data and makes predictions for all unseen points.
    - Classification (discrete)
    - Regression (continuous)
- **Unsupervised learning:** The learner exclusively receives unlabeled training data and makes predictions for all unseen points. 
    - Clustering
- **Semi-supervised learning:** The learner receives a training sample consisting of both labeled and unlabed data, and makes predictions for all unseen points.
- **Transductive inference:** The learner receives a labeled training sample along with a set of unlabeled test points and makes predicition only these particular test points.
- **On-line learning:** Learning is conducted over multiple rounds with intermixed training and testing phases. At each round, the learner receives an unlabeled training point, makes a prediction, receives the true label, and incurs a loss. The objective is to minimize the cumulative loss over all rounds. No distributional assumption is made; instances and their labels may be chosen adversarially.
- **Reinforcement learning:** To collect information, the learner actively interacts with the environment and in some cases affects the environment, and receives an immediate reward for each action. The goal is to maximize the reward over a course of actions and iterations with the environment. However, no long-term reward feedback is provided by the environment, and the learner is faced with the *exploration versus exploitation* dilemma, since it must choose between exploring unknown actions to gain more information versus exploiting the information already collected.
- **Active learning:** The learner adaptively or interactively collects training examples, typically by quering an oracle to request labels for new points. The goal is to achieve a performance comparable to supervised learning.

# Probably approximately correct (PAC) learning framework

Let $X$ be the set of all possible examples, also referred to as the input space, and let $Y$ be the set of all possible labels or target values. A *concept* is a mapping $c:X\to Y$, while a concept class $C$ is a set of concepts that can be learnt. We assume the examples are independently and identically distributed (i.i.d.) according to some fixed yet unknown distribution $D$. 

The learning problem is formulated as follows. The learner considers a fixed set of possible concepts $H$, called a *hypothesis set*, which may not coincide with $C$. It receives a samle $S = (x_1,\dots,x_n)$ drawn i.i.d. according to $D$ as well as the labels $(c(x_1),\dots,c(x_n))$, which are based on a specific target concept $c \in C$ to learn. Its task is use the labeled sample $S$ to select a hypothesis $h_s \in H$ that has a small *generalization error* with respect to the concept $c$. 

<MathBox title='Generalization error' boxType='definition'>
Given a hypothesis $h\in H$, a target concept $c\in C$, and an underlying distribution $D$, the *generalization error/risk* of $h$ is defined by

$$
  R(h) = \Pr(h(x) \neq c(x)) = \mathbb{E}(\mathbf{1}_{h(x)\neq c(x)}),\; x\sim D
$$

Note that by definition, both the hypothesis set $H$ and concept class $C$ are measurable.
</MathBox>

The generalization error of a hypothesis is not directly accessible to the learner since both the distribution $D$ and the taget concept $c$ are unknown. However, the learner can measure the *empirical error* of a hypothesis on the labeled sample $S$.

<MathBox title='Empirical error' boxType='definition'>
Given a hypothesis $h\in H$, a target concept $c\in C$, and a sample $S = (x_1,\dots,x_n)$, the *empirical error/risk* of $h$ is defined by

$$
  \hat{R}(h) = \frac{1}{n} \sum_{i=1}^n \mathbf{1}_{h(x_i) \neq c(x_i)}
$$

Note that by definition, both the hypothesis set $H$ and concept class $C$ are measurable.
</MathBox>

The empirical error of $h\in H$ is its average error over a sample $S$, while the generalization error is its expected error based on the distribution $D$. Note that for a fixed $h\in H$, the expectation of the empirical error based on an i.i.d. sample $S$ is equal to the generalization error, i.e. $\mathbb{E}(\hat{R}(h)) = R(h)$. By linearity to the expectation and the fact that the sample is drawn i.i.d., we can write for any $x\in S$

$$
\begin{align*}
  \underset{x\sim D^n}{\mathbb{E}}(\hat{R}(h)) =& \frac{1}{n} \sum_{i=1}^n \underset{x\sim D^n}{\mathbb{E}}(\mathbf{1}_{h(x_i) \neq c(x_i)}) \\
  =& \frac{1}{n} \sum_{i=1}^n \underset{x\sim D^n}{\mathbb{E}}(\mathbf{1}_{h(x_i) \neq c(x_i)})
\end{align*}
$$

Thus,

$$
\begin{align*}
  \underset{x\sim D^n}{\mathbb{E}}(\hat{R}(h)) =& \underset{x\sim D^n}{\mathbb{E}} (\mathbf{1}_{\Set{h(x) \neq c(x)}}) \\
  =& \underset{x\sim D^n}{\mathbb{E}}(\mathbf{1}_{\Set{h(x) \neq c(x)}}) = R(h)
\end{align*}
$$

<MathBox title='PAC-learning' boxType='definition'>
A concept class $C$ is PAC-learnable if there exists an algorithm $A$ and a polynomial function $p(\cdot,\cdot,\cdot,\cdot)$ such that for any $\varepsilon > 0$ and $\delta > 0$, for all distributions $D$ on $X$ and for any target concept $c\in C$, the following holds for any sample size $n \geq p\left(\frac{1}{\varepsilon}, \frac{1}{\delta}, m, s(c) \right)$

$$
  \underset{S\sim D^n}\Pr(R(h_S) \leq \varepsilon) \geq 1 - \delta
$$

where $s(c)$, the size of $c$, is the maximal cost of the computational representation of $c\in C$. If $A$ further runs in $ p\left(\frac{1}{\varepsilon}, \frac{1}{\delta}, m, s(c) \right)$, then $C$ is efficiently PAC-learnable. When such an algorithm $A$ exists, it called a PAC-learning algorithm for $C$.

A concept class $C$ is thus PAC-learnable if the hypothesis returned by the algorithm after observing a number of points polynomial in $\frac{1}{\varepsilon}$ and $\frac{1}{\delta}$ is *approximately correct* (error at most $\varepsilon$) with high *probability* (at least $1-\delta$). In this framework, $\delta > 0$ describes the *confidence* $1-\delta$ and $\varepsilon > 0$ the *accuracy* $1 - \varepsilon$. Note that if the running time of the algorithm is polynomial in $\frac{1}{\varepsilon}$ and $\frac{1}{\delta}$, then the sample size $n$ must also be polynomial if the full sample is received by the algorithm.
</MathBox>

Several characteristics of the PAC definition are worth emphasizing. First, the PAC is framework is a distribution-free model as it makes no particular assumptions about the distribution $D$ from which samples are drawn. Second, the training sample and the test examples used to define the error are drawn according to the same distribution $D$. This is a necessary assumption for generalization to by possible in most cases.

Finally, the PAC framework deals with the question of learnability for a concept class $C$ and not a particular concept. Note that the concept class $C$ is know to the algorithm, while the target concept $c$ is unknown. In many cases, in particular when the computational representation of the concepts is not explicitly discussed or is straightforward, we may omit the polynomial dependency on $n$ and $s(c)$ in the PAC definition and focus only on the sample complexity.

## Learning guarantees for finite hypothesis sets

### Consistent case

<MathBox title='Learning bounds for finite hypotheses sets (consistent case)' boxType='theorem'>
Let $H$ be a finite hypothesis set. Let $A$ be an algorithm that for any target concept $c\in H$ and i.i.d. sample $S$ returns a consistent hypothesis $h_S: \hat{R}(h_s) = 0$. Then, for any $\epsilon, \delta > 0$, the inequality $\Pr_{S\sim D^m} [R(h_s) \leq\epsilon] \geq 1 - \delta$ holds if

$$
\begin{equation}
  m \geq \frac{1}{\epsilon}\left(\ln|H| + \ln\frac{1}{\delta} \right) \tag{\label{equation-1}}
\end{equation}
$$

This sample complexity result admits the following equivalent statement as a generalization bound: for any $\epsilon,\delta > 0$, with probability at least $1 - \delta$,

$$
\begin{equation}
  R(h_S) \leq \frac{1}{m}\left(\ln|H| + \ln\frac{1}{\delta} \right) \tag{\label{equation-2}}
\end{equation}
$$

<details>
<summary>Proof</summary>

Let $H = \Set{h_i}_{i=1}^{|H|}$, and fix $\epsilon > 0$. We do not know which consistent hypothesis $h_S \in H$ is selected by the algorithm $A$. This hypothesis further depends on the training sample $S$. Thus, we need to give a *uniform convergence bound*, i.e. a bound that holds for the set of all consistent hypotheses, which a fortiori includes $h_S$. Consequently, the boundedness of the probability that some $h\in H$ would be consistent and have error more than $\epsilon$ is given by

$$
\begin{align*}
  \Pr[\exists h \in H : \hat{R}(h) = 0 \land R(h) > 0] =& \Pr\left(\bigvee_{i=1}^{|H|} [h_i \in H, \hat{R}(h_i) = 0 \land R(h_i) > \epsilon] \right) \\
  \leq& \sum_{h\in H} \Pr[\hat{R}(h) = 0 \land R(h) > \epsilon] \\
  \leq& \sum_{h\in H} \Pr[\hat{R}(h) = 0 | R(h) > \epsilon]
\end{align*}
$$

where the first inequality follows from union of bounds and the second from the definition of conditional probability. Now, consider any hypothesis $h\in H$ with $R(h) > \epsilon$. The probability that $h$ is consistent on a training sample $S$ drawn i.i.d., that is, it has no error on any pont in $S$, can be bounded as

$$
  \Pr[\hat{R}(h) = 0 | R(h) > \epsilon] \leq (1 - \epsilon)^m
$$

The previous inequality implies

$$
  \Pr[\exists h\in H : \hat{R}(h) = 0 \land R(h) > \epsilon] \leq |H|(1 - \epsilon)^m
$$

Setting the right-hand side to be equal to $\delta$ and solving for $\epsilon$ concludes the proof.
</details>
</MathBox>

The theorem shows that when the hypothesis set $H$ is finite, a consistent algorithm $A$ is a PAC-learning algorithm, since the sample complexity given by $\eqref{equation-1}$ is dominated by a polynomial in $1/\epsilon$ and $1/\delta$. As shown by $\eqref{equation-2}$, the generalization error of consistent hypotheses is upper bounded by a term that decreases as a function of the sample size $m$. As expected, learning algorithms benefit from larger labeled training samples. The decrease rate of $O(1/m)$ guaranteed by this theorem, however, is particularly favorable.

The upper bound increases logarithmically with the cardinality $\ln|H|$. By appropriate scaling, this is related to the term $\log_2 |H|$, which represents the number of bits needed to represent $H$.

### Inconsistent case

<MathBox title='' boxType='corollary'>
Let $S$ be an i.i.d. sample of size $m$. If $\epsilon > 0$, then for any hypothesis $h:X\to\Set{0,1}$, the following inequalities hold

$$
\begin{align*}
  \Pr_{S\sim D^m} [\hat{R}(h) - R(h) \geq\epsilon] \leq& e^{-2m\epsilon^2} \\
  \Pr_{S\sim D^m} [\hat{R}(h) - R(h) \leq -\epsilon] \leq& e^{-2m\epsilon^2}
\end{align*}
$$

By the union bound, this implies the following two-sided inequality

$$
\begin{equation}
  \Pr_{S\sim D^m} [|\hat{R}(h) - R(h)| \geq \epsilon] \leq 2e^{-2m\epsilon^2} \tag{\label{equation-3}}
\end{equation}
$$

<details>
<summary>Proof</summary>

This follows immediately from Hoeffding's inequality.
</details>
</MathBox>

<MathBox title='Generalization bound (single hypothesis)' boxType='corollary' tag='corollary-2'>
Fix a hypothesis $h:X\to\Set{0,1}$. Then, for any $\delta > 0$, the following inequality holds with probability at leas $1 - \delta$

$$
\begin{equation}
  R(h) \leq \hat{R} + \sqrt{\frac{\ln(2/\delta)}{2m}} \tag{\label{equation-4}}
\end{equation}
$$

<details>
<summary>Proof</summary>

Setting the right-hand side of $\eqref{equation-3}$ to be equal to $\delta$ and solving for $\epsilon$ yields $\eqref{equation-4}$.
</details>
</MathBox>

<MathBox title='Learning bounds for finite hypotheses sets (inconsistent case)' boxType='theorem'>
Let $H$ be a finite hypothesis set. Then, for any $\delta > 0$, with probability at least $1 - delta$, the following inequality holds:

$$
  R(h) \leq \hat{R}(h) + \sqrt{\frac{\ln|H| + \ln(2/\delta)}{2m}},\; \forall h\in H
$$

<details>
<summary>Proof</summary>

Let $H = \Set{h_i}_{i=1}^{|H|}$. Using the union bound and applying Corollary $\ref{corollary-2}$ to each hypothesis yields

$$
\begin{align*}
  \Pr[\exists h\in H : |\hat{R}(h) - R(h)| > \epsilon] =& \Pr\left(\bigvee_{i=1}^{|H|} [|\hat{R}(h_i) - R(h_i)| > \epsilon] \right) \\
  \leq& \sum_{h\in H} \Pr[|\hat{R}(h) - R(h)| > \epsilon] \\
  \leq& 2|H|e^{-2m\epsilon^2}
\end{align*}
$$

Setting the right-hand side to be equal to $\delta$ completes the proof.
</details>
</MathBox>

This theorem implies that for a finite hypothesis set $H = \Set{h_i}_{i=1}^{|H|}$

$$
  R(h) \leq \hat{R}(h) + O\left(\sqrt{\frac{\log_2 |H|}{m}}\right)
$$

As for the inconsisten case, a larger sample size $m$ guarantees better generalization, and the bound increases logarithmically with $|H|$. In this case, however, the bound is a less favorable function of $\frac{1}{m} \log_2 |H|$; it varies as the square root of this term. This means that for a fixed $|H|$, a quadratically larger labeled sample is needed for the same consistent learning guarantee.

Note that the bound establishes a trade-off between reducing the empirical error versus controlling the size of the hypothesis set: a larger hypothesis set is penalized by the second term, but could help reduce the empirical error, which is the first term. However, for a similar empirical error, it suggest using a smaller hypothesis set. This can be viewed in light of *Occam's razor principle* commonly stated as *the simplest explanation is best* (original: plurality should not be posited without necessity).