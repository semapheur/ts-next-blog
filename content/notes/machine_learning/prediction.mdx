---
title: 'Prediction'
subject: 'Machine Learning'
showToc: true
---

# Abbreviations
- **ERM:** empirical risk minimization
- **GLM:** generalized linear model
- **i.i.d:** idenpendent and identically distributed
- **MAP:** maximum a posteriori
- **MLE:** maximum likelihood estimation
- **NLL:** negative log-loss
- **OLS:** ordinary least squares
- **RSS:** residual sum of squares
- **SVD:** singular value decomposition
- **TSS:** total sum of squares

# Predictive models

Predictive models seek to predict outputs $\mathbf{y}$ from inputs $\mathbf{x}$ using some function $f$ that is estimated from a labeled training set $S = \Set{(\mathbf{x}_n,\mathbf{y}_n) | \mathbf{x}_n \in X\subseteq\R^P,\, \mathbf{y}_n \in Y \subseteq\R^R}_{n=1}^N$ for $\mathbf{x}$. The uncertainty about the correct output for a given input can be modeled using a conditional probability model of the form $p(\mathbf{y}|f(\mathbf{x}))$. When $Y$ is a discrete set of labels, this called a *discriminative model*. When $Y = \R$, this is called a *regression model*.

Predictive models are generally distinguished between 
- *parametric models*, which have a fixed number of parameters independent of the size of the training set
- *non-parametric models*, which have a variable number of parameters that grows with the size of the training set

Non-parametric models are usually more flexible, but can be slower to use for prediction. Parametric models are usually less flexible, but are faster to use for prediction. Most non-parametric models are based on comparing a test input $\mathbf{x}$ to some or all of the stored training examples $\Set{\mathbf{x}_n}_{n=1}^N$, using som form of similarity, $s_n = K(\mathbf{x}, \mathbf{x}_n) \geq 0$, and then predicting the output using some weighted combination of the training labels, such as $\hat{\mathbf{y}} = \sum_{n=1}^N s_n \mathbf{y}_n$. Typical examples are Gaussian processes, and K-nearest neigbour models.

Most parametric models have the form $p(\mathbf{y}|\mathbf{x}) = p(\mathbf{y}|f(\mathbf{x};\boldsymbol{\theta}))$, where $f$ is a function that predicts the parameters of the output distribution. If $f$ is a linear function of $\boldsymbol{\theta}$, i.e. $f(\mathbf{x}; \boldsymbol{\theta}) = \boldsymbol{\theta}^\top \boldsymbol{\phi}(\mathbf{x})$ for some fixed feature transformation $\boldsymbol{\phi}$, the model is called a generalized linear model (GLM). If $f$ is a non-linear, but differentiable, function of $\boldsymbol{\theta}$, e.g. $f(\mathbf{x},\boldsymbol{\theta}) = \boldsymbol{\theta}_2^\top \boldsymbol{\phi}(\mathbf{x};\boldsymbol{\theta}_1)$ for some learnable function $\boldsymbol{\phi}(\mathbf{x};\boldsymbol{\theta}_1)$, it is common to represent $f$ using a neural network.

## Model fitting using MLE and MRE

The most common approach of fitting parametric predictive models is through *maximum likelihood estimation* (MLE), which amounts to solving the following optimization problem 

$$
  \hat{\boldsymbol{\theta}} = \argmax_{\boldsymbol{\theta}\in\Theta} p(S|\boldsymbol{\theta}) = \argmax_{\boldsymbol{\theta}\in\Theta} \ln p(S|\boldsymbol{\theta})
$$

If the dataset is $N$ independent, identically distributed data samples, the likelihood decomposes into a product of terms $p(S|\boldsymbol{\theta}) = \prod_{n=1}^N p(\mathbf{y}_n | \mathbf{x}_n, \boldsymbol{\theta})$. Thus, we can instead minimize the following (scaled) *negative log likelihood*:

$$
  \hat{\boldsymbol{\theta}} = \argmin_{\boldsymbol{\theta}\in\Theta} -\frac{1}{N} \sum_{n=1}^N -\ln[p(\mathbf{y}_n|\mathbf{x}_n,\boldsymbol{\theta})]
$$

We can generalize this by replacing the *log loss* $\ell_n (\boldsymbol{\theta}) = -\ln[p(\mathbf{y}_n | \mathbf{x}_n,\boldsymbol{\theta})]$, with a more general loss function to get

$$
  \hat{\boldsymbol{\theta}} = \argmin_{\boldsymbol{\theta}\in\Theta} r(\boldsymbol{\theta})
$$

where $r(\boldsymbol{\theta})$ is the *empirical risk*

$$
  r(\boldsymbol{\theta}) = \frac{1}{N} \sum_{n=1}^N \ell_n (\boldsymbol{\theta})
$$

This approach is called *empirical risk minimization* (ERM).

## Regularization and MAP

A fundamental problem of model fitting via MLE and MRE arises when all the probability mass is put on the training examples, leaving no probability for novel data in the future. This problem is known as *overfitting* and occurs when the model has enough parameters to perfectly fit the empirical distribution. However, in most cases the empirical distribution is not the same as the true distribution. Overfitting results in a model that may not generalize.

The main solution to overfitting is to use *regularization*, which means to add a penalty term to the likelihood function or empirical risk. In this case we optimize an objective function of the form

$$
  \mathcal{L}(\boldsymbol{\theta};\lambda) = \left(\frac{1}{N} \sum_{i=1}^N \ell (\boldsymbol{y}_n, \boldsymbol{\theta}; \boldsymbol{x}_n)\right) + \lambda C = r(\boldsymbol{\theta}) + \lambda C(\boldsymbol{\theta})
$$

where $\lambda\geq 0$ is the *regularization parameter* and $C(\boldsymbol{\theta})$ is some form of *complexity penalty*.

A common complexity penalty is $C(\boldsymbol{\theta}) = -\ln [p(\boldsymbol{\theta})]$, where $p(\boldsymbol{\theta})$ is the prior of $\boldsymbol{\theta}$. If $\ell$ is the log loss, the regularized objective function becomes

$$
  \mathcal{L}(\boldsymbol{\theta};\lambda) = -\frac{1}{N} \sum_{n=1}^N \ln [p(\boldsymbol{y}_n|\boldsymbol{x}_n, \boldsymbol{\theta})] - \lambda\ln[p(\boldsymbol{\theta})]
$$

By setting $\lambda = 1$ and rescaling $p(\boldsymbol{\theta})$ appropriatley, we get the objective function

$$
  \mathcal{L}(\boldsymbol{\theta};\lambda) = - \ln(p(S|\boldsymbol{\theta})) + \ln(p(\boldsymbol{\theta}))
$$

Minimizing this is equivalent to maximize the log posterior

$$
  \hat{\boldsymbol{\theta}}_{\mathrm{MAP}} = \argmax_{\boldsymbol{\theta}}[\ln(p(\boldsymbol{\theta}|S))] = \argmax_{\boldsymbol{\theta}} [\ln(p(S|\boldsymbol{\theta}) + \ln(p(\boldsymbol{\theta})))]
$$

which is known as the *maximum a posteriori estimation* (MAP).

### Weight decay ($\ell_2$ regularization)

A common regularization technique is to penalize the magnitude of the weights (regression coefficients) by introducing a zero-mean Gaussian prior $p(\mathbf{w})$. The resulting MAP estimate is given by

$$
  \hat{\mathbf{w}}_\text{MAP} = \argmin_{\mathbf{w}} \operatorname{NLL}(\mathbf{w}) + \lambda\lVert\mathbf{w}\rVert_2^2
$$

where $\lVert\mathbf{w}\rVert_2^2 = \sum_{i=1}^d w_i^2$ is the $\ell_2$ norm. This is known as $\ell_2$ regularization or weight decay. The larger the value of $\lambda$, the more the weights are penalized for deviating from the zero-mean prior, and thus the less flexible the model.

## Cross-validation

A key question when using regularization is how to choose the strength of the regularizer $\lambda$: small values focus on minimizing empirical risk, which may result in overfitting, whereas large values focus on staying close to the prior, which may result in underfitting.

A common method for choosing $\lambda$ is to partition the data $S$ into two disjoint sets, the training set $S_\text{train}$ and a validation set $S_\text{valid}$. We fit the model on $S_\text{train}$ and evaluate its performance on $S_\text{valid}$ for each $\lambda$. We then select the $\lambda$ that results in the best validation performance.

To explain this more rigorously, let us define the regularized empirical risk

$$
  R_\lambda (\boldsymbol{\theta}, S) = \frac{1}{|S|} \sum_{(\mathbf{x},\mathbf{y})\in S} \ell(\boldsymbol{y}, f(\boldsymbol{x};\boldsymbol{\theta})) + \lambda C(\boldsymbol{\theta})
$$

For each $\lambda$, we compute the parameter estimate

$$
  \hat{\boldsymbol{\theta}}_\lambda (S_\text{train}) = \argmin_{\boldsymbol{\theta}} R_\lambda (\boldsymbol{\theta}, D_\text{train})
$$

We then compute the validation risk

$$
  R_\lambda^\text{val} := R_0 (\hat{\boldsymbol{\theta}}_\lambda (S_\text{train}), S_\text{valid})
$$

This is an estimate of the *population risk*, which is the expected loss under the true distribution $p^* (\boldsymbol{x},\boldsymbol{y})$. Finally, we pick

$$
  \lambda^* = \argmin_{\lambda\in S} R_\lambda^\text{val}
$$

After picking $\lambda^*$, we can refit the model to the entire dataset, $S = S_\text{train} \cup S_\text{valid}$, to get

$$
  \hat{\boldsymbol{\theta}}^* = \argmin_{\boldsymbol{\theta}} R_{\lambda^*} (\boldsymbol{\theta}, S)
$$

A small size of the training set can result in an unreliable estimate of the model parameters. A solution to this is *cross-validation* (CV) by splitting the training data into $K$ *folds*. For each fold $k\in\Set{1,\dots,K}$, we train on all the folds but the $k$th, and test on the $k$th. Formally, we define the *cross-validated risk*

$$
  R_\lambda^\text{cv} := \frac{1}{K} \sum_{k=1}^K R_0 (\hat{\boldsymbol{\theta}}_\lambda (S_{-k}), S_k)
$$

where $S_k$ is the data in the $k$th fold, and $S_{-k}$ is all the other data (complement of $S_k$). If we set $K = N$, we get a method known as *leave-one-out-cross-validation*, since we always train on $N-1$ items and validate on the remaining one.

We can use the CV estimate as an objective inside of an optimization routine to pick the optimal hyperparameter

$$
  \hat{\lambda} = \argmin_\lambda R_\lambda^\text{cv}
$$

Finally, we combine all the available data and re-estimate the model parameters using 

$$
  \hat{\boldsymbol{\theta}} = \argmin_{\boldsymbol{\theta}} R_{\hat{\lambda}} R_{\hat{\lambda}} (\boldsymbol{\theta}, S)
$$

# Generalized linear models

A generalized linear model (GLM) is a conditional version of an exponential family distribution with the form

$$
  p(y_n | \mathbf{x}_n, \mathbf{w}, \sigma^2) = \exp\left(\frac{y_n \eta_n - A(\eta_n)}{\sigma^2} + \ln(h(y_n, \sigma^2))\right)
$$

where 
- $\eta_n = \mathbf{x}^\top \mathbf{x}_n$ is the natural parameter for the distributions
- $A(\eta_n)$ is the log normalizer
- $T(y) = y$ is the sufficient statistic
- $\sigma^2$ is the dispersion term

The mean and variance of the response variable are

$$
\begin{align*}
  \mu_n :=& \mathbb{E}[y_n | \mathbf{x}_n, \mathbf{w}, \sigma^2] = A'(\eta_n) := \ell^{-1} (\eta_n) \\
  \operatorname{var}(y_n | \mathbf{x}_n, \mathbf{w}, \sigma^2) =& A'' (\eta_n) \sigma^2 
\end{align*}
$$

The mapping from the linear inputs to the mean of the outputs is denoted $\mu_n = \ell^{-1} (\eta_n)$, where the function $\ell$ is called the *link function*, and $\ell^{-1}$ is known as the *mean function*. This relationship is usually written as

$$
  \ell(\mu_n) = \eta_n = \mathbf{w}^\top \mathbf{x}_n
$$

## Linear regression

Linear regression is the simplest case of a GLM and is of the form

$$
\begin{equation}
  p(y|\mathbf{x},\boldsymbol{\theta}) = \mathcal{N}(y|w_0 + \mathbf{w}^\top \mathbf{x}, \sigma^2) \tag{\label{equation-1}}
\end{equation}
$$

where $\boldsymbol{\theta} = (w_0, \mathbf{w}, \sigma^2)$ are all the parameters of the models. The vector of parameters $\mathbf{w}\in\R^P$ are known as the *weights* or *regression coefficients*. Each coefficient $w_p$ for $p=1,\dots,P$ specifies the change in the output we expect if we change the corresponing input feature $\mathbf{x}_n$ by one unit. The term $w_0$ is the *offset* or *bias* term, and specifies the output value if all the inputs are $0$. This captures the unconditional mean of the response, $w_0 = \mathbb{E}(y)$, and acts as a baseline.

If the input is one-dimensional, i.e. $X\subseteq\R$, the model has the form $f(\mathbf{x};\mathbf{w}) = ax + b$, where $b = w_0$ is the intercept, and $a = w_1$ is the slope. This is called *simple linear regression*. If the input is multi-dimensional, $\mathbf{x}\in\R^p$ where $p > 1$, the method is called *multiple linear regression*. If the output is also multi-dimensional, i.e. $Y \subset\R^R$ for $R > 1$, it is called *multivariate linear regression*, taking the form

$$
  p(\mathbf{y}|\mathbf{x}, \mathbf{W}) = \prod_{j=1}^R \mathcal{N}(y_j | \mathbf{w}_j^\top \mathbf{x}, \sigma_j^2)
$$

Linear regression can be used to fit sample data to other than a linear function by applying a nonlinear transformation to the input featurese, by replacing $\mathbf{x}$ with $\boldsymbol{\phi}(\mathbf{x})$ to get 

$$
  p(y|\mathbf{x},\boldsymbol{\theta}) = \mathcal{N}(y|\mathbf{w}^\top \boldsymbol{\phi}(\mathbf{x}), \sigma^2)
$$

As long as the parameters of the *feature extractor* $\boldsymbol{\phi}$ are fixed, the model remains linear in the parameters, even if it is not linear in the outputs.

### Ordinary least squares

From $\eqref{equation-1}$, we can derive the negative log likelihood of the data as follows

$$
\begin{align*}
  \operatorname{NLL}(\mathbf{w},\sigma^2) =& \sum_{n=1}^N \ln\left[\left(\frac{1}{2\pi\sigma^2} \right)^{1/2} \exp\left(-\frac{1}{2\sigma^2} (y_n - \mathbf{w}^\top \mathbf{x}_n)^2 \right) \right] \\
  =& \frac{1}{2\sigma^2} \sum_{n=1}^N (y_n - \hat{y})^2 + \frac{N}{2}\ln(2\pi\sigma^2)
\end{align*}
$$

where we have defined the predicted response $\hat{y}_n := \mathbf{w}^\top \mathbf{x}_n$. The MLE is the point where

$$
  \nabla_{\mathbf{w},\sigma} \operatorname{NLL}(\mathbf{w},\sigma^2) = 0
$$

and

$$
\begin{align*}
  \operatorname{RSS}(\mathbf{w}) =& \frac{1}{2} \sum_{n=1}^N (y_n - \mathbf{w}^\top \mathbf{x}_n)^2 = \frac{1}{2}\lVert \mathbf{Xw} - \mathbf{y}\rVert_2^2 \\
  =& \frac{1}{2}(\mathbf{Xw} - \mathbf{w})^\top (\mathbf{Xw} - \mathbf{y})
\end{align*}
$$

Using the matrix identity

$$
  \frac{\partial}{\partial x}(\mathbf{x}^\top \mathbf{Ax}) = (\mathbf{A} + \mathbf{A}^\top)\mathbf{x}
$$

we can show that the gradient is given by

$$
  \nabla_\mathbf{w} \operatorname{RSS}(\mathbf{w}) = \mathbf{X}^\top \mathbf{Xw} - \mathbf{X}^\top \mathbf{y}
$$

Solving for $\nabla_\mathbf{w} \operatorname{RSS}(\mathbf{w}) = 0$ gives

$$
  \mathbf{X}^\top \mathbf{Xw} = \mathbf{X}^\top \mathbf{y}
$$

which are known as the *normal equations*. The corresponding solution $\hat{\mathbf{w}}$ is the *ordinary least squares* solution given by

$$
  \hat{\mathbf{w}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}
$$

Identifying $\mathbf{X}^+ = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}$ as the left pseudo inverse of the matrix $\mathbf{X}$, the normal equation can be written as

$$
  \hat{\mathbf{w}} = \mathbf{X}^+ \mathbf{y}
$$

The Hessian of $\operatorname{RSS}(\mathbf{w})$ is given by

$$
  H(\mathbf{w}) = \frac{\partial^2}{\partial\mathbf{w}^2}\operatorname{RSS}(\mathbf{w}) = \mathbf{X}^\top \mathbf{X}
$$

If $\mathbf{X}$ is full rank, $H$ is positive definite since for any $\mathbf{v} \neq 0$, we have

$$
  \mathbf{v}^\top (\mathbf{X}^\top \mathbf{X})\mathbf{v} = (\mathbf{Xv})^\top (\mathbf{Xv}) = \lVert\mathbf{Xv}\rVert^2 > 0
$$

#### Solving using QR decomposition

If $\mathbf{X}$ is tall and skinny, i.e. $N \gg P$, it can be quicker to use QR decomposition to solve the normal equation. For $\mathbf{X} = \mathbf{QR}$, where $\mathbf{Q}^\top \mathbf{Q} = \mathbf{I}$, we can write the system of linear equations $\mathbf{Xw} = \mathbf{y}$ as

$$
\begin{align*}
  (\mathbf{QR})\mathbf{w} =& \mathbf{y} \\
  \mathbf{Q}^\top \mathbf{QRw} =& \mathbf{Q}^\top \mathbf{y} \\
  \mathbf{w} =& \mathbf{R}^{-1} (\mathbf{Q}^\top \mathbf{y})
\end{align*}
$$

Since $\mathbf{R}$ is upper triangular, we can solve this last set of equations using backsubsitution, thus avoiding matrix inversion. The time complexity of this algorithm is $\mathcal{O}((N+D)D^2)$.

#### Solving using SVD

Let $\mathbf{X}$ have the singular value decomposition (SVD) $\mathbf{X} = \mathbf{USV}^\top$, where
- $\mathbf{V}^\top \mathbf{V} = \mathbf{I}_N$
- $\mathbf{UU}^\top = \mathbf{U}^\top \mathbf{U} = \mathbf{I}_N$
- $\mathbf{S}$ is an $N\times P$ matrix, whose upper left block is a $\operatorname{rk}(\mathbf{X}) \times\operatorname{rk}(\mathbf{X})$-dimensional diagonal matrix with the singular values on the diagonal. The remaining block is zero.

Inserting the SVD in the MLE for $\mathbf{w}$ gives

$$
\begin{align*}
  \hat{\mathbf{w}} =& (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y} \\
  =& (\mathbf{VS}^\top \mathbf{U}^\top \mathbf{USV}^\top)^{-1} \mathbf{VD}^\top \mathbf{U}^\top \mathbf{y} \\
  =& (\mathbf{VS}^\top \mathbf{SV}^\top)^{-1} \mathbf{VS}^\top \mathbf{U}^\top \mathbf{y} \\
  =& \mathbf{V}(\mathbf{S}^\top \mathbf{S})^{-1} \mathbf{V}^\top \mathbf{VD}^\top \mathbf{U}^\top \mathbf{y} \\
  =& \mathbf{V}(\mathbf{S}^\top \mathbf{S})^{-1} \mathbf{D}^\top \mathbf{U}^\top \mathbf{y} 
\end{align*}
$$

The block structure of the design matrix implies that $\mathbf{S}^+ = (\mathbf{S}^\top \mathbf{S})^{-1} \mathbf{S}$ is a $p\times n$-matrix with the reciprocal of the non-zero singular values on the diagonal of the left $p\times p$ dimensional upper left block.

### Ridge regression ($\ell_2$ regularization)

To avoid overfitting, maximum likelihood estimation can be modified by introducing regularization. One approach, known as *ridge regression*, is to introduce an $\ell_2$ regularization with a zero-mean Gaussian prior on the weights, $p(\mathbf{w}) = \mathcal{N}(\mathbf{w}|\mathbf{0}, \lambda^{-1} \mathbf{I})$. The MAP estimation becomes

$$
\begin{align*}
  \hat{\mathbf{w}}_\text{MAP} =& \frac{1}{2\sigma^2}(\mathbf{y} - \mathbf{Xw})^\top (\mathbf{y} - \mathbf{Xw}) + \frac{1}{2\tau^2} \mathbf{w}^\top \mathbf{w} \\
  =& \argmin_{\mathbf{w}} \operatorname{RSS}(\mathbf{w}) + \lambda\lVert\mathbf{w}\rVert_2^2
\end{align*}
$$

where $\lambda = \sigma^2 / \tau^2$ is proportional to the strength of the prior, and

$$
  \lVert\mathbf{w}\rVert_2 = \left(\sum_{i=1}^d |w_i|^2 \right)^{1/2} = \sqrt{\mathbf{w}^\top \mathbf{w}}
$$

Note that we do not penalize th offset term $w_0$, since that only affects the global mean of the output, and does not contribute to overfitting.

Taking the gradient of the MAP estimation with respect to $\mathbf{w}$ gives

$$
\begin{align*}
  \nabla_\mathbf{w} (2\operatorname{RSS}(\mathbf{w}) + \lambda\lVert\mathbf{w}\rVert_2^2) =& \nabla_\mathbf{w} \left((\mathbf{y} - \mathbf{Xw})^\top (\mathbf{y} - \mathbf{Xw}) + \lambda\lVert\mathbf{w}\rVert_2^2 \right) \\
  =& 2(\mathbf{X}^\top \mathbf{Xw} - \mathbf{X}^\top \mathbf{y} + \lambda\mathbf{w}) \\
\end{align*}
$$

Solving for $\nabla_\mathbf{w} (\operatorname{RSS}(\mathbf{w}) + \lambda\lVert\mathbf{w}\rVert_2^2) = 0$ gives

$$
  \hat{\mathbf{w}}_\text{MAP} = (\mathbf{X}^\top \mathbf{X} + \lambda \mathbf{I}_P)^{-1} \mathbf{X}^\top \mathbf{y}
$$

#### Solving using QR decomposition

The MAP estimation can be converted to a standard least squares problem, to which we can apply QR decomposition. We assume that the prior has the form $p(\mathbf{w}) = \mathcal{N}(\mathbf{0},\boldsymbol{\Lambda}^{-1})$, where $\boldsymbol{\Lambda} = (1/\tau^2)\mathbf{I}$ is the precision matrix. We can emulate this prior by adding "virtual data" to the training set to get

$$
  \tilde{\mathbf{X}} = \begin{bmatrix} \mathbf{X}/\sigma \\ \sqrt{\boldsymbol{\Lambda}} \end{bmatrix},\quad \tilde{\mathbf{y}} = \begin{bmatrix} \mathbf{y}/\sigma \\ \mathbf{0}_{d\times 1} \end{bmatrix}
$$

where $\boldsymbol{\Lambda} = \sqrt{\boldsymbol{\Lambda}}\sqrt{\boldsymbol{\Lambda}}^\top$ is a Cholesky decomposition of $\boldsymbol{\Lambda}$. We see that $\tilde{\mathbf{X}}$ is $(N+d) \times d$, where the extra rows represent pseudo-data from the prior.

Calculating the residual sum of squares on this expanded data gives

$$
\begin{align*}
  f(\mathbf{w}) =& (\tilde{\mathbf{y}} - \tilde{\mathbf{X}}\mathbf{w})^\top (\tilde{\mathbf{y}} - \tilde{\mathbf{X}}\mathbf{w}) \\
  =& \left(\begin{bmatrix} \mathbf{y}/\sigma \\ \mathbf{0} \end{bmatrix} - \begin{bmatrix} \mathbf{X}/\sigma \\ \sqrt{\boldsymbol{\Lambda}} \end{bmatrix}\mathbf{w} \right)^\top \left(\begin{bmatrix} \mathbf{y}/\sigma \\ \mathbf{0} \end{bmatrix} - \begin{bmatrix} \mathbf{X}/\sigma \\ \sqrt{\boldsymbol{\Lambda}} \end{bmatrix}\mathbf{w} \right) \\
  =& \begin{bmatrix} \frac{1}{\sigma}(\mathbf{y} - \mathbf{Xw}) \\ -\sqrt{\boldsymbol{\Lambda}\mathbf{w}} \end{bmatrix}^\top \begin{bmatrix} \frac{1}{\sigma}(\mathbf{y} - \mathbf{Xw}) \\ -\sqrt{\boldsymbol{\Lambda}\mathbf{w}} \end{bmatrix} \\
  =& \frac{1}{\sigma^2}(\mathbf{y} - \mathbf{Xw})^\top (\mathbf{y} - \mathbf{Xw}) + (\sqrt{\boldsymbol{\Lambda}}\mathbf{w})^\top (\sqrt{\boldsymbol{\Lambda}}\mathbf{w}) \\
  =& \frac{1}{\sigma^2} (\mathbf{y} - \mathbf{Xw})^\top (\mathbf{y} - \mathbf{Xw}) + \mathbf{w}^\top \boldsymbol{\Lambda}\mathbf{w}
\end{align*}
$$

Hence, the MAP estimate is given by

$$
  \hat{\mathbf{w}}_\text{MAP} = (\tilde{\mathbf{X}}^\top \tilde{\mathbf{X}})^{-1} \tilde{\mathbf{X}}^\top \tilde{\mathbf{y}}
$$

which can be solved using standard OLS methods.

#### Solving using singular value decomposition

If $D > N$, it is faster to use singular value decomposition (SVD) than QR decomposition. Let $\mathbf{X} = \mathbf{USV}^\top$ be the SVD of $\mathbf{X}$, where
- $\mathbf{V}^\top \mathbf{V} = \mathbf{I}_N$
- $\mathbf{UU}^\top = \mathbf{U}^\top \mathbf{U} = \mathbf{I}_N$
- $\mathbf{S}$ is a diagonal $N\times N$ matrix

Inserting this into the MAP estimate gives

$$
\begin{align*}
  \hat{\mathbf{w}}_\text{MAP} =& \mathbf{V}(\mathbf{US}^\top \mathbf{USV}^\top + \lambda \mathbf{I}_P)^{-1} \mathbf{V}\mathbf{S}^\top \mathbf{U}^\top \mathbf{y} \\
  =& (\mathbf{VS}^\top \mathbf{DV}^\top + \lambda \mathbf{VV}^\top)^{-1} \mathbf{VD}^\top \mathbf{U}^\top \mathbf{y} \\
  =& \mathbf{V} (\mathbf{S}^\top \mathbf{S} + \lambda \mathbf{I}_P)^{-1} \mathbf{V}^\top \mathbf{VS}^\top \mathbf{U}^\top \mathbf{y} \\
  =& \mathbf{V}(\mathbf{S}^\top \mathbf{S} + \lambda \mathbf{I}_P)^{-1} \mathbf{S}^\top \mathbf{U}^\top \mathbf{y} 
\end{align*}
$$

in other words, we can replace the $P$-dimensional vectors $\mathbf{x}_i$ with $N$-dimensional vectors $\mathbf{r}_i$, and perform our penalized fit as before. The time complexity for this algorithm is $\mathcal{O}(PN^2)$, which is less than $\mathcal{O}(D^3)$ if $P > N$.

### LASSO regression ($\ell_1$ regularization)

An $\ell_1$ regularization can be applied to the maximum likelihood estimation by introducing a zero-mean Laplacian prior on the weights,

$$
  p(\mathbf{w}|\lambda) = \prod_{p=1}^P \operatorname{Laplace}(w_p|0, 1/\lambda) \propto \prod_{p=1}^P e^{-\lambda|w_p|}
$$

where $\lambda$ is the sparsity parameter. To perform a MAP estimation of a linear regression model with this prior, we just have to minimize the following objective function

$$
  \operatorname{PNN}(\mathbf{w}) = -\ln(p(S|\mathbf{w})) - \ln(p(\mathbf{w}|\lambda)) = \operatorname{NLL}(\mathbf{w}) + \lambda\lVert\mathbf{w}\rVert_1 = \lVert \mathbf{Xw} - \mathbf{y}\rVert_2^2 + \lambda\lVert\mathbf{w}\rVert_1
$$

where $\lVert\mathbf{w}\rVert = \sum_{p=1}^P |w_p|$ is the $\ell_1$ norm of $\mathbf{w}$. This method is called *LASSO regression*, which stands for "least absolute shrinkage and selection operator".

#### Thresholding

The partial derivative of $\operatorname{NLL}(\mathbf{w})$ is given by

$$
\begin{align*}
  \frac{\partial}{\partial w_p} \operatorname{NLL}(\mathbf{w}) =& a_p w_p - c_p \\
  a_p =& \sum_{n=1}^N x_{np}^2 \\
  c_p =& \sum_{n=1}^N x_{np} (y_n - \mathbf{w}_{-p}^\top \mathbf{x}_{n,-p})
\end{align*}
$$

where $\mathbf{w}_{-p}$ is $\mathbf{w}$ without the $p$th component, and similarly $\mathbf{x}_{n,-p}$ is $\mathbf{x}_n$ withouth the $p$th component. We see that $c_p$ is proportional to the correlation between $p$th column of features $\mathbf{x}_{:,p}$ and the residual error obtained by predicting using all the other features, $\mathbf{r}_{-p} = \mathbf{y} - \mathbf{X}_{:,-p} \mathbf{w}_{-p}$. Hence the magnitude of $c_p$ is an indication of how relevant feature $p$ is for predictin $\mathbf{y}$, relative to the other features and the current parameters. Setting the gradient to $0$ gives the optimal update for $w_p$, keeping all other weights fixed

$$
  w_p = c_p / a_p = \frac{\mathbf{x}_{:,p}^\top \mathbf{r}_{-p}}{\lVert\mathbf{x}_{:,p}\rVert_2^2}
$$

The corresponding new prediction for $\mathbf{r}_{-p}$ becomes $\hat{\mathbf{r}}_{-p} = w_p \mathbf{x}_{:,p}$, which is the orthogonal projection of the residual onto the column vector $\mathbf{x}_{:,p}$.

Unfortunately, the $\lVert\mathbf{w}\rVert_1$ term is not differentiable whenever $w_p = 0$. However, we can compute a subgradient at this point

$$
\begin{align*}
  \frac{\partial}{\partial w_p} \operatorname{NLL}(\mathbf{w}) + \lambda\lVert\mathbf{w}\rVert_1 =& (a_p w_p - c_p) + \lambda \partial_{w_p} \lVert\mathbf{w}\rVert_1 \\
  =& \begin{cases}
    a_p w_p - c_p - \lambda, \quad& w_p < 0 \\
    [-c_p - \lambda, -c_p + \lambda] \quad& w_p = 0 \\
    a_p w_p - c_p + \lambda \quad& w_p > 0
  \end{cases}
\end{align*}
$$

Depending on the value of $c_p$, the solutino to $\partial_{w_p} \operatorname{NLL}(\mathbf{w}) + \lambda\lVert\mathbf{w}\rVert_1 = 0$ can occur at $3$ different values of $w_p$

1. If $c_p < -\lambda$, the feature is strongly negatively correlated with the residual, such that the subgradient is zero at $\hat{w}_p = (c_p + \lambda)/a_p < 0$
2. If $c_p \in [-\lambda, \lambda]$, the feature is only weakly correlated with the residual, such that the subgradient is zero at $\hat{w}_p = 0$
3. If $c_p > \lambda$, the feature is strongly positively correlated with the residual, such that the subgradient is zero at $\hat{w}_p = (c_p - \lambda)/a_p > 0$

In summary, we have

$$
  \hat{w}_p (c_p) = \begin{cases}
    (c_p + \lambda)/a_p,\quad& c_p < -\lambda \\
    0, \quad& c_p \in [-\lambda, \lambda] \\
    (c_p - \lambda)/a_p,\quad& c_p > \lambda
  \end{cases}
$$

We can write this as

$$
  \hat{w}_p = \operatorname{SoftThreshold}\left(\frac{c_p}{a_p}, \frac{\lambda}{a_p} \right)
$$

where

$$
  \operatorname{SoftThreshold}(x,\delta) := \operatorname{sign}(x) (|x| - \delta)_+
$$

and $x_+ = \max(x, 0)$ is the positive part of $x$. This is called *soft thresholding*. In contrast, *hard thresholding* sets values of $w_p$ to $0$ if $-\lambda \leq c_p \leq \lambda$, but it does not shrink the values of $w_p$ outside this interval. The slope of the soft thresholding line does does not coincide with the diagonal, which means that even large coefficients are shrunk towards zero. Consequently, LASSO regression is a biased estimator.

# Logistic regression

Logistic regression is a discriminative classification model $p(y|\mathbf{x};\boldsymbol{\theta})$, where $\mathbf{x}\in\R^p$ is a fixed-dimensional input, $y\in\Set{1,\dots,C}$ is the class label, $\boldsymbol{\theta}$ are the parameters. If $C = 2$ this is known as *binary logistic regression*, and if $C > 2$, it is known as *multinomial logistic regression*.

## Binary logistic regression

Binary logistic regression uses class labels $y\in\set{0,1}$ and corresponds to the model

$$
\begin{equation}
  p(y|\mathbf{x};\boldsymbol{\theta}) = \operatorname{Bernoulli}(y|\sigma(\mathbf{w}^\top \mathbf{x} + b)) \tag{\label{equation-2}}
\end{equation}
$$

where $\operatorname{Bernoulli}(y|\theta) = \theta^y (1 - \theta)^{-1}$ is the Bernoulli distribution and $\sigma$ is the sigmoid function

$$
  \sigma(\lambda) = \frac{1}{1 + e^{-\lambda}}
$$

Pluging the sigmoid function into $\eqref{equation-2}$ with $a = \mathbf{w}^\top \mathbf{x} + b$ we get

$$
\begin{align*}
  p(y = 1|\mathbf{x};\boldsymbol{\theta}) =& \frac{1}{1 + e^{-a}} = \sigma(a) \\
  p(y = 0|\mathbf{x};\boldsymbol{\theta}) =& 1 - \frac{1}{e + e^{-a}} = \frac{1}{1 + e^a} = \sigma(-a)
\end{align*}
$$

Note that for $p = p(y = 1|\mathbf{x};\boldsymbol{\theta})$

$$
  \ln\left(\frac{1}{1 - p}\right) = \ln\left(\frac{e^a}{1 + e^a} \frac{1 + e^a}{1}\right) = \ln(e^a) = a
$$

This shows that $a$ is equal to the *log-odds* $\ln[1/(1 - p)]$. The inverse of the sigmoid function is called the *logit function*, defined by

$$
  \operatorname{logit}(p) = \sigma^{-1} (p) := \ln\left(\frac{p}{1 - p}\right)
$$

Sometimes we choose to use the labels $\tilde{y}\in\set{\pm 1}$ instead of $y\in\set{0, 1}$. We can compute the probability of these alternative labels using

$$
  p(\tilde{y}|\mathbf{x};\boldsymbol{\theta}) = \sigma(\tilde{y}a)
$$

since $\sigma(-a) = 1 - \sigma(a)$.

### Linear classifier

The sigmoid gives the probability that the class label is $y = 1$. If the loss for misclassifying each class is the same, then the optimal decision rule is to predict $y = 1$ if and only if class $1$ is more likely than class $0$. Thus

$$
\begin{align*}
  f(\mathbf{x}) =& \mathbf{1}[p(y = 1|\mathbf{x}) > p(y = 0|\mathbf{x})] \\
  =& \mathbf{1}\left(\ln\frac{p(y = 1)}{}\right) = \mathbf{1}_{a > 0}
\end{align*}
$$

where $a = \mathbf{w}^\top \mathbf{x} + b$. Writing the prediction function as

$$
  f(\mathbf{x};\boldsymbol{\theta}) = b + \mathbf{w}^\top \mathbf{x} = b + \langle\mathbf{w},\mathbf{x}\rangle
$$

we see that $f$ defines a linear hyperplane with normal vector $\mathbf{w} \in\R^p$ and an offset $b\in\R$ from the origin. This linear plane is known as a *decision boundary* and splits the output space into two *half-spaces*. If we can perfectly separate the training examples by such a linear boundary without any classification errors, we say that the data is *linearly separable*.

In general, there will be uncertainty about the correct class label, so we need to predict a probability distribution over labels, and not just decide which side of the decision boundary we are on. The normal $\mathbf{w}$ defines the orientation of the decision boundary, while its magnitude $\lVert\mathbf{w}\rVert$ controls the steepness of the sigmoid, and hence the confidence of the predictions.

### Nonlinear classifiers

A logistical regression problem can be made linearly separable by preprocessing the inputs with a suitable transformation $\phi(\mathbf{x})$. The transformed prediction function takes the form

$$
  f(\phi(\mathbf{x});\boldsymbol{\theta}) = b + \mathbf{w}^\top \phi(\mathbf{x})
$$

which remains linear in $\mathbf{w}$.

<MathBox title='Quadratic decision boundary' boxType='example'>
Suppose we apply a transformation $\phi(x_1,x_2) = [1, x_1^2, x_2^2]$ and let $\mathbf{w} = [-r^2, 1, 1]$. Then $\mathbf{w}^\top \phi(\mathbf{x}) = x_1^2 + x_2^2 - r^2$. In this case the decision boundary $f(\mathbf{x}) = b + \mathbf{w}^\top \phi(\mathbf{x})$ defines a circle with radius $r$.
</MathBox>

### Maximum likelihood estimation

Suppose we have a sample $\set{(\mathbf{x}_n, y_n) | y_n\in\set{0,1}}_{n=1}^N$ of $N$ binary classification. Assuming that the bias term $b$ is absorbed into the weight vector $\mathbf{w}\in\R^p$, the scaled negative log likelihood is given by

$$
\begin{align*}
  -\ln(\mathbf{w}) =& -\frac{1}{N} \ln P(\mathcal{D}|\mathbf{w}) = -\frac{1}{N}\ln\prod_{n=1}^N \operatorname{Bernoulli}(y_n | \mu_n) \\
  =& -\frac{1}{N} \sum_{n=1}^N \ln[\mu_n^{y_n} (1 - \mu_n)^{1 - y_n}] \\
  =& -\frac{1}{N} \sum_{n=1}^N [y_n \ln\mu_n + (1 - y_n) \ln(1 - \mu_n)] \\
  =& \frac{1}{N} \sum_{n=1}^N H_{ce} (y_n, \mu_n)
\end{align*}
$$

where $\mu_n = \sigma(a_n)$ is the probability of class $1$, $a_n = \mathbf{w}^\top \mathbf{x}_n$ is the logit, and $H_{ce} (y_n, \mu_n)$ is the binary cross entropy defined by

$$
  H_{ce}(p,q) = -\left[p \ln q + (1 - p) \ln(1 - q) \right]
$$

If we use $\tilde{y}_n \in\set{\pm 1}$ instead of $y\in\set{0,1}$, we can rewrite this as follows

$$
\begin{align*}
  -\ln(\mathbf{w}) =& -\frac{1}{N} \sum_{n=1}^N [\mathbf{1}(\tilde{y}_n = 1) \ln(\sigma(a_n)) + \mathbf{1}(\tilde{y}_n = -1)\ln(\sigma(-a_n))] \\
  =& -\frac{1}{N}\sum_{n=1}^N \ln(\sigma(\tilde{y}_n a_n)) \\
  =& \frac{1}{N} \sum_{n=1}^N \ln(1 + e^{-\tilde{y}_n a_n})
\end{align*}
$$

To find the maximum likelihood estimation we must solve $\nabla_\mathbf{w} (-\ln(\mathbf{w})) = \mathbf{0}$. To start note that

$$
  \frac{\d\mu_n}{\d a_n} = \sigma(a_n)(1 - \sigma(a_n))
$$

Thus, by the chain rule we have

$$
  \frac{\partial}{\partial w_i}\mu_n = \frac{\partial}{\partial} \sigma(\mathbf{w}^\top \mathbf{x}_n) = \frac{\partial}{\partial a_n}\sigma(a_n) \frac{\partial a_n}{\partial w_i} = \mu_n (1 - \mu_n) x_{ni}
$$

The gradient for the bias term can be derived in the same way, by using the input $x_{n0} = 1$ in the above equation. Combining the results we get

$$
  \nabla_\mathbf{w} \ln(\mathbf{\mu_n}) = \frac{1}{\mu_n} \nabla_\mathbf{w} \mu_n = (1 - \mu_n)\mathbf{x}_n
$$

Similarly,

$$
  \nabla_\mathbf{w} \ln(1 - \mu_n) = -\frac{\mu_n (1 - \mu_n)\mathbf{x}_n}{1 - \mu_n} = -\mu_n \mathbf{x}_n
$$

Thus, the gradient of the negative log-likelihood is given by

$$
\begin{equation}
\begin{split}
  \nabla_\mathbf{w} (-\ln(\mathbf{w})) =& -\frac{1}{N}\sum_{n=1}^N [y_n (1 - \mu_n)\mathbf{x}_n - (1 - y_n)\mu_n \mathbf{x}_n] \\
  =& -\frac{1}{N} \sum_{n=1}^N [y_n \mathbf{x}_n - y_n \mathbf{x}_n \mu_n - \mathbf{x}_n \mu_n + y_n \mathbf{x}_n \mu_n] \\
  =& \frac{1}{N} \sum_{n=1}^N (\mu_n - y_n)\mathbf{x}_n
\end{split}\tag{\label{equation-3}}
\end{equation}
$$

If we interpret $\epsilon_n = \mu_n - y_n$ as an error signal, the gradient weights each input $\mathbf{x}_n$ by its error, and then averages the result. In matrix form, the gradient can be written as

$$
  \nabla_\mathbf{w} (-\ln (\mathbf{w})) = \frac{1}{N} (\mathbf{1}_N^T \operatorname{diag}(\boldsymbol{\mu} - y)\mathbf{X})^T
$$

where $\mathbf{X}$ is the $N\times p$ design matrix containin the examples $\mathbf{x}_n$ in each row.

We can show that the Hessian is given by

$$
  \mathbf{H}(\mathbf{w}) = \nabla_\mathbf{w} \nabla_\mathbf{w}^\top (-\ln(\mathbf{w})) = \sum_{n=1}^N (\mu_n (1 - \mu_n)\mathbf{x}_n)\mathbf{x}_n^\top = \frac{1}{D} \mathbf{X}^\top \mathbf{SX}
$$

where $\mathbf{S} := \operatorname{diag}(\mu_n (1 - \mu_n))_{n=1}^N$. We see that $\mathbf{H}$ is positive definite, since for any nonzero vector $\mathbf{v}\in\R^p$ we have

$$
  \mathbf{v}^\top \mathbf{X}^\top \mathbf{SXv} = (\mathbf{v}^\top \mathbf{X}^\top \mathbf{S}^{1/2})(\mathbf{S}^{1/2} \mathbf{Xv}) = \lVert \mathbf{v}^\top \mathbf{X}^\top \mathbf{S}^{1/2} \rVert_2^2 > 0
$$

This follows since $\mu_n > 0$ for all $n$, because of the use of the sigmoid function. Consequently the negative log-likelihood function is strictly convex. However, in practice, values of $\mu_n$ which are close to $0$ and $1$ might cause the Hessian to be close to singular. This can be avoided by using $\ell_2$ regularization.

### Stochastic gradient descent update (SGD)

Using stochastic gradient descent to minimize the negative log likelihood with minibatch of size $1$ gives the following update

$$
\begin{equation}
  \mathbf{w}_{t+1} = \mathbf{w}_t - \eta_t \nabla_\mathbf{w} (-\ln(\mathbf{w})) = \mathbf{w}_t - \eta_t (\mu_n - y_n)\mathbf{x}_n \tag{\label{equation-4}}
\end{equation}
$$

where we replaced the average over all $N$ examples in the gradient $\eqref{equation-3}$ with a single stochastically chose sample $n$.

### Perceptron algorithm

A perceptron is a deterministic binary classifier of the following form

$$
  f(\mathbf{x}_n; \boldsymbol{\theta}) = \mathbf{1}(\mathbf{w}^\top \mathbf{x}_n + b > 0)
$$

This can be seen to be a limiting case of a binary logistic regression classifier, in which the sigmoid function $\sigma(a)$ is replace by the Heaviside step function $H(a) := \mathbf{1}_{a > 0}$. Since the Heaviside function is not differentiable, we cannot use gradient-based optimization methods to fit this model. However, Rosenblatt proposed the *perceptron learning algorithm* instead. The basic idea ist to start with random weights, and then iteratively update them whenever the model makes a predicition mistake. The weights are updated via

$$
\begin{equation}
  \mathbf{w}_{t + 1} = \mathbf{w}_t - \eta_t (\hat{y}_n - y_n)\mathbf{x}_n \tag{\label{equation-5}}
\end{equation}
$$

The perceptron update rule can be interpreted intuitively: if the prediction is correct, no change is made, otherwise we move the weights in a direction so as to make the correct answer more likely.

Comparing $\eqref{equation-5}$ to $\eqref{equation-4}$, we see that the perceptron update rule is equivalent to the SGD update rule for binary logistic regression using the approximation where we replace the soft probabilities $\mu_n = p(y_n = 1|\mathbf{x}_n)$ with hard labels $\hat{y}_n = f(\mathbf{x}_n)$. The advantage of the perceptron method is that we do not need to compute probabilities, which can be useful when the label space is very large. The disadvantage is that the method will only converge when the data is linearly separable, whereas SGD will always converge to the globaly optimal MLE, even if the data is not linearly separable. 

## Multinomial classification

The categorical distribution is a discrete probability distribution with one parameter per class

$$
  \operatorname{Cat}(y|\boldsymbol{\theta}) := \prod_{c=1}^C \theta_c^{\mathbf{1}_{y = c}}
$$

In other words, $p(y = c|\boldsymbol{\theta}) = \theta_c$. Note that the parameters are constrained so that $\theta_c \in [0,1]$ and $\sum_{c=1}^C \theta_c = 1$. Thus, there are only $C - 1$ independent parameters.

We can write the categorical distribution in another way by converting the discrete variable $y$ into a *one-hot vector* with $C$ elements, all of which are $0$ except for the entry corresponding to the class label. More generally, we can encode the classes using *unit vectors*, where $\mathbf{e}_c$ is all zeroes except for dimension $c$. Using one-hot encodings, we can write the categorical distribution as

$$
  \operatorname{Cat}(\mathbf{y}|\boldsymbol{\theta}) := \prod_{c=1}^C \theta_c^{y_c}
$$

The categorical distribution is a special case of the *multinomial distribution*.

In the conditional case, we can define

$$
  p(y|\mathbf{x};\boldsymbol{\theta}) = \operatorname{Cat}(y|f(\mathbf{x};\boldsymbol{\theta})) = \mathcal{M}(\mathbf{y}|1, f(\mathbf{x};\boldsymbol{\theta}))
$$

We require that $f_c(\mathbf{x};\boldsymbol{\theta}) \in [0,1]$ and $\sum_{c=1}^C f_c (\mathbf{x};\boldsymbol{\theta}) = 1$. This can be achieved by applying the softmax function on $f$. The softmax function $\operatorname{softmax}:\R^n \to [0,1]^n$ is also called the multinomial logit and is defined as

$$
  \operatorname{softmax}(\mathbf{a}) := \left[\frac{e^{a_1}}{\sum_{c'=1}^C e^{a_{c'}}},\dots,\frac{e^{a_C}}{\sum_{c' = 1}^C e^{a_{c'}}} \right]
$$

The inputs to the softmax, $\mathbf{a} = f(\mathbf{x};\boldsymbol{\theta})$, are called logits and are a generalization of the log odds. 

If we use a linear predictor of the form $f(\mathbf{x};\boldsymbol{\theta}) = \mathbf{Wx} + \mathbf{b}$, where $\mathbf{W}$ is a $C\times P$ matrix, and $\mathbf{b}\in\R^C$ is a bias vector, the final model becomes

$$
  p(y|\mathbf{x};\boldsymbol{\theta}) = \operatorname{Cat}(y|\operatorname{softmax}(\mathbf{Wx} + \mathbf{b}))
$$

Let $\mathbf{a} = \mathbf{Wx} + \mathbf{b}$ be the $C$-dimensional vector of logits. Then we can rewrite the above as follows

$$
  p(y = c|\mathbf{x};\boldsymbol{\theta}) = \frac{e^{a_c}}{\sum_{c'=1}^C e^{a_{c'}}}
$$

