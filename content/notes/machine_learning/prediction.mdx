---
title: 'Prediction'
subject: 'Machine Learning'
showToc: true
---

# Abbreviations
- **ERM:** empirical risk minimization
- **GLM:** generalized linear model
- **i.i.d:** idenpendent and identically distributed
- **MLE:** maximum likelihood estimation
- **MAP:** maximum a posteriori

# Predictive models

Predictive models seek to predict outputs $\mathbf{y}$ from inputs $\mathbf{x}$ using some function $f$ that is estimated from a labeled training set $D = \Set{(\mathbf{x}_n,\mathbf{y}_n) | \mathbf{x}_n \in X\subseteq\R^D,\, \mathbf{y}_n \in Y \subseteq\R^C}_{n=1}^N$ for $\mathbf{x}$. The uncertainty about the correct output for a given input can be modeled using a conditional probability model of the form $p(\mathbf{y}|f(\mathbf{x}))$. When $Y$ is a discrete set of labels, this called a *discriminative model*. When $Y = \R$, this is called a *regression model*.

Predictive models are generally distinguished between 
- *parametric models*, which have a fixed number of parameters independent of the size of the training set
- *non-parametric models*, which have a variable number of parameters that grows with the size of the training set

Non-parametric models are usually more flexible, but can be slower to use for prediction. Parametric models are usually less flexible, but are faster to use for prediction. Most non-parametric models are based on comparing a test input $\mathbf{x}$ to some or all of the stored training examples $\Set{\mathbf{x}_n}_{n=1}^N$, using som form of similarity, $s_n = K(\mathbf{x}, \mathbf{x}_n) \geq 0$, and then predicting the output using some weighted combination of the training labels, such as $\hat{\mathbf{y}} = \sum_{n=1}^N s_n \mathbf{y}_n$. Typical examples are Gaussian processes, and K-nearest neigbour models.

Most parametric models have the form $p(\mathbf{y}|\mathbf{x}) = p(\mathbf{y}|f(\mathbf{x};\boldsymbol{\theta}))$, where $f$ is a function that predicts the parameters of the output distribution. If $f$ is a linear function of $\boldsymbol{\theta}$, i.e. $f(\mathbf{x}; \boldsymbol{\theta}) = \boldsymbol{\theta}^\top \boldsymbol{\phi}(\mathbf{x})$ for some fixed feature transformation $\boldsymbol{\phi}$, the model is called a generalized linear model (GLM). If $f$ is a non-linear, but differentiable, function of $\boldsymbol{\theta}$, e.g. $f(\mathbf{x},\boldsymbol{\theta}) = \boldsymbol{\theta}_2^\top \boldsymbol{\phi}(\mathbf{x};\boldsymbol{\theta}_1)$ for some learnable function $\boldsymbol{\phi}(\mathbf{x};\boldsymbol{\theta}_1)$, it is common to represent $f$ using a neural network.

## Model fitting using MLE and MRE

The most common approach of fitting parametric predictive models is through *maximum likelihood estimation* (MLE), which amounts to solving the following optimization problem 

$$
  \hat{\boldsymbol{\theta}} = \argmax_{\boldsymbol{\theta}\in\Theta} p(D|\boldsymbol{\theta}) = \argmax_{\boldsymbol{\theta}\in\Theta} \ln p(D|\boldsymbol{\theta})
$$

If the dataset is $N$ independent, identically distributed data samples, the likelihood decomposes into a product of terms $p(D|\boldsymbol{\theta}) = \prod_{n=1}^N p(\mathbf{y}_n | \mathbf{x}_n, \boldsymbol{\theta})$. Thus, we can instead minimize the following (scaled) *negative log likelihood*:

$$
  \hat{\boldsymbol{\theta}} = \argmin_{\boldsymbol{\theta}\in\Theta} -\frac{1}{N} \sum_{n=1}^N -\ln[p(\mathbf{y}_n|\mathbf{x}_n,\boldsymbol{\theta})]
$$

We can generalize this by replacing the *log loss* $\ell_n (\boldsymbol{\theta}) = -\ln[p(\mathbf{y}_n | \mathbf{x}_n,\boldsymbol{\theta})]$, with a more general loss function to get

$$
  \hat{\boldsymbol{\theta}} = \argmin_{\boldsymbol{\theta}\in\Theta} r(\boldsymbol{\theta})
$$

where $r(\boldsymbol{\theta})$ is the *empirical risk*

$$
  r(\boldsymbol{\theta}) = \frac{1}{N} \sum_{n=1}^N \ell_n (\boldsymbol{\theta})
$$

This approach is called *empirical risk minimization* (ERM).

## Regularization and MAP

A fundamental problem of model fitting via MLE and MRE arises when all the probability mass is put on the training examples, leaving no probability for novel data in the future. This problem is known as *overfitting* and occurs when the model has enough parameters to perfectly fit the empirical distribution. However, in most cases the empirical distribution is not the same as the true distribution. Overfitting results in a model that may not generalize.

The main solution to overfitting is to use *regularization*, which means to add a penalty term to the likelihood function or empirical risk. In this case we optimize an objective function of the form

$$
  \mathcal{L}(\boldsymbol{\theta};\lambda) = \left(\frac{1}{N} \sum_{i=1}^N \ell (\boldsymbol{y}_n, \boldsymbol{\theta}; \boldsymbol{x}_n)\right) + \lambda C = r(\boldsymbol{\theta}) + \lambda C(\boldsymbol{\theta})
$$

where $\lambda\geq 0$ is the *regularization parameter* and $C(\boldsymbol{\theta})$ is some form of *complexity penalty*.

A common complexity penalty is $C(\boldsymbol{\theta}) = -\ln [p(\boldsymbol{\theta})]$, where $p(\boldsymbol{\theta})$ is the prior of $\boldsymbol{\theta}$. If $\ell$ is the log loss, the regularized objective function becomes

$$
  \mathcal{L}(\boldsymbol{\theta};\lambda) = -\frac{1}{N} \sum_{n=1}^N \ln [p(\boldsymbol{y}_n|\boldsymbol{x}_n, \boldsymbol{\theta})] - \lambda\ln[p(\boldsymbol{\theta})]
$$

By setting $\lambda = 1$ and rescaling $p(\boldsymbol{\theta})$ appropriatley, we get the objective function

$$
  mathcal{L}(\boldsymbol{\theta};\lambda) = - \ln(p(D|\boldsymbol{\theta})) + \ln(p(\boldsymbol{\theta}))
$$

Minimizing this is equivalent to maximize the log posterior

$$
  \hat{\boldsymbol{\theta}}_{\mathrm{MAP}} = \argmax_{\boldsymbol{\theta}}[\ln(p(\boldsymbol{\theta}|D))] = \argmax_{\boldsymbol{\theta}} [\ln(p(D|\boldsymbol{\theta}) + \ln(p(\boldsymbol{\theta})))]
$$

which is known as the *maximum a posteriori estimation* (MAP).

## Cross-validation

A key question when using regularization is how to choose the strength of the regularizer $\lambda$: small values focus on minimizing empirical risk, which may result in overfitting, whereas large values focus on staying close to the prior, which may result in underfitting.

A common method for choosing $\lambda$ is to partition the data $D$ into two disjoint sets, the training set $D_\text{train}$ and a validation set $D_\text{valid}$. We fit the model on $D_\text{train}$ and evaluate its performance on $D_\text{valid}$ for each $\lambda$. We then select the $\lambda$ that results in the best validation performance.

To explain this more rigorously, let us define the regularized empirical risk

$$
  R_\lambda (\boldsymbol{\theta}, D) = \frac{1}{|D|} \sum_{(\mathbf{x},\mathbf{y})\in D} \ell(\boldsymbol{y}, f(\boldsymbol{x};\boldsymbol{\theta})) + \lambda C(\boldsymbol{\theta})
$$

For each $\lambda$, we compute the parameter estimate

$$
  \hat{\boldsymbol{\theta}}_\lambda (D_\text{train}) = \argmin_{\boldsymbol{\theta}} R_\lambda (\boldsymbol{\theta}, D_\train)
$$

We then compute the validation risk

$$
  R_\lambda^\text{val} := R_0 (\hat{\boldsymbol{\theta}}_\lambda (D_\text{train}), D_\text{valid})
$$

This is an estimate of the *population risk*, which is the expected loss under the true distribution $p^* (\boldsymbol{x},\boldsymbol{y})$. Finally, we pick

$$
  \lambda^* = \argmin_{\lambda\in S} R_\lambda^\text{val}
$$

After picking $\lambda^*$, we can refit the model to the entire dataset, $D = D_\text{train} \cup D_\text{valid}$, to get

$$
  \hat{\boldsymbol{\theta}}^* = \argmin_{\boldsymbol{\theta}} R_{\lambda^*} (\boldsymbol{\theta}, D)
$$

A small size of the training set can result in an unreliable estimate of the model parameters. A solution to this is *cross-validation* (CV) by splitting the training data into $K$ *folds*. For each fold $k\in\Set{1,\dots,K}$, we train on all the folds but the $k$th, and test on the $k$th. Formally, we define the *cross-validated risk*

$$
  R_\lambda^\text{cv} := \frac{1}{K} \sum_{k=1}^K R_0 (\hat{\boldsymbol{\theta}}_\lambda (D_{-k}), D_k)
$$

where $D_k$ is the data in the $k$th fold, and $D_{-k}$ is all the other data (complement of $D_k$). If we set $K = N$, we get a method known as *leave-one-out-cross-validation*, since we always train on $N-1$ items and validate on the remaining one.

We can use the CV estimate as an objective inside of an optimization routine to pick the optimal hyperparameter

$$
  \hat{\lambda} = \argmin_\lambda R_\lambda^\text{cv}
$$

Finally, we combine all the available data and re-estimate the model parameters using 

$$
  \hat{\boldsymbol{\theta}} = \argmin_\boldsymbol{\theta} R_{\hat{\lambda}} R_{\hat{\lambda}} (\boldsymbol{\theta}, D)
$$

# Generalized linear models

A generalized linear model (GLM) is a conditional version of an exponential family distribution with the form

$$
  p(y_n | \mathbf{x}_n, \mathbf{w}, \sigma^2) = \exp\left(\frac{y_n \eta_n - A(\eta_n)}{\sigma^2} + \ln(h(y_n, \sigma^2))\right)
$$

where 
- $\eta_n = \mathbf{x}^\top \mathbf{x}_n$ is the natural parameter for the distributions
- $A(\eta_n)$ is the log normalizer
- $T(y) = y$ is the sufficient statistic
- $\sigma^2$ is the dispersion term

The mean and variance of the response variable are

$$
\begin{align*}
  \mu_n :=& \mathbb{E}[y_n | \mathbf{x}_n, \mathbf{w}, \sigma^2] = A'(\eta_n) := \ell^{-1} (\eta_n) \\
  \operatorname{var}(y_n | \mathbf{x}_n, \mathbf{w}, \sigma^2) =& A'' (\eta_n) \sigma^2 
\end{align*}
$$

The mapping from the linear inputs to the mean of the outputs is denoted $\mu_n = \ell^{-1} (\eta_n)$, where the function $\ell$ is called the *link function*, and $\ell^{-1}$ is known as the *mean function*. This relationship is usually written as

$$
  \ell(\mu_n) = \eta_n = \mathbf{w}^\top \mathbf{x}_n
$$

## Linear regression

Linear regression is the simplest case of a GLM and is of the form

$$
\begin{equation}
  p(y|\mathbf{x},\boldsymbol{\theta}) = \mathcal{N}(y|w_0 + \mathbf{w}^\top \mathbf{x}, \sigma^2) \tag{\label{equation-1}}
\end{equation}
$$

where $\boldsymbol{\theta} = (w_0, \mathbf{w}, \sigma^2)$ are all the parameters of the models. The vector of parameters $\mathbf{w}\in\R^n$ are known as the *weights* of *regression coefficients*. Each coefficient $w_i$ for $i=1,\dots,n$ specifies the change in the output we expect if we change the corresponing input feature $x_d$ by one unit. The term $w_0$ is the *offset* or *bias* term, and sepcfieis the output value if all the inputs are $0$. This captures the unconditional mean of the response, $w_0 = \mathbb{E}(y)$, and acts as a baseline.

If the input is one-dimensional, i.e. $X\subseteq\R$, the model has the form $f(\mathbf{x};\mathbf{w}) = ax + b$, where $b = w_0$ is the intercept, and $a = w_1$ is the slope. This is called *simple linear regression*. If the input is multi-dimensional, $\mathbf{x}\in\R^D$ where $D > 1$, the method is called *multiple linear regression*. If the output is also multi-dimensional, i.e. $Y \subset\R^J$ for $J > 1$, it is called *multivariate linear regression*, taking the form

$$
  p(\mathbf{y}|\mathbf{x}, \mathbf{W}) = \prod_{j=1}^J \mathcal{N}(y_j | \mathbf{w}_j^\top \mathbf{x}, \sigma_j^2)
$$

Linear regression can be used to fit sample data to other than a linear function by applying a nonlinear transformation to the input featurese, by replacing $\mathbf{x}$ with $\boldsymbol{\phi}(\mathbf{x})$ to get 

$$
  p(y|\mathbf{x},\boldsymbol{\theta}) = \mathcal{N}(y|\mathbf{w}^\top \boldsymbol{\phi}(\mathbf{x}), \sigma^2)
$$

As long as the parameters of the *feature extractor* $\boldsymbol{\phi}$ are fixed, the model remains linear in the parameters, even if it is not linear in the outputs.

### Ordinary least squares

From $\eqref{equation-1}$, we can derive the negative log likelihood of the data as follows

$$
\begin{align*}
  \operatorname{NLL}(\mathbf{w},\sigma^2) =& \sum_{n=1}^N \ln\left[\left(\frac{1}{2\pi\sigma^2} \right)^{1/2} \exp\left(-\frac{1}{2\sigma^2} (y_n - \mathbf{w}^\top \mathbf{x}_n)^2 \right) \right] \\
  =& \frac{1}{2\sigma^2} \sum_{n=1}^N (y_n - \hat{y})^2 + \frac{N}{2}\ln(2\pi\sigma^2)
\end{align*}
$$

where we have defined the predicted response $\hat{y}_n := \mathbf{w}^\top \mathbf{x}_n$. The MLE is the point where

$$
  \nabla_{\mathbf{w},\sigma} \operatorname{NLL}(\mathbf{w},\sigma^2) = 0
$$

and

$$
\begin{align*}
  \operatorname{RSS}(\mathbf{w}) =& \frac{1}{2} \sum_{n=1}^N (y_n - \mathbf{w}^\top \mathbf{x}_n)^2 = \frac{1}{2}\lVert \mathbf{Xw} - \mathbf{y}\rVert_2^2 \\
  =& \frac{1}{2}(\mathbf{Xw} - \mathbf{w})^\top (\mathbf{Xw} - \mathbf{y})
\end{align*}
$$

Using the matrix identity

$$
  \frac{\partial}{\partial x}(\mathbf{x}^\top \mathbf{Ax}) = (\mathbf{A} + \mathbf{A}^\top)\mathbf{x}
$$

we can show that the gradient is given by

$$
  \nabla_\mathbf{w} \operatorname{RSS}(\mathbf{w}) = \mathbf{X}^\top \mathbf{Xw} - \mathbf{X}^\top \mathbf{y}
$$

Solving for $\nabla_\mathbf{w} \operatorname{RSS}(\mathbf{w})$ gives

$$
  \mathbf{X}^\top \mathbf{Xw} = \mathbf{X}^\top \mathbf{y}
$$

which are known as the *normal equations*. The corresponding solution $\hat{\mathbf{w}}$ is the *ordinary least squares* solution given by

$$
  \hat{\mathbf{w}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}
$$

The Hessian of $\operatorname{RSS}(\mathbf{w})$ is given by

$$
  H(\mathbf{w}) = \frac{\partial^2}{\partial\mathbf{w}^2}\operatorname{RSS}(\mathbf{w}) = \mathbf{X}^\top \mathbf{X}
$$

If $\mathbf{X}$ is full rank, $H$ is positive definite since for any $\mathbf{v} \neq 0$, we have

$$
  \mathbf{v}^\top (\mathbf{X}^\top \mathbf{X})\mathbf{v} = (\mathbf{Xv})^\top (\mathbf{Xv}) = \lVert\mathbf{Xv}\rVert^2 > 0
$$
