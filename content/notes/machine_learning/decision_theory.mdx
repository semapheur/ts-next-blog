---
title: 'Decision Theory'
subject: 'Machine Learning'
showToc: true
references:
  - book_murphy_2022
---

# Bayesian decision theory

In decision theory, we asumme the decision maker, or *agent*, has a set of possible actions $A$ to choose from. Each action $a\in A$ has costs and benefits depending on the underlying state $h \in H$. This can be encoded into a *loss function* $\ell(h,a)$ specifying the loss we incur if we take action $a\in A$ when the state is $h\in H$.

The *posterior expected loss* measures the *risk* for each possible action $a\in A$ given all the relevant evidence $\mathbf{x}\in E$, and is defined as

$$
  \rho(a|\mathbf{x}) := \mathbb{E}_{p(h|\mathbf{x})} [\ell(h,a)]
$$

The *optimal policy* $\pi^* (\mathbf{x})$, also called the *Bayes estimator* or *Bayes decision rule* $\delta^* (\mathbf{x})$, specifies what action to take when presented with evidence so as to minimize the risk, i.e.

$$
\begin{equation}
  \pi^* (\mathbf{x}) = \argmin{a\in A} \mathbb{E}_{p(h|\mathbf{x})} [\ell(h,a)] \tag{\label{equation-1}}
\end{equation}
$$

Equivalently, we can define a *utility function* $U(h,a)$ to be the desirability of each possible action in each possible state. Setting $U(h,a) = -\ell(h,a)$, the optimal policy is

$$
  \pi^* (\mathbf{x}) = \argmax{a\in A} \mathrm{E}_h [U(h,a)]
$$

This is called the *maximum expected utility principle*.

Standard Bayesian decision theory implicitly assumes that the agent is *risk neutral*, which means that its decision is not affected by the degree of certainty in a set of outcomes. The theory can be generalized by introducing *risk sensitivity*, in which case the agent can have the following risk profiles:
- *risk averse*

## Classification problems

### Zero-one loss

Suppose the states correspond to class labels, i.e. $H = Y = \set{1,\dots, C}$. Furthermore, suppose the actions also class labels, i.e. $A = Y$. In this setting, a very commonly used loss function is the *zero-one-loss* $\ell_{01} (y^*, \hat{y})$ defined as

$$
\begin{array}{c|cc}
  & \hat{y} = 0 & \hat{y} = 1 \\ \hline
  y^* = 0 & 0 & 1 \\
  y^* = 1 & 1 & 0 
\end{array}
$$

We can write this concisely as $\ell_{01} (y^*, \hat{y}) = \mathbf{1}(y^* \neq\hat{y})$. In this case, the posterior expected loss is

$$
  \rho(\hat{y}|\mathbf{x}) = p(\hat{y}\neq y^* | \mathbf{x}) = 1 - p(y^* = \hat{y} | \mathbf{x})
$$

The action that minimizes the expected loss is to choose the most probable label, i.e. $\pi^* (\mathbf{x}) = \argmax_{y\in Y} p(y|\mathbf{x})$. This corresponds to the *mode* of the posterior distribution, also known as the *maximum a posteriori estimate*. 

### Cost-sensitive classification

Consider a binary classification problem where the loss function $\ell(y^*, \hat{y})$ is defined as

$$
  \begin{bmatrix}
    \ell_{00} & \ell_{01} \\
    \ell_{10} & \ell_{11}
  \end{bmatrix}
$$

Let $p_0 = p(y^* = 0 | \mathbf{x})$ and $p_1 = 1 - p_0$. Thus we should choose label $\hat{y} = 0$ if and only if

$$
  \ell_{00} p_0 + \ell_{10} p_1 < \ell_{01} p_0 + \ell_{11} p_1
$$

If $\ell_{00} = \ell_{11} = 0$, this simplifies to

$$
  p_1 < \frac{\ell_{01}}{\ell_{01} + \ell_{10}}
$$

Now suppose $\ell_{10} = c\ell_{01}$, so a false negative costs $c$ times more than a false position. In this case, the optimal policy is to pick $a = 0$ if and only if $p_1 < 1/(1 + c)$.

### Classification with rejection

A risk averse agent may choose to ignore a classification due to high uncertainty, which is called the *reject option*. Suppose the states are $H = Y = \set{1,\dots,C}$, and the actions are $A = Y\cup\set{0}$, where action $0$ represents the reject action. Define the following loss function:

$$
  \ell(y^*, a) = \begin{cases}
    0,\quad& y^* = a \in\set{1,\dots,C} \\
    \lambda_r,\quad& a=&0 \\
    \lambda_e,\quad& \text{otherwise}
  \end{cases}
$$

where $\lambda_r$ is the cost of the rejection action, and $\lambda_e$ is the cost of a classification error. In this case, the optimal policy is given by *Chow's rule*

$$
  a^* = \begin{cases}
    y^*, \quad& p^* > y^* \\
    0, \quad& \text{otherwise}
  \end{cases}
$$

where

$$
\begin{align*}
  y^* =& \argmax_{y\in\set{1,\dots,C}} \\
  p^* =& p(y^* | \mathbf{x}) = \max_{y\in\set{1,\dots,C}} p(y|\mathbf{x}) \\
  \lambda^* = 1 - \frac{\lambda_r}{\lambda_e}
\end{align*}
$$

### Class confusion matrix

For any fixed threshold $\tau\in(0,1)$, we consider the decision rule

$$
  \hat{y}_\tau (\mathbf{x}) = \mathbf{1}(p(y = 1|\mathbf{x})\geq 1 - \tau)
$$

We can compute the empirical number of false positives (FP) that arise from using this policy on a set of $N$ labeled examples as follows:

$$
  \operatorname{FP}_\tau = \sum_{n=1}^N \mathbf{1}(\hat{y}_\tau(\mathbf{x}) = 1, y_n = 0)
$$

Similarly, we can compute the empirical number of false negatives (FN), true positives (TP) and true negatives. We can store these result in a $2\times 2$ *class confution matrix* $C$, where $C_{ij}$ is the number of times an item with true class label $i$ wa (mis)classified as having label $j$. In the case of binary classification problems, the class confusion matrix is

$$
\begin{array}{cc|c|c}
  & & \text{Estimate} & \text{Row sum} \\
  & & \begin{array}{cc} 0 & 1 \end{array} & \\ \hline
  \text{True} & \begin{array}{cc} 0 \\ 1 \end{array} & \begin{array}{c} \operatorname{TN} & \operatorname{FP} \\ \operatorname{FN} & \operatorname{TP} \end{array} & \begin{array}{c} N \\ P \end{array} \\ \hline \\
  \text{Col. sum} & & \begin{array}{cc} \hat{N} & \hat{P} \end{array}
\end{array}
$$

We can derive various summary statistics from these distributions. In terms of $p(\hat{y}|y)$, we have the following statistics:
- The *true negative rate* (TNR), also called the *specificity* given by
$$
  \operatorname{TNR}_\tau := p(\hat{y} = 0 | y = 0, \tau) = \frac{\operatorname{TN}_\tau}{\operatorname{TN}_\tau + \operatorname{FP}_\tau}
$$
- The *false positive rate* (FPR), also called the *false alarm rate*, or the *type I error rate* given by
$$
  \operatorname{FPR}_\tau := p(\hat{y} = 1 | y = 0, \tau) = \frac{\operatorname{FP}_\tau}{\operatorname{FP}_\tau + \operatorname{TN}_\tau}
$$
- The *false negative rate* (FNR), also called the *miss rate*, or the *type II error rate* given by
$$
  \operatorname{FNR}_\tau := p(\hat{y} = 0 | y = 1, \tau) = \frac{\operatorname{FN}_\tau}{\operatorname{FN}_\tau + \operatorname{TP}_\tau}
$$
- The *true positive rate* (TPR), also known as the *sensitivity*, *recall* or *hit rate* given by
$$
  \mathcal{R}(\tau) = \operatorname{TPR}_\tau := p(\hat{y} = 1 | y = 1, \tau) = \frac{\operatorname{TP}_\tau}{\operatorname{FN}_\tau + \operatorname{TP}_\tau}
$$

<TableFigure caption="Class confusion matrix for a binary classification problem normalized per row to get $p(\hat{y}|y)$.">
$$
\begin{array}{cc|c}
  & & \text{Estimate} \\
  & & \begin{array}{cc} 0 & 1 \end{array} \\ \hline
  \text{Truth} & \begin{array}{c} 0 \\ 1 \end{array} & \begin{array}{cc}
    \operatorname{TN}/N = \operatorname{TNR} & \operatorname{FP}/N = \operatorname{FPR} \\
    \operatorname{FN}/P = \operatorname{FNR} & \operatorname{TP}/P = \operatorname{TPR}
  \end{array}
\end{array}
$$
</TableFigure>

For $p(y|\hat{y})$, we have the following statistics
- The *negative predictive value* (NPV) given by
$$
  \operatorname{NPV}_\tau = p(y = 0 | \hat{y} = 0, \tau) := \frac{TN_\tau}{\operatorname{TN}_\tau + \operatorname{FN}_\tau}
$$
- The *false discovery rate* (FDR) given by
$$
  \operatorname{FDR}_\tau = p(y = 0 | \hat{y} = 1, \tau ) := \frac{FP_\tau}{\operatorname{FP}_\tau + \operatorname{TP}_\tau}
$$
- The *false omission rate* (FOR) given by
$$
  \operatorname{FOR}_\tau = p(y = 1 | \hat{y} = 0, \tau ) := \frac{FN}{\operatorname{TN}_\tau + \operatorname{FN}_\tau}
$$
- The *positive predictive value* (PPV), also known as the precision, given by
$$
  \mathcal{P}(\tau) = \operatorname{PPV}_\tau := p(y = 1 | \hat{y} = 1, \tau ) = \frac{TP}{\operatorname{FP}_\tau + \operatorname{TP}_\tau}
$$

<TableFigure caption="Class confusion matrix for a binary classification problem normalized per columb to get $p(y|\hat{y})$.">
$$
\begin{array}{cc|c}
  & & \text{Estimate} \\
  & & \begin{array}{cc} 0 & 1 \end{array} \\ \hline
  \text{Truth} & \begin{array}{c} 0 \\ 1 \end{array} & \begin{array}{cc}
    \operatorname{TN}/\hat{N} = \operatorname{NPV} & \operatorname{FP}/\hat{P} = \operatorname{FDR} \\
    \operatorname{FN}/\hat{N} = \operatorname{FOR} & \operatorname{TP}/\hat{P} = \operatorname{PPV}
  \end{array}
\end{array}
$$
</TableFigure>

### Receiver operating characteristic (ROC) curves

The relation between the true positive and false positive rates is called *receiver operating characteristic* (ROC). The quality of a ROC curve is often summarized as a single number using the area under the curve (AUC). Higher AUC scores are better. Another summary statistic is the *equal error rate* (EER), also called the *cross-over rate*, defined as the value which satisfies $\operatorname{FPR} = \operatorname{FNR}$. Since $\operatorname{FNR} = 1 - \operatorname{TPR}$, we can compute the EER by drawing a line from the top left to the bottom right and seeing where it intersects the ROC curve. Lower EER scores are better.

The ROC curve is unaffected by class imbalance, as the TPR and FRP are fractions within the positive and negatives, respectively. The usefulness of the ROC curve may be reduced in such cases, since a large change in the absolute number of false positives will not change the false positive rate very much.

### Precision-recall (PR) curves

An alternative to ROC curves are precision-recall (PR) curves. The quality of a PR curve is often summarized in a single number. First, we can quote the precision for a fixed recall level. This is called the precision at $K$ score. Alternatively, we can compute the are under the PR curve. However, it is possible that the precision does not drop monotonically with recal. In this case, rather than measuring the precision at $K$ score. This is called the *interpolated precision*. The average of the interpolated precision is called the average precision; it is equal to are under the interpolated PR curve, but may not be equal to the area under the PR curve. The *mean average precision* (mAP) is the mean of the AP over a set of different PR curves.

### F-scores

For a fixed threshold, corresponding to a single point on the PR curve, we compute a single precision and recall value, which we wil denote by $\mathcal{P}$ and $\mathcal{R}$. These are often combined into a single statistic called $F_\beta$, defined as follows

$$
  \frac{1}{F_\beta} = \frac{1}{1 + \beta^2}\frac{1}{\mathcal{P}} + \frac{\beta^2}{1 + \beta^2}\frac{1}{\mathcal{R}}
$$

Setting $\beta = 1$, we get the harmonic mean of presicion and recall

$$
\begin{align*}
  \frac{1}{F_1} = \frac{1}{2}\left(\mathbf{1}{\mathcal{P}} + \frac{1}{\mathcal{R}}) \\
  F_1 = \frac{2}{1/\mathcal{R} + 1/\mathcal{P}} = 2\frac{\mathcal{P}\mathcal{R}}{\mathcal{p} + \mathcal{R}} = \frac{\operatorname{TP}}{\operatorname{TP} + \frac{1}{2}(\operatorname{FP} + \operatorname{FN})}
\end{align*}
$$

In general, the harmonic mean is more conservative thant then the artithmetic mean.

Contrary to ROC case, PR curves are sensitive to class imbalance. To see this, let the fration of positive in the dataset be $\pi = P/(P + N)$, and define the ratio $r = P/N = \pi/(1 - \pi)$. Let $n = P+N$ be the population size. ROC curves are not affected by changes in $r$, since the TPR is defined as a ratio within the positive examples, and FPR is defined as a ratio within the negative examples. This means it does not matter which class we define as positive, and which we define as negative.

Now consider PR curves. The precision can be expressed as

$$
  \operatorname{PPV} = \frac{\operatorname{TP}}{\operatorname{TP} + \operatorname{FP}} = \frac{P\cdot\operatorname{TPR}}{P\cdot\operatorname{TPR} - N\cdot\operatorname{FPR}} = \frac{\operatorname{TPR}}{\operatorname{TPR} + \frac{1}{r}\operatorname{FPR}}
$$

Thus, $\operatorname{PPV}\xrightarrow{\pi\to 1, r\to\infty} 1$ and $\operatorname{PPV}\xrightarrow{\pi\to 0, r\to 0}$. The $F$-score is also affected by class imbalance. To see this, note that we can rewrite the $F$-score as

$$
\begin{align*}
  \frac{1}{F_\beta} =& \frac{1}{1 + \beta^2}\frac{1}{\mathcal{P}} + \frac{\beta^2}{1 + \beta^2}\frac{1}{\mathcal{R}} \\
  =& \frac{1}{1 + \beta^2} \frac{\operatorname{TPR} + \frac{N}{P}\operatorname{FPR}}{\operatorname{TPR}} + \frac{1}{1 + \beta} \frac{1}{\operatorname{TPR}} \\
  F_\beta =& \frac{(1 + \beta^2) \operatorname{TPR}}{\operatorname{TPR} + \frac{1}{r}\operatorname{FPR} + \beta^2}
\end{align*}
$$

## Regression problems

In regression problems the action and state spaces continuous, i.e. $A, H\subseteq\R$.

### Quadratic loss ($\ell_2$ loss)

The quadratic loss, also known as the $\ell_2$ loss or squared error, is defined as

$$
  \ell_2 (h,a) = (h - a)^2
$$

In this case, the risk is given by

$$
  \rho(a|\mathbf{x}) = \mathbb{E}[(h-a)^2 | \mathbf{x}] = \mathbb{E})(h^2 | \mathbf{x}) - 2a\mathbb{E}[h|\mathbf{x}] + a^2
$$

The optimal action must satisfy that the derivative of the risk is zero. Hence, the optimal action is to pick the posterior mean

$$
  \frac{\partial}{\partial a} \rho(a|\mathbf{x}) = -2\mathbb{E}(h|\mathbf{x}) + 2a \implies \pi(\mathbf{x}) = \mathbb{E}[h|\mathbf{x}] = \int h p(h|\mathbf{x})\;\d h
$$

This called the minimum mean squared error (MMSE) estimate. Since the $\ell_2$ loss penalizes deviations from the truth quadratically, it is sensitive to *outliers*.

### Absolute loss ($\ell_1$ loss)

The absolute loss, or $\ell_1$ loss is given by

$$
  \ell_1 (h, a) = |h-a|
$$

The optimal estimate is the *posterior medien*, i.e. value such that $p(h < a|\mathbf{x}) = p(h\geq a | \mathbf{x}) = 0.5$

### Huber loss

The *Huber loss* is defined as follows

$$
  \ell_\delta(h,a) = \begin{cases}
    r^2 / 2,\quad |r|\leq\delta \\
    \delta|r| - \delta^2 / 2,\quad |r| > \delta
  \end{cases}
$$

## Probabilistic prediction problems

In probabilistic predictions problem, we assume that the state and action spaces are distributions, denoted $h = p(Y|\mathbf{x})$ dn $a = q(Y|\mathbf{x})$, respectively. In this setting, we want to pick $q$ to to minimize $\mathbb{E}[\ell(p,q)]$ for each given $\mathbf{x}$.

### Proper scoring rule

A loss function for a probabilistic prediction problem is known as a *scoring rule*. A scoring rule $\ell(p,q)$ is *proper* if it satisfies the following propert: the minimum expectation of $\ell$ occurs for the distribution $q$ that matches the true distribution $p$, i.e. $\ell(p,p) \leq \ell(p,q)$, with equality if and only if $p = q$.

### Kullback-Leibler divergence

A common loss function for comparing two distributions is the *Kullback-Leibler (KL) divergence*, which is defined as

$$
  D_{\text{KL}} (p \Vert q) := \sum_{y\in Y} p(y) \ln\frac{p(y)}{q(y)}
$$

where we have assumed that $Y$ is discrete. We can expand the KL as follows

$$
\begin{align*}
  D_{KL} (p \Vert q) =& \sum_{y\in Y} p(y) \ln[p(y)] - \sum_{y\in Y} p(y) \ln[q(y)] \\
  =& -\mathbb{H}(p) + \mathbb{H}_{\text{ce}} (p,q) \\
  \mathbb{H}(p) :=& -\sum_{y\in Y} p(y) \ln[p(y)]\\
  \mathbb{H}_{\text{ce}}(p,q) := -\sum_{y\in Y} p(y) \ln[q(y)] 
\end{align*}
$$

The $\mathbb{H}(p)$ term is known as the entropy. This is a measure of uncertainty or variance of $p$; it is maximal if $p$ is uniform, and is zero if $p$ is a degenerate or deterministic delta function. The $\mathbb{H}_{\text{ce}}(p,q)$ term is known as the cross-entropy. This measures the expected number of bits we need to use to compess a dataset coming from distribution $p$ if we design our code using distribution $q$. Thus, the KL is the extra number of bits we need to use to compress the data due to using the incorrect distribution $q$. If the KL is zero, it means that we can correctly predict the probabilities of all possible future events. The $\ln[p(y)/q(y)]$ term in the KL loss can be quite sensitive to errors for low probability events.

To find the optimal distribution to use when predicting futute data, we can minimize $D_{\text{KL}} (p \lVert q)$. Since the entropy $\mathbb{H}(p)$ is independent of $q$, we can equivalently minimize the cross-entropy $\mathbb{H}_{\text{ce}} (p,q)$ 

$$
  q^* (Y) = \argmin_q \mathbb{H}_{\text{ce}}(p(Y), q(Y))
$$

Now consider the secial case in which the true state is a degenerate distribution, which put all its mass on a single outcome, say $c$, i.e. $h = p(Y) = \mathbf{1}(Y = c)$. This is called a *one-hot* distribution, since it "turns on" the $c$th element of the vector, and leaves the other elements "off". In this case, the cross entropy becomes

$$
  \mathbb{H}_{\text{ce}} (\delta(Y = c), q) = -\sum_{y\in Y} \delta (y = c) \ln[q(y)] = -\ln[q(y)]
$$

This is known as the *log loss* of the predictive distribution $q$ when given target label $c$.

The cross-entropy loss is a scoring rule by virtue of the fact that $0 = D_{\text{KL}} (p \Vert p) \leq D_{\text{KL}} (p \Vert q)$.

### Brier scores

The Brier score (BS) is defined the special case that the true distribution $p$ is a set of $N$ delta functions, $\mathbf{p}_n (\mathbf{Y}_n) = \delta(\mathbf{Y}_n - \mathbf{y}_n)$, where $\mathbf{y}_n$ is the observed outcome in one-form, so $y_{nc} = 1$ if the $n$th observed outcome is class $c$. The corresponding predictive distribution is assumed to be as set of $N$ distributions $\mathbf{q}_n (\mathbf{Y}_n)$, which can be conditioned on covariates $\mathbf{x}_n$. In this setting, the Brier score is defined as

$$
  \operatorname{BS}(\mathbf{p},\mathbf{q}) := \frac{1}{N} \sum_{n=1}^N \sum_{c=1}^C (q_{nc} - p_{nc})^2 = \frac{1}{N} \sum_{n=1}^N \sum_{c=1}^C (q_{nc} - y_{nc})^2
$$

This is just the mean squared error of the predictive distributions compared to the true distributions, when viewed as vectors. Since it is based on squared error, the Brier score is less sensitive to extremely rare or extremely common classes.

IN the special case of binary classification, where we use class labels $c\in\set{0,1}$, we define $y_n = y_{n1}$ and $q_n = q(Y_{n1})$, so the summand becomes $(q_n - y_n)^2 + (1 - q_n - (1 - y_n))^2 = 2(q_n - y_n)^2$. Consequently, in the binary csae, we often divide the multi-class definition by $2$, to get the binary Brier score

$$
  \operatorname{BS}(\mathbf{p},\mathbf{q}) = \frac{1}{N}\sum_{n=1}^N (q_n - y_n)^2 \in [0,1]
$$

Since it can be hard to interpret absolyte Brier score values, a relative performance measure, known as the *Brier skill score* (BSS), is used. This is defined as $\operatorname{BSS} = 1 - \operatorname{BS}/\operatorname{BS}_{\text{ref}}$, where $\operatorname{BS}_{\text{ref}}$ is the BS of a reference model. The range over score is $\mathbf{BSS}\in[-1,1]$, with $1$ being the best, $0$ meaning no improvement over the baseline, and $-1$ being the worst. In the case of binary predictors, a common reference model is the baseline empirical probability $\bar{q} = \frac{1}{N} \sum_{n=1}^N y_n$. In the meteorological community, this is called the "in-sample climatology" prediction, where "in-sample" means based on the observed data, and "climatology" refers to long run average behaviour.

# Model selection

## Bayesian hypothesis testing

Suppose we have to hypotheses (models), commonly called the *null hypothesis* $M_0$, and the *alternative hypothesis* $M_1$. The process of finding the hypothesis which is more likely to be true is called *hypothesis testing*.

If we use $0$-$1$ loss, the optimal decision is to pick the alternative hypothesis if and only if $p(M_1|\mathcal{D}) > p(M_0|\mathcal{D})$, or equivalently, if $p(M_1 | \mathcal{D}) / p(M_0 | \mathcal{D}) > 1$. If we use a uniform prior, $p(M_0) = p(M_1) = 0.5$, the decision rule becomes: select $M_1$ if and only if $p(\mathcal{D}|M_1)/p(\mathbb{D}|M_0) > 0$. This quantity, which is the ratio of marginal likelihoods of the two models, is known as the *Bayes factor*

$$
  \operatorname{BF}(1,0) := \frac{p(\mathcal{D}|M_0)}{p(\mathcal{D}|M_1)}
$$

This is like a *likelihood ratio*, except we integrate out the parameters, which allows us to compare models of different complexity, due to the Bayesian Occam's razor effect. If $\operatorname{BF}(1,0) > 1$, then we prefer model $1$, otherwise we prefer model $0$.  

<TableFigure caption="Jeffrey scale of evidence for interpreting Bayes factor">
| Bayes factor $BF(1,0)$ | Intepretation |
| :-: | --- |
| $BF < 1/100$ | Decisive evidence for $M_0$ |
| $BF < 1/10$ | Strong evidence for $M_0$ |
| $1/10 < BF < 1/3$ | Moderate evidence for $M_0$ |
| $1/3 < BF < 1$ | Weak evidence for $M_0$ |
| $1 < BF < 3$ | Weak evidence for $M_1$ |
| $3 < BF < 10$ | Moderate evidence for $M_1$ |
| $BF > 10$ | Strong evidence for $M_1$ |
| $BF > 100$ | Decisive evidence for $M_1$ |
</TableFigure>

## Bayesian model selection

Suppose we have a set $\mathcal{M}$ of more than two models. The process of picking the most likely model is called *model selection*. If we have a $0$-$1$ loss, the optimal action is to pick the most probable mode

$$
  \hat{m} = \argmax_{m\in\mathcal{M}} p(m|\mathcal{D})
$$

where

$$
  p(m|\mathcal{D}) = \frac{p(\mathcal{D}|m) p(m)}{\sum_{m\in\mathcal{M}} p(\mathcal{D}|m)p(m)}
$$

is the posterior over model. If the prior model is uniform, $p(m) = 1/|\mathcal{M}|$, then the MAP model is given by

$$
  \hat{m}  = argmax_{m\in\mathcal{M}} p(\mathcal{D}|m)
$$

The posterior over models is given by

$$
  p(\mathcal{D}|m) = \int p(\mathcal{D}|\boldsymbol{\theta},m) p(\boldsymbol{\theta}|m)\;\d\boldsymbol{\theta}
$$

This is known as the *marginal likelihood*, or the *evidence* for model $m$. Intuitively, it is the likelihood of the data averages over all parameter values, weighted by the prior $p(\operatorname{\theta}|m)$. If all settings of $\boldsymbol{\theta}$ assigns high probability to the data, then this is probably a good model. 

The marginal likelihood, $p(\mathcal{D}|m) = \int p(\mathcal{D}|\boldsymbol{\theta}, m)p(\boldsymbol{\theta})\;\d\boldsymbol{\theta}$, can be difficult to compute, since it requires marginalizing over the entire parameter space. Furthermore, the result can be quite sensitive to the choice of prior.

### Occam's razor

Consider two models, a simple one, $m_1$, and a more complex one, $m_2$. Suppose that both can explain the data by suitable optimizing their parameters, i.e. for which $p(\mathcal{D}|\hat{\boldsymbol{\theta}}_1, m_1)$ and $p(\mathcal{D}|\hat{\boldsymbol{\theta}}_2, m_2)$ are both large. Intuitively, we should prefer $m_1$, since it is simpler and just as good as $m_2$. This principle is known as *Occam's razor*.

Let us see how ranking models based on their marignal likelihood, which involces averaging the likelihood with respect to the prior, explains this behaviour. The complex model will put less prior probability on the "good" parameters that explain the data, $\hat{\boldsymbol{\theta}_2}$, since the prior must integrate to $1$ over the entire parameter space. Thus, it will take averages in parts of parameter space with low likelihood. By contrast, the simpler model has fewer parameters, so the prior is concentrated over a smaller volume. Thus, its averages will mostly be on the goor parts of parameters space, near $\hat{\boldsymbol{\theta}_1}$. Hence, we see that the marignal likelihood will prefer the simpler model.

Another way to understand the Bayesian Occam's razor effect is to compare the relative predictive abilities of simple and complex models. Since probabilities must sum to one, we have $\sum_{\mathcal{D}'} p(\mathcal{D}'|m) = 1$, where the sum is over all possible datasets. Complex models, which can predict many things, must spread their predicted probability mas thinly, and hence will not obtain a large a probability for any given data set as simples models. This is sometimes called the *conservation of probability mass* principle.

### Connection between cross validation and margianl likelihood

Marginal likelihood is closely related to the leave-one-out cross-validation estimate. Too see this, we start with the marginal likelihood for model $m$, which we write in sequential form as follows

$$
  p(\mathcal{D}|m) = \prod_{n=1}^N p(y_n|y_{1:n-1}, x_{1:N}, m) = \prod_{n=1}^N p(y_n|x_n, \mathcal{D}_{1:n-1}, m)
$$

where

$$
  p(y|\mathbf{x}, \mathcal{D}_{1:n-1}, m) = \int p(y|\mathbf{x},\boldsymbol{\theta}) p(\boldsymbol{\theta}|\operatorname{D}_{1:n-1}, m)\; \d\boldsymbol{\theta}
$$

Suppose we use a plugin approximation to the above distribution to get

$$
\beign{align*}
  p(y|\mathbf{x}, \mathcal{D}_{1:n-1}, m) \approx& \int p(y|\mathbf{x},\boldsymbol{\theta}) \delta(\boldsymbol{\theta} - \hat{\boldsymbol{\theta}}_m (\mathcal{D}_{1:n-1}))\;\d\boldsymbol{\theta} \\
  =& p(y|\mathbf{x}, \hat{\boldsymbol{\theta}}_m (\mathcal{D}_{1:n-1}))
\end{align*}
$$

Then we get

$$
  \ln[p(\mathcal{D}|m)] \approx \sum_{n=1}^N \ln[p(y_n | x_n, \hat{\boldsymbol{\theta}}_m (\mathcal{D}_{1:n-1}))]
$$

This is similar to a leave-one-out cross-validation estimate of the likelihood, which has the form

$$
  \frac{1}{N} \sum_{n=1}^N \ln[p(y_n | x_n, \hat{\boldsymbol{\theta}} (\mathcal{D}_{1:n-1, n+1:N}))]
$$

except we ignore the $\mathcal{D}_{n+1:N}$ part. This connection can be explained as follows: an overly comples model will overfit the "early" examples and will then predict the remaining ones poorly, and thus will also get a low cross-validation score.

## Information criteria

Information criteria are metric for model selection of the following form

$$
  \mathcal{L}(m) = -log[p(\mathcal{D}\hat{\boldsymbol{\theta}}, m)] + C(m)
$$

where $C(m)$ is a *complexity penalty* term added to the negative log likelihood. It is convenction, when working with information criteria, to scale the negative log likelihood by $-2$ to get the *deviance*, $\operatorname{dev}(m) = -2\ln[p(\mathcal{D}|\hat{\boldsymbol{\theta}}, m)]$.

### Bayesian information criterion (BIC)

The *Bayesian information criterion* (BIC) can be thought of as a simple approximation to the log marginal likelihood. In particular, if we make a Gaussian approximation to the posterior, we get

$$
  \ln[p(\mathcal{D}|m)] \approx \ln[p(\mathcal{D}|\hat{\boldsymbol{\theta}}_{\text{map}})] + \ln[p(\hat{\boldsymbol{\theta}}_{\text{map}})] - \frac{1}{2}\ln|\mathbf{H}|
$$

where $\mathbf{H}$ is the Hessian of the negative log joint, $-\ln[p(\mathcal{D},\boldsymbol{\theta})]$, evaluated at the maximum a posterior estimate $\hat{\boldsymbol{\theta}}_{\text{map}}$. If we have a uniform prior, $p(\boldsymbol{\theta}) \propto 1$, we can drop the prior term, and replace the MAP estime with the MLE, $\hat{\boldsymbol{\theta}}$, yielding

$$
  \ln[p(\mathcal{D}|m)] \approx \ln[p(\mathcal{D}|\hat{\boldsymbol{\theta}})] - \frac{1}{2}\ln|\mathbf{H}|
$$

The $ln|\mathbf{H}|$ term is sometimes called the *Occam factor*, since it is a measure of model complexity. Furthermore, we have $\mathbf{H} = \sum_{n=1}^N \mathbf{H}_n$, where $\boldsymbol{H}_i = \nabla^2 \ln[p(y_n|\boldsymbol{\theta})$ is the empirical Fisher information matrix. Approximating each $\mathbf{H}_n$ by a fixed matrix $\hat{\mathbf{H}}$ yields

$$
  \ln|\mathbf{H}| = \ln|N\hat{\mathbf{H}}| = \ln(N^{D_m} |\hat{\mathbf{H}}|) = D_m \ln N + \ln|\mathbf{H}|
$$

where $D_m = \dim(\boldsymbol{\theta})$ and we have assume $\mathbf{H}$ is full rank. We can drop the $\ln|\hat{\mathbf{H}}|$ term, since it is independent of $N$, and thus will get dominated by the likelihood. Combining the steps, we the BIC score

$$
  J_{\text{BIC}} (m) = \ln[p(\mathcal{D}|m)] \approx \ln[p(\mathcal{D}|\hat{\boldsymbol{\theta}}, m)] - \frac{D_m}{2} \ln N
$$

We can also define the BIC loss, by scaling by $-2$

$$
  \mathcal{L}_{\text{BIC}} = -2\ln[p(\mathcal{D}|\hat{\boldsymbol{\theta}}, m)] + D_m \ln N
$$

### Akaike information criterion (AIC)

The *Akaike information criterion* (AIC) is close related to the BIC. It has the form

$$
  \mathcal{L}_{\text{AIC}} (m) = -2\ln[p(\mathcal{D}|\hat{\boldsymbol{\theta}}, m)] + 2D_m
$$

where $D_m = \dim(\boldsymbol{\theta})$. This penalizes complex models less heavily than BIC, since the regularization term is independent of $N$. This estimator can be derived from a frequentist perspective.

### Minimum description length (MDL)

Suppose we want to choose a model so that the sender can send some data to the receiver using the fewest number of bits. Choosing models this way is known as the *minimum description length* (MDL) principle. This principle can be derived as follows. Let $\hat{\boldsymbol{\theta}}\in\R^{D_m}$ be the parameters estimated using $N$ data samples. Since we can only reliably estimate each parameter to an accuracy of $\mathcal{O}(1/\sqrt{N})$, we only need to use $\log_2 (1/\ln(N)) = \frac{1}{2}\log_2 (N)$ bits to encode each parameter. The sender needs to use this model to encode the data, which takes $-\ln[p(\mathcal{D}|\hat{\boldsymbol{\theta}}, m)] = -\sum_n \ln[p(y_n |\hat{\boldsymbol{\theta}},m)]$ bits. The total cost is

$$
  \mathcal{L}_{\text{MDL}} (m) = -\ln[p(\mathcal{D}|\hat{\boldsymbol{\theta}}, m)] + \frac{D_m}{2}\ln N
$$

### Widely applicable information criterion (WAIC)

The main problem with BIC, AIC and MDL is that it can be har to compute the degrees of a freedom of a model, needed to define the complexity term, since most parameters are highly correlated and not uniquely identifiable from the likelihood. In particular, if the mapping from parameters to the likelihood is not injective, the model is called a *singular static model*, since the corresponding Fisher information matrix may be singular.

An alternative criterion that works even in the singular case is known as the *widely applicable information criterion* (WAIC), also known as the *Watanabe-Akaike information criterion*. The WAIC replaces the plug-in approximation to the marginal likelihood $\ell(m) = \sum_{n=1}^N \ln[p(y_n|\hat{\boldsymbol{\theta}}, m)$ iwth the *expected log pointwise density* (ELPD) defined as

$$
  \operatorname{ELPD}(m) = \sum_{n=1}^N \ln[p(y_n|\mathcal{D},m)] = \sum_{n=1}^N \ln \mathbb{E}_{\boldsymbol{\theta}|\mathbb{D}, m}[p(y_n|\boldsymbol{\theta}, m)]
$$

which is usually approximated by Monte Carlo. Additionally, the complexity term is defined by $C(m) = \sum_{n=1}^N$, which again is usually approximated by Monte Carlo. The intuition for this is a follows: if, for a given datapoint $y_n$, the different posterior samples $\boldsymbol{\theta}_s$ make very different predictions, then the model is uncertain, and likely too flexible. The WAIC loss is defined as

$$
  \mathcal{L}_{\text{WAIC}} (m) = -2\operatorname{LPPD}(m) + 2C(m)
$$

Note that the WAIC evaluates the expected log likelihood using the posterior of the parameters. By contrast, the marginal likelihood averages the log likelihood with respect to the prior. This makes the marginal likelihood more sensitive to the prior. It is therefore generally better to use WAIC for model selection.

## Posterior influence over effect sizes and Bayesian significance testing

Recall that Bayesian hypothesis testing relies on computing the Bayes factors for the null vs the alternative model $p(\mathbb{D}|H_0)/p(\mathcal{D}|H_1)$. Computing the necessary marginal likelihoods can be computationally difficul, and results can be sensitive to the choice of prior. Furthermore, we are often more interested in estimating an *effect size*, which is the difference in magnitude between two parameters, rather than in deciding if an effect size is $0$ (null hypothesis) or not (alternative hypothesis). The latter is called a *point null hypothesis*.

Suppose we have two classifiers, $m_1$ and $m_2$, and we want to know which one is better. That is, we want to perform a comparison of classifiers. Let $\mu_1$ and $\mu_2$ be their average accuracies, and let $\Delta = \mu_1 - \mu_2$ be the difference in their accuracies. The probability that model $1$ is more accurate, on average, than model $2$ is given by $p(\Delta > 0|\mathcal{D})$. However, even if this probability is large, the improvement may be not practically significant. So it is better to compute a probability such as $p(\Delta > \epsilon | \mathcal{D})$ or $p(|\Delta| > \epsilon|\mathcal{D}|)$, where $\epsilon$ represents the minimal magnitude of effect size that is meaningful for the problem at hand. This a called a *one-sided test* or *two-sided test*.

More generally, let $R = [-\epsilon,\epsilon]$ represent a *region of practical equivalence* (ROPE). We can define $3$ events of interest:
- the null hypothesis $H_0 : \Delta\in R$, which says both methods are practically the same (which is a more realistic assumption than $H_0: \Detla = 0$)
- $H_A: \Delta > \epsilon$, which says $m_1$ is better than $m_2$
- $H_B: \Delta < -\epsilon$, which says $m_2$ is better than $m_1$

To choose amongst these $3$ hypotheses, we just have to compute $p(\Delta|\mathcal{D})$, which avoids the need to compute Bayes factors. 

### Bayesian t-test for difference in means

Suppose we have two classifiers, $m_1$ and $m_2$, which are evaluated on the same set of $N$ test examples. Let $e_i^{(m)}$ be the error of method $m$ on the test example $i$ (or this could be the conditional log likelihood $e_i^{(m)} = \ln[p^{(m)}(y_n|x_n)]$). Since the classifiers are applied to the same data, we can use a *paired test* for comparing them, which is more sensitive than looking at average performance, since the factors that make one example easy or hard to classify will be shared by both methods. Thus we will work with the differences $d_n = e_n^{(1)} - e_n^{(2)}$. We assume $d_n \sim\mathcal{N}(\Delta, \sigma^2)$. We are interested in $p(\Delta|\mathbf{d})$, where $\mathbf{d} = (d_n)_{n=1}^N$.

If we use an uninformative prior for the unknown parameters $(\Delta, \sigma)$, one can show that the posterior marginal for the mean is given by a Student distribution

$$
  p(\Delta|\mathbf{d}) = \mathcal{T}_{N-1} (\Delta | \mu, s^2/N)
$$

where $\mu = \frac{1}{N}\sum_{n=1}^N d_n$ is the sample mean, and $s^2 = \frac{1}{N-1} \sum_{n=1}^N (d_n - \mu)^2$ is an unbiased estimate of the variance. Hence, we can easily compute $p(|\Delta| > \epsilon|\mathbf{d}|)$ for a given ROPE $\epsilon$. This is known as a *Bayesian t-test*.

An alternative to a formal test is to examine the distribution of the posterior $p(\Delta|\mathbf{d})$. If this distribution is tightly centered on $0$, we can conclude that there is no significant difference between the methods. 

### Bayesian $\chi^2$-test for difference in rates

Suppose we have two classifiers which are evaluated on different test sets. Let $y_m$ be the number of correct examples from method $m\in\Set{1,2}$ out of $N_m$ trials, so the accuracy rate is $y_m / N_m$. We assume $y_m\sim\operatorname{Bin}(N_m, \theta_m)$, so we are interested in $p(\Delta|\mathcal{D})$, where $\Delta = \theta_1 - \theta_2$ and $\mathcal{D} = (y_1, N_1, y_2, N_2)$ is all the data.

If we use a uniform prior for $\theta_1$ and $\theta_2$, i.e. $p(\theta_j) = \operatorname{Beta}(\theta_j|1,1)$, the posterior is given by

$$
  p(\theta_1, \theta_2|\mathcal{D}) = \operatorname{Beta}(\theta_1 | y_1 + 1, N_1 - y_1 + 1)\operatorname{Beta}(\theta_2|y_2 + 1, N_2 - y_2 + 1)
$$

The posterior for $\Delta$ is given by

$$
\begin{align*}
  p(\Delta|\mathcal{D}) =& \int_0^1 \int_0^1 \mathbf{1}(\Delta = \theta_1 - \theta_2)p(\theta_1 | \mathcal{D}_1)p(\theta_2 | \mathcal{D}_2) \\
  =& \int_0^1 \operatorname{Beta}(\theta_1 | y_1 + 1, N_1 - y_1 + 1)\operatorname{Beta}(\theta_1 - \Delta | y_2 + 1, N_2 - y_2 + 1)\;\d\theta_1
\end{align*}
$$

We can then evaluate this for any value of $\Delta$ that we choose. For example, we can compute

$$
  p(\Delta > \epsilon|\mathcal{D}) = \int_\epsilon^\infty p(\Delta|\mathcal{D})\;\d\Delta
$$

This is called a *Bayesian $\chi^2$-test*.

# Frequentist decision theory

In frequentist decision theory, we treat the unknown state, denoted by $\boldsymbol{\theta}$, as a fixed but unkown quantity, and we treat the data $\mathbf{x}\in\mathcal{D}$ as a random. Thus, instead of conditioning on $\mathbf{x}$ as in Bayesian decision theory, we average over it, to compute the loss we expect to incur if we apply our decision estimator $\delta(\mathbf{x})$ to many different datasets.

The frequentist *risk* of an estimator $\delta$ given an unknown state $\boldsymbol{\theta}$ is defined as the expected loss when applying that estimator to data $\mathbf{x}$, where the expectation is over the data, sampled from $p(\mathbf{x}|\boldsymbol{\theta})$

$$
\begin{equation}
  R(\boldsymbol{\theta}, \delta) := \mathbb{E}_{p(\mathbf{x}|\boldsymbol{\theta})}[\ell(\boldsymbol{\theta}, \delta(\mathbf{x}))]\tag{\label{equation-2}}
\end{equation}
$$

<MathBox title='Mean estimator of a Gaussian' boxType='example'>
Consider a Gaussian sampled data set $\mathbf{x} = (x_n)_{n=1}^N$ with $x_n \sim\mathcal{N}(\theta^*, \sigma^2 = 1)$. Using a quadratic loss $\ell_2 (\theta,\hat{\theta}) = (\theta - \hat{\theta})$ for estimating the mean $\theta$, the corresponding risk function is the mean squared error. Let $\hat{\theta}(\mathbf{x}) = \delta(\mathbf{x})$ be the estimated mean. The risk of this estimator is given by the mean squared error, which can be decomposed into squared bias and variance, i.e.

$$
  \operatorname{MSE}(\hat{\theta}|\theta^*) = \operatorname{var}(\hat{theta}) + \operatorname{bias}^2 (\hat{\theta})
$$

where, $\operatorname{bias}(\hat{\theta}) = \mathbb{E}(\hat{\theta} - \theta^*)$. We now consider $5$ different estimators for computing $\theta$:
- $\delta_1 (\mathbf{x}) = \bar{x}$, the sample mean. 

The sample mean is an unbiased estimator, so its risk is

$$
  \operatorname{MSE}(\delta_1|\theta^*) = \operatorname{var}(\bar{x}) = \frac{\sigma^2}{N}
$$

- $\delta_2 (\mathbf{x}) = \operatorname{median}(\mathbf{x})$, the sample median

The sample median is an unbiased estimator with variance $\pi/(2N)$, so the risk is

$$
  \operatorname{MSE}(\delta_2 | \theta^*) = \frac{\pi}{2N}
$$

- $\delta_3 (\mathbb{x}) = \theta_0$, a fixed value

The fixed estimator returns the constant $\theta_0$, so its bias if $(\theta^* - \theta_0)$ and its variance is null. Hence the risk is

$$
  \operatorname{MSE}(\delta_3 | \theta^*) = (\theta^* - \theta_0)^2
$$

- $\delta_\kappa (\mathbf{x})$, the posterior mean under a $\mathcal{N}(\theta|theta_0, \sigma^2 /\kappa)$ prior
$$
  \delta_\kappa (\mathbf{x}) = \frac{N}{N + \kappa}\bar{x} + \frac{\kappa}{N + \kappa}\theta_0 = w\bar{x} + (1 - w)\theta_0
$$

For $\delta_\kappa$, we use $\theta_0 = 0$, and consider a weak prior, $\kappa = 1$, and a stronger prior, $\kappa = 5$. In this case we derive the MSE as follows:

$$
\begin{align*}
  \operatorname{MSE}(\delta_k | \theta^*) =& \mathbb{E}\left[(w\bar{x} + (1 - w)\theta_0 - \theta^*)^2\right] \\
  =& \mathbb{E}\left[(w(\bar{x} - \theta^*) + (1 - w)(\theta_0 - \theta^*))^2 \right] \\
  =& w^2 \frac{\sigma^2}{N} + (1 - w)^2 (\theta_0 - \theta^*)^2 \\
  =& \frac{1}{(N + \kappa)^2} (N\sima^2 + \kappa^2 (\theta_0 - \theta^*)^2)
\end{align*}
$$
</MathBox>

## Bayes risk

In general, the true state $\boldsymbol{\theta}$ that generates the data $\mathbf{x}$ is unknown, so we cannot compute the risk given in $eqref{equation-2}$. One solution to this is to assume a prior $\pi_0$ for $\boldsymbol{\theta}$, and then average it out. This gives the *Bayes risk*, or the *integrated risk*, defined as

$$
  R_{\pi_0} (\delta) := \mathbb{E}_{\pi_0 (\boldsymbol{\theta})} [R(\boldsymbol{\theta}, \delta)] = \int \pi_0(\boldsymbol{\theta}) p(\mathbf{x}|\boldsymbol{\theta}) \ell(\boldsymbol{\theta},\delta(\mathbf{x}))\;\d\boldsymbol{\theta}\;\d\mathbf{x}
$$

A decision rule that minimizes the Bayes risk is called a *Bayes estimator*. This is equivalent to the optimal policy given by Bayesian decision theory in equation $\eqref{equation-1}$ since

$$
\begin{align*}
  \delta(\boldsymbol{\theta}) =& \argmin_a \int \pi_0(\boldsymbol{\theta}) p(\mathbf{x}|a) \ell(\boldsymbol{\theta},a)\;\d\boldsymbol{\theta}\;\d\mathbf{x} \\
  =& \argmin_a \int p(\boldsymbol{\theta}|\mathbf{x}) \ell(\boldsymbol{\theta}, a)
\end{align*}
$$

Hence, we see that picking the optimal action on a case-by-case basis (as in the Bayesian approach) is optimal on average (as in the frequentist approach).

## Maxumum risk

The use of a propr seem undesirable in the context of frequentist statistics. We can avoid this by defining the *maximum risk*

$$
  R_{\text{max}} (\delta) := \sup_{\boldsymbol{\theta}} R(\boldsymbol{\theta}, \delta)
$$

A decision rule that minimizes the maximum risk is called a *minimax estimator*, and is denoted $\delta_{\text{MM}}$. Minimax estimators have a certain appeal. However, computing them can be hard. They are also very pessimistic. In fact, one can show that all minima estimators are equivalent to Bayes estimators under a *least favorable prior*.

## Consistent estimators

Suppose we have a dataset $\mathbf{x} = \Set{x_n}_{n=1}^N$ where the samples $x_n \in\mathcal{X}$ are generated i.i.d. from a distribution $p(\mathbf{X}|\operatorname{\theta}^*)$, where $\boldsymbol{\theta}^* \in\Theta$ is the true parameter. Furthermore, suppose the parameters are *identifiable*, meaning that $p(\mathbf{x}|\boldsymbol{\theta}) = p(\mathbf{x}|\theta')$ if and only if $\boldsymbol{\theta} = \boldsymbol{\theta}^*$. Then we say that an estimator $\delta:\mathcal{X}^N \to\Theta$ is a *consistent estimator* if $\hat{\boldsymbol{\theta}}(\mathbf{x}) \xrightarrow{N\to\infty} \boldsymbol{\theta}$ converges in probability. In other words, the estimator $\delta$ recovers the true parameter (or a subset of it) in the limit of infinite data. This is equivalent to minimizing the $0-1$ loss, $\mathcal{L}(\boldsymbol{\theta}^*, \hat{\boldsymbol{\theta}}) = \mathbf{1}(\boldsymbol{\theta}^* \neq\hat{\boldsymbol{\theta}}$.

Note that an estimator can be unbiased but not consistent. For example, consider the estimator $\delta(\mathbf{x}) = \delta(\set{\mathbf{x}}_{i=1}^N)$. This is an unbiased estimator of the true mean $\boldsymbol{\theta}$, since $\mathbb{E}[\delta(\mathbf{x})] = \boldsymbol{\mu}$. However, the sampling distribution of $\delta(\mathbf{x})$ does not converge to a fixed values, so it cannot converge to the point $\boldsymbol{\theta}^*$.

Although consistency is a desirable property, it is of samewhot limited usefulness in practice since most real datasets od not come from our chosen model family, i.e. there is no $\boldsymbol{\theta}^*$ such that $p(\cdot|\boldsymbol{\theta}^*)$.

## Admissible estimators

Let $\delta_1, \delta_2$ be two random variables. We say that $\delta_1$ dmoniates $\delta_2$ if $R(\boldsymbol{\theta}, \delta_1) < R(\boldsymbol{\theta}, \delta_2)$ for all $\boldsymbol{\theta}$. The domination is strict if the inequality of strict for some $\boldsymbol{\theta}^*$. An estimator is *admissible* if it is not stricty dominated by any athor estimator. It can be shown that all admissible decision rules are equivalent to some kind of Bayesian decision rule, under some technical conditions.

<MathBox title='James-Stein estimator' boxType='example'>
Let $\mathbf{X} = (X_n)_{n=1}^N$ with $X_n \sim\mathcal{N}(\theta_n, 1)$ be independent Gaussian variables with unit variance. Assuming a quadratic loss function $\ell_2 (\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}) = \lVert\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}\rVert^2$, the corresponding risk function $R(\hat{\boldsymbol{\theta}}, \boldsymbol{\theta}) = \mathbb{E}[\ell(\hat{\boldsymbol{\theta}}, \boldsymbol{\theta})]$ is the mean squared error. It can be shown that the unbiased ordinary estimator $\hat{\boldsymbol{\theta}}_0 (\mathbf{X}) = \mathbf{X}$ with $R(\hat{\boldsymbol{\theta}}_0, \boldsymbol{\theta}) = N$ is admissible for squared error loss when $N = \Set{1,2}$. However, for $N \geq 3$, the James-Stein estimator 

$$
  \hat{\boldsymbol{\theta}}_{\text{JS}}(\mathbf{X}) = \left(1 - \frac{N - 2}{\lVert\mathbf{X}\rVert^2} \right)\mathbf{X}
$$

dominates $\boldsymbol{\theta}^0$, i.e. $R(\hat{\boldsymbol{\theta}}_{\text{JS}}, \boldsymbol{\theta}) \leq R(\hat{\boldsymbol{\theta}}_0, \boldsymbol{\theta})$. Geometrically, the James-Stein estimator shrinks each component of $\mathbf{X}$ towards $\boldsymbol{\theta}$ by reducing the variance (at the expense of introducing bias). This can be shown by computing the risk of the James-Stein estimator:

$$
\begin{align*}
  R(\hat{\boldsymbol{\theta}}_{\text{JS}}, \boldsymbol{\theta}) =& \mathbb{E}\left(\left\lVert \left(1 - \frac{N - 2}{\lVert\mathbf{X}\rVert^2} \right)\mathbf{X} - \boldsymbol{\theta} \right\rVert^2 \right) \\
  =& \mathbb{E}\left(\left\lVert \mathbf{X} - \boldsymbol{\theta} - \frac{(N - 2)\mathbf{X}}{\lVert\mathbf{X}\rVert^2} \right\rVert^2 \right) \\
  =& \mathbb{E}(\lVert\mathbf{X} - \boldsymbol{\theta}\rVert^2) + \mathbb{E}\left(\left\lVert \frac{(N - 2)\mathbf{X}}{\lVert\mathbf{X}\rVert^2} \right\rVert^2 \right) \\
  &- \mathbb{E}\left(2(N - 2) \sum_{n=1}^N \frac{(X_n - \theta_n)X_n}{\lVert\mathbf{X}\rVert^2} \right) \\
  =& N + (N - 2)^2 \mathbb{E}\left(\frac{1}{\lVert\mathbf{X}\rVert^2}\right) - 2(N - 2) \sum_{n=1}^N \mathbb{E}\left(\frac{(X_n - \theta_n)X_n}{\lVert\mathbf{X}\rVert^2}\right)
\end{align*}
$$

For $n=1$, the expectation inside the sum is given by

$$
\begin{align*}
  \mathbb{E}\left(\frac{(X_1 - \theta_1)X_1}{\lVert\mathbf{X}\rVert^2} \right) =& \int_{-\infty}^\infty \cdots \int_{-\infty}^\infty \frac{(x_1 - \theta_1)x_1}{\lVert\mathbf{x}\rVert^2} \frac{1}{(2\pi)^{N/2}}e^{-\lVert\mathbf{x} - \boldsymbol{\theta}\rVert^2 / 2} \;\d x_1\cdots\d x_N
\end{align*}
$$

Noting that

$$
  \frac{\partial}{\partial x_1} \frac{1}{(2\pi)^{N/2}}e^{-\lVert\mathbf{x} - \boldsymbol{\theta}\rVert^2 / 2} = -\frac{x_1 -\ theta_1}{(2\pi)^{N/2}}e^{-\lVert\mathbf{x} - \boldsymbol{\theta}\rVert^2 / 2}
$$

we integrate by parts to obtain

$$
\begin{align*}
  \mathbb{E}\left(\frac{(X_1 - \theta_1)X_1}{\lVert\mathbf{X}\rVert^2} \right) =& \frac{1}{(2\pi)^{N/2}} \int_{-\infty}^\infty \cdots \big(\underbrace{\left[-\frac{x_1}{\lVert\mathbf{x}\rVert^2} e^{-\lVert\mathbf{x} - \boldsymbol{\theta}\rVert^2 / 2} \right]_{-\infty}^\infty}_{=0} \\
  &+ \left. \int_{-\infty}^\infty e^{-\lVert\mathbf{x} - \boldsymbol{\theta}\rVert^2 / 2} \d\left(\frac{x_1}{\lVert\mathbf{x}\rVert^2}\right) \right) \cdots\d x_N \\
  =& \int_{-\infty}^\infty \cdots \int_{-\infty}^\infty \frac{\lVert\mathbf{x}\rVert^2 - 2x_1^2}{\lVert\mathbf{x}\rVert^4} \frac{1}{(2\pi)^{N/2}} e^{-\lVert\mathbf{x} - \boldsymbol{\theta}\rVert^2 / 2} \;\d x_1 \cdots\d x_N \\
  =& \mathbb{E}\left(\frac{\lVert\mathbf{X}\rVert^2 - 2X_1^2}{\lVert\mathbf{X}\rVert^4}\right)
\end{align*}
$$

where we have used that 

$$
\frac{\d}{\d x_1} \left(\frac{x_1}{\lVert\mathbf{x}\rVert^2}\right) = \frac{\lVert\mathbf{x}\rVert^2 - 2x_1^2}{\lVert\mathbf{x}\rVert^4}
$$  

Repeating the process for $n = 2,\dots,N$, we can write the summed expectation as

$$
\begin{align*}
  \sum_{n=1}^N \mathbb{E}\left(\frac{(X_n - \theta_n)X_n}{\lVert\mathbf{X}\rVert^2}\right) = \mathbb{E}\left(\sum_{n=1}^N \frac{\lVert\mathbf{X}\rVert^2 - 2X_i^2}{\lVert\mathbf{X}\rVert^4} \right) \\
  =& \mathbb{E}\left(\frac{N\lVert\mathbf{X}\rVert^2 - 2\lVert\mathbf{X}\rVert^2}{\lVert\mathbf{X}\rVert^4}\right) \\
  =& (N - 2)\mathbb{E}\left(\frac{1}{\lVert\mathbf{X}\rVert^2} \right)
\end{align*}
$$

Inserting back into $R(\hat{\boldsymbol{\theta}}_{\text{JS}}, \boldsymbol{\theta})$ yields

$$
\begin{align*}
  R(\hat{\boldsymbol{\theta}}_{\text{JS}}, \boldsymbol{\theta}) =& N + (N - 2)^2 \mathbb{E}\left(\frac{1}{\lVert\mathbf{X}\rVert^2} \right) - 2(N - 2)^2 \mathbb{E}\left(\frac{1}{\lVert\mathbf{X}\rVert^2} \right) \\
  =& N - (N - 2)^2 \mathbb{E}\left(\frac{1}{\lVert\mathbf{X}\rVert^2} \right)
\end{align*}
$$

Hence, it follows that

$$
  R(\hat{\boldsymbol{\theta}}_{\text{JS}}, \boldsymbol{\theta}) = N - (N - 2)\mathbb{E}\left(\frac{1}{\lVert\mathbf{X}\rVert^2} \right) < N = R(\hat{\boldsymbol{\theta}}_0, \boldsymbol{\theta})
$$

for $N \geq 3$.
</MathBox>

# Empirical risk minimization

## Empirical risk

## Structural risk

## Cross-validation