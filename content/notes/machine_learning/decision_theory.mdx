---
title: 'Decision Theory'
subject: 'Machine Learning'
showToc: true
references:
  - book_murphy_2022
---

# Bayesian decision theory

In decision theory, we asumme the decision maker, or *agent*, has a set of possible actions $A$ to choose from. Each action $a\in A$ has costs and benefits depending on the underlying state $h \in H$. This can be encoded into a *loss function* $\ell(h,a)$ specifying the loss we incur if we take action $a\in A$ when the state is $h\in H$.

The *posterior expected loss* measures the *risk* for each possible action $a\in A$ given all the relevant evidence $\mathbf{x}\in E$, and is defined as

$$
  \rho(a|\mathbf{x}) := \mathbb{E}_{p(h|\mathbf{x})} [\ell(h,a)]
$$

The *optimal policy* $\pi^* (\mathbf{x})$, also called the *Bayes estimator* or *Bayes decision rule* $\delta^* (\mathbf{x})$, specifies what action to take when presented with evidence so as to minimize the risk, i.e.

$$
  \pi^* (\mathbf{x}) = \argmin{a\in A} \mathbb{E}_{p(h|\mathbf{x})} [\ell(h,a)]
$$

Equivalently, we can define a *utility function* $U(h,a)$ to be the desirability of each possible action in each possible state. Setting $U(h,a) = -\ell(h,a)$, the optimal policy is

$$
  \pi^* (\mathbf{x}) = \argmax{a\in A} \mathrm{E}_h [U(h,a)]
$$

This is called the *maximum expected utility principle*.

Standard Bayesian decision theory implicitly assumes that the agent is *risk neutral*, which means that its decision is not affected by the degree of certainty in a set of outcomes. The theory can be generalized by introducing *risk sensitivity*, in which case the agent can have the following risk profiles:
- *risk averse*

## Classification problems

### Zero-one loss

Suppose the states correspond to class labels, i.e. $H = Y = \set{1,\dots, C}$. Furthermore, suppose the actions also class labels, i.e. $A = Y$. In this setting, a very commonly used loss function is the *zero-one-loss* $\ell_{01} (y^*, \hat{y})$ defined as

$$
\begin{array}{c|cc}
  & \hat{y} = 0 & \hat{y} = 1 \\ \hline
  y^* = 0 & 0 & 1 \\
  y^* = 1 & 1 & 0 
\end{array}
$$

We can write this concisely as $\ell_{01} (y^*, \hat{y}) = \mathbf{1}(y^* \neq\hat{y})$. In this case, the posterior expected loss is

$$
  \rho(\hat{y}|\mathbf{x}) = p(\hat{y}\neq y^* | \mathbf{x}) = 1 - p(y^* = \hat{y} | \mathbf{x})
$$

The action that minimizes the expected loss is to choose the most probable label, i.e. $\pi^* (\mathbf{x}) = \argmax_{y\in Y} p(y|\mathbf{x})$. This corresponds to the *mode* of the posterior distribution, also known as the *maximum a posteriori estimate*. 

### Cost-sensitive classification

Consider a binary classification problem where the loss function $\ell(y^*, \hat{y})$ is defined as

$$
  \begin{bmatrix}
    \ell_{00} & \ell_{01} \\
    \ell_{10} & \ell_{11}
  \end{bmatrix}
$$

Let $p_0 = p(y^* = 0 | \mathbf{x})$ and $p_1 = 1 - p_0$. Thus we should choose label $\hat{y} = 0$ if and only if

$$
  \ell_{00} p_0 + \ell_{10} p_1 < \ell_{01} p_0 + \ell_{11} p_1
$$

If $\ell_{00} = \ell_{11} = 0$, this simplifies to

$$
  p_1 < \frac{\ell_{01}}{\ell_{01} + \ell_{10}}
$$

Now suppose $\ell_{10} = c\ell_{01}$, so a false negative costs $c$ times more than a false position. In this case, the optimal policy is to pick $a = 0$ if and only if $p_1 < 1/(1 + c)$.

### Classification with rejection

A risk averse agent may choose to ignore a classification due to high uncertainty, which is called the *reject option*. Suppose the states are $H = Y = \set{1,\dots,C}$, and the actions are $A = Y\cup\set{0}$, where action $0$ represents the reject action. Define the following loss function:

$$
  \ell(y^*, a) = \begin{cases}
    0,\quad& y^* = a \in\set{1,\dots,C} \\
    \lambda_r,\quad& a=&0 \\
    \lambda_e,\quad& \text{otherwise}
  \end{cases}
$$

where $\lambda_r$ is the cost of the rejection action, and $\lambda_e$ is the cost of a classification error. In this case, the optimal policy is given by *Chow's rule*

$$
  a^* = \begin{cases}
    y^*, \quad& p^* > y^* \\
    0, \quad& \text{otherwise}
  \end{cases}
$$

where

$$
\begin{align*}
  y^* =& \argmax_{y\in\set{1,\dots,C}} \\
  p^* =& p(y^* | \mathbf{x}) = \max_{y\in\set{1,\dots,C}} p(y|\mathbf{x}) \\
  \lambda^* = 1 - \frac{\lambda_r}{\lambda_e}
\end{align*}
$$

### Class confusion matrix

For any fixed threshold $\tau\in(0,1)$, we consider the decision rule

$$
  \hat{y}_\tau (\mathbf{x}) = \mathbf{1}(p(y = 1|\mathbf{x})\geq 1 - \tau)
$$

We can compute the empirical number of false positives (FP) that arise from using this policy on a set of $N$ labeled examples as follows:

$$
  \operatorname{FP}_\tau = \sum_{n=1}^N \mathbf{1}(\hat{y}_\tau(\mathbf{x}) = 1, y_n = 0)
$$

Similarly, we can compute the empirical number of false negatives (FN), true positives (TP) and true negatives. We can store these result in a $2\times 2$ *class confution matrix* $C$, where $C_{ij}$ is the number of times an item with true class label $i$ wa (mis)classified as having label $j$. In the case of binary classification problems, the class confusion matrix is

$$
\begin{array}{cc|c|c}
  & & \text{Estimate} & \text{Row sum} \\
  & & \begin{array}{cc} 0 & 1 \end{array} & \\ \hline
  \text{True} & \begin{array}{cc} 0 \\ 1 \end{array} & \begin{array}{c} \operatorname{TN} & \operatorname{FP} \\ \operatorname{FN} & \operatorname{TP} \end{array} & \begin{array}{c} N \\ P \end{array} \\ \hline \\
  \text{Col. sum} & & \begin{array}{cc} \hat{N} & \hat{P} \end{array}
\end{array}
$$

We can derive various summary statistics from these distributions. In terms of $p(\hat{y}|y)$, we have the following statistics:
- The *true negative rate* (TNR), also called the *specificity* given by
$$
  \operatorname{TNR}_\tau := p(\hat{y} = 0 | y = 0, \tau) = \frac{\operatorname{TN}_\tau}{\operatorname{TN}_\tau + \operatorname{FP}_\tau}
$$
- The *false positive rate* (FPR), also called the *false alarm rate*, or the *type I error rate* given by
$$
  \operatorname{FPR}_\tau := p(\hat{y} = 1 | y = 0, \tau) = \frac{\operatorname{FP}_\tau}{\operatorname{FP}_\tau + \operatorname{TN}_\tau}
$$
- The *false negative rate* (FNR), also called the *miss rate*, or the *type II error rate* given by
$$
  \operatorname{FNR}_\tau := p(\hat{y} = 0 | y = 1, \tau) = \frac{\operatorname{FN}_\tau}{\operatorname{FN}_\tau + \operatorname{TP}_\tau}
$$
- The *true positive rate* (TPR), also known as the *sensitivity*, *recall* or *hit rate* given by
$$
  \mathcal{R}(\tau) = \operatorname{TPR}_\tau := p(\hat{y} = 1 | y = 1, \tau) = \frac{\operatorname{TP}_\tau}{\operatorname{FN}_\tau + \operatorname{TP}_\tau}
$$

<TableFigure caption="Class confusion matrix for a binary classification problem normalized per row to get $p(\hat{y}|y)$.">
$$
\begin{array}{cc|c}
  & & \text{Estimate} \\
  & & \begin{array}{cc} 0 & 1 \end{array} \\ \hline
  \text{Truth} & \begin{array}{c} 0 \\ 1 \end{array} & \begin{array}{cc}
    \operatorname{TN}/N = \operatorname{TNR} & \operatorname{FP}/N = \operatorname{FPR} \\
    \operatorname{FN}/P = \operatorname{FNR} & \operatorname{TP}/P = \operatorname{TPR}
  \end{array}
\end{array}
$$
</TableFigure>

For $p(y|\hat{y})$, we have the following statistics
- The *negative predictive value* (NPV) given by
$$
  \operatorname{NPV}_\tau = p(y = 0 | \hat{y} = 0, \tau) := \frac{TN_\tau}{\operatorname{TN}_\tau + \operatorname{FN}_\tau}
$$
- The *false discovery rate* (FDR) given by
$$
  \operatorname{FDR}_\tau = p(y = 0 | \hat{y} = 1, \tau ) := \frac{FP_\tau}{\operatorname{FP}_\tau + \operatorname{TP}_\tau}
$$
- The *false omission rate* (FOR) given by
$$
  \operatorname{FOR}_\tau = p(y = 1 | \hat{y} = 0, \tau ) := \frac{FN}{\operatorname{TN}_\tau + \operatorname{FN}_\tau}
$$
- The *positive predictive value* (PPV), also known as the precision, given by
$$
  \mathcal{P}(\tau) = \operatorname{PPV}_\tau := p(y = 1 | \hat{y} = 1, \tau ) = \frac{TP}{\operatorname{FP}_\tau + \operatorname{TP}_\tau}
$$

<TableFigure caption="Class confusion matrix for a binary classification problem normalized per columb to get $p(y|\hat{y})$.">
$$
\begin{array}{cc|c}
  & & \text{Estimate} \\
  & & \begin{array}{cc} 0 & 1 \end{array} \\ \hline
  \text{Truth} & \begin{array}{c} 0 \\ 1 \end{array} & \begin{array}{cc}
    \operatorname{TN}/\hat{N} = \operatorname{NPV} & \operatorname{FP}/\hat{P} = \operatorname{FDR} \\
    \operatorname{FN}/\hat{N} = \operatorname{FOR} & \operatorname{TP}/\hat{P} = \operatorname{PPV}
  \end{array}
\end{array}
$$
</TableFigure>

### Receiver operating characteristic (ROC) curves

The relation between the true positive and false positive rates is called *receiver operating characteristic* (ROC). The quality of a ROC curve is often summarized as a single number using the area under the curve (AUC). Higher AUC scores are better. Another summary statistic is the *equal error rate* (EER), also called the *cross-over rate*, defined as the value which satisfies $\operatorname{FPR} = \operatorname{FNR}$. Since $\operatorname{FNR} = 1 - \operatorname{TPR}$, we can compute the EER by drawing a line from the top left to the bottom right and seeing where it intersects the ROC curve. Lower EER scores are better.

The ROC curve is unaffected by class imbalance, as the TPR and FRP are fractions within the positive and negatives, respectively. The usefulness of the ROC curve may be reduced in such cases, since a large change in the absolute number of false positives will not change the false positive rate very much.

### Precision-recall (PR) curves

An alternative to ROC curves are precision-recall (PR) curves. The quality of a PR curve is often summarized in a single number. First, we can quote the precision for a fixed recall level. This is called the precision at $K$ score. Alternatively, we can compute the are under the PR curve. However, it is possible that the precision does not drop monotonically with recal. In this case, rather than measuring the precision at $K$ score. This is called the *interpolated precision*. The average of the interpolated precision is called the average precision; it is equal to are under the interpolated PR curve, but may not be equal to the area under the PR curve. The *mean average precision* (mAP) is the mean of the AP over a set of different PR curves.

### F-scores

For a fixed threshold, corresponding to a single point on the PR curve, we compute a single precision and recall value, which we wil denote by $\mathcal{P}$ and $\mathcal{R}$. These are often combined into a single statistic called $F_\beta$, defined as follows

$$
  \frac{1}{F_\beta} = \frac{1}{1 + \beta^2}\frac{1}{\mathcal{P}} + \frac{\beta^2}{1 + \beta^2}\frac{1}{\mathcal{R}}
$$

Setting $\beta = 1$, we get the harmonic mean of presicion and recall

$$
\begin{align*}
  \frac{1}{F_1} = \frac{1}{2}\left(\mathbf{1}{\mathcal{P}} + \frac{1}{\mathcal{R}}) \\
  F_1 = \frac{2}{1/\mathcal{R} + 1/\mathcal{P}} = 2\frac{\mathcal{P}\mathcal{R}}{\mathcal{p} + \mathcal{R}} = \frac{\operatorname{TP}}{\operatorname{TP} + \frac{1}{2}(\operatorname{FP} + \operatorname{FN})}
\end{align*}
$$

In general, the harmonic mean is more conservative thant then the artithmetic mean.

Contrary to ROC case, PR curves are sensitive to class imbalance. To see this, let the fration of positive in the dataset be $\pi = P/(P + N)$, and define the ratio $r = P/N = \pi/(1 - \pi)$. Let $n = P+N$ be the population size. ROC curves are not affected by changes in $r$, since the TPR is defined as a ratio within the positive examples, and FPR is defined as a ratio within the negative examples. This means it does not matter which class we define as positive, and which we define as negative.

Now consider PR curves. The precision can be expressed as

$$
  \operatorname{PPV} = \frac{\operatorname{TP}}{\operatorname{TP} + \operatorname{FP}} = \frac{P\cdot\operatorname{TPR}}{P\cdot\operatorname{TPR} - N\cdot\operatorname{FPR}} = \frac{\operatorname{TPR}}{\operatorname{TPR} + \frac{1}{r}\operatorname{FPR}}
$$

Thus, $\operatorname{PPV}\xrightarrow{\pi\to 1, r\to\infty} 1$ and $\operatorname{PPV}\xrightarrow{\pi\to 0, r\to 0}$. The $F$-score is also affected by class imbalance. To see this, note that we can rewrite the $F$-score as

$$
\begin{align*}
  \frac{1}{F_\beta} =& \frac{1}{1 + \beta^2}\frac{1}{\mathcal{P}} + \frac{\beta^2}{1 + \beta^2}\frac{1}{\mathcal{R}} \\
  =& \frac{1}{1 + \beta^2} \frac{\operatorname{TPR} + \frac{N}{P}\operatorname{FPR}}{\operatorname{TPR}} + \frac{1}{1 + \beta} \frac{1}{\operatorname{TPR}} \\
  F_\beta =& \frac{(1 + \beta^2) \operatorname{TPR}}{\operatorname{TPR} + \frac{1}{r}\operatorname{FPR} + \beta^2}
\end{align*}
$$

## Regression problems

In regression problems the action and state spaces continuous, i.e. $A, H\subseteq\R$.

### Quadratic loss ($\ell_2$ loss)

The quadratic loss, also known as the $\ell_2$ loss or squared error, is defined as

$$
  \ell_2 (h,a) = (h - a)^2
$$

In this case, the risk is given by

$$
  \rho(a|\mathbf{x}) = \mathbb{E}[(h-a)^2 | \mathbf{x}] = \mathbb{E})(h^2 | \mathbf{x}) - 2a\mathbb{E}[h|\mathbf{x}] + a^2
$$

The optimal action must satisfy that the derivative of the risk is zero. Hence, the optimal action is to pick the posterior mean

$$
  \frac{\partial}{\partial a} \rho(a|\mathbf{x}) = -2\mathbb{E}(h|\mathbf{x}) + 2a \implies \pi(\mathbf{x}) = \mathbb{E}[h|\mathbf{x}] = \int h p(h|\mathbf{x})\;\d h
$$

This called the minimum mean squared error (MMSE) estimate. Since the $\ell_2$ loss penalizes deviations from the truth quadratically, it is sensitive to *outliers*.

### Absolute loss ($\ell_1$ loss)

The absolute loss, or $\ell_1$ loss is given by

$$
  \ell_1 (h, a) = |h-a|
$$

The optimal estimate is the *posterior medien*, i.e. value such that $p(h < a|\mathbf{x}) = p(h\geq a | \mathbf{x}) = 0.5$

### Huber loss

The *Huber loss* is defined as follows

$$
  \ell_\delta(h,a) = \begin{cases}
    r^2 / 2,\quad |r|\leq\delta \\
    \delta|r| - \delta^2 / 2,\quad |r| > \delta
  \end{cases}
$$

## Probabilistic prediction problems

In probabilistic predictions problem, we assume that the state and action spaces are distributions, denoted $h = p(Y|\mathbf{x})$ dn $a = q(Y|\mathbf{x})$, respectively. In this setting, we want to pick $q$ to to minimize $\mathbb{E}[\ell(p,q)]$ for each given $\mathbf{x}$.

### Proper scoring rule

A loss function for a probabilistic prediction problem is known as a *scoring rule*. A scoring rule $\ell(p,q)$ is *proper* if it satisfies the following propert: the minimum expectation of $\ell$ occurs for the distribution $q$ that matches the true distribution $p$, i.e. $\ell(p,p) \leq \ell(p,q)$, with equality if and only if $p = q$.

### Kullback-Leibler divergence

A common loss function for comparing two distributions is the *Kullback-Leibler (KL) divergence*, which is defined as

$$
  D_{\text{KL}} (p \Vert q) := \sum_{y\in Y} p(y) \ln\frac{p(y)}{q(y)}
$$

where we have assumed that $Y$ is discrete. We can expand the KL as follows

$$
\begin{align*}
  D_{KL} (p \Vert q) =& \sum_{y\in Y} p(y) \ln[p(y)] - \sum_{y\in Y} p(y) \ln[q(y)] \\
  =& -\mathbb{H}(p) + \mathbb{H}_{\text{ce}} (p,q) \\
  \mathbb{H}(p) :=& -\sum_{y\in Y} p(y) \ln[p(y)]\\
  \mathbb{H}_{\text{ce}}(p,q) := -\sum_{y\in Y} p(y) \ln[q(y)] 
\end{align*}
$$

The $\mathbb{H}(p)$ term is known as the entropy. This is a measure of uncertainty or variance of $p$; it is maximal if $p$ is uniform, and is zero if $p$ is a degenerate or deterministic delta function. The $\mathbb{H}_{\text{ce}}(p,q)$ term is known as the cross-entropy. This measures the expected number of bits we need to use to compess a dataset coming from distribution $p$ if we design our code using distribution $q$. Thus, the KL is the extra number of bits we need to use to compress the data due to using the incorrect distribution $q$. If the KL is zero, it means that we can correctly predict the probabilities of all possible future events. The $\ln[p(y)/q(y)]$ term in the KL loss can be quite sensitive to errors for low probability events.

To find the optimal distribution to use when predicting futute data, we can minimize $D_{\text{KL}} (p \lVert q)$. Since the entropy $\mathbb{H}(p)$ is independent of $q$, we can equivalently minimize the cross-entropy $\mathbb{H}_{\text{ce}} (p,q)$ 

$$
  q^* (Y) = \argmin_q \mathbb{H}_{\text{ce}}(p(Y), q(Y))
$$

Now consider the secial case in which the true state is a degenerate distribution, which put all its mass on a single outcome, say $c$, i.e. $h = p(Y) = \mathbf{1}(Y = c)$. This is called a *one-hot* distribution, since it "turns on" the $c$th element of the vector, and leaves the other elements "off". In this case, the cross entropy becomes

$$
  \mathbb{H}_{\text{ce}} (\delta(Y = c), q) = -\sum_{y\in Y} \delta (y = c) \ln[q(y)] = -\ln[q(y)]
$$

This is known as the *log loss* of the predictive distribution $q$ when given target label $c$.

The cross-entropy loss is a scoring rule by virtue of the fact that $0 = D_{\text{KL}} (p \Vert p) \leq D_{\text{KL}} (p \Vert q)$.

### Brier scores

The Brier score (BS) is defined the special case that the true distribution $p$ is a set of $N$ delta functions, $\mathbf{p}_n (\mathbf{Y}_n) = \delta(\mathbf{Y}_n - \mathbf{y}_n)$, where $\mathbf{y}_n$ is the observed outcome in one-form, so $y_{nc} = 1$ if the $n$th observed outcome is class $c$. The corresponding predictive distribution is assumed to be as set of $N$ distributions $\mathbf{q}_n (\mathbf{Y}_n)$, which can be conditioned on covariates $\mathbf{x}_n$. In this setting, the Brier score is defined as

$$
  \operatorname{BS}(\mathbf{p},\mathbf{q}) := \frac{1}{N} \sum_{n=1}^N \sum_{c=1}^C (q_{nc} - p_{nc})^2 = \frac{1}{N} \sum_{n=1}^N \sum_{c=1}^C (q_{nc} - y_{nc})^2
$$

This is just the mean squared error of the predictive distributions compared to the true distributions, when viewed as vectors. Since it is based on squared error, the Brier score is less sensitive to extremely rare or extremely common classes.

IN the special case of binary classification, where we use class labels $c\in\set{0,1}$, we define $y_n = y_{n1}$ and $q_n = q(Y_{n1})$, so the summand becomes $(q_n - y_n)^2 + (1 - q_n - (1 - y_n))^2 = 2(q_n - y_n)^2$. Consequently, in the binary csae, we often divide the multi-class definition by $2$, to get the binary Brier score

$$
  \operatorname{BS}(\mathbf{p},\mathbf{q}) = \frac{1}{N}\sum_{n=1}^N (q_n - y_n)^2 \in [0,1]
$$

Since it can be hard to interpret absolyte Brier score values, a relative performance measure, known as the *Brier skill score* (BSS), is used. This is defined as $\operatorname{BSS} = 1 - \operatorname{BS}/\operatorname{BS}_{\text{ref}}$, where $\operatorname{BS}_{\text{ref}}$ is the BS of a reference model. The range over score is $\mathbf{BSS}\in[-1,1]$, with $1$ being the best, $0$ meaning no improvement over the baseline, and $-1$ being the worst. In the case of binary predictors, a common reference model is the baseline empirical probability $\bar{q} = \frac{1}{N} \sum_{n=1}^N y_n$. In the meteorological community, this is called the "in-sample climatology" prediction, where "in-sample" means based on the observed data, and "climatology" refers to long run average behaviour.

# Model selection

## Information criteria

### Bayesian information criterion (BIC)

### Akaike information criterion (AIC)

### Minimum description length (MDL)

### Widely applicable information criterion (WAIC)

# Frequentist decision theory

