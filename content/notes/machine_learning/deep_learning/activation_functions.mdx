---
title: 'Activation Functions'
subject: 'machine learning/deep learning'
showToc: true
references:
  - book_calin_2020
  - book_mohri_etal_2012
  - book_murphy_2022
  - book_shalev-schwartz_ben-david_2014
  - book_ye_2022
---

<MathBox title="Multidimensional extension of a real-valued function" boxType="definition">
Let $f:\R\to\R$ be a real-valued function, and for $T\in\N_+$ let $d_1,\dots,d_T \in\N_+$. Define the map

$$
  \mathbf{M}_{f, d_1,\dots, d_T} : \R^{d_1 \times\cdots\times d_T} \to\R^{d_1 \times\cdots\times d_T}
$$

by its action on arbitrary multidimensional arrays

$$
  \mathbf{x} = \left(x_{k_1,\dots,k_T \right)_{(k_1,\dots,k_T)\in \prod_{t=1}^T \set{1,\dots,d_t}} \in \R^{d_1 \times\cdots\times d_T}
$$

according to

$$
  \mathbf{M}_{f, d_1,\dots,d_T} (\mathbf{x}) := \left(f(x_{k_1,\dots,k_T}) \right)_{(k_1,\dots,k_T)\in\prod={t=1}^T \set{1,\dots, d_t}}
$$

Equivalently, for all index tuples

$$
  (k_1,\dots,k_T) \in \Prod_{t=1}^T \set{1,\dots,d_t}
$$

the components of $\mathbf{M}_{f, d_1,\dots,d_T} (\mathbf{x})$ satisfy

$$
  [\mathbf{M}_{f, d_1,\dots,d_T} (\mathbf{x})]_{k_1,\dots,k_T} = f(x_{k_1,\dots,k_T})
$$

The map $\mathbf{M}_{f, d_1,\dots, d_T}$ is called the multidimensional (or componentwise) extension of the real-valued function $f$ to multidimensional arrays of shape $d_1 \times\cdots\times d_T$.
</MathBox>

# Heaviside function

$$
  H(x) = \mathbf{1}_{x\geq 0} (x) := \begin{cases}
    1, \quad x\geq& 0 \\
    0, \quad x < 0
  \end{cases}
$$

An artificial neuron using the Heaviside activitation function is known as a *perceptron*.

# Rectified linear unit (ReLU)

$$
  \sigma(\lambda) = \operatorname{ReLU}(\lambda) := \max\set{0,\lambda}
$$

# Sigmoid

The (logistic) sigmoid function $\sigma:\R\to[0,1]$ is given by 

$$
  \sigma(\lambda) := \frac{1}{1 + e^{-\lambda}}
$$

The term "sigmoid" means S-shaped. The sigmoid function can be thought of as a "soft" version of the Heaviside step function.

<GraphFigure 
  expression="1/(1 + exp(-x))"
  points={100}
  xAxis={{scale: "linear", domain: [-2,2]}}
  yAxis={{scale: "linear", domain: [0,1]}}
  caption="Sigmoid function"
/>

<MathBox title='Properties of the sigmoid function' boxType='proposition'>
1. $\sigma'(\lambda) = \sigma(\lambda)(1 - \sigma(\lambda))$
2. $1 - \sigma(\lambda) = \sigma(\lambda)$
3. $\sigma^{-1} (\lambda) = \ln\left(\frac{\lambda}{1 - \lambda} \right) =: \operatorname{logit}(\lambda)$
4. $\sigma_+ (\lambda) := \ln(1 + e^\lambda) =: \operatorname{softplus}(\lambda)$
5. $\sigma'_+ (\lambda) = \sigma(\lambda)$
</MathBox>

# Hyperbolic tangent

$$
  \tanh(\lambda) := \frac{e^\lambda - e^{-\lambda}}{e^\lambda + e^{-\lambda}}
$$

<GraphFigure 
  expression="tanh(x)"
  points={100}
  xAxis={{scale: "linear", domain: [-2,2]}}
  yAxis={{scale: "linear", domain: [-1,1]}}
  caption="Hyperbolic tangent"
/>

# Swish

$$
  \operatorname{swish}_\beta (\lambda) := \lambda \sigma(\beta\lambda) := \frac{\lambda}{1 + e^{-\beta\lambda}},\; \beta > 0
$$
In case of $\beta = 1$, this function is also called the *sigmoid-weighted linear unit* (SiLU).

# Softmax

Softmax also known as the normalized exponential function: $\operatorname{softmax}:\R^n \to[0,1]^n$

$$
  \operatorname{softmax}(\mathbf{x}) := \frac{e^\mathbf{x}}{\sum_{i=1}^n e^{x_i}}
$$
Softmax has the useful property that its output is a discrete probability distribution.

# Maxpool

$\operatorname{maxpool}:\R^n \to\R^m$

$$
  \operatorname{maxpool}\left(\begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix} \right) := \begin{bmatrix}
    \max_{j\in I_1} x_j \\
    \vdots \\
    \max_{j\in I_m} x_j
  \end{bmatrix}
$$

where for each $i\in\set{1,\dots,m}$ we have a $I_i \subset\set{1,\dots,n}$ that specifies over which inputs to take the maximum for each output. Maxpooling can easily be generalized by replacing the max operation.

# Normalization

$\operatorname{normalize}:\R^n \to\R^n$:

$$
  \operatorname{normalize}(\mathbf{x}) := \frac{\mathbf{x} - \mu\mathbf{1}_n}{\sigma}
$$
where $\mu = \mathbb{E}(\mathbf{x})$ and $\sigma^2 = \operatorname{var}(\mathbf{x})$.

# Dropout

Dropout is a stochastic function that is often used during the training process but is removed once the training is finished. It works by randomly setting individuals values a signal to zero with probability $p$

$$
  \operatorname{dropout}_p (\mathbf{x})_i := \begin{cases}
    0 \quad &\text{with probability $p$} \\
    x_1 \quad &\text{with probability $p - 1$}
  \end{cases}
$$

# Heatbath

Heatbath is a scalar function that outputs $1$ or $-1$ with a probability that depends on the input
$$
  \operatorname{heatbath}(\lambda) := \begin{cases}
    1 \quad &\text{with probability $p$} \\
    -1 \quad &\text{otherwise}
  \end{cases}
$$
