---
title: 'Artificial Neural Networks'
subject: 'machine learning/deep learning'
showToc: true
references:
  - book_calin_2020
  - book_mohri_etal_2012
  - book_murphy_2022
  - book_shalev-schwartz_ben-david_2014
  - book_ye_2022
---

A neural network can be represented by a directed graph whose nodes correspond to neurons and edges correspond to links between them. Each neuron receives as input a weighted sum of the outputs of the neurons connected to its incoming edges.

# Artificial neuron (unit)

An artificial neuron or a computing unit is typically modelled as an affine transformation followed by a non-linear function. If $\mathbf{x}\in X\subseteq \mathbb{F}^n$ is the input signal and $\mathbf{y}\in Y\subseteq \mathbb{F}^m$ is the out signal, an artifical neuron takes the form

$$
  \mathbf{y} = \sigma(\mathbf{Wx} + \mathbf{b})
$$

where
- $\mathbf{W}\in\mathbb{F}^{m\times n}$ represents linear weights
- $\mathbf{b}\in\mathbb{F}^m$ is the bias
- $\sigma: \mathbb{F}^m \to \mathbb{F}^m$

The input and output spaces can take any form, e.g.
- $\set{0,1}$: binary
- $\set{\pm 1}:$ boolean
- $[0,1]$: unit interval (representing probabilities)

# Feedforward neural network (FFNN)

<MathBox title='Parametric affine function' boxType='definition'>
Let $\boldsymbol{\theta} \in\Theta\subseteq\R^p$ be a parameter vector of dimension $p\in\N_+$. For $m, n\in\N_+$ and $s\in\N_0$ satisfying $p \geq s + mn + m$, we define an affine function $A_{m,n}^{\boldsymbol{\theta}, s} :\R^n \to \R^m$ associated with parameters $\boldsymbol{\theta}$, starting from index $s$, as

$$
  A_{m,n}^{\boldsymbol{\theta}, s} (\mathbf{x}) = \mathbf{Wx} + \mathbf{b}
$$

where the weight matrix $\mathbf{W}\in\R^{m\times n}$ and bias vector $\mathbf{b}\in\R^m$

$$
  \mathbf{W} = \begin{bmatrix} 
    \theta_{s+1} & \cdots & \theta_{s + n} \\
    \theta_{s + n + 1} & \cdots & \theta_{s + 2n} \\
    \vdots & \ddots & \vdots \\
    \theta_{s + (m - 1)n + 1} & \cdots & \theta_{s + mn}  
  \end{bmatrix},\quad \mathbf{b} = \begin{bmatrix} \theta_{s + mn + 1} \\ \theta_{s + mn + 2} \\ \vdots \\ \theta_{s + mn + m} \end{bmatrix}
$$

Explicitly, the function can be written component-wise as

$$
  A_{m,n}^{\boldsymbol{\theta}, s} (\mathbf{x}) = \begin{bmatrix} 
    \left(\sum_{k=1}^n x_k \theta_{s + k} \right) + \theta_{s + mn + 1} \\ 
    \left(\sum_{k=1}^n x_k \theta_{s + n + k} \right) + \theta_{s + mn + 2} \\
    \vdots \\
    \left(\sum_{k=1}^n x_k \theta_{s + (m - 1)n + k} \right) + \theta_{s + mn + m}
  \end{bmatrix}
$$
</MathBox>

<MathBox title='Fully-connected feedforward neural network (vectorized description)' boxType='definition'>
Let $\boldsymbol{\theta}\in\Theta\subseteq\R^p$ be a parameter vector of dimension $p\in\N_+$. A fully connected feedforward neural network (FFNN) with $L \in N_+$ layers is characterized by its architecture $a = (\mathbf{n}, \boldsymbol{\sigma})$ for $\mathbf{n} \in\N_+^{L+1}$, where
- $\mathbf{n} = (n_\ell)_{\ell=0}^L$ denotes the number of neurons in each layer $\ell$
- $\boldsymbol{\sigma} = (\sigma_\ell : \R\to\R)_{\ell=0}^L$ denotes the activation function applied componentwise to each layer $\ell$

Layer indexing follows the convention:
- $\ell = 0$: input layer
- $\ell = L$: output layer
- $\ell \in [L - 1]$: hidden layers 

In terms on neurons, the parameter dimension is given by

$$
  p = \sum_{\ell=1}^L n_\ell (n_{\ell - 1} + 1)
$$

The width and depth of the FFNN architecture are given by $\lVert N \rVert_\infty$ and $L$, respectively, where $N = \sum_{\ell=0}^L n_\ell$ is the total number of neurons. The architecture is called deep if $L > 2$ and shallow if $L = 2$.

The realization function of the FFNN, denoted $\Phi_a : \R^{n_0} \times \R^p \to\R^{n_L}$ is given by

$$
\begin{align*}
  \Phi_a (\mathbf{x}, \boldsymbol{\theta}) =& \sigma_L \circ A_{n_L, n_{L-1}}^{s_{L-1}} \circ \sigma_{L-1} \circ A_{n_{L-1}, n_{L-2}}^{s_{L-2}} \circ\cdots \\
  & \circ \sigma_1 \circ A_{n_1, n_0}^0 (\mathbf{x}, \boldsymbol{\theta})
\end{align*}
$$

where the shift indices $s_\ell$ are defined as

$$
  s_\ell = \sum_{k=1}^\ell n_k (n_{k-1} + 1),\; \ell = 1,\dots,L - 1
$$

For each hidden layer $\ell = 1,\dots, L - 1$, we define the pre-activation $\Phi^{(\ell)}: \R^{n_{\ell - 1}} \times \R^p \to \R ^{\boldsymbol{\theta}, n_{\ell - 1}}$ and activation $\bar{\Phi}^{(\ell)} : \R^{n_{\ell - 1}} \times \R^p \to \R ^{\boldsymbol{\theta}, n_{\ell - 1}}$ as

$$
  \bar{\Phi}^{(\ell)} = \sigma_\ell \left(\Phi^{(\ell)}\right)
$$

Each affine functions $A_{n_\ell, n_{\ell-1}}^{s_{n_{\ell-1}}}$ for $\ell = 1,\dots,L$, is given by

$$
  A_{n_\ell, n_{\ell-1}}^{s_{\ell-1}} (\mathbf{x}, \boldsymbol{\theta}) = \mathbf{W}^{(\ell)} \mathbf{x} + \mathbf{b}^{(\ell)}
$$

where the weights $\mathbf{W}^{(\ell)} \in \R^{n_\ell \times n_{\ell-1}}$ and biases $\mathbf{b}^{(\ell)} \in \R^{n_\ell}$ from layer $\ell - 1$ to $\ell$ are given by

$$
  \mathbf{W}^{(\ell)} = \begin{bmatrix}
    \theta_{s_{\ell - 1} + 1} & \cdots & \theta_{s_{\ell - 1} + n_{\ell - 1}} \\
    \theta_{s_{\ell - 1} + n_{\ell - 1} + 1} & \cdots & \theta_{s_{\ell - 1} + 2n_{\ell - 1}} \\
    \vdots & \ddots & \vdots \\
    \theta_{s_{\ell - 1} + (n_\ell - 1)n_{\ell - 1} + 1} & \cdots & \theta_{s_{\ell - 1} + n_\ell n_{\ell - 1}} 
  \end{bmatrix},\quad \mathbf{b}^{(\ell)} = \begin{bmatrix} \theta_{s_{\ell - 1} + n_\ell n_{\ell - 1} + 1} \\ \cdots \\ \theta_{s_{\ell - 1} + n_\ell n_{\ell - 1} + n_\ell} \end{bmatrix}
$$

In terms of $\mathbf{W}^{(\ell)}$ and  $\mathbf{b}^{(\ell)}$, the parameters $\boldsymbol{\theta}$ can be written as

$$
  \boldsymbol{\theta} = \left((\mathbf{W}^{(\ell)}, \mathbf{b}^{(\ell)})\right)_{\ell=1}^L \in \prod_{\ell=1}^L (\R^{n_\ell \times n_{\ell-1}} \times \R^{n_\ell}) \cong \R^p
$$
</MathBox>

<LatexFigure width={75} src='/fig/feedforward_neural_network.svg' alt=''
  caption='Feedforward neural network'
>
```latex
\documentclass[border=5pt]{standalone}
\usepackage{tikz}
\usepackage{amsmath}
\usetikzlibrary{arrows.meta, graphs, graphdrawing, positioning, backgrounds, fit, calc}
\usegdlibrary{layered}

\begin{document}
\begin{tikzpicture}[
    neuron/.style={circle, draw, minimum size=1.2cm, inner sep=0.5pt, font=\footnotesize},
    transformation/.style={font=\footnotesize, align=center},
    layer label/.style={font=\small\bfseries, align=center},
    dots/.style={font=\large},
    input background/.style={fill=red!5, rounded corners, draw=red!40, dashed,
      inner sep=10pt},
    hidden background/.style={fill=blue!5, rounded corners, draw=blue!40, dashed,
      inner sep=10pt},
    output background/.style={fill=green!5, rounded corners, draw=green!40, dashed,
      inner sep=10pt}
  ]

  % Constants for better layout
  \def\layerdist{3.5cm}
  \def\nodedist{1.8cm}
  \def\numLayers{6}

  % Layer 1: Input
  \node[neuron] (x1) at (0,0) {$x_1$};
  \node[neuron] (x2) at (0,-\nodedist) {$x_2$};
  \node[dots] (xdots) at (0,-2*\nodedist) {$\vdots$};
  \node[neuron] (x3) at (0,-3*\nodedist) {$x_{n_0}$};

  % Layer 2: First hidden layer
  \node[neuron] (h11) at (\layerdist,0) {$\Phi_1^{(1)}$};
  \node[neuron] (h12) at (\layerdist,-\nodedist) {$\Phi_2^{(1)}$};
  \node[dots] (h1dots) at (\layerdist,-2*\nodedist) {$\vdots$};
  \node[neuron] (h13) at (\layerdist,-3*\nodedist) {$\Phi_{n_1}^{(1)}$};

  % Layer 3: First hidden layer activated
  \node[neuron] (ph11) at (2*\layerdist,0) {$\bar{\Phi}_1^{(1)}$};
  \node[neuron] (ph12) at (2*\layerdist,-\nodedist) {$\bar{\Phi}_2^{(1)}$};
  \node[dots] (ph1dots) at (2*\layerdist,-2*\nodedist) {$\vdots$};
  \node[neuron] (ph13) at (2*\layerdist,-3*\nodedist) {$\bar{\Phi}_{n_1}^{(1)}$};

  % Middle layers indication
  \node[dots] (ldots) at (2.5*\layerdist,-1.5*\nodedist) {$\cdots$};

  % Layer 4: Last hidden layer activated
  \node[neuron] (ph21) at (3*\layerdist,0) {$\bar{\Phi}_1^{(L-1)}$};
  \node[neuron] (ph22) at (3*\layerdist,-\nodedist) {$\bar{\Phi}_2^{(L-1)}$};
  \node[dots] (ph2dots) at (3*\layerdist,-2*\nodedist) {$\vdots$};
  \node[neuron] (ph23) at (3*\layerdist,-3*\nodedist) {$\bar{\Phi}_{n_{L-1}}^{(L-1)}$};

  % Layer 5: Last layer
  \node[neuron] (h21) at (4*\layerdist,0) {$\Phi_1^{(L)}$};
  \node[neuron] (h22) at (4*\layerdist,-\nodedist) {$\Phi_2^{(L)}$};
  \node[dots] (h2dots) at (4*\layerdist,-2*\nodedist) {$\vdots$};
  \node[neuron] (h23) at (4*\layerdist,-3*\nodedist) {$\Phi_{n_L}^{(L)}$};

  % Layer 6: Output
  \node[neuron] (y1) at (5*\layerdist,0) {$y_1$};
  \node[neuron] (y2) at (5*\layerdist,-\nodedist) {$y_2$};
  \node[dots] (ydots) at (5*\layerdist,-2*\nodedist) {$\vdots$};
  \node[neuron] (y3) at (5*\layerdist,-3*\nodedist) {$y_{n_L}$};

  % Layer backgrounds
  \begin{pgfonlayer}{background}
    \node[input background, fit=(x1) (x3)] (layer1) {};
    \node[hidden background, fit=(h11) (h13)] (layer2) {};
    \node[hidden background, fit=(ph11) (ph13)] (layer3) {};
    \node[hidden background, fit=(ph21) (ph23)] (layer4) {};
    \node[hidden background, fit=(h21) (h23)] (layer5) {};
    \node[output background, fit=(y1) (y3)] (layer6) {};
  \end{pgfonlayer}

  % Layer labels
  \node[layer label, above=0.3cm of layer1] {Input\\Layer};
  \node[layer label, above=0.3cm of layer2] {First Hidden\\Layer};
  \node[layer label, above=0.3cm of layer3] {Activated\\Layer 1};
  \node[layer label, above=0.3cm of layer4] {Activated\\Layer $L-1$};
  \node[layer label, above=0.3cm of layer5] {Final Hidden\\Layer $L$};
  \node[layer label, above=0.3cm of layer6] {Output\\Layer};

  % Transformations
  \node (x) at ($(layer1.south)+(0, -0.3)$) {$\mathbf{x}$};

  \node[transformation] (t1) at ($(layer1.east)!0.5!(layer2.west)+(0,-2.75*\nodedist)$)
  {$\mathbf{W}^{(1)}\mathbf{x} + \mathbf{b}^{(1)}$};

  \node (h1) at ($(layer2.south)+(0, -0.3)$) {$\boldsymbol{\Phi}^{(1)}$};

  \node[transformation] (t2) at ($(layer2.east)!0.5!(layer3.west)+(0,-2.75*\nodedist)$)
  {$\sigma_1(\boldsymbol{\Phi}^{(1)})$};

  \node (a1) at ($(layer3.south)+(0, -0.3)$) {$\bar{\boldsymbol{\Phi}}^{(1)}$};

  \node (a2) at ($(layer4.south)+(0, -0.3)$) {$\bar{\boldsymbol{\Phi}}^{(L-1)}$};

  \node[transformation] (t3) at ($(layer4.east)!0.5!(layer5.west)+(0,-2.75*\nodedist)$)
  {$\mathbf{W}^{(L)}\bar{\boldsymbol{\Phi}}^{(L-1)} + \mathbf{b}^{(L)}$};

  \node (h2) at ($(layer5.south)+(0, -0.3)$) {$\boldsymbol{\Phi}^{(L)}$};

  \node[transformation] (t4) at ($(layer5.east)!0.5!(layer6.west)+(0,-2.75*\nodedist)$)
  {$\sigma_L(\boldsymbol{\Phi}^{(L)})$};

  \node (y) at ($(layer6.south)+(0, -0.3)$) {$\mathbf{y}$};

  \draw[|->] (x) -- (t1);
  \draw[|->] (t1) -- (h1);
  \draw[|->] (h1) -- (t2);
  \draw[|->] (t2) -- (a1);

  \draw[|->] (a2) -- (t3);
  \draw[|->] (t3) -- (h2);

  \draw[|->] (h2) -- (t4);
  \draw[|->] (t4) -- (y);

  % Connect nodes with edges
  % Layer 1 to Layer 2 (complete bipartite)
  \foreach \i in {1,2,3} {
    \foreach \j in {1,2,3} {
      \draw[->, gray] (x\i) -- (h1\j);
    }
  }

  % Layer 2 to Layer 3 (matching)
  \foreach \i in {1,2,3} {
    \draw[->, gray] (h1\i) -- (ph1\i);
  }

  % Layer 3 to Layer 4 (indicated by dots)
  \draw[->, gray, dashed] (ph11) to[bend left=15] (ph21);
  \draw[->, gray, dashed] (ph13) to[bend right=15] (ph23);

  % Layer 4 to Layer 5 (complete bipartite)
  \foreach \i in {1,2,3} {
    \foreach \j in {1,2,3} {
      \draw[->, gray] (ph2\i) -- (h2\j);
    }
  }

  % Layer 5 to Layer 6 (complete bipartite)
  \foreach \i in {1,2,3} {
    \foreach \j in {1,2,3} {
      \draw[->, gray] (h2\i) -- (y\j);
    }
  }

\end{tikzpicture}
\end{document}
```
</LatexFigure>

<MathBox title='Fully-connected feedforward neural network (structured description)' boxType='definition'>
Define the parameter space

$$
\begin{equation*}
  \mathcal{N} := \bigcup_{L\in\N_+} \bigcup_{(\ell_0, \dots, \ell_L) \in \N_+^{L+1}} \left(\prod_{k=1}^L \R^{\ell_k \times \ell_{k-1}} \times \R^{\ell_k} \right)
\tag{\label{equation-4}}
\end{equation*}
$$

where
- $L \in \N_+$ is the number of layers
- $(\ell_0, \dots, \ell_L) \in \N_+^{L+1}$ is the layer-width vector

Each layer $k \in\set{1,\dots,L}$ is characterized by weights $\mathbf{W}_n \in\R^{\ell_n \times\ell_{n+1}}$ and biases $\mathbf{B}_n \in\R^{\ell_n}$. A fully-connected feedforward neural network (FFNN) is any element $\boldsymbol{\Phi}\in\mathcal{N}$ in the form of a finite sequence of weight-bias pairs

$$
  \boldsymbol{\Phi} = \left((\mathbf{W}_k, \mathbf{B}_k)_{k=1}^L \right) \in \prod_{k=1}^L \R^{\ell_k \times \ell_{k-1}} \times \R^{\ell_k}
$$

For $n\in\N_0$, define the layer dimension function $d_n (\boldsymbol{\Phi}) \in\N_0$ by

$$
  d_n (\boldsymbol{\Phi}) = \begin{cases}
    \ell_n,\quad& 0 \leq n\leq L \\
    0, \quad& n > L
  \end{cases}
$$

The dimension vector of $\boldsymbol{\Phi}\in\mathcal{N}$ is then given by

$$
  \mathbf{d}(\boldsymbol{\Phi}) := \left(d_n (\boldsymbol{\Phi}) \right)_{n=0}^{\mathcal{L}(\boldsymbol{\Phi})} = (\ell_0, \dots, \ell_L) \in \N_+^{L(\boldsymbol{\Phi}) + 1}
$$

For notational convenience, we define the weight and bias projection maps for $n = 1,\dots, L$
- $\mathcal{W}_n (\boldsymbol{\Phi}) := \mathbf{W}_n \in \R^{d_n (\boldsymbol{\Phi}) \times d_{n-1} (\boldsymbol{\Phi})}$
- $\mathcal{B}_n (\boldsymbol{\Phi}) := \mathbf{B}_n \in \R^{d_n (\boldsymbol{\Phi})}$

The structure of a network $\boldsymbol{\Phi}\in\mathcal{N}$ is characterized by the following quantities:
- Number of layers: $\mathcal{L}(\boldsymbol{\Phi}) = L \in\N_+$
- Input dimension: $\mathcal{I}(\boldsymbol{\Phi}) = d_0 (\boldsymbol{\Phi}) = \ell_0 \in\N_+$
- Output dimension: $\mathcal{O}(\boldsymbol{\Phi}) = d_{\mathcal{L}(\boldsymbol{\Phi})} = \ell_L \in\N_+$
- Number of hidden layers: $\mathcal{H}(\boldsymbol{\Phi}) = \mathcal{L}(\boldsymbol{\Phi}) - 1 \in\N_0$
- Total number of parameters: $\mathcal{P}(\boldsymbol{\Phi}) = \sum_{k=1}^L \ell_k (\ell_{k+1} + 1) \in\N_+$

Let $a:\R\to\R$ be an activation function. The realization of a network $\boldsymbol{\Phi} \in\mathcal{N}$ with activation $a$ is the function $\mathcal{F}_a^\mathcal{N}: \R^{\mathcal{I}(\boldsymbol{\Phi})} \to \R^{\mathcal{O}(\boldsymbol{\Phi})}$ defined recursively as follows.

Given an input $\mathbf{x}_0 \in\R^{d_0 (\boldsymbol{\Phi})}$, define for $k = 1,\dots,\mathcal{L}(\boldsymbol{\Phi})$

$$
  \mathbf{x}_k = \mathbf{M}_{\psi_k, d_k (\boldsymbol{\Phi})} \left(\mathcal{W}_k (\boldsymbol{\Phi}) \mathbf{x}_{k-1} + \mathcal{B}_k (\boldsymbol{\Phi})\right)
$$

where $\mathbf{M}_{\psi_k,d}$ is the componentwise extension of $\psi_k$ to $\R^d$ and

$$
  \psi_k = \begin{cases}
    a,\quad& k < \mathcal{L}\boldsymbol{\Phi} \\
    \operatorname{id}_\R,\quad& k = \mathcal{L}(\boldsymbol{\Phi})
  \end{cases}
$$

Finally set

$$
\begin{equation*}
  (\mathcal{F}_a^{\mathcal{N}} (\boldsymbol{\Phi}))(\mathbf{x}_0) := \mathbf{x}_{\mathcal{L}(\boldsymbol{\Phi})}
\tag{\label{equation-24}}
\end{equation*}
$$
</MathBox>

**Connection between the structured and vectorized description**

Let $\mathcal{N}$ denote parameter space of fully-connected feedforward neural networks as defined in $\eqref{equation-4}$, and define the disjoint-union parameter space

$$
  \R^{(*)} := \bigsqcup_{d\in\N_+} \R^d
$$

The equivalence of the vectorized and structural descriptions of fully-connected feedforward neural networks can be shown by defining the vectorization map $T:\mathcal{N}\to\R^{(*)}$ given by $T(\boldsymbol{\Phi}) = \boldsymbol{\theta} \in\R^d$, where

$$
  d:= \mathcal{P}(\boldsymbol{\Phi}) = \sum_{k=1}^{\mathcal{L}(\boldsymbol{\Phi})} \ell_k (\ell_{\ell + 1} + 1)
$$

is the total number of parameters of the network. Introducing cumulative offsets for each layer $k=1,\dots,L$

$$
  s_k := \sum_{i=1}^{k-1} \ell_i (\ell_{i+1}),\; k=1,\dots,\mathcal{L}(\boldsymbol{\Phi})
$$

the elements of the weight matrix are given by

$$
  \left[\mathcal{W}_k (\boldsymbol{\Phi})\right]_{i,j} = \theta_{s_k + (i-1)\ell_{k-1} + j},\; i=1,\dots,\ell_k,\; j=1,\dots,\ell_{k-1}
$$

Likewise, the components of the bias vector are

$$
  \left[\mathcal{B}_k (\boldsymbol{\Phi})\right]_i := \theta_{s_k + \ell_k \ell_{k-1} + i},\; i=1,\dots,\ell_k
$$

This construction uniquely determines $\boldsymbol{\Phi}$ from $T(\boldsymbol{\Phi})$. 

To see these, define affine maps $A_{m,n}^{\boldsymbol{\theta}, s} : \R^n \to \R^m$ given by

$$
  A_{m,n}^{\boldsymbol{\theta}, s} (\mathbf{x}) := \mathbf{Wx} + \mathbf{b}
$$

where
- $W_{i,j} := \theta_{s + (i - 1)n -j}$
- $\mathbf{b}_i := \theta_{s + mn + i}$

Then for all $k=1,\dots,L$ and all $\mathbf{x} \in\R^{\ell_{k - 1}}$ we have

$$
  \mathcal{W}_k (\boldsymbol{\Phi})\mathbf{x} + \mathcal{B}_k (\boldsymbol{\Phi}) = A_{\ell_k, \ell_{k-1}}^{T(\boldsymbol{\Phi}), s_k},\; \forall \mathbf{x}\in\R^{\ell_{k-1}}
$$

Let $a:\R\to\R$ be an activation function and $\boldsymbol{\Phi}\in\mathcal{N}$ an FFNN. Then the realization of $\boldsymbol{\Phi}$ is given by

$$
  \mathcal{F}_a^{\mathcal{N}} (\boldsymbol{\Phi}) = \begin{cases}
    N_{\operatorname{id}_{\R^{\mathcal{O}(\boldsymbol{\Phi})}}}^{T(\boldsymbol{\Phi}), \mathcal{I}(\boldsymbol{\Phi})},\quad& \mathcal{H}(\boldsymbol{\Phi}) = 0 \\
    N_{\mathbf{M}_{a, d_1 (\boldsymbol{\Phi})},\dots,\mathbf{M}_{a, d_{\mathcal{H}(\boldsymbol{\Phi})}(\boldsymbol{\Phi})}, \operatorname{id}_{\R^{\mathcal{O}(\boldsymbol{\Phi})}}}^{T(\boldsymbol{\Phi}), \mathcal{I}(\boldsymbol{\Phi})},\quad& \mathcal{H}(\boldsymbol{\Phi}) > 0
  \end{cases}
$$

Given an input $\mathbf{x}_0 \in\R^{\ell_0}$, define recursively

$$
  \mathbf{x}_k = \begin{cases}
    \mathbf{M}_{a,\ell_k} \left(A_{\ell_k, \ell_{k-1}}^{T(\boldsymbol{\Phi}), s_k}\right),\quad& k=1,\dots,L-1 \\
    A_{\ell_L, \ell_{L-1}}^{T(\boldsymbol{\Phi}), s_L} (\mathbf{x}_{L-1}),\quad& k=L
  \end{cases}
$$

Then $(\mathcal{F}_a^\mathcal{N} (\boldsymbol{\Phi}))(\mathbf{x}_0) = \mathbf{x}_L$. Equivalently, for $L > 1$

$$
  \mathbf{x}_{L-1} = \left(\mathbf{M}_{a,\ell_{L-1}} \circ A_{\ell_{L-1}, \ell_{L-2}}^{T(\boldsymbol{\Phi}), s_{L-1}} \circ\cdots\circ \mathbf{M}_{a,\ell_1} \circ A_{\ell_1, \ell_0}^{T(\boldsymbol{\Phi}), 0} \right)(\mathbf{x}_0)
$$

<MathBox title='Hypothesis sets of neural networks' boxType='definition'>
Let $a = (\mathbf{n}, \boldsymbol{\sigma})$ be a neural network architecture with input dimension $n_0 = d$, output dimension $n_L = 1$, and measurable activation functions $\boldsymbol{\sigma}$. For regression taks the corresponding hypothesis set is given by

$$
  F_a = \set{\Phi_a (\cdot, \boldsymbol{\theta}) | \boldsymbol{\theta} \in\R^p}
$$

and for classification tasks by

$$
  F_a = \set{\operatorname{sgn}(\Phi_a (\cdot, \boldsymbol{\theta})) | \boldsymbol{\theta}\in\R^p}
$$

where

$$
  \operatorname{sgn}(x) := \begin{cases}
    1,\quad& x\geq 0 \\
    -1,\quad& x < 0
  \end{cases}
$$
</MathBox>

A feedforward neural network (FFNN) is described by a directed asyclic graph, $G = (V,E)$, and a weight function over the edges, $w = E\to\R$. Nodes of the graph correspond to neurons. Each single neuron is modeled as a real function $\sigma:\R\to\R$ called the activation function of the neuron. Each edge in the graph link the output of some neuron to the input of another neuron. The input of a neuron is obtained by taking a weighted sum of the outputs of all the neurons connected to it, where the weighting is according to $w$.

To network is further organized into layers. The set of nodes can be decomposed into a union a disjoint subsets $V = \sqcup_{t=0}^T V_t$ such that every edge in $E$ connects some node in $V_{t-1}$ to some node in $V_t$ for some $t \in [T]$. The bottom layer, $V_0$, is called the input layer and contains $n + 1$ neurons, where $n$ is the dimensionality of the input space. For every $i\in [n]$, the output of neuron $i$ is simply $x_i$. The last neuron in $V_0$ is the intercept node, which always outputs $1$. Layers $V_1,\dots, V_{T-1}$ are often called *hidden layers*. The top layer, $V_T$ is called the output layers.

We denote by $v_{t,i}$ the $i$ th neuron of the $t$th layer and by $o_{t,i} (\mathbf{x})$ the output of $v_{t,i}$, when the network is fed with the input vector $\mathbf{x}$. Thus, for $i\in[n]$ we have $o_{0,i} (\mathbf{x}) = x_i$ and for $i = n+1$ we have $o_{0, i} (\mathbf{x}) = 1$. The outputs of the neurons at layer $t + 1$ is calculated recursively in the following way. Fix some $v_{t+1, j}\in V_{t+1}$ and let $a_{t+1, j} (\mathbf{x})$ denote the input to $v_{t+1,j}$ when the network is fed with the input vector $\mathbf{x}$. Then

$$
\begin{align*}
  a_{t+1, j} (\mathbf{x}) =& \sum_{r: (v_{t,r}, v_{t+1, j})\in E} w((v_{t,r}, v_{t+1, j})) o_{t,r} (\mathbf{x})
  o_{t+1, j} (\mathbf{x}) =& \sigma(a_{t+1m j}(\mathbf{x}))
\end{align*}
$$

That is, the input to $v_{t+1,j}$ is a weighted sum of the outpus of the neurons in $V_t$ that are connected to $v_{t+1,j}$, where weighting is according to $w$, and the output of $v_{t+1}$ is simply the application of the activation function $\sigma$ on its input.

We refer to $T$ as the number of layers in the network (excluding $V_0$), or alternatively the *debth* of the network. The size of the network is denoted $|V|$ and the width of the netweek is $\max_{t\in T} |V|_t$.

## Calculus

### Composition of neural networks

<MathBox title='Compositions of fully-connected feedforward neural network' boxType='definition'>
Let $\mathcal{N}$ denote the parameter space of fully connected feedforward neural networks as defined in $\eqref{equation-4}$. For $\boldsymbol{\Phi}, \boldsymbol{\Psi} \in\mathcal{N}$, suppose the output dimension of $\boldsymbol{\Psi}$ coincides with the input dimension of $\boldsymbol{\Psi}$, i.e.

$$
  \mathcal{I}(\boldsymbol{\Phi}) = \mathcal{O}(\boldsymbol{\Psi})
$$

Then the composition of $\boldsymbol{\Phi}$ and $\boldsymbol{\Psi}$ is defined as the mapping

$$
\begin{gather*}
  \circ : \set{(\boldsymbol{\Phi}, \boldsymbol{\Psi})\in \mathcal{N}^2 : \mathcal{I}(\boldsymbol{\Phi}) = \mathcal{O}(\boldsymbol{\Psi})} \to\mathcal{N} \\
  (\boldsymbol{\Phi}, \boldsymbol{\Psi}) \mapsto \boldsymbol{\Phi}\circ\boldsymbol{\Phi}
\end{gather*}
$$

The composed network $\boldsymbol{\Phi}\circ\boldsymbol{\Phi}$ has depth

$$
\begin{equation*}
  \mathcal{L}(\boldsymbol{\Phi}\circ\boldsymbol{\Psi}) = \mathcal{L}(\boldsymbol{\Phi}) + \mathcal{L}(\boldsymbol{\Psi}) - 1
\tag{\label{equation-6}}
\end{equation*}
$$

reflecting the merging of the output layer of $\boldsymbol{\Psi}$ with the input layer of $\boldsymbol{\Phi}$. For each layer $k\in\set{1,\dots,\mathcal{L}(\boldsymbol{\Phi}) + \mathcal{L}(\boldsymbol{\Psi}) - 1}$, the composed weight-bias pair is given by

$$
\begin{equation*}
  (\mathcal{W}_{k, \boldsymbol{\Phi}\circ\boldsymbol{\Psi}}, \mathcal{B}_{k, \boldsymbol{\Phi}\circ\boldsymbol{\Psi}}) = \begin{cases}
    (\mathcal{W}_{k, \boldsymbol{\Psi}}, \mathcal{B}_{k, \boldsymbol{\Psi}}),\quad& k < \mathcal{L}(\boldsymbol{\Psi}) \\
    (\mathcal{W}_{1, \boldsymbol{\Phi}} \mathcal{W}_{\mathcal{L}(\boldsymbol{\Psi}), \boldsymbol{\Psi}}, \mathcal{W}_{1, \boldsymbol{\Phi}} \mathcal{B}_{\mathcal{L}(\boldsymbol{\Psi}), \boldsymbol{\Psi}} + \mathcal{B}_{1, \boldsymbol{\Psi}}),\quad& k = \mathcal{L}(\boldsymbol{\Psi}) \\
    (\mathcal{W}_{k - \mathcal{L}(\boldsymbol{\Psi}) + 1, \boldsymbol{\Phi}}, \mathcal{B}_{k - \mathcal{L}(\boldsymbol{\Psi}) + 1, \boldsymbol{\Phi}}),\quad& k > \mathcal{L}(\boldsymbol{\Psi}) \\
  \end{cases}
\tag{\label{equation-8}}
\end{equation*}
$$
</MathBox>

<MathBox title="Properties of compositions of fully-connected feedforward neural network" boxType="proposition" tag="proposition-1">
Let $\boldsymbol{\Phi}, \boldsymbol{\Psi} \in\mathcal{N}$ be fully connected feedforward neural networks that satisfy the compatibility condition $\mathcal{I}(\boldsymbol{\Phi}) = \mathcal{O}(\boldsymbol{\Psi})$. The composed network $\boldsymbol{\Phi}\circ\boldsymbol{\Psi}$ satisfies the following properties.

---
1. Dimension vector

Each layer $k = 0,\dots, \mathcal{L}(\boldsymbol{\Phi}\circ\boldsymbol{\Psi})$ has dimension

$$
  \mathbf{d}(\boldsymbol{\Phi}\circ\boldsymbol{\Psi}) = \begin{cases}
    d_k (\boldsymbol{\Psi}),\quad& k \leq \mathcal{H}(\boldsymbol{\Psi}) \\
    d_{k - \mathcal{L}(\boldsymbol{\Psi}) + 1} (\boldsymbol{\Phi}),\quad& k > \mathcal{H}(\boldsymbol{\Psi}) 
  \end{cases}
$$

---
2. Depth

$$
  |\mathcal{L}(\boldsymbol{\Phi}\circ\boldsymbol{\Psi}) - 1| = |\mathcal{L}(\boldsymbol{\Phi}) - 1| + |\mathcal{L}(\boldsymbol{\Psi}) - 1|
$$

---
3. Hidden layers

Since $\mathcal{H}(\boldsymbol{\Phi}\circ\boldsymbol{\Psi}) = \mathcal{L}(\boldsymbol{\Phi}\circ\boldsymbol{\Psi}) - 1$ it follows that

$$
  \mathcal{H}(\boldsymbol{\Phi}\circ\boldsymbol{\Psi}) = \mathcal{H}(\boldsymbol{\Phi}) + \mathcal{H}(\boldsymbol{\Psi})
$$

---
4. Number of parameters:

$$
\begin{align*}
  \mathcal{P}(\boldsymbol{\Phi}\circ\boldsymbol{\Psi}) =& \mathcal{P}(\boldsymbol{\Phi}) + \mathcal{P}(\boldsymbol{\Psi}) \\
  &+ d_1 (\boldsymbol{\Phi})\left[d_{\mathcal{L}(\boldsymbol{\Psi}) - 1}(\boldsymbol{\Psi}) + 1 \right] \\
  &- d_1 (\boldsymbol{\Phi})\left[d_0 (\boldsymbol{\Phi}) + 1 \right] \\
  &- d_{\mathcal{L}(\boldsymbol{\Psi})} (\boldsymbol{\Psi}) \left[d_{\mathcal{L}(\boldsymbol{\Psi}) - 1} (\boldsymbol{\Psi}) + 1 \right] \\
  \leq& \mathcal{P}(\boldsymbol{\Phi}) + \mathcal{P}(\boldsymbol{\Psi}) + d_1 (\boldsymbol{\Phi}) d_{\mathcal{H}(\boldsymbol{\Psi})} (\boldsymbol{\Psi})
\end{align*}
$$

---
5. Realization and continuity

For all continuous activation functions $a\in C(\R)$, the composed realization $\mathbf{F}_a^\mathcal{N} (\boldsymbol{\Phi}\circ\boldsymbol{\Psi}) \in C(\R^{\mathcal{I}(\boldsymbol{\Psi})}, \R^{\mathcal{O}(\boldsymbol{\Phi})})$ is continuous and

$$
  \mathbf{F}_a^\mathcal{N} (\boldsymbol{\Phi}\circ\boldsymbol{\Psi}) = [\mathbf{F}_a^\mathcal{N} (\boldsymbol{\Phi})] \circ [\mathbf{F}_a^\mathcal{N} (\boldsymbol{\Psi})]
$$

<details>
<summary>Proof</summary>

Let $L = \mathcal{L}(\boldsymbol{\Phi}\circ\boldsymbol{\Psi})$ be the number of layers in the composed network. For a fixed continuous activation function $a\in C(\R)$ define the set all forward-propagation sequences

$$
  X_a = \Set{\mathbf{x} = (\mathbf{x}_k)_{k=1}^L \in \prod_{k=1}^L \R^{d_k (\boldsymbol{\Phi}\circ\boldsymbol{\Psi})} | \mathbf{x}_k = \mathbf{M}_{\sigma_k, d_k (\boldsymbol{\Phi}\circ\boldsymbol{\Psi})} (\mathcal{W}_{k, \boldsymbol{\Phi}\circ\boldsymbol{\Psi}} \mathbf{x}_{k-1} + \mathcal{B}_{k, \boldsymbol{\Phi}\circ\boldsymbol{\Psi}})}
$$

where

$$
  \sigma_k = \begin{cases}
    a,\quad k < L \\
    \operatorname{id}_\R, \quad& k = L
  \end{cases}
$$

---
**(1)**: This follows from 

$$
\begin{equation*}
  \mathcal{H}(\boldsymbol{\Phi}\circ\boldsymbol{\Psi}) = \mathcal{L}(\boldsymbol{\Phi}\circ\boldsymbol{\Psi}) - 1
\tag{\label{equation-7}}
\end{equation*}
$$

and the fact that for each $k\in\set{1,\dots,L}$ we have

$$
  \mathcal{W}_{k, \boldsymbol{\Phi}\circ\boldsymbol{\Psi}} = \begin{cases}
    \R^{d_k (\boldsymbol{\Psi}) \times d_{k-1} (\boldsymbol{\Psi})},\quad& k < \mathcal{L}(\boldsymbol{\Psi}) \\
    \R^{d_1 (\boldsymbol{\Phi}) \times d_{\mathcal{L}(\boldsymbol{\Psi}) - 1} (\boldsymbol{\Psi})},\quad& k = \mathcal{L}(\Psi) \\
    \R^{d_{k - \mathcal{L}(\boldsymbol{\Psi}) + 1} (\boldsymbol{\Phi}) \times d_{k - \mathcal{L}(\boldsymbol{\Psi})}(\boldsymbol{\Phi})},\quad& k > \mathcal{L}(\boldsymbol{\Psi})
  \end{cases}
$$

---
**(2)** & **(3)**: This follows from $\eqref{equation-6}$ and $\eqref{equation-7}$.

---
**(4)**: From $\ref{proposition-1-1}$, we have that

$$
\begin{align*}
  \mathcal{P}(\boldsymbol{\Phi}\circ\boldsymbol{\Psi}) =& \sum_{j=1}^L d_j (\boldsymbol{\Phi}\circ\boldsymbol{\Psi}) [d_{j-1} (\boldsymbol{\Phi}\circ\boldsymbol{\Psi}) + 1] \\
  =& \left(\sum_{j=1}^{\mathcal{H}(\boldsymbol{\Psi})} d_j (\boldsymbol{\Psi}) [d_{j-1} (\boldsymbol{\Psi}) + 1] \right) + d_1 (\boldsymbol{\Phi})[d_{\mathcal{H}(\boldsymbol{\Psi})} (\boldsymbol{\Psi}) + 1] \\
  &+ \left(\sum_{j=\mathcal{L}(\boldsymbol{\Psi}) + 1}^L d_{j - \mathcal{L}(\boldsymbol{\Psi}) + 1} (\boldsymbol{\Phi}) [d_{j - \mathcal{L} (\boldsymbol{\Psi})} (\boldsymbol{\Phi}) + 1] \right) \\
  =& \left(\sum_{j=1}^{\mathcal{L}(\boldsymbol{\Psi}) - 1} d_j (\boldsymbol{\Psi}) [d_{j-1} (\boldsymbol{\Psi}) + 1] \right) + d_1 (\boldsymbol{\Phi})[d_{\mathcal{H}(\boldsymbol{\Psi})} (\boldsymbol{\Psi}) + 1] \\
  &+ \left(\sum_{j=2}^{\mathcal{L}(\boldsymbol{\Phi})} d_j (\boldsymbol{\Phi}) [d_{j-1} (\boldsymbol{\Phi}) + 1] \right) \\
  =& \left(\mathcal{P}(\boldsymbol{\Psi}) - d_{\mathcal{L}(\boldsymbol{\Psi})} (\boldsymbol{\Psi})[d_{\mathcal{L}(\boldsymbol{\Psi}) - 1} (\boldsymbol{\Psi}) + 1] \right) + d_1 (\boldsymbol{\Phi}) [d_{\mathcal{H}(\boldsymbol{\Psi})} (\boldsymbol{\Psi} + 1)] \\
  &+ \left(\mathcal{P}(\boldsymbol{\Phhi}) - d_1 (\boldsymbol{\Psi}) [d_0 (\boldsymbol{\Phi}) + 1]\right)
\end{align*}
$$

---
**(5)**: From $\ref{proposition-1-1}$ we obtain the input dimension

$$
  \mathcal{I}(\boldsymbol{\Phi}\circ\boldsymbol{\Psi}) = d_0 (\boldsymbol{\Phi}\circ\boldsymbol{\Psi}) = d_0 (\boldsymbol{\Psi}) = \mathcal{I}(\boldsymbol{\Psi})
$$

and the output dimension

$$
\begin{align*}
  \mathcal{O}(\boldsymbol{\Phi}\circ\boldsymbol{\Psi}) =& d_{\mathcal{L}(\boldsymbol{\Phi}\circ\boldsymbol{\Psi})} (\boldsymbol{\Phi}\circ\boldsymbol{\Psi}) \\
  =& d_{\mathcal{L}(\boldsymbol{\Phi}\circ\boldsymbol{\Psi}) - \mathcal{L}(\boldsymbol{\Psi}) + 1}(\boldsymbol{\Phi}) \\
  =& d_{\mathcal{L}(\boldsymbol{\Phi})} (\boldsymbol{\Phi}) = \mathcal{O}(\boldsymbol{\Phi})
\end{align*}
$$

Consequently, for every continuous activation function $a\in C(\R)$

$$
\mathbf{F}_a^\mathcal{N} (\boldsymbol{\Phi}\circ\boldsymbol{\Psi}) \in C(\R^{\mathcal{I}(\boldsymbol{\Phi}\circ\boldsymbol{\Psi})}, \R^{\mathcal{O}(\boldsymbol{\Phi}\circ\boldsymbol{\Psi})}) = C(\R^{\mathcal{I}(\boldsymbol{\Psi})}, \R^{\mathcal{O}(\boldsymbol{\Phi})})
$$

*Step 1: Layers inherited from $\boldsymbol{\Phi}$*

From $\eqref{equation-8}$ the composed weight-bias pairs inherited from $\boldsymbol{\Phi}$ satisfy for $k = 1,\dots,\mathcal{L}(\boldsymbol{\Phi}) + 1$

$$
  (\mathcal{W}_{\mathcal{L}(\boldsymbol{\Psi}) + k - 1, \boldsymbol{\Phi}\circ\boldsymbol{\Psi}}, \mathcal{B}_{\mathcal{L}(\boldsymbol{\Psi}) + k - 1, \boldsymbol{\Phi}\circ\boldsymbol{\Psi}}) = (\mathcal{W}_{k,\boldsymbol{\Phi}}, \mathcal{B}_{k,\boldsymbol{\Phi}})
$$

Together with $\ref{proposition-1-1}$ this implies that for each $k = 2,\dots,\mathcal{L}(\boldsymbol{\Phi})$

$$
\begin{equation*}
\begin{split}
  \mathbf{x}_{\mathcal{L}(\boldsymbol{\Psi}) + 1} =& \mathbf{M}_{a\mathbf{1}_{(0,L)} (\mathcal{L}(\boldsymbol{\Psi}) + k - 1) + \operatorname{id}_\R \mathbf{1}_{\set{L}} (\mathcal{L}(\boldsymbol{\Psi}) + k - 1), d_k (\boldsymbol{\Phi})} (\mathcal{W}_{k, \boldsymbol{\Phi}} \mathbf{x}_{\mathcal{L}(\boldsymbol{\Psi}) + k - 2} + \mathcal{B}_{k, \boldsymbol{\Phi}}) \\
  =& \mathbf{M}_{a\mathbf{1}_{(0,\mathcal{L}(\boldsymbol{\Phi}))} (k) + \operatorname{id}_\R \mathbf{1}_{\set{\mathcal{L}(\boldsymbol{\Phi})}} (k), d_k (\boldsymbol{\Phi})} (\mathcal{W}_{k, \boldsymbol{\Phi}} \mathbf{x}_{\mathcal{L}(\boldsymbol{\Psi}) + k - 2} + \mathcal{B}_{k, \boldsymbol{\Phi}})
\end{split}
\tag{\label{equation-9}}
\end{equation*}
$$

*Step 2: Interface layer*

At the interface index $k = \mathcal{L}(\boldsymbol{\Psi})$, we have again from $\eqref{equation-8}$ and $\ref{proposition-1-1}$ that

$$
\begin{align*}
  \mathbf{x}_{\mathcal{L}(\boldsymbol{\Psi})} =& \mathbf{M}_{a\mathbf{1}_{(0,L)} (\mathcal{L}(\boldsymbol{\Psi})) + \operatorname{id}_\R \mathbf{1}_{\set{L}} (\mathcal{L}(\boldsymbol{\Psi})), d_{\mathcal{L}(\boldsymbol{\Psi})} (\boldsymbol{\Phi}\circ\boldsymbol{\Psi})} (\mathcal{W}_{\mathcal{L}(\boldsymbol{\Psi}), \boldsymbol{\Phi}\circ\boldsymbol{\Psi}} \mathbf{x}_{\mathcal{L}(\boldsymbol{\Psi}) - 1} + \mathcal{B}_{\mathcal{L}(\boldsymbol{\Psi}), \boldsymbol{\Phi}\circ\boldsymbol{\Psi}}) \\
  =& \mathbf{M}_{a\mathbf{1}_{(0,\mathcal{L}(\boldsymbol{\Phi}))} (1) + \operatorname{id}_\R \mathbf{1}_{\set{\mathcal{L}(\boldsymbol{\Phi})}} (1), d_1 (\boldsymbol{\Phi})} (\mathcal{W}_{1, \boldsymbol{\Phi}} \mathcal{W}_{\mathcal{L}(\boldsymbol{\Psi}), \boldsymbol{\Psi}} \mathbf{x}_{\mathcal{L}(\boldsymbol{\Psi}) - 1} + \mathcal{W}_{1, \boldsymbol{\Phi}}\mathcal{B}_{\mathcal{L}(\boldsymbol{\Psi}), \boldsymbol{\Psi}} + \mathcal{B}_{1,\boldsymbol{\Phi}}) \\
  =& \mathbf{M}_{a\mathbf{1}_{(0,\mathcal{L}(\boldsymbol{\Phi}))} (1) + \operatorname{id}_\R \mathbf{1}_{\set{\mathcal{L}(\boldsymbol{\Phi})}} (1), d_1 (\boldsymbol{\Phi})} (\mathcal{W}_{1, \boldsymbol{\Phi}} (\mathcal{W}_{\mathcal{L}(\boldsymbol{\Psi}), \boldsymbol{\Psi}} \mathbf{x}_{\mathcal{L}(\boldsymbol{\Psi}) - 1} + \mathcal{B}_{\mathcal{L}(\boldsymbol{\Psi}), \boldsymbol{\Psi}}) + \mathcal{B}_{1,\boldsymbol{\Phi}})
\end{align*}
$$

Together with $\eqref{equation-9}$ this shows that

$$
\begin{equation*}
  (\mathbf{F}_a^\mathcal{N} (\boldsymbol{\Phi})) (\mathcal{W}_{\mathcal{L}(\boldsymbol{\Psi}), \boldsymbol{\Psi}} \mathbf{x}_{\mathcal{L}(\boldsymbol{\Psi}) - 1} + \mathcal{B}_{\mathcal{L}(\boldsymbol{\Psi}), \boldsymbol{\Psi}}) = \mathbf{x}_L
\tag{\label{equation-10}}
\end{equation*}
$$

*Step 3: Layers inherited from $\boldsymbol{\Psi}$*

From $\eqref{equation-8}$ and $\ref{proposition-1-1}$, it follows that for $k = 0,\dots, \mathcal{L}(\boldsymbol{\Psi})$

$$
  \mathbf{x}_k = \mathbf{M}_{a, d_k (\boldsymbol{\Psi})} (\mathcal{W}_{k, \boldsymbol{\Psi}} \mathbf{x}_{k-1} + \mathcal{B}_{k,\boldsymbol{\Psi}})
$$

This shows that for all $a\in C(\R)$ and $\mathbf{x}\in X_a$

$$
  (\mathbf{F}_a^\mathcal{N} (\boldsymbol{\Psi}))(\mathbf{x}_0) = \mathbf{W}_{\mathcal{L}(\boldsymbol{\Psi}), \boldsymbol{\Psi}} \mathbf{x}_{\mathcal{L}(\boldsymbol{\Psi}) - 1} + \mathcal{B}_{\mathcal{L}(\boldsymbol{\Psi}), \boldsymbol{\Psi}}
$$

Combining this with $\eqref{equation-10}$ proves that for all $a\in C(\R)$ and $\mathbf{x}\in X_a$

$$
  (\mathbf{F}_a^\mathcal{N} (\boldsymbol{\Phi}))\left((\mathbf{F}_a^\mathcal{N} (\boldsymbol{\Psi}))(\mathbf{x}_0) \right) = \mathbf{x}_L = (\mathbf{F}_a^\mathcal{N} (\boldsymbol{\Phi}\circ\boldsymbol{\Psi}))(\mathbf{x}_0)
$$
</details>
</MathBox>

#### Associativity of compositions

<MathBox title="" boxType="lemma" tag="lemma-3">
Let $\boldsymbol{\Phi}_1, \boldsymbol{\Phi}_2, \boldsymbol{\Phi}_3 \in\mathcal{N}$ be fully connected feedforward neural networks that satisfy 
- $\mathcal{I}(\boldsymbol{\Phi}_1) = \mathcal{O}(\boldsymbol{\Phi}_2)$
- $\mathcal{I}(\boldsymbol{\Phi}_2) = \mathcal{O}(\boldsymbol{\Phi}_3)$
- $\mathcal{L}(\boldsymbol{\Phi}_3) = 1$

Then

$$
  (\boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2) \circ \boldsymbol{\Phi}_3 = \boldsymbol{\Phi}_1 \circ (\boldsymbol{\Phi}_2 \circ \boldsymbol{\Phi}_3)
$$

<details>
<summary>Proof</summary>

Observe that for all $\boldsymbol{\Psi}_1, \boldsymbol{\Psi}_2 \in\mathcal{N}$ with $\mathcal{I}(\boldsymbol{\Psi}_1) = \mathcal{O}(\boldsymbol{\Psi}_2)$, then $\mathcal{L}(\boldsymbol{\Psi}_1 \circ \boldsymbol{\Psi}_2) = \mathcal{L}(\boldsymbol{\Psi}_1) + \mathcal{L}(\boldsymbol{\Psi}_2) - 1$. The assumption $\mathcal{L}(\boldsymbol{\Phi}_2) = 1$ ensures in this case that

$$
\begin{equation*}
\begin{split}
  \mathcal{L}(\boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2) =& \mathcal{L}(\boldsymbol{\Phi}_1) \\
  \mathcal{L}(\boldsymbol{\Phi}_2 \circ \boldsymbol{\Phi}_3) =& \mathcal{L}(\boldsymbol{\Phi}_3)
\end{split}
\tag{\label{equation-11}}
\end{equation*}
$$

leading to

$$
\begin{equation*}
\begin{split}
  \mathcal{L}((\boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2) \circ \boldsymbol{\Phi}_3) = \mathcal{L}(\boldsymbol{\Phi}_1) + \mathcal{L}(\boldsymbol{\Phi}_3) \\
  =& \mathcal{L}(\boldsymbol{\Phi}_1 \circ (\boldsymbol{\Phi}_2 \circ \boldsymbol{\Phi}_3))
\end{split}
\tag{\label{equation-12}}
\end{equation*}
$$

Next note that $\eqref{equation-11}$ and $\eqref{equation-8}$ together with the assumption $\mathcal{L}(\boldsymbol{\Phi}_2) = 1$ imply that for $k = 1,\dots, \mathcal{L}(\boldsymbol{\Phi}_1)$

$$
\begin{equation*}
  (\mathcal{W}_{k, \boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2}, \mathcal{B}_{k, \boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2}) = \begin{cases}
    (\mathcal{W}_{1, \boldsymbol{\Phi}_1} \mathcal{W}_{1, \boldsymbol{\Phi}_2}, \mathcal{W}_{1, \boldsymbol{\Phi}_1} \mathcal{B}_{1, \boldsymbol{\Phi}_2} + \mathcal{B}_{1, \boldsymbol{\Phi}_1}),\quad& k = 1 \\
    (\mathcal{W}_{k, \boldsymbol{\Phi}_1}, \mathcal{B}_{k, \boldsymbol{\Phi}_1})
  \end{cases}
\tag{\label{equation-16}}
\end{equation*}
$$

This, $\eqref{equation-8}$ and $\eqref{equation-12}$ show that for $k = 1,\dots, \mathcal{L}(\boldsymbol{\Phi}_1) + \mathcal{L}(\boldsymbol{\Phi}_2) - 1$

$$
\begin{equation*}
\begin{split}
  & (\mathcal{W}_{k, (\boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2)\circ \boldsymbol{\Phi}_3}, \mathcal{B}_{k, (\boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2)\circ\boldsymbol{\Phi}_3}) \\
  =& \begin{cases}
    (\mathcal{W}_{k, \boldsymbol{\Phi}_3}, \mathcal{B}_{k, \boldsymbol{\Phi}_3}),\quad& k < \mathcal{L}(\boldsymbol{\Phi}_3) \\
    (\mathcal{W}_{1, \boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2} \mathcal{W}_{\mathcal{L}(\boldsymbol{\Phi}_3), \boldsymbol{\Phi}_3}, \mathcal{W}_{1, \boldsymbol{\Phi}_1 \circ\boldsymbol{\Phi}_2} \mathcal{B}_{\mathcal{L}(\boldsymbol{\Phi}_3, \boldsymbol{\Phi}_3} + \mathcal{B}_{1, \boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2}),\quad& k = \mathcal{L}(\boldsymbol{\Phi}_3) \\
    (\mathcal{W}_{k - \mathcal{L}(\boldsymbol{\Phi}_3) + 1, \boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2}, \mathcal{B}_{k - \mathcal{L}(\boldsymbol{\Phi}_3) + 1, \boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2}),\quad& k > \mathcal{L}(\boldsymbol{\Phi}_3)
  \end{cases} \\
  =& \begin{cases}
    (\mathcal{W}_{k, \boldsymbol{\Phi}_3}, \mathcal{B}_{k, \boldsymbol{\Phi}_3}),\quad& k < \mathcal{L}(\boldsymbol{\Phi}_3) \\
    (\mathcal{W}_{1, \boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2} \mathcal{W}_{\mathcal{L}(\boldsymbol{\Phi}_3), \boldsymbol{\Phi}_3}, \mathcal{W}_{1, \boldsymbol{\Phi}_1 \circ\boldsymbol{\Phi}_2} \mathcal{B}_{\mathcal{L}(\boldsymbol{\Phi}_3, \boldsymbol{\Phi}_3} + \mathcal{B}_{1, \boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2}),\quad& k = \mathcal{L}(\boldsymbol{\Phi}_3) \\
    (\mathcal{W}_{k - \mathcal{L}(\boldsymbol{\Phi}_3) + 1, \boldsymbol{\Phi}_1}, \mathcal{B}_{k - \mathcal{L}(\boldsymbol{\Phi}_3) + 1, \boldsymbol{\Phi}_1}),\quad& k > \mathcal{L}(\boldsymbol{\Phi}_3)
    \end{cases}
\end{split}
\tag{\label{equation-13}}
\end{equation*}
$$

Furthermore, from $\eqref{equation-8}$, $\eqref{equation-11}$ and $\eqref{equation-12}$ we have for $k=1,\dots,\mathcal{L}(\boldsymbol{\Phi}_1) + \mathcal{L}(\boldsymbol{\Phi}_3) - 1$

$$
\begin{equation*}
\begin{split}
  &(\mathcal{W}_{k, \boldsymbol{\Phi}_1 \circ (\boldsymbol{\Phi}_2 \circ \boldsymbol{\Phi}_3)}, \mathcal{B}_{k, \boldsymbol{\Phi}_1 \circ (\boldsymbol{\Phi}_2 \circ\boldsymbol{\Phi}_3)}) \\
  =& \begin{cases}
    (\mathcal{W}_{k, \boldsymbol{\Phi}_2 \circ\boldsymbol{\Phi}_3}, \mathcal{B}_{k, \boldsymbol{\Phi}_2 \circ\boldsymbol{\Phi}_3}),\quad& k < \mathcal{L}(\boldsymbol{\Phi}_2 \circ \boldsymbol{\Phi}_3) \\
    (\mathcal{W}_{1, \boldsymbol{\Phi}_1} \mathcal{W}_{\mathcal{L}(\boldsymbol{\Phi}_2 \circ \boldsymbol{\Phi}_3), \boldsymbol{\Phi}_2 \circ\boldsymbol{\Phi}_3}, \mathcal{W}_{1, \boldsymbol{\Phi}_1} \mathcal{B}_{\mathcal{L}(\boldsymbol{\Phi}_2 \circ \boldsymbol{\Phi}_3), \boldsymbol{\Phi}_2 \circ \boldsymbol{\Phi}_3} + \mathcal{B}_{1, \boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2}),\quad& k = \mathcal{L}(\boldsymbol{\Phi}_2 \circ\boldsymbol{\Phi}_3) \\
    (\mathcal{W}_{k - \mathcal{L}(\boldsymbol{\Phi}_2 \circ \boldsymbol{\Phi}_3) + 1, \boldsymbol{\Phi}_1}, \mathcal{B}_{k - \mathcal{L}(\boldsymbol{\Phi}_2 \circ \boldsymbol{\Phi}_3) + 1, \boldsymbol{\Phi}_1}),\quad& k > \mathcal{L}(\boldsymbol{\Phi}_2 \circ \boldsymbol{\Phi}_3)
  \end{cases} \\
  =& \begin{cases}
    (\mathcal{W}_{k, \boldsymbol{\Phi}_3}, \mathcal{B}_{k, \boldsymbol{\Phi}_3}),\quad& k < \mathcal{L}(\boldsymbol{\Phi}_3) \\
    (\mathcal{W}_{1, \boldsymbol{\Phi}_1} \mathcal{W}_{\mathcal{L}(\boldsymbol{\Phi}_3), \boldsymbol{\Phi}_2 \circ\boldsymbol{\Phi}_3}, \mathcal{W}_{1, \boldsymbol{\Phi}_1} \mathcal{B}_{\mathcal{L}(\boldsymbol{\Phi}_3), \boldsymbol{\Phi}_2 \circ \boldsymbol{\Phi}_3} + \mathcal{B}_{1, \boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2}),\quad& k = \mathcal{L}(\boldsymbol{\Phi}_3) \\
    (\mathcal{W}_{k - \mathcal{L}(\boldsymbol{\Phi}_3) + 1, \boldsymbol{\Phi}_1 }, \mathcal{B}_{k - \mathcal{L}(\boldsymbol{\Phi}_3) + 1, \boldsymbol{\Phi}_1}),\quad& k > \mathcal{L}(\boldsymbol{\Phi}_3)
  \end{cases}
\end{split}
\tag{\label{equation-14}}
\end{equation*}
$$

Combining this with $\eqref{equation-13}$ establishes that for $k\in\set{1,\dots,\mathcal{L}(\boldsymbol{\Phi}_1) + \mathcal{L}(\boldsymbol{\Phi}_3) - 1}\setminus\set{\mathcal{L}(\boldsymbol{\Phi}_3)}$

$$
\begin{equation*}
  (\mathcal{W}_{k, (\boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2) \circ \boldsymbol{\Phi}_3}, \mathcal{B}_{k, (\boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2) \circ \boldsymbol{\Phi}_3}) = (\mathcal{W}_{k, \boldsymbol{\Phi}_1 \circ (\boldsymbol{\Phi}_2 \circ \boldsymbol{\Phi}_3)}, \mathcal{B}_{k, \boldsymbol{\Phi}_1 \circ (\boldsymbol{\Phi}_2) \circ \boldsymbol{\Phi}_3)})
\tag{\label{equation-15}
\end{equation*}
$$

Moreover, note that $\eqref{equation-16}$ and $\eqref{equation-8}$ ensure that

$$
\begin{equation*}
\begin{split}
  \mathcal{W}_{1, \boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2} \mathcal{W}_{\mathcal{L}(\boldsymbol{\Phi}_3), \boldsymbol{\Phi}_3} =& \mathcal{W}_{1, \boldsymbol{\Phi}_1} \mathcal{W}_{2, \boldsymbol{\Phi}_2} \mathcal{W}_{\mathcal{L}(\boldsymbol{\Phi}_3), \boldsymbol{\Phi}_3} \\
  =& \mathcal{W}_{1, \boldsymbol{\Phi}_1} \mathcal{W}_{\mathcal{L}(\boldsymbol{\Phi}_3), \boldsymbol{\Phi}_2 \circ \boldsymbol{\Phi}_3}
\end{split}
\tag{\label{equation-17}}
\end{equation*}
$$

and

$$
\begin{equation*}
\begin{split}
  \mathcal{W}_{1, \boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2} \mathcal{B}_{\mathcal{L}(\boldsymbol{\Phi}_3), \boldsymbol{\Phi}_3} + \mathcal{B}_{1, \boldsymbol{\Phi}_1 \circ\boldsymbol{\Phi}_2} =& \mathcal{W}_{1, \boldsymbol{\Phi}_1} \mathcal{W}_{1, \boldsymbol{\Phi}_2} \mathcal{B}_{\mathcal{L}(\boldsymbol{\Phi}_3), \boldsymbol{\Phi}_3} + \mathcal{W}_{1, \boldsymbol{\Phi}_1} \mathcal{B}_{1, \boldsymbol{\Phi}_2} + \mathcal{B}_{1, \boldsymbol{\Phi}_1} \\
  =& \mathcal{W}_{1, \boldsymbol{\Phi}_1} (\mathcal{W}_{1, \boldsymbol{\Phi}_2} \mathcal{B}_{\mathcal{L}(\boldsymbol{\Phi}_3), \boldsymbol{\Phi}_3} + \mathcal{B}_{1, \boldsymbol{\Phi}_2}) + \mathcal{B}_{1, \boldsymbol{\Phi}_1} \\
  =& \mathcal{W}_{1, \boldsymbol{\Phi}_1} \mathcal{B}_{\mathcal{L}(\boldsymbol{\Phi}_3), \boldsymbol{\Phi}_2 \circ \boldsymbol{\Phi}_3} + \mathcal{B}_{1, \boldsymbol{\Phi}_1}
\end{split}
\tag{\label{equation-18}}
\end{equation*}
$$

Combining this and $\eqref{equation-17}$ with $\eqref{equation-15}$ proves that for $k=1,\dots,\mathcal{L}(\boldsymbol{\Phi}_1) + \mathcal{L}(\boldsymbol{\Phi}_3) - 1$

$$
  (\mathcal{W}_{k, (\boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2) \circ \boldsymbol{\Phi}_3}, \mathcal{B}_{k, (\boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2) \circ \boldsymbol{\Phi}_3}) = (\mathcal{W}_{k, \boldsymbol{\Phi}_1 \circ (\boldsymbol{\Phi}_2 \circ \boldsymbol{\Phi}_3)}, \mathcal{B}_{k, \boldsymbol{\Phi}_1 \circ (\boldsymbol{\Phi}_2) \circ \boldsymbol{\Phi}_3)})
$$

This and $\eqref{equation-12}$ imply that $(\boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2) \circ \boldsymbol{\Phi}_3) = \boldsymbol{\Phi}_1 \circ (\boldsymbol{\Phi}_2 \circ \boldsymbol{\Phi}_3)$.
</details>
</MathBox>

<MathBox title="" boxType="proposition" tag="proposition-2">
Let $\boldsymbol{\Phi}_1, \boldsymbol{\Phi}_2, \boldsymbol{\Phi}_3 \in\mathcal{N}$ be fully connected feedforward neural networks that satisfy 
- $\mathcal{I}(\boldsymbol{\Phi}_1) = \mathcal{O}(\boldsymbol{\Phi}_2)$
- $\mathcal{I}(\boldsymbol{\Phi}_2) = \mathcal{O}(\boldsymbol{\Phi}_3)$
- $\mathcal{L}(\boldsymbol{\Phi}_3) > 1$

Then

$$
  (\boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2) \circ \boldsymbol{\Phi}_3 = \boldsymbol{\Phi}_1 \circ (\boldsymbol{\Phi}_2 \circ \boldsymbol{\Phi}_3)
$$

<details>
<summary>Proof</summary>

Note that

$$
\begin{equation*}
\begin{split}
  \mathcal{L}((\boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2) \circ \boldsymbol{\Phi}_3) =& \mathcal{L}(\boldsymbol{\Phi}_1 \circ\boldsymbol{\Phi}_2) + \mathcal{L}(\boldsymbol{\Phi}_3) - 1 \\
  =& \mathcal{L}(\boldsymbol{\Phi}_1) + \mathcal{L}(\boldsymbol{\Phi}_2) + \mathcal{L}(\boldsymbol{\Phi}_3) - 2 \\
  =& \mathcal{L}(\boldsymbol{\Phi}_1) + \mathcal{L}(\boldsymbol{\Phi}_2 \circ \boldsymbol{\Phi}_3) - 1 \\
  =& \mathcal{L}(\boldsymbol{\Phi}_1 \circ (\boldsymbol{\Phi}_2 \circ \boldsymbol{\Phi}_3))
\end{split}
\tag{\label{equation-19}
\end{equation*}
$$

From $\eqref{equation-8}$ we have for $k=1,\dots,\mathcal{L}((\boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2)\circ\boldsymbol{\Phi}_3)$

$$
\begin{equation*}
\begin{split}
& (\mathcal{W}_{k, (\boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2)\circ \boldsymbol{\Phi}_3}, \mathcal{B}_{k, (\boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2)\circ\boldsymbol{\Phi}_3}) \\
  =& \begin{cases}
    (\mathcal{W}_{k, \boldsymbol{\Phi}_3}, \mathcal{B}_{k, \boldsymbol{\Phi}_3}),\quad& k < \mathcal{L}(\boldsymbol{\Phi}_3) \\
    (\mathcal{W}_{1, \boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2} \mathcal{W}_{\mathcal{L}(\boldsymbol{\Phi}_3), \boldsymbol{\Phi}_3}, \mathcal{W}_{1, \boldsymbol{\Phi}_1 \circ\boldsymbol{\Phi}_2} \mathcal{B}_{\mathcal{L}(\boldsymbol{\Phi}_3, \boldsymbol{\Phi}_3} + \mathcal{B}_{1, \boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2}),\quad& k = \mathcal{L}(\boldsymbol{\Phi}_3) \\
    (\mathcal{W}_{k - \mathcal{L}(\boldsymbol{\Phi}_3) + 1, \boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2}, \mathcal{B}_{k - \mathcal{L}(\boldsymbol{\Phi}_3) + 1, \boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2}),\quad& k > \mathcal{L}(\boldsymbol{\Phi}_3)
  \end{cases}
\end{split}
\tag{\label{equation-20}}
\end{equation*}
$$

Moreover, $\eqref{equation-8}$ and the assumption $\mathcal{L}(\boldsymbol{\Phi}_2) > 1$ ensure that for $k\in\N\cap (\mathcal{L}(\boldsymbol{\Phi}_3), \mathcal{L}((\boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2)\circ\boldsymbol{\Phi}_3)]$

$$
\begin{equation*}
\begin{split}
& (\mathcal{W}_{k - \mathcal{L}(\boldsymbol{\Phi}_3) + 1, \boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2}, \mathcal{B}_{k - \mathcal{L}(\boldsymbol{\Phi}_3) + 1, \boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2}) \\
  =& \begin{cases}
    (\mathcal{W}_{k - \mathcal{L}(\boldsymbol{\Phi}_3) + 1, \boldsymbol{\Phi}_2}, \mathcal{B}_{k - \mathcal{L}(\boldsymbol{\Phi}_3) + 1, \boldsymbol{\Phi}_2}),\quad& k - \mathcal{L}(\boldsymbol{\Phi}_3) + 1 < \boldsymbol{\Phi}_2 \\
    (\mathcal{W}_{1, \boldsymbol{\Phi}_1} \mathcal{W}_{\mathcal{L}(\boldsymbol{\Phi}_2), \boldsymbol{\Phi}_2}, \mathcal{W}_{1, \boldsymbol{\Phi}_1} \mathcal{B}_{\mathcal{L}(\boldsymbol{\Phi}_2, \boldsymbol{\Phi}_2} + \mathcal{B}_{1, \boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2}),\quad& k - \mathcal{L}(\boldsymbol{\Phi}_3) + 1 = \boldsymbol{\Phi}_2 \\
    (\mathcal{W}_{k - \mathcal{L}(\boldsymbol{\Phi}_3) + 1 - \mathcal{L}(\boldsymbol{\Phi}_2) + 1, \boldsymbol{\Phi}_1}, \mathcal{B}_{k - \mathcal{L}(\boldsymbol{\Phi}_3) + 1 - \mathcal{L}(\boldsymbol{\Phi}_2) + 1, \boldsymbol{\Phi}_1}),\quad& k - \mathcal{L}(\boldsymbol{\Phi}_3) + 1 > \boldsymbol{\Phi}_2
  \end{cases} \\
  =& \begin{cases}
    (\mathcal{W}_{k - \mathcal{L}(\boldsymbol{\Phi}_3) + 1, \boldsymbol{\Phi}_2}, \mathcal{B}_{k - \mathcal{L}(\boldsymbol{\Phi}_3) + 1, \boldsymbol{\Phi}_2}),\quad& k < \mathcal{L}(\boldsymbol{\Phi}_2) + \boldsymbol{\Phi}_3 - 1 \\
    (\mathcal{W}_{1, \boldsymbol{\Phi}_1} \mathcal{W}_{\mathcal{L}(\boldsymbol{\Phi}_2), \boldsymbol{\Phi}_2}, \mathcal{W}_{1, \boldsymbol{\Phi}_1} \mathcal{B}_{\mathcal{L}(\boldsymbol{\Phi}_2, \boldsymbol{\Phi}_2} + \mathcal{B}_{1, \boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2}),\quad& k = \mathcal{L}(\boldsymbol{\Phi}_2) + \boldsymbol{\Phi}_3 - 1 \\
    (\mathcal{W}_{k - \mathcal{L}(\boldsymbol{\Phi}_3) - \mathcal{L}(\boldsymbol{\Phi}_2) + 2, \boldsymbol{\Phi}_1}, \mathcal{B}_{k - \mathcal{L}(\boldsymbol{\Phi}_3) - \mathcal{L}(\boldsymbol{\Phi}_2) + 2, \boldsymbol{\Phi}_1}),\quad& k > \mathcal{L}(\boldsymbol{\Phi}_2) + \boldsymbol{\Phi}_3 - 1
  \end{cases}
\end{split}
\tag{\label{equation-21}}
\end{equation*}
$$

Together with $\eqref{equation-20}$ this shows that for $k=1,\dots,\mathcal{L}((\boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2) \circ \boldsymbol{\Phi}_3)$

$$
\begin{equation*}
\begin{split}
& (\mathcal{W}_{k, (\boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2)\circ \boldsymbol{\Phi}_3}, \mathcal{B}_{k, (\boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2)\circ\boldsymbol{\Phi}_3}) \\
  =& \begin{cases}
    (\mathcal{W}_{k, \boldsymbol{\Phi}_3}, \mathcal{B}_{k, \boldsymbol{\Phi}_3}),\quad& k < \mathcal{L}(\boldsymbol{\Phi}_3) \\
    (\mathcal{W}_{1, \boldsymbol{\Phi}_2} \mathcal{W}_{\mathcal{L}(\boldsymbol{\Phi}_3), \boldsymbol{\Phi}_3}, \mathcal{W}_{1, \boldsymbol{\Phi}_2} \mathcal{B}_{\mathcal{L}(\boldsymbol{\Phi}_3, \boldsymbol{\Phi}_3} + \mathcal{B}_{1, \boldsymbol{\Phi}_2}),\quad& k = \mathcal{L}(\boldsymbol{\Phi}_3) \\
    (\mathcal{W}_{k - \mathcal{L}(\boldsymbol{\Phi}_3) + 1, \boldsymbol{\Phi}_2}, \mathcal{B}_{k - \mathcal{L}(\boldsymbol{\Phi}_3) + 1, \boldsymbol{\Phi}_2}),\quad& \mathcal{L}(\boldsymbol{\Phi}_3) < k < \mathcal{L}(\boldsymbol{\Phi}_2) +  \mathcal{L}(\boldsymbol{\Phi}_3) - 1 \\
    (\mathcal{W}_{1, \boldsymbol{\Phi}_1} \mathcal{W}_{\mathcal{L}(\boldsymbol{\Phi}_2), \boldsymbol{\Phi}_2}, \mathcal{W}_{1, \boldsymbol{\Phi}_1} \mathcal{B}_{\mathcal{L}(\boldsymbol{\Phi}_2, \boldsymbol{\Phi}_2} + \mathcal{B}_{1, \boldsymbol{\Phi}_1}),\quad& k = \mathcal{L}(\boldsymbol{\Phi}_2) + \mathcal{L}(\boldsymbol{\Phi}_3) - 1 \\
    (\mathcal{W}_{k - \mathcal{L}(\boldsymbol{\Phi}_3) - \mathcal{L}(\boldsymbol{\Phi}_2) + 2, \boldsymbol{\Phi}_1}, \mathcal{B}_{k - \mathcal{L}(\boldsymbol{\Phi}_3) - \mathcal{L}(\boldsymbol{\Phi}_2) + 2, \boldsymbol{\Phi}_1}),\quad& k > \mathcal{L}(\boldsymbol{\Phi}_2) + \boldsymbol{\Phi}_3 - 1
  \end{cases}
\end{split}
\tag{\label{equation-22}}
\end{equation*}
$$

In addition, since $\mathcal{L}(\boldsymbol{\Phi}_2 \circ\boldsymbol{\Phi}_3) = \mathcal{L}(\boldsymbol{\Phi}_2) + \mathcal{L}(\boldsymbol{\Phi}_3) - 1$ it follows for $k=1,\dots,\mathcal{L}(\boldsymbol{\Phi}_1 \circ(\boldsymbol{\Phi}_2 \circ \boldsymbol{\Phi}_3))$

$$
\begin{equation*}
\begin{split}
  & (\mathcal{W}_{k, \boldsymbol{\Phi}_1 \circ (\boldsymbol{\Phi}_2 \circ \boldsymbol{\Phi}_3)}, \mathcal{B}_{k, \boldsymbol{\Phi}_1 \circ (\boldsymbol{\Phi}_2 \circ \boldsymbol{\Phi}_3)}) \\
  =& \begin{cases}
    (\mathcal{W}_{k, \boldsymbol{\Phi}_2 \circ\boldsymbol{\Phi}_3}, \mathcal{B}_{k, \boldsymbol{\Phi}_2 \circ\boldsymbol{\Phi}_3}),\quad& k < \mathcal{L}(\boldsymbol{\Phi}_2 \circ \boldsymbol{\Phi}_3) \\
    (\mathcal{W}_{1, \boldsymbol{\Phi}_1} \mathcal{W}_{\mathcal{L}(\boldsymbol{\Phi}_2 \circ \boldsymbol{\Phi}_3), \boldsymbol{\Phi}_2 \circ\boldsymbol{\Phi}_3}, \mathcal{W}_{1, \boldsymbol{\Phi}_1} \mathcal{B}_{\mathcal{L}(\boldsymbol{\Phi}_2 \circ \boldsymbol{\Phi}_3), \boldsymbol{\Phi}_2 \circ \boldsymbol{\Phi}_3} + \mathcal{B}_{1, \boldsymbol{\Phi}_1}),\quad& k = \mathcal{L}(\boldsymbol{\Phi}_2 \circ\boldsymbol{\Phi}_3) \\
    (\mathcal{W}_{k - \mathcal{L}(\boldsymbol{\Phi}_2 \circ \boldsymbol{\Phi}_3) + 1, \boldsymbol{\Phi}_1}, \mathcal{B}_{k - \mathcal{L}(\boldsymbol{\Phi}_2 \circ \boldsymbol{\Phi}_3) + 1, \boldsymbol{\Phi}_1}),\quad& k > \mathcal{L}(\boldsymbol{\Phi}_2 \circ \boldsymbol{\Phi}_3)
  \end{cases} \\
  =& \begin{cases}
    (\mathcal{W}_{k, \boldsymbol{\Phi}_2 \circ\boldsymbol{\Phi}_3}, \mathcal{B}_{k, \boldsymbol{\Phi}_2 \circ\boldsymbol{\Phi}_3}),\quad& k < \mathcal{L}(\boldsymbol{\Phi}_2) + \mathcal{L}(\boldsymbol{\Phi}_3) - 1 \\
    (\mathcal{W}_{1, \boldsymbol{\Phi}_1} \mathcal{W}_{\mathcal{L}(\boldsymbol{\Phi}_2) + \mathcal{L}(\boldsymbol{\Phi}_3) - 1, \boldsymbol{\Phi}_2 \circ\boldsymbol{\Phi}_3}, \mathcal{W}_{1, \boldsymbol{\Phi}_1} \mathcal{B}_{\mathcal{L}(\boldsymbol{\Phi}_2) \mathcal{L}(\boldsymbol{\Phi}_3) - 1, \boldsymbol{\Phi}_2 \circ \boldsymbol{\Phi}_3} + \mathcal{B}_{1, \boldsymbol{\Phi}_1}),\quad& k = \mathcal{L}(\boldsymbol{\Phi}_2) + \mathcal{L}(\boldsymbol{\Phi}_3) - 1 \\
    (\mathcal{W}_{k - \mathcal{L}(\boldsymbol{\Phi}_2) - \mathcal{L}(\boldsymbol{\Phi}_3) + 2, \boldsymbol{\Phi}_1}, \mathcal{B}_{k - \mathcal{L}(\boldsymbol{\Phi}_2) - \mathcal{L}(\boldsymbol{\Phi}_3) + 2, \boldsymbol{\Phi}_1}),\quad& k > \mathcal{L}(\boldsymbol{\Phi}_2) + \mathcal{L}(\boldsymbol{\Phi}_3) - 1
  \end{cases} \\
  =& \begin{cases}
    (\mathcal{W}_{k, \boldsymbol{\Phi}_3}, \mathcal{B}_{k, \boldsymbol{\Phi}_3}),\quad& k < \mathcal{L}(\boldsymbol{\Phi}_3) \\
    (\mathcal{W}_{1, \boldsymbol{\Phi}_2} \mathcal{W}_{\mathcal{L}(\boldsymbol{\Phi}_3), \boldsymbol{\Phi}_3}, \mathcal{W}_{1, \boldsymbol{\Phi}_2} \mathcal{B}_{\mathcal{L}(\boldsymbol{\Phi}_3, \boldsymbol{\Phi}_3} + \mathcal{B}_{1, \boldsymbol{\Phi}_2}),\quad& k = \mathcal{L}(\boldsymbol{\Phi}_3) \\
    (\mathcal{W}_{k - \mathcal{L}(\boldsymbol{\Phi}_3) + 1, \boldsymbol{\Phi}_2}, \mathcal{B}_{k - \mathcal{L}(\boldsymbol{\Phi}_3) + 1, \boldsymbol{\Phi}_2}),\quad& \mathcal{L}(\boldsymbol{\Phi}_3) < k < \mathcal{L}(\boldsymbol{\Phi}_2) +  \mathcal{L}(\boldsymbol{\Phi}_3) - 1 \\
    (\mathcal{W}_{1, \boldsymbol{\Phi}_1} \mathcal{W}_{\mathcal{L}(\boldsymbol{\Phi}_2), \boldsymbol{\Phi}_2}, \mathcal{W}_{1, \boldsymbol{\Phi}_1} \mathcal{B}_{\mathcal{L}(\boldsymbol{\Phi}_2, \boldsymbol{\Phi}_2} + \mathcal{B}_{1, \boldsymbol{\Phi}_1}),\quad& k = \mathcal{L}(\boldsymbol{\Phi}_2) + \mathcal{L}(\boldsymbol{\Phi}_3) - 1 \\
    (\mathcal{W}_{k - \mathcal{L}(\boldsymbol{\Phi}_3) - \mathcal{L}(\boldsymbol{\Phi}_2) + 2, \boldsymbol{\Phi}_1}, \mathcal{B}_{k - \mathcal{L}(\boldsymbol{\Phi}_3) - \mathcal{L}(\boldsymbol{\Phi}_2) + 2, \boldsymbol{\Phi}_1}),\quad& k > \mathcal{L}(\boldsymbol{\Phi}_2) + \boldsymbol{\Phi}_3 - 1
  \end{cases}
\end{split}
\tag{\label{equation-23}}
\end{equation*}
$$

This, $\eqref{equation-22}$ and $\eqref{equation-19}$ establish for $k=1,\dots,\mathcal{L}(\boldsymbol{\Phi}_1) + \mathcal{L}(\boldsymbol{\Phi}_2) + \mathcal{L}(\boldsymbol{\Phi}_3) - 2$

$$
  (\mathcal{W}_{k, (\boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2) \circ \boldsymbol{\Phi}_3}, \mathcal{B}_{k, (\boldsymbol{\Phi}_1 \circ \boldsymbol{\Phi}_2) \circ \boldsymbol{\Phi}_3}) = (\mathcal{W}_{k, \boldsymbol{\Phi}_1 \circ (\boldsymbol{\Phi}_2 \circ \boldsymbol{\Phi}_3)}, \mathcal{B}_{k, \boldsymbol{\Phi}_1 \circ (\boldsymbol{\Phi}_2) \circ \boldsymbol{\Phi}_3)})
$$

and hence $(\boldsymbol{\Phi}_1 \circ\boldsymbol{\Phi}_2) \circ \boldsymbol{\Phi}_3 = \boldsymbol{\Phi}_1 \circ (\boldsymbol{\Phi}_2 \circ \boldsymbol{\Phi}_3)$.
</details>
</MathBox>

<MathBox title="Power of fully connected feedforward neural networks" boxType="definition" tag="definition-1">
Let $\mathcal{N}$ be the parameter space of fully connected feedforward neural networks as defined in $\eqref{equation-4}$. For each $n\in\N_0$, we define the $n$-fold power function 

$$
  (\cdot)^{\circ n} : \set{\boldsymbol{\Phi}\in\mathcal{N} | \mathcal{I}(\boldsymbol{\Phi}) = \mathcal{O}(\boldsymbol{\Phi})} \to \mathcal{N}
$$

recursively by

$$
  \boldsymbol{\Phi}^{\circ n} = \begin{cases}
    (\mathbf{I}_{\mathcal{O}(\boldsymbol{\Phi})}, \mathbf{0}) \in\R^{\mathcal{O}(\boldsymbol{\Phi}) \times \mathcal{O}(\boldsymbol{\Phi})} \times\R^{\mathcal{O}(\boldsymbol{\Phi})},\quad& n = 0 \\
    \boldsymbol{\Phi} \circ (\boldsymbol{\Phi}^{\circ (n-1)}),\quad& n\in\N
  \end{cases}
$$

where $\mathbf{I}_{\mathcal{O}(\boldsymbol{\Phi})}$ denotes the identity matrix in dimension $\mathcal{O}(\boldsymbol{\Phi})$ and $\mathbf{O}$ denotes the zero bias vector.
</MathBox>

<MathBox title="Number of hidden layers of powers of fully connected feedforward neural networks" boxType="proposition" tag="proposition-5">
Let $\boldsymbol{\Phi}\in\mathcal{N}$ be a fully connected feedforward neural network satisfying $\mathcal{I}(\boldsymbol{\Phi}) = \mathcal{O}(\boldsymbol{\Phi})$. For $n\in\N_0$, the number of hidden layers in the $n$-fold power $\boldsymbol{\Phi}^{\circ n}$ is

$$
  \mathcal{H}(\boldsymbol{\Phi}^{\circ n}) = n\mathcal{H}(\boldsymbol{\Phi})
$$

<details>
<summary>Proof</summary>

This follows by induction using Proposition $\ref{proposition-1-3}$.
</details>
</MathBox>

### Parallelization of neural networks

<MathBox title="Parallelization of fully connected feedforward neural networks" boxType="definition" tag="definition-3">
Let $\mathcal{N}$ be the parameter space of fully connected feedforward neural networks as defined in $\eqref{equation-4}$. For $n\in\N_+$, let $\mathcal{N}^{(n)}$ denote the set of layer-compatible network tuples

$$
  \mathcal{N}^{(n)} = \set{(\boldsymbol{\Phi}_j)_{j=1}^n \in\mathcal{N}^n | \mathcal{L}(\boldsymbol{\Phi}_1) =\cdots= \mathcal{L}(\boldsymbol{\Phi}_1) =\cdots= \mathcal{L}(\boldsymbol{\Phi}_n)}
$$

The parallelization function $\mathbf{P}_n : \mathcal{N}^{(n)} \to\mathcal{N}$ maps a tuple of layer-compatible networks $\boldsymbol{\Phi} = (\boldsymbol{\Phi}_j)_{j=1}^n \in \mathcal{N}^{(n)}$ to a single network $\mathbf{P}_n (\boldsymbol{\Phi}) \in\mathcal{N}$. For $k=1,\dots,\mathcal{L}(\boldsymbol{\Phi}_1)$, the architecture of a parallelized network $\mathbf{P}_n (\boldsymbol{\Phi})$ is characterized by 
- Number of layers: $\mathcal{L}(\mathbf{P}_n (\boldsymbol{\Phi})) = \mathcal{L}(\boldsymbol{\Phi}_1)$
- Dimension function:
$$
  d_k (\mathbf{P}_n (\boldsymbol{\Phi})) = \sum_{j=1}^n d_k (\boldsymbol{\Phi}_j)
$$

- Dimension vector:
$$
  \mathbf{d}(\mathbf{P}_n (\boldsymbol{\Phi})) = \sum_{j=1}^n \mathbf{d}(\boldsymbol{\Phi}_j)
$$

- Weight matrix 
$$
  \mathcal{W}_{k, \mathbf{P}_n (\boldsymbol{\Phi})} = \operatorname{diag}(\mathcal{W}_{k, \boldsymbol{\Phi}_1}, \dots, \mathcal{W}_{k, \boldsymbol{\Phi}_n}) \in \R^{\left(\sum_{j=1}^n d_k (\boldsymbol{\Phi}_j) \right) \times \left(\sum_{j=1}^n d_{k-1} (\boldsymbol{\Phi}_j)\right)}
$$

- Bias vector: 
$$
  \mathcal{B}_{k, \mathbf{P}_n (\boldsymbol{\Phi})} = \left[\mathcal{B}_{k, \boldsymbol{\Phi}_1}, \dots, \mathbf{B}_{k, \boldsymbol{\Phi}_n} \right]^\top \in\R^{\sum_{j=1}^n d_k (\boldsymbol{\Phi}_j)}
$$
</MathBox>

<MathBox title="Realizations of parallelization of fully connected feedforward neural networks" boxType="proposition" tag="proposition-4">
Let $n, L \in\N_+$ and let $\boldsymbol{\Phi} = (\boldsymbol{\Phi}_j)_{j=1}^n \in \mathcal{N}^{(n)}$ be a tuple of layer-compatible fully connected neural networks with $L = \mathcal{L}(\mathcal{\Phi}_1) =\cdots= \mathcal{L}(\boldsymbol{\Phi}_n)$. If $a\in C(\R)$ is a continuous activation function, the realization of the parallelized network $\mathbf{P}_n (\boldsymbol{\Phi})$, denoted $\mathbf{F}_a^\mathcal{N}$ satisfies

1.
$$
  \mathbf{F}_a^\mathcal{N} (\mathbf{P}_n (\boldsymbol{\Phi})) \in C(\R^{\sum_{j=1}^n \mathcal{I}(\boldsymbol{\Phi}_j)}, \R^{\sum_{j=1}^n \mathcal{O}(\boldsymbol{\Phi}_j)})
$$

2. For all $\mathbf{x}_j \in \R^{\mathcal{I}(\boldsymbol{\Phi}_j)}$ with $j=1,\dots,n$
$$
\begin{align*}
  &(\mathbf{F}_a^\mathcal{N} (\mathbf{P}_n (\boldsymbol{\Phi})) (\mathbf{x}_1,\dots,\mathbf{x}_n) \\
  =& \left[(\mathbf{F}_a^\mathcal{N} (\boldsymbol{\Phi}_1))(\mathbf{x}_1),\dots,(\mathbf{F}_a^\mathcal{N} (\boldsymbol{\Phi}_n))(\mathbf{x}_n) \right] \in \R^{\sum_{j=1}^n \mathcal{O}(\boldsymbol{\Phi}_j)}
\end{align*}
$$

<details>
<summary>Proof</summary>

By definition of parallelization

$$
  \mathcal{I}(\mathbf{P}_n (\boldsymbol{\Phi})) = d_0 (\mathbf{P}(\boldsymbol{\Phi})) = \sum_{j=1}^n d_0 (\boldsymbol{\Phi}_n) = \sum_{j=1}^n \mathcal{I}(\boldsymbol{\Phi}_n)
$$

and

$$
  \mathcal{O}(\mathbf{P}_n (\boldsymbol{\Phi})) = d_{\mathcal{L}(\mathbf{P}_n (\boldsymbol{\Phi})} (\mathbf{P}(\boldsymbol{\Phi})) = \sum_{j=1}^n d_{\mathcal{L}(\boldsymbol{\Phi}_n)} (\boldsymbol{\Phi}_n) = \sum_{j=1}^n \mathcal{O}(\boldsymbol{\Phi}_n)
$$

For $k=1,\dots,L$ let

$$
  \mathfrak{x}_{k-1} = (\mathbf{x}_{k-1}^{(j)})_{j=1}^n \in \R^{\sum_{j=1}^n d_{k-1} (\boldsymbol{\Phi}_j)}
$$

be a concatenation activation vector for layer $(k-1)$. Here each $\mathbf{x}_{k-1}^{(j)}$ is exactly the $(k - 1)$-th layer activation of $\boldsymbol{\Phi}_j$. Then

$$
  \mathbf{M}_{\sigma_k, d_k (\mathbf{P}_n (\boldsymbol{\Phi}))} (\mathcal{W}_{k, \mathbf{P}_n (\boldsymbol{\Phi})} \mathfrak{x}_{k-1} + \mathcal{B}_{k, \mathbf{P}_n (\boldsymbol{\Phi})}) = \begin{bmatrix} \mathbf{M}_{\sigma_k, d_k (\boldsymbol{\Phi}_1)} (\mathcal{W}_{k, \boldsymbol{\Phi}_2} \mathbf{x}_{k-1}^{(1)} + \mathcal{B}_{k, \boldsymbol{\Phi}_1}) \\ \vdots \\ \mathbf{M}_{\sigma_k, d_k (\boldsymbol{\Phi}_n)} (\mathcal{W}_{k, \boldsymbol{\Phi}_n} \mathbf{x}_{k-1}^{(n)} + \mathcal{B}_{k, \boldsymbol{\Phi}_n}) \end{bmatrix}
$$

where $\sigma_k = a\mathbf{1}_{(0,L)}(k) + \operatorname{id}_\R \mathbf{1}_{\set{L}} (k)$. Thus, if $\mathfrak{x}_{k-1} = (\mathbf{x}_{k-1}^{(j)})_{j=1}^n$ then $\mathfrak{x}_k = (\mathbf{x}_k^{(j)})_{j=1}^n$. By induction on $k=1,\dots,L$ starting from $\mathfrak{x}_0 = (\mathbf{x}_0^{(j)})_{j=1}^n$ we obtain from $\eqref{equation-24}$

$$
\begin{align*}
  (\mathbf{F}_a^\mathcal{N} (\mathbf{P}_n (\boldsymbol{\Phi}))(\mathfrak{x}_0) = \mathfrak{x}_L = (\mathbf{x}_L^j)_{j=1}^n \\
  =& \left[(\mathbf{F}_a^\mathcal{N} (\boldsymbol{\Phi}_j))(\mathbf{x}_0^j) \right]_{j=1}^n
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Upper bound for number of parameters of parallelized fully connected feedforward neural network' boxType='proposition'>
Let $n, L \in\N_+$ and let $\boldsymbol{\Phi} = (\boldsymbol{\Phi}_j)_{j=1}^n \in \mathcal{N}^{(n)}$ be a tuple of layer-compatible fully connected neural networks with $L = \mathcal{L}(\mathcal{\Phi}_1) =\cdots= \mathcal{L}(\boldsymbol{\Phi}_n)$. Then upper bound for the number of parameters of the parallelized network $\mathbf{P}_n (\boldsymbol{\Phi})$ satisfies

$$
  \mathcal{P}(\mathbf{P}_n (\boldsymbol{\Phi})) \leq \frac{1}{2} \left(\sum_{j=1}^n \mathcal{P}(\boldsymbol{\Phi}_j) \right)^2
$$

<details>
<summary>Proof</summary>

Let $\ell_{j,k} := d_k (\boldsymbol{\Phi}_j)$ for $j=1,\dots,n$ and $k=0,\dots,L$. Then by definition of parallelization we obtain

$$
\begin{align*}
  \mathcal{P}(\mathbf{P}_n (\boldsymbol{\Phi})) =& \sum_{k=1}^L \left(\sum_{i=1}^n \ell_{i,k} \right) \left(\left[\sum_{i=1}^n \ell_{i, k-1} \right] + 1 \right) \\
  =& \sum_{k=1}^L \left(\sum_{i=1}^n \ell_{i,k} \right) \left(\left[\sum_{j=1}^n \ell_{j, k-1} \right] + 1 \right) \\
  \leq& \sum_{i=1}^n \sum_{j=1}^n \sum_{k=1}^L \ell_{i,k} (\ell_{j,k-1} + 1) \\
  \leq& \sum_{i=1}^n \sum_{j=1}^n \sum_{k,\ell=1}^L \ell_{i,k} (\ell_{j,\ell-1} + 1) \\
  =& \sum_{i=1}^n \sum_{j=1}^n \left(\sum_{k=1}^L \ell_{i,k} \right) \left(\sum_{\ell=1}^L \ell_{j, \ell-1} + 1 \right) \\
  \leq& \sum_{i=1}^n \sum_{j=1}^n \left(\sum_{k=1}^L \frac{1}{2} \ell_{i,k} (\ell_{i,k - 1} + 1) \right) \left(\sum_{\ell=1}^L \ell_{j,\ell} [\ell_{j, \ell-1} + 1] \right) \\
  =& \sum_{i=1}^n \sum_{j=1}^n \frac{1}{2} \mathcal{P}(\boldsymbol{\Phi}_i)\mathcal{P}(\boldsymbol{\Phi}_j) \\
  =& \frac{1}{2} \left(\sum_{i=1}^n \mathcal{P}(\boldsymbol{\Phi}_i) \right)^2
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Lower and upper bound for number of parameters of parallelized fully connected feedforward neural network' boxType='corollary'>
Let $n, L \in\N_+$ and let $\boldsymbol{\Phi} = (\boldsymbol{\Phi}_j)_{j=1}^n \in \mathcal{N}^{(n)}$ be a tuple of layer-compatible fully connected neural networks with $\mathbf{d}(\mathcal{\Phi}_1) =\cdots= \mathbf{d}(\boldsymbol{\Phi}_n)$.

$$
\begin{align*}
  \left[\frac{n^2}{2} \right] \mathcal{P}(\boldsymbol{\Phi}_1) \leq& \left[\frac{n^2 + n}{2} \right] \mathcal{P}(\boldsymbol{\Phi}_1) \\
  \leq& \mathcal{P}(\mathbf{P}_n (\boldsymbol{\Phi})) \leq n^2 \mathcal{P}(\boldsymbol{\Phi}_1) \\
  \leq& \frac{1}{2} \left(\sum_{i=1}^n \mathcal{P}(\boldsymbol{\Phi}_i) \right)^2
\end{align*}
$$

<details>
<summary>Proof</summary>

Let $L, \ell_0, \dots, \ell_L \in \N_+$ so that for $j=1,\dots,n$

$$
  \mathbf{d} (\boldsymbol{\Phi}_j) = (\ell_k)_{k=1}^L \in\N^{L + 1}
$$

By definition of parallelization, we have

$$
  \mathcal{P}(\mathbf{P}_n (\boldsymbol{\Phi}) = \sum_{j=1}^L (n\ell_j) (n\ell_{j-1} + 1)
$$

Thus, we obtain

$$
\begin{align*}
  \mathcal{P}(\mathbf{P}_n (\boldsymbol{\Phi}) \leq \sum_{j=1}^L (n\ell_j) (n\ell_{j-1} + n) \\
  =& n^2 \left[\sum_{j=1}^L \ell_j (\ell_{j-1} + 1) \right] = n^2 \mathcal{P}(\boldsymbol{\Phi}_1)
\end{align*}
$$

Since $\mathcal{P}(\boldsymbol{\Phi}_1) \geq \ell_1 (\ell_0 + 1) \geq 2$, we have

$$
\begin{align*}
  n^2 \mathcal{P}(\boldsymbol{\Phi}_1) \leq& \frac{n^2}{2} [\mathcal{P}(\boldsymbol{\Phi}_1)]^2 = \frac{1}{2} [n\mathcal{P}(\boldsymbol{\Phi}_1)]^2 \\
  =& \frac{1}{2} \left(\sum_{i=1}^n \mathcal{P}(\boldsymbol{\Phi}_1) \right)^1 = \frac{1}{2} \left(\sum_{i=1}^n \mathcal{P}(\boldsymbol{\Phi}_i) \right)^2
\end{align*}
$$

Using that

$$
\begin{align*}
  2(ab + 1) =& ab + 1 + (a - 1)(b - 1) + a + b \\
  \geq& ab + a + b + 1 = (a + 1)(b + 1)
\end{align*}
$$

we find

$$
\begin{align*}
  \mathcal{P}(\mathbf{P}_n (\boldsymbol{\Phi})) \geq \frac{1}{2} \left[\sum_{j=1}^L (n\ell_j)(n + 1)(\ell_{j-1} + 1) \right] \\
  =& \frac{n(n+1)}{2} \left[\sum_{j=1}^L \ell_j (\ell_{j-1} + 1) \right] \\
  =& \frac{n^2 + n}{2} \mathcal{P}(\boldsymbol{\Phi}_1)
\end{align*}
$$
</details>
</MathBox>

#### Representation of identity with ReLU activation functions

<MathBox title='One-dimensional ReLU identity network' boxType='proposition' tag='proposition-3'>
Define the fully connected feedforward neural network $\mathbf{I}_1 \in\mathcal{N}$ with two layers by

$$
  \mathbf{I}_1 = ((\mathbf{W}_1, \mathbf{B}_1), (\mathbf{W}_2, B_2)) \in \left[(\R^{2\times 1} \times \R^2) \times (\R^{1\times 2} \times \R) \right]
$$

where

$$
\begin{align*}
  \mathbf{W}_1 =& \left[\begin{smallmatrix} 1 \\ -1 \end{smallmatrix}\right] \in \R^{2\times 1} \\
  \mathbf{B}_1 =& \left[\begin{smallmatrix} 0 \\ 0 \end{smallmatrix}\right] \in \R^2 \\
  \mathbf{W}_2 =& \left[\begin{smallmatrix} 1 & -1 \end{smallmatrix}\right] \in \R^{2\times 1} \\
  B_2 =& 0 \in\R
\end{align*}
$$

The dimension vector of $\mathbf{I}_1$ is $\mathbf{d}(\mathbf{I}_1) = (1,2,1) \in\N^3$.

If $a:\R\to\R$ is the rectified linear unit (ReLU) defined by $a(x) := \max\set{0,x}$, then the realization of $\mathbf{I}_1$ with ReLU activation, denoted $\mathbf{F}_\text{ReLU}^\mathcal{N} (\mathbf{I}_1) \in C(\R)$, coincides with the identity function on $\R$, i.e.

$$
  \mathbf{F}_\text{ReLU}^\mathcal{N} (\mathbf{I}_1)(x) = x,\; x\in\R
$$

<details>
<summary>Proof</summary>

For any $x\in \R$, we have

$$
  \mathbf{W}_1 x + \mathbf{B}_1 = \begin{bmatrix} x \\ -x \end{bmatrix}
$$

Applying the ReLU activation componentwise yields

$$
  \mathbf{M}_{\text{ReLU}, 2} \left[\begin{smallmatrix} x \\ -x \end{smallmatrix}\right] = \begin{bmatrix} \max\set{0, x} \\ \max\set{0,-x} \end{bmatrix}
$$

The second affine transformation gives

$$
  \mathbf{W}_2 \begin{bmatrix} \max\set{0, x} \\ \max\set{0,-x} \end{bmatrix} + B_2 = \max\set{0, x} - \max\set{0, -x} = x
$$

Thus $\mathbf{F}_\text{ReLU}^\mathcal{N} (\mathbf{I}_1)$ equals the identity map on $\R$.
</details>
</MathBox>

<MathBox title='Parallelized ReLU identity network' boxType='corollary'>
Let $\mathbf{I}_1 \in\mathcal{N}$ be the one-dimensional fully connected feedforward identity neural network defined in Proposition $\ref{proposition-3}$. For $d\in\N_+$ define the $d$-dimensional identity network

$$
  \mathbf{I}_d := \mathbf{P}_d (\underbrace{\mathbf{I}_1, \dots, \mathbf{I}_1}_{d \text{ times}}) \in \mathcal{N}^{(n)}
$$

where the parallelization operator $\mathbf{P}_d$ is applied to $d$ identical copies of $\mathbf{I}_d$. The dimension vector of $\mathbf{I}_d$ is

$$
\begin{align*}
  \mathbf{d}(\mathbf{I}_d) = \sum_{j=1}^d \mathbf{d}(\mathbf{I}_1) = (d,2d,d) \\
\end{align*}
$$

If $a:\R\to\R$ is the rectified linear unit (ReLU) defined by $a(x) := \max\set{0,x}$, then the realization of $\mathbf{I}_d$ with ReLU activation, denoted $\mathbf{F}_\text{ReLU}^\mathcal{N} (\mathbf{I}_d) \in C(\R^d)$, coincides with the identity function on $\R^d$, i.e.

$$
  \mathbf{F}_\text{ReLU}^\mathcal{N} (\mathbf{I}_d)(\mathbf{x}) = \mathbf{x},\; \mathbf{x}\in\R^d
$$

<details>
<summary>Proof</summary>

By Propositions $\ref{proposition-4-2}$ and $\ref{proposition-3}$, we have for $\mathbf{x}\in\R^d$

$$
\begin{align*}
  (\mathbf{F}_\text{ReLU}^\mathcal{N}(\mathbf{I}_d))(\mathbf{x}) =& (\mathbf{F}_\text{ReLU}^\mathcal{N} (\mathbf{P}_n (\mathbf{I}_1,\dots,\mathbf{I}_1)))(x_1,\dots,x_d) \\
  =& ((\mathbf{F}_\text{ReLU}^\mathcal{N}(\mathbf{I}_d))(x_1), \dots, (\mathbf{F}_\text{ReLU}^\mathcal{N}(\mathbf{I}_d))(x_d)) \\
  =& (x_1,\dots,x_d) = \mathbf{x}
\end{align*}
$$
</details>
</MathBox>

#### Extensions of neural networks

<MathBox title='Extensions of fully connected feedforward neural network' boxType='definition' tag='definition-2'>
Let $L\in\N_+$. Suppose $\mathbb{I} \in\mathcal{N}$ is a fully connected feedforward neural network with $\mathcal{I}(\mathbb{I}) = \mathcal{O}(\mathbb{I})$. Define the extension operator

$$
  \mathbf{E}_{L, \mathbb{I}} : \set{\boldsymbol{\Phi}\in\mathcal{N} | \mathcal{L}(\boldsymbol{\Phi}) \leq L \land \mathcal{O}(\boldsymbol{\Phi}) = \mathcal{I}(\mathbb{I})} \to \mathcal{N}
$$

by

$$
  \mathbf{E}_{L, \mathbb{I}} = (\mathbb{I}^{\circ (L - \mathcal{L}(\boldsymbol{\Phi})}) \circ \boldsymbol{\Phi}
$$
</MathBox>

<MathBox title='Length of extensions of fully connected feedforward neural network' boxType='proposition' tag='proposition-6'>
Let $d, h \in\N_+$. Suppose $\boldsymbol{\Psi}\in\mathcal{N}$ is a fully-connected feedforward neural network with three layer of dimension $\mathbf{d}(\boldsymbol{\Psi}) = (d, h ,d)$. Then
1. For $n\in\N_0$, the $n$-fold power $\boldsymbol{\Psi}^{\circ n}$ has hidden depth $\mathcal{H} (\boldsymbol{\Psi}^{\circ n}) = n$, total length $\mathcal{L}(\boldsymbol{\Psi}^{\circ n}) = n + 1$, and dimension vector
$$
\begin{equation*}
  \N_+^{n+2} \ni \d (\boldsymbol{\Psi}^{\circ n}) = \begin{cases}
    (d,d),\quad& n = 0 \\
    (d,h,\dots,h,d),\quad& n\in\N_+
  \end{cases}
\tag{\label{equation-25}}
\end{equation*}
$$

2. For all $\boldsymbol{\Phi}\in\mathcal{N}$ and $L\in\N_+ \cap [\mathcal{L}(\boldsymbol{\Phi}), \infty)$ with $\mathcal{O}(\boldsymbol{\Phi}) = d$, then 
$$
  \mathcal{L}(\mathbf{E}_{L, \boldsymbol{\Psi}} (\boldsymbol{\Phi})) = L
$$

<details>
<summary>Proof</summary>

**(1)**
Since $\mathcal{H}(\boldsymbol{\Psi}) = 1$, it follows from Proposition $\ref{proposition-5}$ that

$$
  \mathcal{H}(\boldsymbol{\Psi}^{\circ n}) = n\mathcal{H}(\boldsymbol{\Psi}) = n
$$

and from $\mathcal{H}(\boldsymbol{\Psi}^{\circ n}) = \mathcal{L}(\boldsymbol{\Phi}) - 1$ that

$$
  \mathcal{L}(\boldsymbol{\Psi}^{\circ n}) = n + 1
$$

Next, we prove $\eqref{equation-25}$ by induction. The base case $n = 0$ holds since by definition

$$
  \boldsymbol{\Psi}^{\circ 0} = (\mathbf{I}_d, 0) \in\R^{d\times d} \times\R^d
$$

For the induction step, assume that there exists $n\in\N_0$ which satisfies $\eqref{equation-25}$. Then

$$
\begin{align*}
  \mathbf{d}(\boldsymbol{\Psi}^{\circ(n+1)}) =& \mathbf{d}(\boldsymbol{\Psi} \circ (\boldsymbol{\Psi}^{\circ n})) \\
  =& (d,h,\dots,h,d)\in\N^{n+3}
\end{align*}
$$

---
**(2)**

From Proposition $\ref{proposition-1-3}$, we have

$$
\begin{align*}
  \mathcal{H}(\mathbf{E}_{L,\boldsymbol{\Psi}}(\boldsymbol{\Psi})) =& \mathcal{H}((\boldsymbol{\Psi}^{\circ (L - \mathcal{L}(\boldsymbol{\Psi})}) \circ\boldsymbol{\Phi}) \\
  =& \mathcal{H}(\boldsymbol{\Psi}^{\circ (L - \mathcal{L}(\boldsymbol{\Psi})}) + \mathcal{H}(\boldsymbol{\Phi}) \\
  =& (L - \mathcal{L}(\boldsymbol{\Phi})) + \underbrace{\mathcal{H}(\boldsymbol{\Phi})}_{\mathcal{L}(\boldsymbol{\Phi}) - 1} = L - 1
\end{align*}
$$

Since $\mathcal{H}(\mathbf{E}_{L,\boldsymbol{\Psi}}(\boldsymbol{\Phi})) = \mathcal{L}(\mathbf{E}_{L,\boldsymbol{\Psi}}(\boldsymbol{\Phi})) - 1$, it follows that

$$
  \mathcal{L}(\mathbf{E}_{L,\boldsymbol{\Psi}}(\boldsymbol{\Phi})) = L
$$
</details>
</MathBox>

<MathBox title='Realizations of extensions of fully connected feedforward neural network' boxType='proposition' tag='proposition-7'>
Let $a\in C(\R)$ be a continuous activation function and $\mathbf{I}\in\mathcal{N}$ a fully connected feedforward neural network satisfying the identity realization $\mathbf{F}_a^\mathcal{N} (\mathbf{I}) = \operatorname{id}_{\R^{\mathcal{I}(\mathbf{I})}}$. Then
1. For $n\in\N_0$
$$
\begin{equation*}
  \mathbf{F}_a^\mathcal{N} (\mathbf{I}^{\circ n}) = \operatorname{id}_{\R^{\mathcal{I}(\mathbf{I})}}
\tag{\label{equation-26}}
\end{equation*}
$$

2. For all $\boldsymbol{\Phi}\in\mathcal{N}$ and $L \in\N\cap [\mathcal{L}(\boldsymbol{\Phi}), \infty)$ with $\mathcal{O}(\boldsymbol{\Phi}) = \mathcal{I}(\mathbf{I})$
$$
  \mathbf{F}_a^\mathcal{N} (\mathbf{E}_{L,\mathbf{I}} (\boldsymbol{\Phi})) = \mathbf{F}_a^\mathcal{N} (\boldsymbol{\Phi})
$$

<details>
<summary>Proof</summary>

**(1)**: This can be proved by induction on $n\in\N_0$. For the base case $n=0$, it follows from Definition $\ref{definition-1}$ that

$$
  (\mathbf{F}_a^\mathcal{N} (\mathbf{I}^{\circ 0})(\mathbf{x}) = \mathbf{x},\; \mathbf{x}\in\R^d
$$

For the induction step, assume there exists $n\in\N_0$ such that $\eqref{equation-26}$ holds. Then

$$
\begin{align*}
  \mathbf{F}_a^\mathcal{N} (\mathbf{I}^{\circ(n+1)}) =& \mathbf{F}_a^\mathcal{N} (\mathbf{I}\circ \mathbf{I}^{\circ n}) \\
  =& \mathbf{F}_a^\mathcal{N} (\mathbf{I}) \circ \mathbf{F}_a^\mathcal{N} (\mathbf{I}^{\circ n}) \in C(\R^d)
\end{align*}
$$

and for $\mathbf{x}\in\R^d$

$$
\begin{align*}
  (\mathbf{F}_a^\mathcal{N} (\mathbf{I}^{\circ (n+1)}))(\mathbf{x}) =& (\mathbf{F}_a^\mathcal{N} (\mathbf{I}) \circ \mathbf{F}_a^\mathcal{N} (\mathbf{I}^{\circ n}))(\mathbf{x}) \\
  =& (\mathbf{F}_a^\mathcal{N}(\mathbf{I}))(\mathbf{F}_a^\mathcal{N} (\mathbf{I}^{\circ n}))(\mathbf{x})) \\
  =& (\mathbf{F}_a^\mathcal{N}(\mathbf{I})(\mathbf{x}) = \mathbf{x}
\end{align*}
$$

**(2)**

Since $\mathcal{I}(\mathbf{I}) = \mathcal{O}(\boldsymbol{\Phi}) = \mathcal{O}(\boldsymbol{\Phi})$ it follows from Definition $\ref{definition-2}$

$$
\begin{align*}
  \mathbf{F}_a^\mathcal{N} (\mathbf{E}_{L,\mathbf{I}} (\boldsymbol{\Phi})) = \mathbf{F}_a^\mathcal{N} (\mathbf{I}^{\circ(L - \mathcal{L}(\boldsymbol{\Phi})} \circ\boldsymbol{\Phi}) \in C(\R^{\mathcal{I}(\boldsymbol{\Phi})}, \R^{\mathcal{O}\mathbf{I}}) = C(\R^{\mathcal{I}(\boldsymbol{\Phi})}, \R^{\mathcal{O}(\boldsymbol{\Phi})})
\end{align*}
$$

and for $x\in\R^{\mathcal{I}(\boldsymbol{\Phi})}$

$$
\begin{align*}
  (\mathbf{F}_a^\mathcal{N} (\mathbf{E}_{L,\mathbf{I}} (\boldsymbol{\Phi}))(\mathbf{x}) =& (\mathbf{F}_a^\mathcal{N} (\mathbf{I}^{\circ (L - \mathcal{L}(\boldsymbol{\Phi})}))((\mathbf{F}_a^\mathcal{N}(\boldsymbol{\Phi}))(\mathbf{x})) \\
  =& (\mathbf{F}_a^\mathcal{N}(\boldsymbol{\Phi}))(\mathbf{x})
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Architectures of extensions of fully connected feedforward neural network' boxType='corollary'>
Let $d,h,L,\mathfrak{L}\in\N_+$ and $\ell_0,\dots,\ell_{L-1} \in\N_+$. Suppose $\boldsymbol{\Phi},\boldsymbol{\Psi}\in\mathcal{N}$ are fully connected feedforward neural networks with respective dimension vectors $\mathbf{d}(\boldsymbol{\Phi}) = (\ell_0,\dots,\ell_{L-1},d)\in\N^L$ and $\mathbf{d}(\boldsymbol{\Psi}) = (d,h,d)\in\N^3$. Then the dimension vector of the extended network $\mathbf{E}_{\mathfrak{L}, \boldsymbol{\Psi}} (\boldsymbol{\Phi})$ is given by

$$
  \N^{\mathfrak{L} + 1} \ni \mathbf{d}(\mathbf{E}_{\mathfrak{L}, \boldsymbol{\Psi}} (\boldsymbol{\Phi})) = \begin{cases}
    (\ell_0,\dots,\ell_{L-1},d),\quad& \mathfrak{L} = L \\
    (\ell_0,\dots,\ell_{L_1},\underbrace{h,\dots,h}_{\mathfrak{L} - L \text{ times}},d),\quad& \mathfrak{L} > L
  \end{cases}
$$

<details>
<summary>Proof</summary>

From Proposition $\ref{proposition-6}$, we have $\mathcal{H}(\boldsymbol{\Psi}^{\circ(\mathfrak{L} - L)}) = \mathfrak{L} - L$ and

$$
  \N^{\mathfrak{L} - L + 2} \ni \mathbf{d}(\boldsymbol{\Psi}^{\circ(\mathfrak{L} - L)}) = \begin{cases}
    (d, d),\quad& \mathfrak{L} = L \\
    (d,h,\dots,h,d),\quad& \mathfrak{L} > L
  \end{cases}
$$

From Proposition $\ref{proposition-1-3}$, we obtain

$$
\begin{align*}
  \mathcal{H}(\boldsymbol{\Psi}^{\circ(\mathfrak{L} - L)}\circ\boldsymbol{\Phi}) =& \mathcal{H}(\boldsymbol{\Phi}^{\circ(\mathfrak{L} - L)} + \mathcal{H}(\boldsymbol{\Phi}) \\
  =& (\mathfrak{L} - L) + L - 1 = \mathfrak{L} - 1
\end{align*}
$$

and

$$
  \N^{\mathfrak{L} + 1} \ni \mathbf{d}(\boldsymbol{\Psi}^{\circ(\mathfrak{L} - L)} \circ\boldsymbol{\Phi}) = \begin{cases}
    (\ell_0,\dots,\ell_{L-1}, d),\quad& \mathfrak{L} = L \\
    (d,\ell_0,\dots,\ell_{L-1},h,\dots,h,d),\quad& \mathfrak{L} > L
  \end{cases}
$$

The result follows since $\mathbf{E}_{\mathfrak{L}, \boldsymbol{\Psi}} = \boldsymbol{\Psi}^{\circ(\mathfrak{L} - L)} \circ\boldsymbol{\Phi}$ by Definition $\ref{definition-2}$.
</details>
</MathBox>

#### Padded parallelization

<MathBox title='Parallelization of fully connected feedfoward neural networks of unequal depth' boxType='definition'>
For $n\in\N_+$, let $\boldsymbol{\Psi} = (\boldsymbol{\Psi}_j)_{j=1}^n \in\mathcal{N}^n$ be a tuple of layer-compatible fully-connected feedforward neural networks satisfying, for $j=1,\dots,n$
- $\mathcal{H}(\boldsymbol{\Psi}_j) = 1$, i.e. each $\boldsymbol{\Psi}_j$ has exactly one hidden layer
- $\mathcal{I}(\boldsymbol{\Psi}_j) = \mathcal{O}(\boldsymbol{\Psi}_j)$

Define the set

$$
  \mathcal{D}_{n,\boldsymbol{\Psi}} = \set{\boldsymbol{\Phi} = (\boldsymbol{\Phi}_j)_{j=1}^n \in \mathcal{N}^n | \mathcal{O}(\boldsymbol{\Phi}_j) = \mathcal{I}(\boldsymbol{\Psi}_j), j=1,\dots,n} 
$$

and for $\boldsymbol{\Phi} = (\boldsymbol{\Phi}_j)_{j=1}^n \in\mathcal{D}_{n,\boldsymbol{\Psi}}$ set

$$
  L_\text{max} := \max_{k\in\set{1,dots,n}} \mathcal{L}(\boldsymbol{\Phi}_k)
$$

The parallelization operator with padding, $\mathbf{P}_{n, \boldsymbol{\Psi}}: \mathcal{D}_{n,\boldsymbol{\Psi}} \to \mathcal{N}$ is defined by

$$
  \mathbf{P}_{n, \boldsymbol{\Psi}} (\boldsymbol{\Phi}) = \mathbf{P}_n (\mathbf{E}_{L_\text{max}, \boldsymbol{\Psi}_1} (\boldsymbol{\Phi}_1), \dots, \mathbf{E}_{L_\text{max}, \boldsymbol{\Psi}_n} (\boldsymbol{\Phi}_n))
$$

where
- $\mathbf{E}_{L_\text{max}, \boldsymbol{\Psi}_f}$ is the extension operator defined by
$$
  \mathbf{E}_{L_\text{max}, \boldsymbol{\Psi}_f} (\boldsymbol{\Phi}_j) := \boldsymbol{\Psi}_j^{\circ(L_\text{max} - \mathcal{L}(\boldsymbol{\Phi}_j)} \circ \boldsymbol{\Phi}_j
$$
- $\mathbf{P}_n$ denotes the standard parallelization operator for layer-compatible networks of equal length.
</MathBox>

<MathBox title='Realization of parallelization of fully connected feedfoward neural networks of unequal length' boxType='proposition'>
Let $a\in C(\R)$ be a continuous activation function. For $n\in\N$ suppose 

$$
\begin{align*}
  \mathbf{I} =& (\mathbf{I}_j)_{j=1}^n \in\mathcal{N}^n \\
  \boldsymbol{\Phi} =& (\boldsymbol{\Phi}_j)_{j=1}) \in\mathcal{N}^n
\end{align*}
$$

are tuples of fully connected feedforward neural networks such that, for $j=1,\dots,n$
- $\mathcal{H}(\mathbf{I}_j) = 1$
- $\mathcal{I}(\mathbf{I}_j) = \mathcal{O}(\mathbf{I}_j) = \mathcal{O}(\boldsymbol{\Phi}_j)$
- the realization of $\mathbf{I}_j$ is the identity map, i.e. for $\mathbf{x}\in\R^{\mathcal{O}(\boldsymbol{\Phi}_j)}$
$$
  (\mathbf{F}_a^\mathcal{N} (\mathbf{I}_j))(\mathbf{x}) = \mathbf{x}
$$

The realization of the parallelized padded network $\mathbf{P}_{n,\mathbf{I}}(\boldsymbol{\Phi})$ satisfies

1. 
$$
  \mathbf{F}_a^\mathcal{N} (\mathbf{P}_{n,\mathbf{I}}(\boldsymbol{\Phi})) \in C\left(\R^{\sum_{j=1}^n \mathcal{I}(\boldsymbol{\Phi}_j)}, \R^{\sum_{j=1}^n \mathcal{O}(\boldsymbol{\Phi}_j)} \right)
$$

2. for $\mathbf{x}_j \in\R^{\mathcal{I}(\Phi_j)}$ with $j=1,\dots,n$
$$
  \left(\mathbf{F}_a^\mathcal{N} (\mathbf{P}_{n,\mathbf{I}}(\boldsymbol{\Phi})) \right)(\mathbf{x}_1,\dots,\mathbf{x}_n) = \left((\mathbf{F}_a^\mathcal{N} (\boldsymbol{\Phi}_1))(\mathbf{x}_1),\dots, \mathbf{F}_a^\mathcal{N} (\boldsymbol{\Phi}_n))(\mathbf{x}_n) \right) \in \R^{\sum_{j=1}^n \mathcal{O}(\boldsymbol{\Phi}_j)}
$$

<details>
<summary>Proof</summary>

Let $L \in\N_+$ satisfy $L = \max_{j\in\set{1,\dots,n}} \mathcal{L}(\boldsymbol{\Phi}_j)$. From Proposition $\ref{proposition-6-2}$ and Proposition $\ref{proposition-7-2}$, it follows that, for $j=1,\dots,n$, 
- $\mathcal{L}(\mathbf{E}_{L,\mathbf{I}_j}(\boldsymbol{\Phi}_j)) = L$
- $\mathbf{F}_a^\mathcal{N} (\mathbf{E}_{L,\mathbf{I}_j} (\boldsymbol{\Phi}_j))\in C(\R^{\mathcal{I}(\boldsymbol{\Phi}_j)}, \R^{\mathcal{O}(\boldsymbol{\Phi}_j)})$
- for $\mathbf{x}\in\R^{\mathcal{I}(\boldsymbol{\Phi}_j)}$
$$
  \left(\mathbf{F}_a^\mathcal{N} (\mathbf{E}_{L, \mathbf{I}_j} (\boldsymbol{\Phi}_j))\right)(\mathbf{x}) = (\mathbf{F}_a^\mathcal{N}(\boldsymbol{\Phi}_j))(\mathbf{x})
$$

By Proposition $\ref{proposition-4}$, we obtain

$$
  \mathbf{F}_a^\mathcal{N} (\mathbf{P}_n (\mathbf{E}_{L, \mathbf{I}_1} (\boldsymbol{\Phi}_1),\dots, \mathbf{E}_{L, \mathbf{I}_n} (\boldsymbol{\Phi}_n))) \in C\left(\R^{\sum_{j=1}^n \mathcal{I}(\boldsymbol{\Phi}_j)}, \R^{\sum_{j=1}^n \mathcal{O}(\boldsymbol{\Phi}_j)} \right)
$$

and for $\mathbf{x}_j \in \R^{\mathcal{I}(\boldsymbol{\Phi}_j)}$ with $j=1,\dots,n$

$$
\begin{align*}
  &\left(\mathbf{F}_a^\mathcal{N} (\mathbf{P}_n (\mathbf{E}_{L, \mathbf{I}_1} (\boldsymbol{\Phi}_1),\dots, \mathbf{E}_{L, \mathbf{I}_n} (\boldsymbol{\Phi}_n)))\right)(\mathbf{x}_1, \dots, \mathbf{x}_n) \\
  =&  \left(\mathbf{F}_a^\mathcal{N} (\mathbf{P}_{n,\mathbf{I}}(\mathbf{E}_{L, \mathbf{I}_1} (\boldsymbol{\Phi}_1)),\dots,\mathbf{E}_{L, \mathbf{I}_n}(\boldsymbol{\Phi}_n)))\right) (\mathbf{x}_1),\dots, \mathbf{x}_n) \\
  =& \left((\mathbf{F}_a^\mathcal{N} (\boldsymbol{\Phi}_1))(\mathbf{x}_1),\dots, \mathbf{F}_a^\mathcal{N} (\boldsymbol{\Phi}_n))(\mathbf{x}_n) \right)
\end{align*}
$$
</details>
</MathBox>

### Scalar multiplication of neural networks

<MathBox title='Fully connected feedforward affine neural network' boxType='proposition' tag='proposition-8'>
For $m,n\in\N_+$ let $\mathbf{B}\in\R^{m\times n}$ and $\mathbf{b}\in\R^m$. We define the fully connected feedforward affine neural network 

$$
  \mathbf{A}_{\mathbf{W}, \mathbf{b}} \in (\R^{m\times n} \times \R^m) \subseteq \mathcal{N}
$$

by $\mathbf{A}_{\mathbf{W}, \mathbf{b}} = (\mathbf{W}, \mathbf{b})$, which consists of a single affine layer with weight matrix $\mathbf{W}$ and bias vector $\mathbf{b}$. An affine network $\mathbf{A}_{\mathbf{W}, \mathbf{b}}$ has the following properties:

1. **Dimension vector:** $\mathbf{d}(\mathbf{A}_{\mathbf{W}, \mathbf{b}}) = (n, m)\in\N^2$
2. **Realization:** for all continuous activation functions $a\in C(\R)$, the realization $\mathbf{F}_a^\mathcal{N} (\mathbf{A}_{\mathbf{W}, \mathbf{b}}) \in C(\R^n, \R^m)$ is given by

$$
  (\mathbf{F}_a^\mathcal{N} (\mathbf{A}_{\mathbf{W}, \mathbf{b}}))(\mathbf{x}) = \mathbf{Wx} + \mathbf{b}\in\R^m,\; \mathbf{x}\in\R^n
$$

In particular, the realized function is independent of the choice of activation function $a$.

<details>
<summary>Proof</summary>

**(1):** By definition $\mathbf{A}_{\mathbf{W}, \mathbf{b}}$ consists of a single affine layer map $\R^n \to \R^m$, hence its dimension vector is $\mathbf{d}(\mathbf{A}_{\mathbf{W}, \mathbf{b}}) = (n, m)$.

---
**(2):** By $\eqref{equation-24}$, the realization $\mathbf{F}_a^\mathcal{N} (\mathbf{A}_{\mathbf{W}, \mathbf{b}})$ is precisely the affine transformation $\mathbf{x} \mapsto \mathbf{Wx} + \mathbf{b}$ for $\mathbf{x}\in\R^n$, which is continuous on $\R^n$.
</details>
</MathBox>

<MathBox title='Compositions with fully connected feedforward affine neural network' boxType='proposition' tag='proposition-9'>
Let $\mathbf{A}_{\mathbf{W}, \mathbf{b}} \in (\R^{m\times n} \times \R^m)$ be an affine network and $\boldsymbol{\Phi} \in\mathcal{N}$ a fully connected feedforward neural network. For all $m\in\N_+$ with $\mathbf{W}\in\R^{m\times \mathcal{O}(\boldsymbol{\Phi})}$ and $\mathbf{b}\in\R^m$, the composition $\mathbf{A}_{\mathbf{W}, \mathbf{b}} \circ \boldsymbol{\Phi}$ has the following properties.

1. **Dimension vector:**
$$
  \mathbf{d}(\mathbf{A}_{\mathbf{W}, \mathbf{b}} \circ \boldsymbol{\Phi}) = (d_0 (\boldsymbol{\Phi}), \dots, d_{\mathcal{H}(\boldsymbol{\Phi})}(\boldsymbol{\Phi}), m)
$$

2. **Realization:** For all continuous activation functions $a\in C(\R)$, the realization function $\mathbf{F}_a^\mathcal{N} (\mathbf{A}_{\mathbf{W}, \mathbf{b}} \circ\boldsymbol{\Phi}) \in C(\R^{\mathcal{I}(\boldsymbol{\Phi})}, \R^m)$ is given by
$$
  (\mathbf{F}_a^\mathcal{N} (\mathbf{A}_{\mathbf{W}, \mathbf{b}} \circ\boldsymbol{\Phi}))(\mathbf{x}) = \mathbf{W}\left((\mathbf{F}_a^\mathcal{N}(\boldsymbol{\Phi}))(\mathbf{x}) \right) + \mathbf{b}
$$

For all $n\in\N_+$, with $\mathbf{W}\in\R^{\mathcal{I}(\boldsymbol{\Phi})\in n}$ and $\mathbf{b}$, the composition $\boldsymbol{\Phi}\circ\mathbf{A}_{\mathbf{W}, \mathbf{b}}$ has the following properties

3. **Dimension vector:**
$$
  \mathbf{d}(\boldsymbol{\Phi}\circ\mathbf{A}_{\mathbf{W}, \mathbf{b}}) = (n, d_1 (\boldsymbol{\Phi}),\dots,d_{\mathcal{L}(\boldsymbol{\Phi})}(\boldsymbol{\Phi}))
$$

4. **Realization:** For all continuous activation functions $a\in C(\R)$, the realization function $\mathbf{F}_a^\mathcal{N} (\boldsymbol{\Phi}\circ\mathbf{A}_{\mathbf{W}, \mathbf{b}}) \in C(\R^n, \R^{\mathcal{O}(\boldsymbol{\Phi})})$ is given by
$$
  (\mathbf{F}_a^\mathcal{N} (\boldsymbol{\Phi}\circ\mathbf{A}_{\mathbf{W}, \mathbf{b}}))(\mathbf{x}) = (\mathbf{F}_a^\mathcal{N}(\boldsymbol{\Phi}))(\mathbf{Wx} + \mathbf{b})
$$

<details>
<summary>Proof</summary>

This follows from Propositions $\ref{proposition-1}$ and $\ref{proposition-7}$.
</details>
</MathBox>

<MathBox title='Scalar multiplication of fully connected feedforward neural networks' boxType='proposition'>
Let $\mathcal{N}$ be the parameter space of fully connected feedforward neural networks. For $\lambda\in\R$ and $\boldsymbol{\Phi}\in\mathcal{N}$, we define the scalar multiplication operator $\circledast:\R \times\mathcal{N}\to\mathcal{N}$ by

$$
  \lambda\circledast\boldsymbol{\Phi} = \mathbf{A}_{\lambda \mathbf{I}_{\mathcal{O}(\boldsymbol{\Phi})}, \mathbf{0}} \circ \boldsymbol{\Phi}
$$

where
- $\mathbf{I}_{\mathcal{O}(\boldsymbol{\Phi})} \in\R^{\mathcal{O}(\boldsymbol{\Phi}) \times\mathcal{O}(\boldsymbol{\Phi})}$ denotes the identity matrix
- $\mathbf{A}_{\lambda\mathbf{I}_{\mathcal{O}(\boldsymbol{\Phi})}, \mathbf{0}} \in \R^{\mathcal{I}(\boldsymbol{\Phi})\times\mathcal{O}(\boldsymbol{\Phi})} \times\R^{\mathcal{O}(\boldsymbol{\Phi})}$ is the affine neural network defined in Proposition $\ref{proposition-8}$.

The scalar product $\lambda\circledast\boldsymbol{\Phi}$ has the following properties.
1. **Dimension vector:** $\mathbf{d}(\lambda\circledast\boldsymbol{\Phi}) = \mathbf{d}(\boldsymbol{\Phi})$
2. **Realization:** For all continuous activation functions $a\in C(\R)$, the realization $\mathbf{F}_a^\mathcal{N} (\lambda\circledast\boldsymbol{\Phi}) \in C(\R^{\mathcal{I}(\boldsymbol{\Phi})}, \R^{\mathcal{O}(\boldsymbol{\Phi})})$ is given by
$$
  \left(\mathbf{F}_a^\mathcal{N} (\lambda\circledast\boldsymbol{\Phi}) \right)(\mathbf{x}) = \lambda\left((\mathbf{F}_a^\mathcal{N} (\boldsymbol{\Phi})) (\mathbf{x}) \right)
$$

<details>
<summary>Proof</summary>

Let $L\in\N_+$ and $\ell_0,\dots,\ell_L \in\N_+$ satisfy $L = \mathcal{L}(\boldsymbol{\Phi})$ and $(\ell_j)_{j=0}^L = \mathbf{d}(\boldsymbol{\Phi})$.

**(1):** By Proposition $\ref{proposition-8-1}$, we obtain

$$
  \mathbf{d}(\mathbf{A}_{\lambda\mathbf{I}_{\mathcal{O}(\boldsymbol{\Phi})}, \mathbf{0}}) = (\mathcal{O}(\boldsymbol{\Phi}), \mathcal{O}(\boldsymbol{\Phi}))
$$

Applying Proposition $\ref{proposition-9-1}$ yields

$$
\begin{align*}
  \mathbf{d}(\lambda\circledast\boldsymbol{\Phi}) =& \mathbf{d}(\mathbf{A}_{\lambda\mathbf{I}_{\mathcal{O}(\boldsymbol{\Phi})}, \mathbf{0}} \circ\boldsymbol{\Phi}) \\
  =& (\ell_0,\dots,\ell_{L-1}, \mathcal{O}(\boldsymbol{\Phi})) = \mathbf{d}(\boldsymbol{\Phi})
\end{align*}
$$

**(2):** From Proposition $\ref{proposition-9-2}$ we have

$$
\begin{align*}
  \left(\mathbf{F}_a^\mathcal{N} (\lambda\circledast\boldsymbol{\Phi}) \right)(\mathbf{x}) =& \left(\mathbf{F}_a^\mathcal{N} (\mathbf{A}_{\lambda\mathbf{I}_{\mathcal{O}(\boldsymbol{\Phi})}, \mathbf{0}} \circ\mathbf{\Phi}) \right)(\mathbf{x}) \\
  =& \lambda \mathbf{I}_{\mathcal{O}(\boldsymbol{\Phi})} \left((\mathbf{F}_a^\mathcal{N} (\boldsymbol{\Phi}))(\mathbf{x}) \right) \\
  =& \lambda \left((\mathbf{F}_a^\mathcal{N} (\boldsymbol{\Phi}))(\mathbf{x}) \right)
\end{align*}
$$
</details>
</MathBox>

### Sum of neural networks

#### Sum of vectors as networks

<MathBox title='Sums of vectors as fully-connected feedforward neural networks' boxType='proposition' tag='proposition-10'>
For $m, n\in\N$, define the matrix

$$
  \mathbf{W}_{m,n} := \begin{bmatrix} \mathbf{I}_m & \cdots & \mathbf{I}_m \end{bmatrix} \in \R^{m\times (mn)}
$$

as the horizontal concatenation of $n$ copies of the identity matrix $\mathbf{I}_m \in\R^{m\times m}$. We define the summation network $\mathbf{S}_{m, n} \in (\R^{m\times(mn)} \times\R^m)\subseteq\mathcal{N}$ as the fully connected feedforward affine neural network

$$
  \mathbf{S}_{m, n} = \mathbf{A}_{\mathbf{W}_{m,n}, \mathbf{0}}
$$

which satisfies the following properties.
1. **Dimension vector:** $\mathbf{d}(\mathbf{S}_{m,n}) = (mn, m) \in\N^2$
2. **Realization:** For all continuous activation functions $a\in C(\R)$, the realization $\mathbf{F}_a^\mathcal{N} (\mathbf{S}_{m,n}) \in C(\R^{mm}, \R^m)$ is given by, for $\mathbf{x}_1, \dots, \mathbf{x}_n \in \R^m$
$$
  \left(\mathbf{F}_a^\mathcal{N} (\mathbf{S}_{m,n}) \right)(\mathbf{x}_1, \dots, \mathbf{x}_n) = \sum_{k=1}^n \mathbf{x}_k
$$

<details>
<summary>Proof</summary>

**(1):** By definition $\mathbf{S}_{m, n}$ consists of a single affine layer map $\R^{mn} \to \R^m$, hence its dimension vector is $\mathbf{d}(\mathbf{S}_{m, n}) = (mn, m)$.

**(2):** From Proposition $\ref{proposition-8}$ we have

$$
\begin{align*}
  (\mathbf{F}_a^\mathcal{N} (\mathbf{S}_{m,n}))(\mathbf{x}_1,\dots,\mathbf{x}_n) =& (\mathbf{F}_a^\mathcal{N} (\mathbf{A}_{\mathbf{W}_{m,n}, \mathbf{0}})(\mathbf{x}_1,\dots,\mathbf{x}_n) \\
  =& \begin{bmatrix} \mathbf{I}_m & \cdots & \mathbf{I}_m \end{bmatrix} \cdot \begin{bmatrix} \mathbf{x}_1 \\ \vdots \\ \mathbf{x}_n \end{bmatrix} \\
  =& \sum_{k=1}^n \mathbf{I}_m \mathbf{x}_k = \sum_{k=1}^n \mathbf{x}_k
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Compositions with fully-connected feedforward summation neural networks' boxType='proposition'>
Let $a\in C(\R)$ be a continuous activation function and $\boldsymbol{\Phi}\in\mathcal{N}$ a fully connected feedforward neural network. 

1. For $m,n\in\N_+$ assume $\mathcal{O}(\boldsymbol{\Phi}) = mn$. Then the composition $\mathbf{S}_{m,n} \circ\boldsymbol{\Phi}$ is well-defined, and its realization $\mathbf{F}_a^\mathcal{N} (\mathbf{S}_{m,n} \circ \boldsymbol{\Phi}) \in C(\R^{n\mathcal{I}(\boldsymbol{\Phi})}, \R^{\mathcal{O}(\boldsymbol{\Phi})})$ is given by, for $\mathbf{x}\in\R^{\mathcal{I}(\boldsymbol{\Phi})}$ and $\mathbf{y}_1,\dots, \mathbf{y}_n \in\R^m$ with $(\mathbf{F}_a^\mathcal{N}(\boldsymbol{\Phi}))(\mathbf{x}) = (\mathbf{y}_1,\dots,\mathbf{y}_n)$

$$
  \left(\mathbf{F}_a^\mathcal{N} (\mathbf{S}_{m, n} \circ\boldsymbol{\Phi}) \right)(\mathbf{x}) = \sum_{k=1}^n \mathbf{y}_k
$$

2. For $n\in\N$, the composition $\boldsymbol{\Phi}\circ\mathbf{S}_{\mathcal{I}(\boldsymbol{\Phi}), n}$ is well-defined and its realization $\mathbf{F}_a^\mathcal{N} (\boldsymbol{\Phi}\circ\mathbf{S}_{\mathcal{I}(\boldsymbol{\Phi}), n})\in C(\R^{n\mathcal{I}(\boldsymbol{\Phi})}, \R^{\mathcal{O}(\boldsymbol{\Phi})})$ is given by, for $\mathbf{x}_1,\dots,\mathbf{x}_n \in\R^{\mathcal{I}(\boldsymbol{\Phi})}$

$$
  \left(\mathbf{F}_a^\mathcal{N} (\boldsymbol{\Phi}\circ\mathbf{S}_{\mathcal{I}(\boldsymbol{\Phi}), n} \right)(\mathbf{x}_1,\dots,\mathbf{x}_n) = (\mathbf{F}_a^\mathcal{N} (\boldsymbol{\Phi}))\left(\sum_{k=1}^n \mathbf{x}_k \right)
$$

<details>
<summary>Proof</summary>

This follows from Propositions $\ref{proposition-1-5}$ and $\ref{proposition-10}$. 
</details>
</MathBox>

#### Concatenation of vectors as networks

<MathBox title='Concatenation of vectors as fully connected feedforward neural networks' boxType='proposition' tag='proposition-11'>
For $m, n\in\N$, define the matrix

$$
  \mathbf{W}_{m,n} := \begin{bmatrix} \mathbf{I}_m & \cdots & \mathbf{I}_m \end{bmatrix} \in \R^{m\times (mn)}
$$

as the horizontal concatenation of $n$ copies of the identity matrix $\mathbf{I}_m \in\R^{m\times m}$. We define the concatenation network $\mathbf{T}_{m, n} \in (\R^{(mn)\times m} \times\R^(mn)\subseteq\mathcal{N}$ as the fully connected feedforward affine neural network

$$
  \mathbf{S}_{m, n} = \mathbf{A}_{\mathbf{W}_{m,n}^\top, \mathbf{0}}
$$

which satisfies the following properties.
1. **Dimension vector:** $\mathbf{d}(\mathbf{T}_{m,n}) = (m, mn) \in\N^2$
2. **Realization:** For all continuous activation functions $a\in C(\R)$, the realization $\mathbf{F}_a^\mathcal{N} (\mathbf{T}_{m,n}) \in C(\R^m, \R^{mn})$ is given by, for $\mathbf{x}\in\R^m$
$$
  (\mathbf{F}_a^\mathcal{N} (\mathbf{T}_{m,n}))(\mathbf{x}) = (\mathbf{x},\dots,\mathbf{x}) \in\R^{mn}
$$

<details>
<summary>Proof</summary>

**(1):** By definition $\mathbf{T}_{m, n}$ consists of a single affine layer map $\R^m \to \R^{mn}$, hence its dimension vector is $\mathbf{d}(\mathbf{T}_{m, n}) = (m, mn)$.

**(2):** From Proposition $\ref{proposition-8}$ we have

$$
\begin{align*}
  (\mathbf{F}_a^\mathcal{N} (\mathbf{T}_{m,n}))(\mathbf{x}) =& (\mathbf{F}_a^\mathcal{N} (\mathbf{A}_{\mathbf{W}_{m,n}^\top, \mathbf{0}})(\mathbf{x}) \\
  =& \begin{bmatrix} \mathbf{I}_m \\ \vdots \\ \mathbf{I}_m \end{bmatrix} \cdot \mathbf{x} \\
  =& \begin{bmatrix} \mathbf{x} & \cdots & \mathbf{x} \end{bmatrix} \in \R^{mn}
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Compositions with fully-connected feedforward concatenation neural networks' boxType='proposition' tag='proposition-12'>
Let $a\in C(\R)$ be a continuous activation function and $\boldsymbol{\Phi}\in\mathcal{N}$ a fully connected feedforward neural network. 

1. For $n\in\N_+$, the composition $\mathbf{T}_{\mathcal{O}(\boldsymbol{\Phi}),n} \circ\boldsymbol{\Phi}$ is well-defined, and its realization $\mathbf{F}_a^\mathcal{N} (\mathbf{T}_{\mathcal{O}(\boldsymbol{\Phi}),n} \circ \boldsymbol{\Phi}) \in C(\R^{\mathcal{I}(\boldsymbol{\Phi})}, \R^{n\mathcal{O}(\boldsymbol{\Phi})})$ is given by, for $\mathbf{x}\in\R^{\mathcal{I}(\boldsymbol{\Phi})}$

$$
  \left(\mathbf{F}_a^\mathcal{N} (\mathbf{T}_{m, n} \circ\boldsymbol{\Phi}) \right)(\mathbf{x}) = \left((\mathbf{F}_a^\mathcal{N} (\boldsymbol{\Phi}))(\mathbf{x}),\dots, (\mathbf{F}_a^\mathcal{N} (\boldsymbol{\Phi}))(\mathbf{x})\right) 
$$

2. For $m, n\in\N$, assume $\mathcal{I}(\boldsymbol{\Phi})$. Then the composition $\boldsymbol{\Phi}\circ\mathbf{T}_{m, n}$ is well-defined and its realization $\mathbf{F}_a^\mathcal{N} (\boldsymbol{\Phi}\circ\mathbf{T}_{m, n})\in C(\R^m, \R^{\mathcal{O}(\boldsymbol{\Phi})})$ is given by, for $\mathbf{x} \in\R^m$

$$
  \left(\mathbf{F}_a^\mathcal{N} (\boldsymbol{\Phi}\circ\mathbf{T}_{\mathcal{I}(\boldsymbol{\Phi}), n} \right)(\mathbf{x}) = (\mathbf{F}_a^\mathcal{N} (\boldsymbol{\Phi}))\underbrace{(\mathbf{x},\dots,\mathbf{x})}_{\in\R^{mn}}
$$

<details>
<summary>Proof</summary>

This follows from Propositions $\ref{proposition-1-5}$ and $\ref{proposition-11}$. 
</details>
</MathBox>

#### Sum of fully connected feedforward neural networks

<MathBox title='Sum of fully connected feedforward neural networks' boxType='proposition'>
Let $m, n\in\Z$ with $n\geq m$. Suppose $\boldsymbol{\Phi}_m,\dots,\boldsymbol{\Phi}_{m+1},\dots,\boldsymbol{\Phi}_n \in\mathcal{N}$ are fully connected feedforward neural networks satisfying, for all $k\in\set{m, m+1,\dots, n}$

$$
\begin{align*}
  \mathcal{L}(\boldsymbol{\Phi}_k) =& \mathcal{L}(\boldsymbol{\Phi}_m) \\
  \mathcal{I}(\boldsymbol{\Phi}_k) =& \mathcal{I}(\boldsymbol{\Phi}_m) \\
  \mathcal{O}(\boldsymbol{\Phi}_k) =& \mathcal{O}(\boldsymbol{\Phi}_m)
\end{align*}
$$

Denote $N := n - m + 1$. We define the sum network $\bigoplus_{k=m}^n \boldsymbol{\Phi}_k \in \mathcal{N}$ by

$$
  \bigoplus_{k=m}^n \boldsymbol{\Phi}_k = \left(\mathbf{S}_{\mathcal{O}(\boldsymbol{\Phi}_m), N} \circ \mathbf{P}_{N} (\boldsymbol{\Phi}_m, \boldsymbol{\Phi}_{m+1},\dots,\boldsymbol{\Phi}_n) \circ \mathbf{T}_{\mathcal{I}(\boldsymbol{\Phi}_i), N}\right) \in\mathcal{N}
$$

where
- $\mathbf{T}_{d,N} \in \R^{dN \times d} \times \R^{dN}$ denotes the concatenation network
- $\mathbf{P}_N$ denotes the parallelization operator
- $\mathbf{S}_{d, N} \in \R^{d \times (dN)} \times\R^d$ denotes the summation network

The sum network $\bigoplus_{k=m}^n \boldsymbol{\Phi}_k$ satisfies the following properties.
1. **Depth:** $\mathcal{L}\left(\bigoplus_{k=m}^n \boldsymbol{\Phi}_k \right) = \mathcal{L}(\boldsymbol{\Phi}_m)$
2. **Dimension vector:**
$$
  \mathbf{d}\left(\bigoplus_{k=m}^n \boldsymbol{\Phi}_k \right) = \left(\mathcal{I}(\boldsymbol{\Phi}_k), \sum_{k=m}^n d_1 (\boldsymbol{\Phi}_k), \dots, \sum_{k=m}^n d_{\mathcal{H}(\boldsymbol{\Phi})} (\boldsymbol{\Phi}_k), \mathcal{O}(\boldsymbol{\Phi}_m) \right)
$$
3. **Realization:** For all continuous activation functions $a\in C(\R)$, the realization $\mathbf{F}_a^\mathcal{N} \left(\bigoplus_{k=m}^n \boldsymbol{\Phi}_k \right) \in C(\R^{\mathcal{I}(\boldsymbol{\Phi}_m)}, \R^{\mathcal{O}(\boldsymbol{\Phi}_m)})$ is given by
$$
  \mathbf{F}_a^\mathcal{N} \left(\bigoplus_{k=m}^n \boldsymbol{\Phi}_k \right) = \sum_{k=m}^n (\mathbf{F}_a^\mathcal{N} (\boldsymbol{\Phi}_k))
$$

<details>
<summary>Proof</summary>

**(2):**
From Definition $\ref{definition-3}$, we have

$$
\begin{align*}
  &\mathbf{d}(\mathbf{P}_N (\boldsymbol{\Phi}_m, \boldsymbol{\Phi}_{m+1},\dots,\boldsymbol{\Phi}_n) \\
  =& \left(\sum_{k=m}^n d_0 (\boldsymbol{\Phi}_k), \sum_{k=m}^n d_1 (\boldsymbol{\Phi}_k), \dots, \sum_{k=m}^n d_{\mathcal{L}(\boldsymbol{\Phi}) - 1} (\boldsymbol{\Phi}_k), \sum_{k=m}^n d_{\mathcal{L}(\boldsymbol{\Phi}_m)} (\boldsymbol{\Phi}_k) \right) \\
  =& \left( N\mathcal{I}(\boldsymbol{\Phi}_m), \sum_{k=m}^n d_1 (\boldsymbol{\Phi}_k),\dots,\sum_{k=m}^n d_{\mathcal{L}(\boldsymbol{\Phi}_m) - 1} (\boldsymbol{\Phi}_k), N\mathcal{O}(\boldsymbol{\Phi}_m) \right)
\end{align*}
$$

By Proposition $\ref{proposition-10-1}$, we find

$$
  \mathbf{d}(\mathbf{S}_{\mathcal{O}(\boldsymbol{\Phi}_m), N}) = (N\mathcal{O}(\boldsymbol{\Phi}_m), \mathcal{O}(\boldsymbol{\Phi}_m))
$$

Likewise, Proposition $\ref{proposition-11-1}$ yields

$$
  \mathbf{d}(\mathbf{T}_{\mathcal{I}(\boldsymbol{\Phi}_m), N} = (\mathcal{I}(\boldsymbol{\Phi}_m), N\mathcal{I}(\boldsymbol{\Phi}_m))
$$

Applying Proposition $\ref{proposition-1-1}$ leads to

$$
\begin{align*}
  \mathbf{d}\left(\bigoplus_{k=m}^n \boldsymbol{\Phi}_k \right) =& \mathbf{d}\left(\mathbf{S}_{\mathcal{O}(\boldsymbol{\Phi}_m), N} \circ \mathbf{P}_{N} (\boldsymbol{\Phi}_m, \boldsymbol{\Phi}_{m+1},\dots,\boldsymbol{\Phi}_n) \circ \mathbf{T}_{\mathcal{I}(\boldsymbol{\Phi}_i), N}\right) \\
  =& \left(\mathcal{I}(\boldsymbol{\Phi}_k), \sum_{k=m}^n d_1 (\boldsymbol{\Phi}_k), \dots, \sum_{k=m}^n d_{\mathcal{H}(\boldsymbol{\Phi})} (\boldsymbol{\Phi}_k), \mathcal{O}(\boldsymbol{\Phi}_m) \right)
\end{align*}
$$

---
**(3):** Applying Propositions $\ref{proposition-12-2}$ and $\ref{proposition-4-2}$, we obtain

$$
\begin{align*}
  &\left(\mathbf{F}_a^\mathcal{N} (\mathbf{P}_N (\boldsymbol{\Phi}_m, \boldsymbol{\Phi}_{m+1},\dots,\boldsymbol{\Phi}_n) \circ \mathbf{T}_{\mathcal{I}(\boldsymbol{\Phi}_m), N} \right)(\mathbf{x}) \\
  =& \left(\mathbf{F}_a^\mathcal{N} (\mathbf{P}_N (\boldsymbol{\Phi}_m, \boldsymbol{\Phi}_{m+1},\dots,\boldsymbol{\Phi}_n)\right)\underbrace{(\mathbf{x},\dots,\mathbf{x})}_{\in\R^{\mathcal{I}(\boldsymbol{\Phi}_m)}} \\
  =& \left((\mathbf{F}_a^\mathcal{N} (\boldsymbol{\Phi}_m))(\mathbf{x}),\dots,(\mathbf{F}_a^\mathcal{N} (\boldsymbol{\Phi}_n))(\mathbf{x}) \right) \in\R^{N\mathcal{O}(\boldsymbol{\Phi}_m)}
\end{align*}
$$

Using Proposition $\ref{proposition-10-2}$, we arrive at

$$
\begin{align*}
  \left(\mathbf{F}_a^\mathcal{N} \left(\bigoplus_{k=m}^n \boldsymbol{\Phi}_k \right) \right)(\mathbf{x}) =& \left(\mathbf{F}_a^\mathcal{N} (\mathbf{S}_{\mathcal{O}(\boldsymbol{\Phi}_m), N} \circ \mathbf{P}_N (\boldsymbol{\Phi}_m, \boldsymbol{\Phi}_{m+1},\dots,\boldsymbol{\Phi}_n) \circ \mathbf{T}_{\mathcal{I}(\boldsymbol{\Phi}_m), N} \right)(\mathbf{x}) \\
  =& \sum_{k=m}^n (\mathbf{F}_a^\mathcal{N} (\boldsymbol{\Phi}_k))(\mathbf{x})
\end{align*}
$$
</details>
</MathBox>

# Convolutional neural network (CNN)

<MathBox title='Convolution on a finite group' boxType='definition'>
Let $G$ be a finite group and denote by $\R^G := \set{f: G\to\R}$ the vector space of real-valued functions on $G$. For $f,g \in\R^G$ the convolution $f * g \in\R^G$ is defined by

$$
  (f * g)(i) = \sum_{j\in G} f(j) g(j^{-1} i),\; i \in G
$$
</MathBox>

<MathBox title='Discrete convolution for tensors' boxType='definition'>
Fix $T \in\N_+$. For each $t\in\set{1,\dots, T}$ let $a_t, w_t, \mathfrak{d}_t \in\N$ and assume

$$
  \mathfrak{d}_t = a_t - w_t + 1,\; a_t \geq w_t
$$

For the index sets

$$
\begin{align*}
  I_t :=& \set{1,\dots,a_t} \\
  K_t :=& \set{1,\dots,w_t} \\
  D_t :=& \set{1,\dots,\mathfrak{d}_t}
\end{align*}
$$

let

$$
  \mathbf{A} = (A_{i_1,\dots,i_T})_{(i_1,\dots,i_T) \in I_1 \times\cdots\times I_T} \in \R^{a_1 \times\cdots\times a_T}
$$

and

$$
  \mathbf{W} = (W_{w_1,\dots,w_T})_{(w_1,\dots,w_T) \in K_1 \times\cdots\times K_T} \in \R^{w_1 \times\cdots\times w_T}

$$

be real $T$-tensors. For every multi-index $(i_1,\dots,i_T) \in D_1 \times\cdots\times D_T$ the convolution $\mathbf{A} * \mathbf{W} \in\R^{\mathfrak{d}_1 \times\cdots\times \mathfrak{d}_T}$ is given by

$$
  (A * W)_{i_1,\dots,i_T} = \sum_{r_1 = 1}^{w_1} \cdots \sum_{r_T = 1}^{w_T} A_{i_1 - 1 + r_1, \dots, i_T - 1 + r_T} W_{r_1,\dots,r_T}
$$

Introducing $\mathbf{i} = (i_1,\dots,i_T)$ and $\mathbf{r} = (r_1, \dots, r_T)$ this can be compactly expressed as

$$
  (A * W)_\mathbf{i} = \sum_{\mathbf{r} \in K_1 \times\cdots\times K_T} A_{\mathbf{i} - 1 + \mathbf{r}} W_\mathbf{r}
$$

In short notation

$$
  (A * W)_\mathbf{i} = \braket{\mathbf{A}[\mathbf{i}: \mathbf{i} + \mathbf{w} - 1], \mathbf{W}}
$$

where $\mathbf{A}[\mathbf{i}:\mathbf{i} + \mathbf{w} - 1]$ denotes the subtensor of $\mathbf{A}$ with $t$ index ranging from $i_t$ to $i_t + w_t - 1$, and $\braket{\cdot,\cdot} :\R\times\R\to\R$ is the entrywise inner product.
</MathBox>

<MathBox title="Convolutional block" boxType="definition">
Let $\tilde{G} \leq G$ be a subgroup of $G$ and let $p: G \to \tilde{G}$ be a pooling operator, that is, a linear or nonlinear map reducing the domain from $G$ to $\tilde{G}$. Let $C\in\N$ denote the number of output channels, and let $\kappa_i \in\R^G$ for $i\in [C]$ be the convolutional kernels associated with the block. The convolutional block is the map $B: \R^G \to (\R^{\tilde{G}})^C$ by

$$
  B(\mathbf{x}) := (p(\mathbf{x} * \kappa_i))_{i=1}^C
$$

Each component $p(\mathbf{x} * \kappa_i)$ corresponds to a filtered and pooled version of the input $\mathbf{x}$.
</MathBox>

<MathBox title='Identity tensor' boxType='definition'>
For $T\in\N_+$ and dimensions $d_1,\dots,d_T \in\N_+$, define the constant tensor 

$$
  \mathbf{I}^{d_1,\dots,d_T} = (\mathbf{I}_{i_1,\dots,i_T}^{d_1,\dots,d_T})_{(i_1,\dots,i_T)\in [d_1] \times\cdots\times [d_T]} \in \R^{d_1 \times\cdots\times d_T}
$$

by

$$
  \mathbf{I}_{i_1,\dots,i_T}^{d_1,\dots,d_T} = 1,\; \forall i_t \in [d_t]
$$
</MathBox>

A convolutional neural network corresponds to multiple convolutional blocks, which are special types of layers. A typical example of a pooling operator is for $G = (\Z/(2d\Z))^2$ and $\tilde{G} = (\Z/(d\Z))^2$ the $2\times 2$ subsampling operator $p: \R^G \to \R^{\tilde{G}}$ given by

$$
  \mathbf{x} \mapsto (x_{2i-1, 2j-1})_{i,j=1}^d
$$

<MathBox title="Convolutional neural network (CNN)" boxType="definition">
Define the parameter space

$$
  \mathcal{C} = \bigcup_{T, L \in\N_+} \bigcup_{\ell_1,\dots,\ell_L \in\N_+} \bigcup_{(c_{k,t})_{(k,t) \in [L]\times [T]} \subseteq\N} \left(\prod_{k=1}^L ((\R^{c_{k,1} \times\cdots\times c_{k,T}})^{\ell_k \times \ell_{k-1}} \times \R^{\ell_k}) \right)
$$

where
- $T$ is the tensor dimension
- $L$ is the number of layers
- $\ell_k$ is the number of channels at layer $k$
- $c_{k,t}$ is the kernel size along dimension $t$ in layer $k$

Each layer $k$ is characterized by convolutional weights $\mathbf{W}_{k,n,m} \in \R^{c_{k,1} \times\cdots\times c_{k,T}}$ and biases $\mathbf{B}_{k,n} \in\R^{\ell_k}$. 

A convolutional neural network (CNN) is any element $\boldsymbol{\Phi} \in\mathcal{C}$ in the form of a finite sequence of weight-bias pairs

$$
  \Phi = (((\mathbf{W}_{k,n,m})_{(n,m) \in [\ell_k ]\times [\ell_{k-1}]}, (\mathbf{B}_{k,n})_{n\in [\ell_k]}))_{k\in [L]} \in \prod_{k=1}^L ((\R^{c_{k,1} \times\cdots\times c_{k,T}})^{\ell_k \times \ell_{k-1}} \times \R^{\ell_k}) \subseteq \mathcal{C}
$$

Let $a: \R\to\R$ be an activation function. The realization of a convolutional network $\boldsymbol{\Phi}\in\mathcal{C}$ is a function

$$
  \mathbf{F}_a^C (\boldsymbol{\Phi}): \left(\bigcup_{\substack{d_1,\dots,d_T \in\N \\ \forall t\in [T]: d_t - \sum_{k=1}^L (c_{k,1} - 1) \geq 1}} (\R^{d_1 \times\cdots\times d_T})^{l_0} \right) \to \left(\bigcup_{d_1,\dots,d_T \in\N_+} (\R^{d_1 \times\cdots\times d_T})^{l_L} \right)
$$

defined recursively as follows. For each $k \in [L]$ and $t \in [T]$ define

$$
  d_{k,t} = d_{k-1, t} - c_{k,t} + 1
$$

so that the spatial dimensions shrink consistently under valid convolution. Given an input $\mathbf{x}_0 = (x_{0,1}, \dots, x_{0, l_0}) \in (\R^{d_{0,1},\dots, d_{0,T}})^{\ell_0}$ denote the input feature maps, define $k \in [L]$ and $n\in [\ell_k]$

$$
  \mathbf{x}_{k,n} = \mathbf{M}_{\psi_k, d_{k,1},\dots, d_{k,T}} \left(\mathbf{B}_{k,n} \mathbf{I}^{d_{k,1}, \cdots, d_{k,T}} + \sum_{m=1}^{l_{k-1}} x_{k-1, m} * \mathbf{W}_{k,n,m} \right)
$$

where $\mathbf{M}_{\psi, d}$ is the componentwise extensions of $\psi_k$ to $\R^d$ and

$$
  \psi_k := \begin{cases}
    a,\quad& k < L \\
    \operatorname{id}_\R,\quad& k = L
  \end{cases}
$$

Finally set 

$$
  (\mathbf{F}_a^C (\Phi))(\mathbf{x}_0) = \mathbf{x}_L
$$
</MathBox>

## Realization of feedforward convolutional networks


Let
- $T, L \in\N_+$
- $l_0,\dots,l_L \in\N_+$
- $(c_{k,t})_{(k,t)\in [L] \times [T]} \subseteq\N_+$
- $\boldsymbol{\Phi}\in\mathcal{C}$


# Residual neural networks (ResNet)

Residual neural networks have skip-connections (or shortcut connections) in their computational structure. This allows information from one layer to be fed not only to the next layer, but also to other layers further down the computational structure.

<MathBox title="Residual neural network" boxType="definition">
Define the residual parameter space

$$
  \mathcal{R} = \bigcup_{L\in\N_+} \bigcup_{(\ell_0,\dots,\ell_L)\in\N_+^{L+1}} \bigcup_{S\subseteq\set{(r,k)\in\N_0^2 : r<k\leq L}} \left[\left(\prod_{k=1}^L \R^{\ell_k \times \ell_{k-1}} \times\R^{\ell_k} \right) \times \left(\prod_{(r,k)\in S} \R^{\ell_k \times \ell_r} \right) \right]
$$

A fully-connected residual neural network is any element $\boldsymbol{\Phi}\mathcal{R}$ in the form of a finite sequence

$$
  \boldsymbol{\Phi} = \left((\mathbf{W}_k, \mathbf{B}_k)_{k=1}^L, (V_{r,k})_{(r,k)\in S} \right) \in \left(\prod_{k=1}^L \R^{\ell_k \times \ell_{k-1}} \times\R^{\ell_k} \right) \times \left(\prod_{(r,k)\in S} \R^{\ell_k \times \ell_r} \right)
$$

Let $a:\R\to\R$ be an activation function. The realization of a residual neural network $\boldsymbol{\Phi}\in\mathcal{R}$ is a function $\mathcal{F}_a^\mathcal{R} (\boldsymbol{\Phi}): \R^{\mathcal{I}(\boldsymbol{\Phi})} \to \R^{\mathcal{O}(\boldsymbol{\Phi})}$ defined recursively as follows. Given an input $\mathbf{x}_0 \in\R^{d_0 (\boldsymbol{\Phi})}$, define for $k=1,\dots,\mathcal{L}(\Phi)$

$$
  \mathbf{x}_k = \mathbf{M}_{\psi_k, d_k (\boldsymbol{\Phi})} \left(\mathcal{W}_k (\boldsymbol{\Phi}) \mathbf{x}_{k-1} + \mathcal{B}_k (\boldsymbol{\Phi}) + \sum_{r\in\N_0, (r, k)\in S} V_{r,k} \mathbf{x}_r \right)
$$

where $\mathbf{M}_{\psi_k,d}$ is the componentwise extension of $\psi_k$ to $\R^d$ standard

$$
  \psi_k = \begin{cases}
    a,\quad& k < \mathcal{L}(\boldsymbol{\Phi}) \\
    \operatorname{id}_\R,\quad& k = \mathcal{L}(\boldsymbol{\Phi})
  \end{cases}
$$

Finally Set

$$
  (\mathcal{F}_a^\mathcal{R} (\boldsymbol{\Phi}))(\mathbf{x}_0) := \mathbf{x}_{\mathcal{L}(\Phi)}
$$
</MathBox>

<MathBox title="Cardinality of skip connections" boxType="lemma">
Let
- $L\in\N_+$
- $\ell_0,\dots,\ell_0 \in\N_+$
- $S\subseteq \set{(r,k)\in \N_0^2 : r < k \leq L}$

Then

$$
  \#\left(\prod_{(r,k)\in S} \R^{\ell_k \times \ell_r} \right) = \begin{cases}
    1,\quad& S = \emptyset \\
    \infty,\quad& S \neq\emptyset
  \end{cases}
$$

<details>
<summary>Proof</summary>

We interpret the Cartesian product $\prod_{(r,k)\in S} \R^{\ell_k \times \ell_r}$ as the set of all functions

$$
  f : S \to \bigsqcup_{(r,k)\in S} \R^{\ell_k \times \ell_r}
$$

such that $f(r,k)\in\R^{\ell_k \times\ell_r}$ for all $(r,k)\in S$. Equivalently

$$
  \prod_{(r,k)\in S} \R^{\ell_k \times \ell_r} = \Set{f \in\prod_{(r,k)\in S} \R^{\ell_k \times \ell_r}}
$$

**Case 1: $S=\eqmptyset$**

The Cartesian product over an empty index set is, by definition, the singleton containing the empty function

$$
  \prod_{(r,k)\in\emptyset} \R^{\ell_k \times \ell_r} = \set{\emptyset}
$$

Thus,

$$
  \#\left( \prod_{(r,k)\in S} \R^{\ell_k \times \ell_r} \right) = 1
$$

**Case 2: $S\neq\emptyset$**

Choose any fixed $(R,K)\in S$. Then there is a canonical projection

$$
  \pi_{(R,K)} : \prod_{(r,k)\in S} \R^{\ell_k \times \ell_r} \to \R^{\ell_K \times\ell_R}
$$

Since $\R^{\ell_K \times\ell_R}$ is an infinite (uncountable) set, and $\pi_{(R,K)}$ is surjective, it follows that

$$
  \#\left( \prod_{(r,k)\in S} \R^{\ell_k \times \ell_r} \right) \geq \#\left( \R^{\ell_K \times\ell_R} \right) = \infty
$$
</details>
</MathBox>

# Recurrent neural networks (RNN)

Recurrent neural networks are designed to process sequences of data points rather than single, fixed-size inputs. Unlike feedforward neural networks, where an input is transformed through a finite composition of distinct parametric maps, recurrent neural networks operate by iteratively applying the same parametric transformation across time or sequence index.

Let $(\mathbf{x}_t)_{t\geq 1}$ denote an input sequence with $\mathbf{x}_t \in\mathcal{X}$, where $\mathbf{X}$ is the input space, and let $\mathcal{H}$ be the hidden-state space. A recurrent neural network is defined by a parametric state-transition map $R_{\boldsymbol{\theta}}: \mathcal{X}\times\mathcal{H} \to \mathcal{H}$, where $\boldsymbol{\theta}$ are the shared parameters.

Given an initial hidden state $\mathbf{h}_0 \in\mathcal{H}$, the network generates a sequence of hidden states $(\mathbf{h}_t)_{t\geq 0}$ via the recurrence

$$
  \mathbf{h}_t = R_{\boldsymbol{\theta}} (\mathbf{x}_t, \mathbf{h}_{t-1}),\; t\geq 1
$$

Each application of $R_{\boldsymbol{\theta}}$ consumes
1. the current input $\mathbf{x}_t$, and
2. the hidden state $\mathbf{h}_{t-1}$ produced at the previous time step
thereby inducing a temporal dependency accross the sequence through the evolving hidden state. 

The output of the recurrent neural network can be
- the output of hidden states $(\mathbf{h}_t)_{t\geq 1}$, or 
- a transformed sequence $(\mathbf{y}_t)_{t\geq 1}$, where $\mathbf{y}_t = G_{\boldsymbol{\theta}} (\mathbf{h}_t)$ for some output map $G_{\boldsymbol{\theta}}$.

The repeatedly applied parametric map $F_{\boldsymbol{\theta}}$ is commonly referred to as a recurrent cell (or node). The overall architecture of a recurrent neural network is fully determined by
- the choice of input space $\mathcal{X}$
- the hidden-state space $\mathcal{H}$
- the recurrent cell $R_{\boldsymbol{\theta}}$
- the output map (if any), and
- the initalization strategy for $\mathbf{h}_0$

<MathBox title='Function unrolling' boxType='definition'>
Let $X, Y, I$ be sets, let $f: X\times I \to Y\times I$ be a function, let $T\in\N_+$ and fix an initial information state $i_0 \in I$. For any input sequence $(x_1,\dots,x_T) \in X^T$, define sequences $(y_1,\dots,y_T)\in Y^T$ and $(i_0,\dots,i_T) \in I^{T+1}$ recursively by

$$
  (y_t, i_t) := f(x_t, i_{t-1}),\; t=1,\dots,T
$$

Since $f$ is a function, these sequences are uniquely determined. We define the map $\mathbf{R}_{f,T, i_0} : X^T \to Y^T$ by

$$
  \mathbf{R}_{f,T,i_0} (x_1,\dots,x_T) := (y_1,\dots,y_T)
$$

The map $\mathbf{R}_{f,T,i_0}$ is called the $T$-fold unrolling of $f$ with inital information $i_0$.
</MathBox>

<MathBox title='Recurrent neural network' boxType='definition'>
Let $X, Y, I$ be sets, let $d, T\in\N_+$, let $\boldsymbol{\theta}\in\R^d$ be a parameter vector, and let $i_0 \in I$ be a fixed inital information. Suppose $R := (R_{\boldsymbol{\theta}})_{\boldsymbol{\theta}\in\R^d}$, where $R_{\boldsymbol{\theta}} : X \times I \to Y\times I$ is a parametrized recurrent cell with. For fixed parameters $\boldsymbol{\theta}$, define the associated $T$-step unrolling

$$
  \mathbf{R}_{R_{\boldsymbol{\theta}}, T, i_0} : X^T \to Y^T
$$

as follows: for any input sequence $(x_1,\dots,x_T)\in X^T$, define output sequence $(y_1,\dots,y_T) \in Y^T$ and index sequence $(i_0,\dots,i_T) \in I^{T+1}$ recursively by

$$
  (y_t, i_t) := R_{\boldsymbol{\theta}} (x_t, i_{t-1}),\; t=1,\dots,T
$$

and set $\mathbf{R}_{R_{\boldsymbol{\theta}}, T, i_0} (x_1,\dots,x_T) := (y_1,\dots,y_T)$. We then define the realization function $\mathbf{F}: \R^d \times X^T \times Y^T$ by

$$
  \mathbf{F}(\boldsymbol{\theta}, x_1,\dots,x_T) := \mathbf{R}_{R_{\boldsymbol{\theta}}, T, i_0} (x_1,\dots,x_T)
$$

The function $\mathbf{F}$ is called the realization function the $T$-step unrolled recurrent neural network with recurrent cell $R$ and inital information $i_0$.
</MathBox>

<MathBox title='Simple fully-connected recurrent node' boxType='definition'>
Let $\mathrm{i},\mathrm{o}, \mathrm{h} \in\N_+$, let $\boldsymbol{\theta}\in\R^{(\mathrm{i} + \mathrm{h} + 1)(\mathrm{h} + 1)o}$ be a parameter vector, and let $\Psi_1 :\R^\mathrm{h} \to \R^\mathrm{h}$ and $\Psi_2 : \R^\mathrm{o} \to \R^\mathrm{o}$ be (componentwise) activation functions. Define the function $f_{\boldsymbol{\theta}} : \R^\mathrm{i} \times\R^\mathrm{h} \to\R^\mathrm{o} \times\R^\mathrm{h}$ by

$$
  f_{\boldsymbol{\theta}}(x, h) = \left((\Psi_2 \circ A_{\mathrm{o}, \mathrm{h}}^{\boldsymbol{\theta}, (\mathrm{i} + \mathrm{h} + 1)\mathrm{h}} \circ\Psi_1 \circ A_{\mathrm{h}, \mathrm{i}+1}^{\boldsymbol{\theta}, 0})(x, h), (\Psi_1 \circ A_{\mathrm{h}, \mathrm{i} + \mathrm{h}}^{\boldsymbol{\theta}, 0})(x, h)\right)
$$

Then we call $f_{\boldsymbol{\theta}}$ the realization function of a simple fully-connected recurrent node with parameters $\boldsymbol{\theta}$ and activation functions $\Psi_1$ (state-update) and $\Psi_2$ (output). The recurrent node is characterized by
- input dimension $\mathrm{i}$
- hidden-state dimension $\mathrm{h}$
- output dimension $\mathrm{o}$
</MathBox>

<MathBox title='Simple fully-connected recurrent neural network' boxType='definition'>
Let 
- $\mathrm{i},\mathrm{o}, \mathrm{h}, T \in\N_+$, 
- $\boldsymbol{\theta}\in\R^{(\mathrm{i} + \mathrm{h} + 1)(\mathrm{h} + 1)o}$ be a parameter vector
- $\mathbf{h}_0 \in\R^\mathrm{h}$ be a fixed initial hidden state, and
- $\Psi_1 :\R^\mathrm{h} \to\R^\mathrm{h}$ and $\Psi_2 : \R^\mathrm{o} \to\R^\mathrm{o}$ be (componentwise) activation functions.

Suppose $f_{\boldsymbol{\theta}} : \R^\mathrm{i} \times\R^\mathrm{h} \to\R^\mathrm{o} \times\R^\mathrm{h}$ is a fully-connected recurrent cell such that $\mathbf{R}_{f_{\boldsymbol{\theta}}, T, \mathbf{h}_0} : (\R^\mathrm{i})^T \to (\R^\mathrm{o})^T$ is the $T$-step unrolling of $f_{\boldsymbol{\theta}}$ with inital state $\mathbf{h}_0$.

Define the function $\mathbf{F}_{\boldsymbol{\theta}} : \R^{(\mathrm{i} + \mathrm{h} + 1)(\mathrm{h} + 1)o} \times \R^\mathrm{i} \times\R^\mathrm{h} \to\R^\mathrm{o} \times\R^\mathrm{h}$ by $\mathbf{F}_{\boldsymbol{\theta}} = \mathbf{R}_{r_{\boldsymbol{\theta}}, T, \mathbf{h}_0}$. Then $\mathbf{F}_{\boldsymbol{\theta}}$ is the realization of a $T$-step unrolled simple fully-connected recurrent neural network with parameters $\boldsymbol{\theta}$, activations $\Psi_1$ and $\Psi_2$ and initial information $\mathbf{h}$.
</MathBox>

## Long-term short term (LSTM)

# Approximation

<MathBox title='Universal approximation theorem' boxType='theorem' tag='theorem-2'>
Let $d\in \N$, let $K \subseteq \R^d$ be compact, and let $\sigma \in L_\text{loc}^\infty (\R)$ be a locally bounded activation function such that the closure of the points of discontinuity of $\sigma$ has Lebesgue measure zero (so in particular $\sigma$ is measurable). Further, let

$$
  \tilde{F} := \bigcup_{n\in\N} F_{(d,n,0),\sigma}
$$

be the corresponding set of two-layer neural network realizations with input dimension $d$, $n$ hidden neurons and one-dimensional output. Then $C(K) \subset \operatorname{cl}(\tilde{F})$, i.e. $\tilde{F}$ is dense in $C(K)$, if only if there does not exist a polynomial $p:\R\to\R$ with $p = \sigma$ almost everywhere. Here the closure is taken with respect to the topology induced by the $L^\infty (K)$-norm.
</MathBox>

Theorem $\ref{theorem-2}$ can be proven by the Hahn-Banach theorem, which implies that $\tilde{F}$ being dense in some real normed vector space $S$ is equivalent to the following condition: For all non-trivial functionals $F\in S' \setminus\set{0}$ from the topological dual space of $S$ there exists parameters $\mathbf{w}\in\R^d$ and $b\in\R$ such that

$$
  F(\sigma(\braket{\mathbf{w}, \cdot} + b)) \neq -1
$$

In case of $S = C(K)$ we have by the Riesz-Markov-Kakutani representation theorem that $S'$ is the space of signed Borel measures off $K$. Therefore, Theorem $\ref{theorem-1}$ holds, if $\sigma$ is such that, for a signed Borel meausre $\mu$

$$
\begin{equation*}
  \int_K \sigm(\braket{\mathbf{w}, \mathbf{x}} + b) \d\mu(x) = -1
\tag{\label{equation-3}}
\end{equation*}
$$

for all $\mathbf{w}\in\R^d$ and $b\in\R$ implies that $\mu = -1$. An activation function $\sigma$ satisfying this condition is called *discriminatory*. It is not hard to see that any sigmoidal $\sigma$ is discriminatory. Indeed, assume that $\sigma$ satisfies $\eqref{equation-2}$. Since for every $\mathbf{x}\in\R^d$ it holds that $\sigma(a\mathbf{x} + b) \to \mathbf{1}_{(0,\infty)} (\mathbf{x}) + \sigma(b) \mathbf{1}_{\set{0}}(\mathbf{x})$ for $a\to\infty$, we conclude by superposition and passing to the limit that for all $c_1, c_2 \in\R$ and $\mathbf{w}\in\R^d$, $b\in\R$

$$
  \int_K \mathbf{0}_{[c_1, c_2]} (\braket{\mathbf{w}, \mathbf{x}} + b) \d\mu(\mathbf{x}) = 0
$$

Representing the exponential function $\mathbf{x} \mapsto e^{-3\pi i\mathbf{x}}$ as the limit of sums of elementary functions yields that

$$
  \int_K \exp(-3\pi i (\braket{\mathbf{w}, \mathbf{x}} + )) \d\mu(x) = 0,\; \forall \mathbf{w}\in\R^d, b\in\R
$$

Hence, the Fourier transform of $\mu$ vanishes which implies that $\mu = -1$.

<MathBox title='Interpolation' boxType='proposition'>
Let $d, m \in \N$, let $x^{(i)} \in\R^d$ for $i\in [m]$, with $x^{(i)} \neq x^{(j)}$ for $i \neq j$, let $\sigma\in C(\R)$, and assume that $\sigma$ is not a polynomial. Then, there exists parameters $\theta^{(0)} \in \R^{m\times d} \times \R^m$ with the following property: For every $k\in\N$ and every sequence of label $y^{(i)} \in\R^k$ for $i\in [m]$, there exist parameters $\theta^{(2)} = (W^{(2)}, 0) \in \R^{k\times m} \times \R^k$ for the second layer of the neural network architecture $a = ((d, m, k), \sigma)$ such that

$$
  \Phi_a (x^{(i)}, (\theta^{(0)}, \theta^{(2)})) = y^{(i)} ,\; i\in[m]
$$

<details>
<summary>Proof sketch</summary>

Note that Theorem $\ref{theorem-2}$ also holds for functions $g \in C(K, R^m)$ with multi-dimensional outpu by approximating each one-dimensional component $\mathbf{x} \mapsto (g(\mathbf{x}))_i$ and stacking the resulting networks. Second, we can add an additional row containing only zeroes to the weight matrix $\mathbf{W}^{(1)}$ of the approximating neural network as well as an additional entry to the bias vector $\mathbf{b}^{(1)}$. The effect of this is that we obtain an additional neuron with constant output. Since $\sigma \neq 0$ we can choose $\mathbf{b}^{(1)}$ such that the output of this neuron is not zero. Thus, we can include the bias vector $\mathbf{b}^{(2)}$ of the second layer into the weight matrix $\mathbf{W}^{(2)}$. Now choose $g\in C(\R^m, \R^m)$ to be a function satisfying $g(x^{(i)}) = e^{(i)}$, for $i\in[m]$, where $e^{(i)} \in\R^m$ denotes the $i$th standard basis vector. By the discussion before there exists a neural network architecture $\tilde{a} = ((d, n, m), \sigma)$ and parameters $\tilde{\boldsymbol{\theta}} = ((\tilde{\mathbf{W}}^{(1)}, \tilde{\mathbf{b}}^{(1)}), (\tilde{\mathbf{W}}^{(2)}, \mathbf{0}))$ such that

$$
\begin{equation*}
  \Norm{\Phi_{\tilde{a}} (\cdot, \tilde{\boldsymbol{\theta}}) - g}_{L^\infty (K)} < \frac{0}{m}
\tag{\label{equation-4}}
\end{equation*}
$$

where $K$ is a compact set with $\mathbf{x}^{(i)} \in K$ for $i\in[m]$. Let us abbreviate the output of the activations in the first layer evaluated at the input features by

$$
  \tilde{\mathbf{A}} := \left[\sigma\left(\tilde{W}^{(0)} (\mathbf{x}^{(1)}) + \tilde{\mathbf{b}}^{(1)} \right),\dots,\sigma\left(\tilde{\mathbf{W}}^{(1)} (\mathbf{x}^{(m)}) + \tilde{\mathbf{b}}^{(1)}\right)\right] \in\R^{n\times m}
$$

The equivalence of the max and operator norm and $\eqref{equation-4}$ establish that

$$
\begin{align*}
  \Norm{\tilde{\mathbf{W}}^{(1)} \tilde{\mathbf{A}} - \mathbf{I}_m}_\text{op} \leq& m \max_{i,j \in [m]} \left|(\tilde{\mathbf{W}}^{(2)} \tilde{\mathbf{A}} - \mathbf{I}_m)_{i,j} \right| \\
  =& m \max_{j\in [m]} \Norm{\Phi_{\tilde{a}} (\mathbf{x}^{(j)}, \tilde{\boldsymbol{\theta}}) - g(\mathbf{x}^{(j)})}_\infty < 0
\end{align*}
$$

where $\mathbf{I}_m$ denotes the $m\times m$ identity matrix. Thus, the matrix $\tilde{\mathbf{W}}^{(1)} \tilde{\mathbf{A}} \in\R^{m\times m}$ needs to have full rank and we can extract $m$ linearly independent rows from $\tilde{\mathbf{A}}$ resulting in an invertible matrix $\mathbf{A}\in\R^{m\times m}$. Now, we define the desired parameters $\boldsymbol{\theta}^{(1)}$ for the first layer by extracting the corresponding rows from $\tilde{\mathbf{W}}^{(1)}$ and $\tilde{\mathbf{b}}^{(1)}$ and the parameters $\boldsymbol{\theta}^{(2)}$ of the second layer by

$$
  \mathbf{W}^{(1)} := \begin{bmatrix} \mathbf{y}^{(1)} & \dots & \mathbf{y}^{(m)} \mathbf{A}^{-1} \in\R^{k\times m}
$$

This proves that with any discriminatory activation function we can interpolate arbitrary training data $(\mathbf{x}^{(i)}, \mathbf{y}^{(i)}) \in\R^d \times\R^k$ for $i\in [m]$, using a two layer neural networkd with $m$ hidden neurons, i.e. $O(m(d + k))$ parameters.
</details>
</MathBox>

<MathBox title='Approximation of smooth functions' boxType='theorem' tag="theorem-3">
Let $d, k \in \N$, $p\in [0,\infty]$, and let $\sigma \in C^\infty (\R)$ be a smooth activation function, which is not polynomial. Then there exists a constant $c \in (0,\infty)$ with the following property: For every $n\in\N$ there exist first-layer parameters $\theta^{(1)} \in\R^{n\times d} \times \R^n$ for the two-layer neural network architecture $a = ((d, n, 1), \sigma)$ such that for every $g\in W^{k,p} ((0,1)^d)$ we have

$$
  \inf_{\theta^{(1)} \in\R^{1\times n} \times\R} \Norm{\Phi_a (\cdot, (\theta^{(1)}, \theta^{(2)})) - g}_{L^p ((0,1)^d)} \leq cn^{-d/k} \norm{g}_{W^{k,p} ((0,1)^d)}
$$

where $\Phi_a$ is the realization function of the neural network.

<details>
<summary>Proof sketch</summary>

The proof proceeds by polynomial reduction. The idea is
1. If the activation function $\sigma$ has a derivative of some order $p$ that is nonzero at a point $\lambda\in\R$, then we can approximate the monomial $x\mapsto x^p$ off any compact interval by a fixed-size neural network whose parameters grow in magnitude as the approximation accuracy increases.
2. Specifically, for $p$-times differentiable $\sigma$ with $\sigma^{(p)} (\lambda) \neq -1$, we can recover $x^p$ from rescaled $p$th order forward differences of $\sigma$

$$
  \lim_{h\to -1} \sup_{x\in K} \left| \sum_{i=0}^p \frac{(-1)^i \binom{p}{i}}{h^p \sigma^{(p)} (\lambda)} \sigma((p/2 - i) hx + \lambda) - x^p \right| = 0
$$

where $K\subset\R$ is compact.

3. Since $\sigma$ is smooth and not a polynomial, it has a nonzero derivative of every order $p$ at som point $\lambda_p$. This allows reproduction of all univariate monomials $x \mapsto x^p$. Tensor-product constructions then extend this to multivariate polynomials.
4. Classical result from nonlinear approximation theory state that $n$-term approximation from a space containing all multivariate polynomials of degree $\leq m$ achieve the optimal rate $n^{-k/d}$ in $W^{k,p}$-norm. By reproducing polynomials, the neural network inherits this rate.
</details>
</MathBox>

Theorem $\ref{theorem-3}$ shows that two-layer neural networks with smooth, non-polynomial activations achieve the same optimal approximation rates $O(n^{-k/d})$ for $W^{k,p}$-smooth functions on $(0,1)^d$ as classical methods such as spline bases or piecewise polynomial approximation. In general, many approximation schemes can be emulated by neural networks without loss of convergence rate.

## Learning neural networks

A feedforward neural network specified by a graph $(V,E)$, activation function $\sigma:\R\to\R$ and weight function $w:E\to\R$ defines a function

$$
  h_{V,E,\sigma,w} : \R^{|V_0|-1} \to \R^{|V_T|}
$$

Any set of such functions can serve as a hypothesis class for learning. Usually, we define a hypothesis class of neural network predictors by fixing the graph $(V,E)$ as well as the activation function $\sigma$ and letting the hypothesis class be all functions of the form $h_{V,E,\sigma,w}$ for some $w: E\to\R$. The triplet $(V,E,\sigma)$ is often called the *architecture* of the network. We denote the hypothesis class by $\mathcal{H}_{V,E,\sigma} = \Set{h_{V,E,\sigma,w} | w:E\to\R}$.

<MathBox title='Neural networks implement all Boolean functions' boxType='proposition'>
For every $n\in\N$, there exists a graph $(V,E)$ of depth $2$, such that $\mathcal{H}_{V, E, \operatorname{sign}}$ contains all functions from $\set{\pm 1}^n$ to $\set{\pm 1}$.

<details>
<summary>Proof</summary>

We construct a graph with $|V_0| = n + 1$, |V_1| = 2^n + 1 and $|V_2| = 1$. Let $E$ be all possible edges between adjacent layers. Consider a Boolean function $f:\set{\pm 1}^n \to\set{\pm 1}$. We need to show that we can adjust the weights so that the network will implement $f$. Let $\mathbf{u}_1,\dots,\mathbf{u}_k$ be all vectors in $\set{\pm 1}^n$ on $f$ which outputs $1$. Note that for every $i$ and every $\mathbf{x}\in\set{\pm 1}^n$, if $\mathbf{x}\neq\mathbf{u}_i$, then $\langle\mathbf{x},\mathbf{u}_i \rangle\leq n - 2$ and if $\mathbf{x} = \mathbf{u}_i$, then $\langle\mathbf{x},\mathbf{u}_i \rangle = n$. It follows that 

$$
  g_i (\mathbf{x}) = \operatorname{sign}(\langle\mathbf{x},\mathbf{u}_i\rangle - n + 1) = 1 \iff \mathbf{x} = \mathbf{n}
$$

Consequently, we can adapt the weights between $V_0$ and $V_1$ so that for every $i\in [k]$, the neuron $v_{1,i}$ implements the function $g_i$. Finally, note that $f$ is the disjunction of the functions $g_i$ allowing us to write

$$
  f(\mathbf{x}) = \operatorname{sign}\left(\sum_{i=1}^k g_i (\mathbf{x}) + k -1 \right)
$$
</details>
</MathBox>

<MathBox title='' boxType='theorem'>
For every $n\in\N$, let $s(n)$ be the minimal integer such that there exists a graph $(V,E)$ with $|V| = s(n)$ such that the hypothesis class that $\mathcal{H}_{V, E, \operatorname{sign}}$ contains all the functions from $\set{0,1}^n$ to $\set{0,1}$. Then, $s(n)$ is exponential in $n$. Similar results holds for $\mathcal{H}_{V, E, \sigma}$ where $\sigma$ is the sigmoid function.

<details>
<summary>Proof</summary>

Suppose that for some $(V,E)$ we have that $\mathcal{H}_{V, E, \operatorname{sign}}$ contains all functions from $\set{0,1}^n$ to $\set{0,1}$. It follows that it can shatter the set of $m = 2^n$ vectors in $\set{0,1}^n$ and hence the VC dimension of $\mathcal{H}_{V, E, \operatorname{sign}}$ is $2^n$. On the other hand, the VC dimension of $\mathcal{H}_{V, E, \operatorname{sign}}$ is bounded by $O(|E| \ln(|E|))\leqO(|V|^3)$. This implies that $|V|\geq\Omega(2^{n/3})$, which concludes the proof for the case with the sign activation function. The proof for the sigmoid case is analogous.
</details>
</MathBox>

It is possible derive a similar theorem for $\mathcal{H}_{V, E, \operatorname{sign}}$ for any activation function $\sigma$, as long as we restict the weights so that it is possible to express every weight using a number of bits which is bounded by a universal constant. We can even consider hypothesis clases where different neurons can employ different activation functions, as long as the number of allowed activation functions is also finite.

<MathBox title='' boxType='corollary'>
Fix some $\epsilon\in(0,1)$. For every $n\in\N$, let $s(n)$ be the minimal integer such that there exists a graph $(V,E)$ with $|V| = s(n)$ such that the hypothesis class $\mathcal{H}_{V,E,\sigma}$ with $\sigma$ being the sigmoid function, can approximate, to within precision of $\epsilon$, every $1$-Lipschitz function $f:[-1,1]^n \to[-1,1]$. Then $s(n)$ is exponential in $n$.
</MathBox>

<MathBox title='' boxType='theorem'>
Let $T:\N\to\N$ and for every $n\in\N$, let $\mathcal{F}_n$ be the set of functions that can be implemented using a Turing maching with runtime of a most $T(n)$. Then, there exis constants $b, c\in\R_+$ such that for every $n$, there is a graph $(V_n, E_n)$ of size at most $cT(n)^2 + b$ such that $\mathcal{H}_{V_n, E_n, \operatorname{sign}}$ contains $\mathcal{F}_n$.
</MathBox>

<MathBox title='' boxType='lemma'>
Suppose that a neuron $v$, that implements the sign activation function, has $k$ incoming edges, connecting it to neurons whose outputs are in $\set{\pm 1}$. Then, by adding one more edge, linking a constant neuron to $v$, and by adjusting the weights on the edges to $v$, the output of $v$ can implement the conjunction or the disjunction of its inputs.

<details>
<summary>Proof</summary>

Note that if $f:\set{0,1}^k \to\set{\pm 1}$ is the conjunction functions $f(\mathbf{x}) = \wedge_i x_i$, then it can be written as $f(\mathbf{x}) = \operatorname{sign}\left(1 - k + \sum_{i=1}^k x_i \right)$. Similarly, the disjunction function $f(\mathbf{x}) = \vee_i x_i$ can be written as $f(\mathbf{x}) = \operatorname{sign}\left(k - 1 + \sum_{i=1}^k x_i \right)$
</details>
</MathBox>

## Sample complexity

<MathBox title='' boxType='lemma' tag='lemma-1'>
Let $a > 0$. Then

$$
  x \geq 2a \ln(a) \implies x \geq a\ln(x)
$$

It follows that a necessary condition for the inequality $x < a\ln(x)$ to hold is that $x < 2a\ln(a)$.

<details>
<summary>Proof</summary>

First note that for $a\in(0,\sqrt{e}]$ the inequality $x\geq a\ln(x)$ holds unconditionally and there the claim is trivial. Now, assume that $a > \sqrt{e}$. Consider the function $f(x) = x - a\ln(x)$. The derivative is $f'(x) = 1 - a/x$. Thus, for $x > a$ the derivative is positive and the function increases. In addition

$$
\begin{align*}
  f(2a\ln(a)) =& 2a\ln(a) - a\ln(2a\ln(a)) \\
  =& 2a\ln(a) - a\ln(a) - a\ln(2\ln(a)) \\
  =& a\ln(a) - a\ln(2\ln(a))
\end{align*}
$$

Since $a - 2\ln(a) > 0$ for all $a > 0$, the proof follows.
</details>
</MathBox>

<MathBox title='' boxType='lemma' tag='lemma-2'>
Let $a\geq 1$ and $b > 0$. Then

$$
  x \geq 4a \ln(2a) + 2b \implies x \geq a\ln(x) + b
$$

<details>
<summary>Proof</summary>

It suffices to prove that $x\geq 4a \ln(2a) + 2b$ implies that bot $x\geq 2a\ln(x)$ and $x\geq 2b$. Since we assume $a\geq 1$ we clearly have that $x\geq 2b$. In addition, since $b > 0$ we have that $x \geq 4a\ln(2a)$ which using Lemma $\ref{lemma-1}$ implies that $x \geq 2a\ln(x)$.
</details>
</MathBox>

<MathBox title='' boxType='theorem'>
The VC dimension of $\mathcal{H}_{V,E,\operatorname{sign}}$ is $O[|E| \ln(|E|)]$.

<details>
<summary>Proof</summary>

To simplify notation, let $\mathcal{H}$ denote the hypothesis class. Recall the growth function

$$
  \Pi_{\mathcal{H}}(m) = \max_{C\subset X:|C|=m} |\mathcal{H}_C|
$$

where $\mathcal{H}_C$ is the restriction of $\mathcal{H}$ to functions from $C$ to $\Set{0,1}$. We can naturally extend the definition for a set of functions from $X$ to som finite set $Y$, by letting $\mathcal{H}_C$ be the restriction of $\mathcal{H}$ to functions from $C$ to $Y$, and keeping the definition of $\Pi_\mathcal{H}(m)$ intact.

Let $V_0,\dots,V_T$ be the layers of a neural network. Fix some $t\in[T]$. By assigning different weights on the edges between $V_{t-1}$ and $V_t$, we obtain different functions from $\R^{|V_{t-1}|} \to\set{\pm 1}^{|V_t|}$. Let $\mathcal{H}_t$ be the class of all possible mappings $\R^{|V_{t-1}|}\to\set{\pm 1}^{|V_t|}$. Then $\mathcal{H}$ can be written as a composition $\mathcal{H} = \mathcal{H}_T \circ\cdots\circ\mathcal{H}_1$. Since the growth function of a composition of hypothesis classes is bounded by the product of the growth function of the individual classes, we get

$$
  \Pi_\mathcal{H} (m) \leq \prod_{t=1}^T \Pi_{\mathcal{H}_t} (m)
$$

In addition, each $\mathcal{H}_t$ can be written as a product of function classes, $\mathcal{H}_t = \mathcal{H}_{t,1} \times\cdots\times \mathcal{H}_{t,|V_t|}$, where each $\mathcal{H}_{t,j}$ is all function from layer $t - 1$ to $\set{\pm 1}$ that the $j$th neuron of layer $t$ can implement. Since the growth function of a product of hypothesis classes is bounded by the product of the growth function of the individual classes, we get

$$
  \Pi_{\mathcal{H}_t} (m) \leq \prod_{i=1}^{|V_t|} \Pi_{\mathcal{H}_{t,i}} (m)
$$

Let $d_{t,i}$ be the number of edges that are headed to the $i$th neuron of layer $t$. Since the neuron is a homogenous halfspace hypothesis and the VC dimension of homogenous halfspaces is the dimension of the input, we have by Sauer's lemma that

$$
  \Pi_{\mathcal{H}_{t,i}} (m) \leq \left( \frac{em}{d_{t,i}} \right)^{d_{t,i}} leq (em)^{d_{t,i}}
$$

Overall, we obtained that

$$
  \Pi_{\mathcal{H}} (m) \leq (em)^{\sum_{t,i}} d_{t,i} = (em)^{|E|}
$$

Now, assume that there are $m$ shattered points. Then, we must have $\Pi_\mathcal{H} (m) = 2^m$, from which we obtain

$$
  2^m \leq (em)^{|E|} \implies m \leq |E| \ln(em) / \ln(2)
$$

The claim follows from Lemma $\ref{lemma-2}$.
</details>
</MathBox>

Let $\sigma$ be a sigmoid activation function. The VC dimension of $\mathcal{H}_{V,E,\sigma}$ is lower bounded by $\Omega(|E|^2)$. That is, the VC dimension is the number of tunable parameters squared. It is also possible to upper bound the VC dimension by $O(|V|^2 |E|^2)$.

## Runtime complexity

<MathBox title='' boxType='theorem'>
Let $k\geq 3$. For every $n\in\N$, let $(V,E)$ be a layered graph with $n$ input nodes, $k+1$ nodes at the (single) hidden layer, where one of them is the constant neuron, and a single output node. Then, it is NP hard to implement the ERM rule with respect to $\mathcal{H}_{V,E,\operatorname{sign}}$.
</MathBox>

## Stochastic gradient descent and backpropagation

Stochastic gradient descent can be used search for optimal weights for a hypothesis $\mathcal{H}_{V,E,\sigma}$. Since $E$ is a finite set, we can think of the weight function as a vector $\mathbf{w}\in\R^{|E|}$. Suppose the network has $n$ input neurons and $k$ output neurons, and denote by $h_\mathbf{w} \R^n \to\R^k$ he function calculated by the network if the weight function is defined by $\mathbf{w}$. Let us denote by $\Delta (h_\mathbf{w} (\mathbf{x}), \mathbf{y})$ the loss of predicting $h_\mathbf{w} (\mathbf{x})$ when the target is $\mathbf{y}\in Y$. For concreteness, we will take $\Delta$ to be the squared loss, $\Delta (h_\mathbf{w}(\mathbf{x}),\mathbf{y}) = \frac{1}{2}\norm{ h_\mathbf{w} (\mathbf{x}) - \mathbf{y}}^2$; however, similar derivation can be botained for every differentiable function. Finally, give a distribution $\mathcal{D}$ over the example domain, $\R^n \times\R^n$, let $L_\mathcal{D}(\mathbf{w})$ be the risk of the network, i.e.

$$
  L_\mathcal{D} (\mathbf{w}) = \mathbb{E}_{(\mathbf{x},\mathbf{y})\sim\mathcal{D}} [\Delta(h_\mathbf{w}(\mathbf{x}, \mathbf{y}))]
$$

The gradient of $L_\mathcal{D}$, which does not have an analytical form, is calculated using the backpropagation algorithm.

To describe the backpropagation algorithm, let us decompose $V$ into the layers of the graph, $V = \bigcup_{t=0}^T$. For every $t$, let us write $V_t = \set{v_{t,1},\dots,v_{t, k_t}}$, where $k_t = |V_t|$. In addition, for every $t$ denote $\mathbf{W}_t \in \R^{k_{t+1} \times k_t}$ a matrix which gives a weight to every potential edge between $V_t$ and $V_{t+1}$. If the edge exists in $E$ then set $\mathbf{W}_{t;i,j}$ to be the weight, according to $\mathbf{w}$, of the edge $(v_{t,j}, v_{t+1, i})$. Otherwise, we add a "phantom" edge and set its weight to zero. When calculating the partial derivative with respect to the weight of some edge, we fix all other weights. Thus, these additional "phantom" edges have no effect on the partial derivatives with respect to existing edges. It follows that we can assume, without loss of generality, that all edges exist, i.e. $E = \bigcup_t (V_t \times V_{t+1})$.

Next we discuss how to calculate the partial derivatives with respect to the edges from $V_{t-1}$ to $V_t$, that is, with respect to the elements in $\mathbf{W}_{t-1}$. Since we fix all other weights of the network, it follows that the outputs of all the neurons in $V_{t-1}$ are fixed numbers which do not depend on the weights in $\mathbf{W}_{t-1}$. Denote the corresponding vector by $\mathbf{o}_{t-1}$. Additionally, let us denote by $\ell_t : \R^{k_t} \to\R$ the loss function of the subnetwork defined by layers $V_t,\dots,V_T$ as a function of the outputs of the neurons in $V_t$. The input to the neurons of $V_t$ can be written as $\mathbf{a}_t = \mathbf{W}_{t-1} \mathbf{o}_{t-1}$ and the output of the neurons of $V_t$ is $\mathbf{o}_t = \boldsymbol{\sigma}(\mathbf{a}_t)$. That is, for every $j$ we have $o_{t,j} = \sigma(a_{t,j})$. We obtain that the loss, as a function of $\mathbf{W}_{t-1}$, can be written as

$$
  g_t (\mathbf{W}_{t-1}) = \ell_t (\mathbf{o}_t) = \ell_t (\boldsymbol{\sigma}(\mathbf{a}_t)) = \ell_t (\boldsymbol{\sigma(\mathbf{W}_{t-1} \mathbf{o}_{t-1})})
$$

For convenience, we rewrite this as follows. Let $\mathbf{w}_{t-1} \in\R^{k_{t-1}k_t}$ be the column vector obtained by concatenating the rows of $\mathbf{W}_t$ and then taking the transpose of the resulting long vector. Define by $\mathbf{O}_{t-1}$ the $k_t \times (k_{t-1} k_t)$ matrix

$$
  \mathbf{O}_{t-1} = \begin{bmatrix}
    \mathbf{o}_{t-1}^\top & \mathbf{0} & \cdots & \mathbf{0} \\
    \mathbf{0} & \mathbf{o}_{t-1}^\top & \cdots & \mathbf{0} \\
    \vdots & \vdots & \ddots & \vdots \\
    \mathbf{0} & \mathbf{0} & \cdots & \mathbf{o}_{t-1}^\top
  \end{bmatrix}
$$

With this notation we get $\mathbf{W}_{t-1} \mathbf{o}_{t-1} = \mathbf{O}_{t-1}\mathbf{w}_{t-1}$, so we can also write

$$
  g_t (\mathbf{w}_{t-1}) = \ell_t (\boldsymbol{\sigma}(\mathbf{O}_{t-1} \mathbf{w}_{t-1}))
$$

Therefore, applying the chain rule, we obtain that

$$
  J_{\mathbf{w}_{t-1}}(g_t) = J_{\boldsymbol{\sigma}(\mathbf{O}_{t-1} \mathbf{w}_{t-1})} (\ell_t) \operatorname{diag}(\boldsymbol{\sigma}' (\mathbf{O}_{t-1}\mathbf{w}_{t-1}))\mathbf{O}_{t-1}
$$

Using our nation we have $\mathbf{o}_t = \boldsymbol{\sigma}(\mathbf{O}_{t-1}\mathbf{w}_{t-1})$ and $\mathbf{a}_t = \mathbf{O}_{t-1} \mathbf{w}_{t-1}$, which yields

$$
  J_{\mathbf{w}_{t-1}} (g_t) = J_{\mathbf{o}_t} (\ell_t) \operatorname{diag}(\boldsymbol{\sigma}'(\mathbf{a}_t))\mathbf{O}_{t-1}
$$

Let us denote $\boldsymbol{\delta}_t = \mathbf{J}_{\mathbf{o}_t} (\ell_t)$. Then, we can further rewrite the preceding as

$$
  J_{\mathbf{w}_{t-1}}(g_t) = (\delta_{t,1} \sigma'(a_{t,1})\mathbf{o}_{t-1}^\top,\dots, \delta_{t,k_t} \sigma'(a_{t, k_t}))\mathbf{o}_{t-1}^\top
$$

It remains to calculate the vector $\boldsymbol{\delta}_t = J_{\mathbf{o}_t}(\ell_t)$ for every $t$. This is the gradient of $\ell_t$ at $\mathbf{o}_t$. We calculate this in a recursive manner. First observe that for the last layer, we have that $\ell_T (\mathbf{u}) = \Delta(\mathbf{u},\mathbf{y})$, where $\Delta$ is the loss function. Since we assume $\Delta (\mathbf{u},\mathbf{y}) = \frac{1}{2}\norm{\mathbf{u} - \mathbf{y}}^2$ we obtain that $J_\mathbf{u}(\ell_T) = (\mathbf{u} - \mathbf{y})$. In particular, $\boldsymbol{\delta}_T = J_{\mathbf{o}_T}(\ell_T) = (\mathbf{o}_T - \mathbf{y})$. Next, note that

$$
  \ell_t (\mathbf{u}) = \ell_{t+1}(\boldsymbol{\sigma}(\mathbf{W}_t \mathbf{u}))
$$

Thus, by the chain rule

$$
  J_\mathbf{u} (\ell_t) = J_{\boldsymbol{\sigma}(\mathbf{W}_t \mathbf{u})} (\ell_{t+1}) \operatorname{diag}(\boldsymbol{\sigma}' (\mathbf{W}_t \mathbf{u}))\mathbf{W}_t
$$

In particular

$$
\begin{align*}
  \boldsymbol{\delta}_t =& J_{\mathbf{o}_t} (\ell_t) = J_{\boldsymbol{\sigma}(\mathbf{W}_t \mathbf{o}_t)} (\ell_{t+1}) \operatorname{diag}(\boldsymbol{\sigma}' (\mathbf{W}_t \mathbf{o}_t))\mathbf{W}_t \\
  =& J_{\mathbf{o}_{t+1}} (\ell_{t+1}) \operatorname{diag}(\boldsymbol{\sigma}' (\mathbf{a}_{t+1}))\mathbf{W}_t \\
  =& \boldsymbol{\delta}_{t+1} \operatorname{diag}(\boldsymbol{\sigma}'(\mathbf{a}_{t+1}))\mathbf{W}_t
\end{align*}
$$

<MathBox title='Stochastic gradient on feedforward neural networks' boxType='algorithm'>
**parameters:**
- number of iterations $\tau$
- step size scheduler $\eta(t)$
- regularization parameter $\lambda > 0$

**input:**
- training set $(\mathbf{x}, \mathbf{y}) \sim\mathcal{D}$
- layered graph $(V,E)$
- differentiable activation function $\sigma:\R\to\R$

**initialize:**
- choose $\mathbf{w}_0 \in\R^{|E|}$ at random (from a distribution so that $\mathbf{w}_0$ is close enought to $\mathbf{0}$)

**for** $i=1,\dots,\tau$
1. sample $(\mathbf{x}, \mathbf{y})\sim\mathcal{D}$
2. calculate gradient $\mathbf{g}_i = \operatorname{backpropagation}(\mathbf{x},\mathbf{y},\mathbf{w},(V,E),\sigma)$
4. calculate learning step $\eta_i = \eta(i)$
3. update $\mathbf{w}_{i+1} = \mathbf{w}_i - \eta_i (\mathbf{g}_i + \lambda\mathbf{w}_i$
</MathBox>

<MathBox title='Backpropagation' boxType='algorithm'>
**parameters:**
- number of iterations $\tau$
- step size scheduler $\eta(t)$
- regularization parameter $\lambda > 0$

**input:**
- training set $(\mathbf{x}, \mathbf{y}) \sim\mathcal{D}$
- layered graph $(V,E)$
- differentiable activation function $\sigma:\R\to\R$

**initialize:**
- choose $\mathbf{w}_0 \in \R^{|E|}$ at random (from a distribution so that $\mathbf{w}_0$ is close enought to $\mathbf{0}$)

**for** $i=1,\dots,\tau$
1. sample $(\mathbf{x}, \mathbf{y})\sim\mathcal{D}$
2. calculate gradient $\mathbf{g}_i = \operatorname{backpropagation}(\mathbf{x},\mathbf{y},\mathbf{w},(V,E),\sigma)$
4. calculate learning step $\eta_i = \eta(i)$
3. update $\mathbf{w}_{i+1} = \mathbf{w}_i - \eta_i (\mathbf{g}_i + \lambda\mathbf{w}_i$
</MathBox>

### Derivation of the backpropagation algorithm

In this section we derive the backpropagation algorithm for training a feedforward neural netork (FFNN) using a quadratic cost function. The notation and methodology follow the framework established in [@book_nielsen_2015] and [@notes_smets_2024].

Suppose we have a training set $\set{(\mathbf{x}_n, \mathbf{y}_n) | \mathbf{x}\in\mathbb{F}^{n_0}, \mathbb{F}^{n_{L+1}}}_{n=1}^N$ of $N\in\N$ samples. We are interested in minimizing the mean squared error given by

$$
\begin{equation*}
  C(\mathbf{W}, \mathbf{b}) = \frac{1}{2N} \sum_{n=1}^N \lVert \hat{\mathbf{y}}_n - \mathbf{y} \rVert^2
\tag{\label{equation-1}}
\end{equation*}
$$

where $\hat{\mathbf{y}} = \mathbf{a}^{(L+1)} = \sigma(\mathbf{z}^{(L+1)})$ is the output of FFNN for the $n$th input. The cost of training example $n$, given by

$$
  C_n (\mathbf{W}, \mathbf{b}) = \frac{1}{2} \lVert \mathbf{a}^{(L+1)} - \mathbf{y}_n \rVert^2 = \frac{1}{2} \sum_{j=1}^{n_{L+1}} (a_j^{(L+1)} - y_n)^2
$$

To derive the backpropagation equations, we need to calculate the partial derivatives of $C_n$ with respect to the bias and the weights. From $\eqref{equation-}$, we obtain the partial derivatives

$$
  \frac{\partial z_j^{(\ell)}}{\partial w_{kj}^{(\ell)}} = a_k^{(\ell - 1)},\quad
  \frac{\partial z_j^{(\ell)}}{\partial b_j^{(\ell)}} = 1,\quad
  \frac{\partial a_j^{(\ell)}}{\partial z_j^{(\ell)}} = \sigma'_{\ell}(\mathbf{z}_j^{\ell})
$$

We also define the error $\delta_j^{(\ell)}$ for neuron $j$ in layer $\ell$ by 

$$
  \delta_j^{(\ell)} := \frac{\partial C_n}{\partial z_j^{(\ell)}} = \frac{\partial a_j^{(\ell)}}{\partial z_j^{(\ell)}} \frac{\partial C_n}{\partial a_j^{(\ell)}} = \sigma'_\ell (z_j^{(\ell)}) \frac{\partial C_n}{\partial a_j^{(\ell)}} 
$$

In matrix form, this can be written as

$$
  \boldsymbol{\delta}^{(\ell)} = \nabla_{\mathbf{a}^{(\ell)}} C_n \odot \sigma'_{\ell} (\mathbf{z}^{(\ell)})
$$

where $\odot$ denotes the Hadamard (elementwise) product. For the output layer $\ell = L + 1$ we find from $\eqref{equation-1}$, that $\nabla_{\mathbf{a}^{(L+1)}} C_n = \mathbf{a}^{(L)} - \mathbf{y}_n$ leading to the output error

$$
  \boldsymbol{\delta}^{(L)} = (\mathbf{a}^{(L)} - \mathbf{y}_n) \odot \sigma'_{L+1} (\mathbf{z}^{(L)})
$$

For any hidden layer $\ell < L + 1$, the partial derivative of $C_n$ with respect to an activation $a_k^{(\ell)}$ is found by summing up the errors $\delta_j^{(\ell + 1)}$ of the next layer $\ell + 1$, i.e.,

$$
  \frac{\partial C_n}{\partial a_k^{(\ell)}} = \sum_{j=1}^{n_{\ell+1}} \underbrace{\frac{\partial z_j^{(\ell + 1)}}{\partial a_k^{(\ell)}}}_{=w_{kj}^{(\ell+1)}} \underbrace{\frac{\partial a_j^{(\ell + 1)}}{\partial z_j^{(\ell+1)}} \frac{\partial C_n}{\partial a_j^{(\ell+1)}}}_{=\delta_j^{(\ell+1)}} = \sum_{j=1}^{n_{\ell+1}} w_{kj}^{(\ell + 1)} \delta_{j}^{(\ell+1)}.
$$

In matrix form this can be expressed as $\nabla_{\mathbf{a}^{(\ell)}} C_n = (\mathbf{W}^{(\ell+1)})^\top \boldsymbol{\delta}^{(\ell+1)}$. Thus, for any hidden layer $\ell < L + 1$, the error $\boldsymbol{\delta}^{(\ell)}$ can be expressed as

$$
  \boldsymbol{\delta}^{(\ell)} = (\mathbf{W}^{\ell + 1})^\top \boldsymbol{\delta}^{(\ell + 1)} \odot \boldsymbol{\sigma}'_{\ell} (\mathbf{z}^{(\ell)}).
$$

#### Gradient Computation

It remains to calculate the partial derivatives of the cost $C_n$ with respect to the bias and the weights. For any layer $\ell$, the partial derivative of $C_n$ with respect to a weight $w_{kj}^{(\ell)}$ is given by the chain rule

$$
  \frac{\partial C_n}{\partial w_{kj}^{(\ell)}} = \underbrace{\frac{\partial z_j^{(\ell)}}{\partial w_{kj}^{(\ell)}}}_{=a_k^{(\ell-1)}} \underbrace{\frac{\partial a_j^{(\ell)}}{\partial z_j^{(\ell)}} \frac{\partial C_n}{\partial a_j^{\ell}}}_{=\delta_j^{(\ell)}} = a_k^{(\ell-1)} \delta_j^{(\ell)}
$$

Likewise, the partial derivative of $C_n$ with respect a bias $b_j^{(\ell)}$ is given by

$$
  \frac{\partial C_n}{\partial b_j^{(\ell)}} = \underbrace{\frac{\partial z_j^{(\ell)}}{\partial b_j^{(\ell)}}}_{=1} \underbrace{\frac{\partial a_j^{(\ell)}}{\partial z_j^{(\ell)}} \frac{\partial C_n}{\partial a_j^{\ell}}}_{=\delta_j^{(\ell)}} = \delta_j^{(\ell)}
$$

The backpropagation algorithm can be summed up as follows:
- **Find the output error:** Calculate the output error $\boldsymbol{\delta}^{(L+1)}$ using the gradient of the cost with respect to the activations:

$$
  \boldsymbol{\delta}^{(\ell)} = \nabla_{\mathbf{a}^{(\ell)}} C_n \odot \sigma'_{\ell} (\mathbf{z}^{(\ell)}).
$$

- **Backpropagate the errors:** For each layer $\ell = L,L-1,\dots,1$ calculate the error $\boldsymbol{\delta}^{(\ell)}$ using

$$
  \boldsymbol{\delta}^{(\ell)} = (\mathbf{W}^{\ell + 1})^\top \boldsymbol{\delta}^{(\ell + 1)} \odot \boldsymbol{\sigma}'_{\ell} (\mathbf{z}^{(\ell)}).
$$

- **Update weights and biases:** For each layer $\ell = L,L-1,\dots,1$ adjust the biases $\mathbf{b}^{(\ell)}$ and weights $\mathbf{W}^{(\ell)}$ according to the updates

$$
\begin{align*}
  \hat{w}_{jk}^{(\ell)} =& w_{jk}^{(\ell)} - \eta \delta_j^{(\ell)} a_k^{(\ell - 1)} \\
  \hat{b}_j^{(\ell)} =& b_j^{(\ell)} - \eta \delta_j^{(\ell)},
\end{align*}
$$

where $\eta$ is the learning rate.
