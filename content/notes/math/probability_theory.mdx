---
title: 'Probability Theory'
subject: 'Mathematics'
showToc: true
---

The subjects of probability and statistics are somewhat inverse of each other. In probability, random experiments are completely specified by mathematical models. By contrast, statistics deals with incompletely specified mathematical models, and experiments are designed to draw inferences about the unknown factors in the mathematical model.

# Probability space

<MathBox title='Probability space' boxType='definition'>
A probability space $(\Omega, \mathcal{F}, \mathbb{P})$ is a measure space such that the measure of the whole space is equal to one. It consists of

1. An outcome space, $\Omega$
2. An event space, a $\sigma$-algebra $\mathcal{F} \subseteq 2^\Omega = \mathcal{P}(\Omega)$, which is a collection of events (subsets) of $\Omega$ satisfying
  - $\Omega, \emptyset \in \mathcal{F}$
  - $A \in \mathcal{F} \implies A^c = \Omega \backslash A \in \mathcal{F}$
  - $\bigcup_{i \in \N} A_i \in \mathcal{F},\; A_i \in \mathcal{F}$
3. A probability measure, $\mathbb{P}:\mathcal{F}\to[0, 1]$ that assigns a probability to each event in the sample space $(\Omega, \mathcal{F})$, satisfying the Kolmogorov axioms
  - $\mathbb{P}(A)\geq 0$ for every event $A\in\mathcal{F}$
  - $\mathbb{P}(\Omega) = 1$
  - $\mathbb{P}\left(\bigcup_{i\in\N} A_i \right) = \sum_{i\in\N} \mathbb{P}(A_i)$ for any sequence $\Set{A_i}_{i\in\N}$ of disjoint sets in $\mathcal{F}$, i.e. $A_i \cap A_j = \emptyset$ if $i\neq j$

Most of the sample spaces that occur in elementary probability fall into two general categories:
1. Discrete: $\Omega$ is countable and $\mathcal{F} = \mathcal{P}(\Omega)$ is the power set of $\Omega$.
2. Euclidean: $\Omega$ is a measurable subset of $\R^n$ for some $n\in\N$ and $\mathcal{F}$ is the collection of measurable subsets of $\Omega$.
</MathBox>

<MathBox title='Coin toss' boxType='example'>
Flipping a fair coin has the following outcomes 
- $H$ if the coin lands on its head
- $T$ if the coin lands on its tail

Flipping the coin once can be described with the following probability space
- Sample space: $\Omega_1 = \Set{H, T}$
- Event space: $\mathcal{F} = \Set{\emptyset, \Omega_1, \Set{H}, \Set{T} } = 2^{\Omega_1}$
- Probability measure $P(\Set{H}) = P(\Set{T}) = \frac{1}{2}$

Flipping the coin $n$ times gives the following probability space
- $\Omega_n = \Set{ \left(\omega_i \right)_{i=1}^n | \omega_i \in\Set{H, T} }$
- $\mathcal{F}_n = 2^{\Omega_n}$
</MathBox>

<MathBox title='Properties of the probability measure' boxType='definition'>
Given a probability space $(\Omega, \mathcal{F}, \mathbb{P})$, the probability measure $\mathbb{P}:\mathcal{F}\to[0, 1]$ has the following properties
- Monotonicity: If $A \subseteq B$ then $\mathbb{P}(A)\leq\mathbb{P}(B)$
</MathBox>

## Filtration

In the following discussion let $(\Omega,\mathcal{F})$ be a sample space and $(T,\mathcal{T})$ an index space.

<MathBox title='Filtration' boxType='definition'>
A collection of $\sigma$-algebras $\mathscr{F}:=\Set{\mathcal{F}_t}_{t\in T}$ is a filtration on $(\Omega,\mathcal{F})$ if $s,t\in T$ and $s\leq t$ imply $\mathcal{F}_s\subseteq\mathcal{F}_t\subseteq\mathcal{F}$. The triple $(\Omega,\mathcal{F},\mathscr{F})$ is a filtered sample space. If $\mathbb{P}$ is a probability measure on $(\Omega,\mathcal{F})$, then $(\Omega,\mathcal{F},\mathscr{F},\mathbb{P})$ is a filtered probability space.
</MathBox>

A filtration is simply an increasing collection of $\sigma$-algebras, indexed by $T$. Each $\mathcal{F}_t$ is a $\sigma$-algebra of events up to time $t\in T$, and contains the information that is available at that given time.

<MathBox title='Partial order on filtrations' boxType='definition'>
Suppose $\mathscr{F} = \Set{F_t}_{t\in T}$ and $\mathscr{G} = \Set{\mathcal{G}_t}_{t\in T}$ are filtrations on $(\Omega,\mathcal{F})$. We say that $\mathscr{F}$ is coarser than $\mathscr{G}$ and $\mathscr{G}$ is finer than $\mathscr{F}$, denoted $\mathscr{F}\leq\mathscr{G}$, if $\mathcal{F}_t\subseteq\mathcal{G}_t$ for all $t\in T$. The relation $\leq$ is a partial order on the collection of filtrations on $(\Omega,\mathcal{F})$. That is, if $\mathscr{F}=\Set{\mathcal{F}_t}_{t\in T}$, $\mathscr{G}=\Set{\mathcal{G}_t}_{t\in T}$ and $\mathscr{H}=\Set{\mathcal{H}_t}_{t\in T}$ are filtrations, then
- reflexivity: $\mathscr{F}\leq\mathscr{F}$
- anti-symmetry: $\mathscr{F}\leq\mathscr{G}\land\mathscr{G}\leq\mathscr{F}\implies\mathscr{F}=\mathscr{G}$
- transitivity: $\mathscr{F}\leq\mathscr{G}\land\mathscr{G}\leq\mathscr{H}\implies\mathscr{F}\leq\mathscr{H}$
</MathBox>

<MathBox title='$\sigma$-algebra generated by the union of a filtration' boxType='proposition'>
For a filtration $\mathscr{F} = \Set{\mathcal{F}_t}_{t\in T}$ on $(\Omega,\mathscr{F})$, defined $\mathscr{F}_\infty:=\sigma\left(\bigcup\Set{\mathcal{F}_t}_{t\in T}\right)$. Then by the increasing monotonicity of $\mathscr{F}$
1. $\mathcal{F}_\infty = \sigma\left(\bigcup \Set{\mathcal{F}_t | t\in T,t\geq s}\right)$ for $s\in T$
2. $\mathcal{F}_t\subseteq\mathcal{F}_\infty$ for $t\in T$
</MathBox>

<MathBox title='Intersection and union of filtrations' boxType='proposition'>
Suppose $\mathscr{F}_i = \Set{F_t}_{t\in T}$ is a filtration on $(\Omega,\mathscr{F})$ for each $i$ in an index set $I$. Then 
- $\mathscr{F}=\Set{\mathcal{F}_t}_{t\in T}$ where $\mathcal{F}_t = \bigcap{i\in I}\mathcal{F}_t^i$ for $t\in T$ is also a filtration on $(\Omega,\mathcal{F})$. This filtration is sometimes denoted $\mathscr{F}=\bigwedge_{i\in I}\mathscr{F}_i$ and is the finest filtration that is coarser than $\mathscr{F}_i$ for every $i\in I$.
- $\mathscr{F}=\Set{\mathcal{F}_t}_{t\in T}$ where $\mathcal{F}_t = \sigma\left(\bigcup_{i\in I} \mathcal{F}_t^i \right)$ for $t\in T$ is also a filtration on $(\Omega,\mathcal{F})$. This filtration is sometimes denoted $\mathscr{F}=\bigvee_{i\in I}\mathscr{F}_i$, and is the coarsest filtration that is finer than $\mathscr{F}_i$ for every $i\in I$. 
<details>
<summary>Proof sketch</summary>

Suppose $s,t\in T$ with $s\leq t$. Then $\mathcal{F}_s^i\subseteq\mathcal{F}_t^i\subseteq\mathcal{F}$ for each $i\in I$. Then
1. $\bigcap_{i\in I}\mathcal{F}_s^i\subseteq\bigcap_{i\in I}\mathcal{F}_t^i\subseteq\mathcal{F}$.
2. $\bigcup_{i\in I}\mathcal{F}_s^i\subseteq\bigcup_{i\in I}\mathcal{F}_t^i\subseteq\mathcal{F}$, and hence $\sigma\left(\bigcup_{i\in I}\mathcal{F}_s^i\subseteq\mathcal{F}\right) = \sigma\left(\bigcup_{i\in I}\mathcal{F}_t^i\subseteq\mathcal{F}\right)\subseteq\mathcal{F}$
</details>
</MathBox>

### Completion

<MathBox title='Complete filtration' boxType='definition'>
The filtration $\mathscr{F} = \Set{\mathcal{F}_t}_{t\in T}$ is complete with respect to a probability measure $\mathbb{P}$ on $(\Omega,\mathcal{F})$ if
1. $\mathcal{F}$ is complete with respect to $\mathbb{P}$
2. If $A\in\mathcal{F}$ and $\mathbb{P}(A) = 0$ then $A\in\mathcal{F}_0$

Since the $\sigma$-algebras in $\mathscr{F}$ are increasing in $t\in T$, it follows that if $A\in\mathcal{F}$ is a null event, i.e. $\mathbb{P}(A) = 0$, or an almost certain event, i.e. $\mathbb{P}(A) = 1$, then $A\in\mathcal{F}_t$ for every $t\in T$.
</MathBox>

<MathBox title='Completion of a filtration' boxType='proposition'>
Suppose $\mathscr{F} = \Set{\mathcal{F}_t}_{t\in T}$ is a filtration on the probability space $(\Omega,\mathcal{F},\mathbb{P})$. Let $\mathcal{F}_t^\mathbb{P} = \sigma(\mathcal{F}_t\cup\mathcal{N})$ for $t\in T$ where $\mathcal{N}$ is the null set of $\Omega$. Then $\mathscr{F}^\mathbb{P} = \Set{\mathcal{F}_t^\mathbb{P}}_{t\in T}$ is a filtration on $\left(\Omega, \mathcal{F}^\mathbb{P} \right)$ that is finer than $\mathscr{F}$ and is complete relative to $\mathbb{P}$.
<details>
<summary>Proof sketch</summary>

If $s,t\in T$ with $s\leq t$ then $\mathcal{F}_s\subseteq\mathcal{F}_t\subseteq\mathcal{F}$ and consequently

$$
  \sigma(\mathcal{F}_s\cup\mathcal{N})\subseteq\sigma(\mathcal{F}_t\cup\mathcal{N})\subseteq\sigma(\mathcal{F}\cup\mathcal{N})
$$

showing that $\mathcal{F}_s^\mathbb{P}\subseteq\mathcal{F}_t^\mathbb{P}\subseteq\mathcal{F}^\mathbb{P}$. The probability measure $\mathbb{P}$ can be extended to $\mathcal{F}^\mathbb{P}$, and hence is defined on $\mathcal{F}_t^\mathbb{P}$ for each $t\in T$. By construction, if $A\in\mathcal{F}^\mathbb{P}$ and $\mathbb{P}(A) = 0$ then $A\in\mathcal{F}_0^\mathbb{P}$ so $\mathscr{F}_0^\mathbb{P}$ is complete with respect to $\mathbb{P}$.
</details>
</MathBox>

<MathBox title='Universal completion' boxType='proposition'>
Let $\mathcal{P}$ denote the collection of probability measures on $(\Omega,\mathcal{F})$ and suppose $\mathscr{F} = \Set{\mathcal{F}_t}_{t\in T}$ is a filtration on $(\Omega,\mathcal{F})$. Let $\mathcal{F}* = \bigcap\Set{\mathcal{F}^\mathbb{P} }_{\mathbb{P}\in\mathcal{P}}$, and let $\mathscr{F}* = \bigwedge\Set{\mathscr{F}^\mathbb{P}}_{\mathbb{P}\in\mathcal{P}}$. Then $\mathscr{F}*$ is a filtration on $(\Omega,\mathcal{F})$, called the universal completion of $\mathscr{F}$.
</MathBox>

### Right continuity

<MathBox title='' boxType='proposition'>
Suppose $\mathscr{F} = \Set{\mathcal{F}_t | t\in[0,\infty)}$ is a filtration on $(\Omega,\mathcal{F})$. For $t\in[0,\infty)$ define $\mathcal{F}_{t+} = \bigcap\Set{\mathcal{F}_t | s\in(t,\infty)}$. Then $\mathscr{F}_+ = \Set{\mathcal{F}_{t+}}_{t\in T}$ is also a filtration on $(\Omega,\mathcal{F})$ that is finer than $\mathscr{F}$.
<details>
<summary>Proof sketch</summary>

For $t\in[0,\infty)$ note that $\mathcal{F}_{t+}$ is a $\sigma$-algebra because it is the intersection of $\sigma$-algebras, and clearly $\mathcal{F}_{t+}\subseteq\mathcal{F}$. Next, if $s,t\in[0,\infty)$ with $s\leq t$, then

$$
  \mathcal{F}_{s+} = \bigcap\Set{\mathcal{F}_r | r\in(s,\infty)}\subseteq\bigcap\Set{\mathcal{F}_r | r\in(t,\infty)} = \mathcal{F}_{t+}
$$

Finally, for $t\in[0,\infty)$, $\mathcal{F}_t\subseteq\mathcal{F}_s$ for every $s\in(t,\infty)$ so $\mathcal{F}_t\subseteq \bigcap\Set{\mathcal{F}_s | s\in(t,\infty)} = \mathcal{F}_{t+}$.
</details>
</MathBox>

<MathBox title='Right continuous filtration' boxType='definition'>
A filtration $\mathscr{F} = \Set{\mathcal{F}_t | t\in[0,\infty)}$ is *right continuous* if $\mathscr{F}_+ = \mathscr{F}$ so that $\mathcal{F}_{t+} = \mathcal{F}_t$ for every $t\in[0, \infty)$. 
</MathBox>

<MathBox title='' boxType='proposition'>
If $\mathscr{F} = \Set{\mathcal{F}_t | t\in[0,\infty)}$ is a filtration, then $\mathscr{F}_+$ is a right continuous filtration.
<details>
<summary>Proof sketch</summary>

For $t\in T$

$$
  \mathcal{F}_{t++} = \bigcap\Set{\mathcal{F}_{s+} | s\in(t,\infty)} = \bigcap\Set{ \mathcal{F}_r | r\in(s,\infty) }\mid s\in(t,\infty)} = \bigcap\Set{\mathcal{F}_{s+} | u\in(t,\infty) } = \mathcal{F}_{t+}
$$
</details>
</MathBox>

### Stopping time

A stopping time is a random variable representing a time at which a specific event or state occurs.

To allow random time converging to infinity, we need to extend the time set $T$ to include $\infty$ by definining $T_\infty = T\cup\Set{t\in T_\infty | t>s}$ is an open neighbourhood of $\infty$. This makes $T_\infty$ a one-point compactification of $T$. The extended space $T_\infty$ can be made measurable by giving it the Borel $\sigma$-algebra generated by the extended topology of $T_\infty$.

<MathBox title='Random time' boxType='definition'>
Let $(\Omega,\mathcal{F})$ be a sample space and $(T_\infty, \mathcal{T}_\infty)$ a time space where $T_\infty = T\cup\Set{\infty}$. A random variable $\tau:\Omega\to T_\infty$ taking values in $T_\infty$ is called a random time. 
</MathBox>

<MathBox title='Stopping time' boxType='definition'>
Suppose $\mathscr{F} = \Set{\mathcal{F}_t}_{t\in T}$ is a filtration on $(\Omega,\mathcal{F})$. A random time $\tau$ is a stopping time relative to $\mathscr{F}$ if $\Set{\tau\leq t}\in\mathcal{F}_t$ for each $t\in T$. Equivalently, $\tau$ is a stopping time relative to $\mathscr{F}$ if $\Set{\tau > t}\in\mathcal{F}_t$ for each $t\in T$ since $\Set{\tau\leq t}^c = \Set{\tau > t}$.
</MathBox>

<MathBox title='Inheritance property of stopping time' boxType='proposition'>
Suppose $\mathscr{F} = \Set{\mathcal{F}_t}_{t\in T}$ and $\mathscr{G} = \Set{\mathcal{G}_t}_{t\in T}$ are filtrations on $(\Omega,\mathcal{F})$ and that $\mathscr{G}$ is finer than $\mathscr{F}$. If a random time $\tau$ is a stopping time relative to $\mathscr{F}$ then $\tau$ is a stopping time relative to $\mathscr{G}$.
<details>
<summary>Proof</summary>

For $t\in T$, then $\Set{\tau\leq t}\in\mathcal{F}_t$ and hence $\Set{\tau\leq t}\in\mathcal{G}_t$ since $\mathcal{F}_t\subseteq\mathcal{G}_t$.
</details>
</MathBox>

<MathBox title='Properties of stopping time' boxType='proposition'>
Suppose $\mathscr{F} = \Set{\mathcal{F}_t}_{t\in T}$ is a filtration on $(\Omega,\mathcal{F})$, and that $\tau$ is a stopping time relative to $\mathscr{F}$. Then for every $t\in T$
1. $\Set{\tau < t}\in\mathcal{F}_t$
2. $\Set{\tau \geq t}\in\mathcal{F}_t$
3. $\Set{\tau = t}\in\mathcal{F}_t$
<details>
<summary>Proof</summary>

1. Suppose $T=\N$. Then $\Set{\tau< t} = \Set{\tau\leq t-1}\in\mathcal{F}_{t-1}\subseteq\mathcal{F}_t$ for $t\in\N$. Next suppose $T=[0,\infty)$. Fix $t\in(0,\infty)$ and let $(s_i)_{i\in\N}$ be a strictly increasing sequence in $[0,\infty)$ with $\s_n\xrightarrow{n\to\infty} t$. Then $\Set{\tau< t} = \bigcup_{n\in\N} \Set{\tau\leq s_n}$. However, $\Set{\tau\leq s_n}\in\mathcal{F}_{s_n}\subseteq\mathcal{F}_t$ for each $n$, so $\Set{\tau < t}\in\mathcal{F}_t$.
2. Note that $\Set{\tau\geq t} = \Set{\tau< t}^c$ for $t\in T$. It follows that $\Set{\tau\geq t}\in\mathcal{F}_t$
3. For $t\in T$ note that $\Set{tau = t} = \Set{\tau\leq t}\setminus\Set{\tau < t}$. Both events in the set difference are in $\mathcal{F}_t$.
</details>
</MathBox>

<MathBox title='Condition for stopping time on right continuous filtrations' boxType='proposition'>
Suppose $T=[0,\infty)$ and that $\mathscr{F} = \Set{\mathcal{F}_t | t\in[0,\infty)}$ is a filtration on $(\Omega,\mathcal{F})$. A random time $\tau$ is a stopping time relative to $\mathscr{F}_+$ if and only if $\Set{\tau< t}\in\mathcal{F}_t$ for every $t\in[0,\infty)$.

If $\mathscr{F}$ is right-continuous, then $\tau$ is a stopping time relative to $\mathscr{F}$ if and only if $\Set{\tau<t}\in\mathcal{F}_t$ for every $t\in[0,\infty)$.
<details>
<summary>Proof</summary>

We need to show that $\Set{\tau\leq t}\in\mathcal{F}_{t+}$ for every $t\in[0,\infty)$ if and only if $\Set{\tau< t}\in\mathcal{F}_t$ for every $t\in[0,\infty)$. Suppose first that $\tau$ is a stopping time relative to $\mathscr{F}$. Fix $t\in[0,\infty)$ and let $(t_n)_{n\in\N}$ be a strictly decreasing sequence in $[0,\infty)$ with $t_n \xrightarrow{n\to\infty} t$. Then for each $k\in\N$, we have $\Set{\tau\leq t} = \bigcap_{n=k}^\infty \Set{\tau < t_n}$. If $s> t$ then there exists $k\in\N$ such that $t_n < s$ for each $n\geq k$. Therefore $\Set{\tau\leq t_n}\in\mathcal{F}_{t_n}\subseteq\mathcal{F}_s$ for $n\geq k$, and it follows that $\Set{\tau\leq t}\in\mathcal{F}_s$. Since this is true for every $s> t$ it follows that $\Set{\tau, t}\in\mathcal{F}_{t+}$.

Conversely, suppose $\Set{\tau\leq t}\in\mathcal{F}_{t+}$ for every $t\in[0,\infty)$. Fix $t\in(0,\infty)$ and let $(t_n)_{n\in\N}$ be a strictly increasing sequence in $(0,\infty)$ with $t_n\xrightarrow{n\to\infty}t$. Then $\bigcup_{n\in\N}\Set{\tau\leq t_n} = {\tau<t}$. However for every $n\in\N$

$$
  \Set{\mathcal{F}_s | s\in(t_n, t)}\subseteq\mathcal{F}_t
$$

Hence $\Set{\tau<t}\in\mathcal{F}_t$.
</details>
</MathBox>

<MathBox title='Condition for stopping time on discrete filtrations' boxType='proposition'>
Suppose $T=\N$ and that $\mathscr{F} = \Set{\mathcal{F}_n }_{n\in\N}$ is a filtration on $(\Omega,\mathcal{F})$. A random time $\tau$ is a stopping time relative to $\mathscr{F}$ if and only if $\Set{\tau = t}\in\mathcal{F}_n$ for every $n\in\N$.
<details>
<summary>Proof</summary>

If $\tau$ is a stopping time then by previous results, $\Set{\tau = t}\in\mathcal{F}_n$ for every $n\in\N$. Conversely, suppose this condition holds. For $n\in\N$, then $\Set{\tau\leq n} = \bigcup_{k=0}^n \Set{\tau = k}$. However, $\Set{\tau = k}\in\mathcal{F}_k\subseteq\mathcal{F}_n$ for $k\in\Set{0,1,\dots,n}$ so $\Set{\tau\leq n}\in\mathcal{F}_n$.
</details>
</MathBox>

#### $\sigma$-algebra

<MathBox title='$\sigma$-algebra of a stopping time' boxType='proposition'>
Suppose $\mathscr{F} = \Set{A\in\mathcal{F} | A\cap\Set{\tau\leq t}\in\mathcal{F}_t\;\forall t\in T}$. Then $\mathcal{F}_\tau$ is a $\sigma$-algebra.
<details>
<summary>Proof</summary>

First $\Omega\in\mathcal{F}_\tau$ since $\Omega\cap\Set{\tau\leq t}\in\mathcal{F}_t$ for $t\in T$. If $A\in\mathcal{F}_\tau$, then $A^c\cap\Set{\tau\leq t} = \Set{\tau\leq t}\setminus(A\cap\Set{\tau\leq t})\in\mathcal{F}_t$ for $t\in T$. Finally, suppose $A_i\in\mathcal{F}_\tau$ for $i$ in a countable index set $I$, then $\left(\bigcup_{i\in I} A_i \right)\cap\Set{\tau\leq t} = \bigcup_{i\in I} (A_i \cap \Set{\tau\leq t})\in\mathcal{F}_t$ for $t\in T$.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Suppose $\mathscr{F} = \Set{\mathcal{F}_t }_{t\in T}$ is a filtration on $(\Omega,\mathcal{F})$. Fix $s\in T$ and define $\tau(\omega) = s$ for all $\omega\in\Omega$. Then $\mathcal{F}_\tau = \mathcal{F}_s$
<details>
<summary>Proof</summary>

Suppose $A\in\mathcal{F}_s$. Then $A\in\mathcal{F}$ and for $t\in T$ we have $A\cap\Set{\tau\leq t}=A$ if $s\leq t$ and $A\cap\Set{\tau\leq t}=\emptyset$ if $s>t$. In either case, $A\cap\Set{\tau\leq t}\in\mathcal{F}_t$ and hence $A\in\mathcal{F}_\tau$. Conversely, suppose $A\in\mathcal{F}_\tau$. Then $A = A\cap\Set{\tau\leq s}\in\mathcal{F}_s$.
</details>
</MathBox>

<MathBox title='Measurability of stopping time' boxType='proposition'>
Suppose $\mathscr{F} = \Set{A\in\mathcal{F} | A\cap\Set{\tau\leq t}\in\mathcal{F}_t\;\forall t\in T}$. Then $\tau$ is measurable with respect to $\mathcal{F}_\tau$.
<details>
<summary>Proof</summary>

If suffices to show that $\Set{\tau\leq s}\in\mathcal{F}_\tau$ for each $s\in T$. For $s,t\in T$

$$
  \Set{\tau\leq t}\cap\Set{\tau\leq s} = \Set{\tau\leq s\wedge t}\in\mathcal{F}_{s\wedge t}\subseteq\mathcal{F}_t
$$
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Suppose $\mathscr{F} = \Set{A\in\mathcal{F} | A\cap\Set{\tau\leq t}\in\mathcal{F}_t\;\forall t\in T}$. If $A\in\mathcal{F}_\tau$ then for $t\in T$
1. $A\cap\Set{\tau\leq T}\in\mathcal{F}_t$
2. $A\cap\Set{\tau = T}\in\mathcal{F}_t$
<details>
<summary>Proof</summary>

1. By definition $A\cap\Set{\tau\leq t}\in\mathcal{F}_t$. However $\Set{\tau< t}\subseteq\Set{\tau\leq t}$ and $\Set{\tau<t}\in\mathcal{F}_t$. Hence $A\cap\Set{\tau<t}=A\cap\Set{\tau\leq t}\cap\Set{\tau<t}\in\mathcal{F}_t$
2. Similarly, $\Set{\tau = t}\subseteq\Set{\tau\in t}$ and $\Set{\tau = t }\in\mathcal{F}_t$. Hence $A\cap\Set{\tau= t} = A\cap\Set{\tau\leq t}\cap\Set{\tau= t}\in\mathcal{F}_t$.
</details>
</MathBox>

<MathBox title='Inheritance property' boxType='proposition'>
Suppose $\mathscr{F}=\Set{\mathcal{F}_t }_{t\in T}$ and $\mathscr{G}=\Set{\mathcal{G}_t }_{t\in T}$ are filtrations on $(\Omega,\mathcal{F})$ and that $\mathscr{G}$ is finer than $\mathscr{F}$. If $\tau$ is a stopping time relative to $\mathscr{F}$, then $\mathcal{F}_\tau\subseteq\mathcal{G}_\tau$. 
<details>
<summary>Proof</summary>

From previous results, $\tau$ is also a stopping time relative to $\mathscr{G}$. If $A\in\mathcal{F}_\tau$ then for $t\in T$ we have $A\cap\Set{\tau\leq t}\in\mathcal{F}_t\subseteq\mathcal{G}_t$, so $A\in\mathcal{G}_\tau$.
</details>
</MathBox>

<MathBox title='Ordering property' boxType='proposition'>
Suppose $\mathscr{F}=\Set{\mathcal{F}_t }_{t\in T}$ is a filtration on $(\Omega,\mathcal{F})$ and that $\rho$ and $\tau$ are stopping times relative to $\mathscr{F}$ with $\rho\leq\tau$. Then $\mathcal{F}_\rho\subseteq\mathcal{F}_\tau$. 
<details>
<summary>Proof</summary>

Suppose $A\in\mathcal{F}_\rho$ and $t\in T$. Note that $\Set{\rho\leq t}\subseteq\Set{\tau\leq t}$. By definition $A\cap\Set{\rho\leq t}\in\mathcal{F}_t$ and $\Set{\tau\leq t}\in\mathcal{F}_t$. Hence $A\cap\Set{\tau\leq t} = A\cap\Set{\rho\cap t}\cap\Set{\tau\leq t}\in\mathcal{F}_t$ so $A\in\mathcal{F}_\tau$.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Suppose $\mathscr{F}=\Set{\mathcal{F}_t }_{t\in T}$ is a filtration on $(\Omega,\mathcal{F})$ and that $\rho$ and $\tau$ are stopping times relative to $\mathscr{F}$ with $\rho\leq\tau$. Then each of the following events is in $\mathcal{F}_\tau$ and $\mathcal{F}_\rho$
1. $\Set{\rho<\tau}$
2. $\Set{\rho=\tau}$
3. $\Set{\rho>\tau}$
4. $\Set{\rho\leq\tau}$
5. $\Set{\rho\geq\tau}$
<details>
<summary>Proof</summary>

For simplicity let $T=\N$
1. Let $t\in T$, then

$$
  \Set{\rho\leq\tau}\cap\Set{\tau\leq t} = \bigcup_{n=0}^t \bigcup_{k=0}^{n-1} \Set{\tau = n,\rho = k}
$$

Each event in the union is in $\mathcal{F}_t$
2. Let $t\in T$, then

$$
  \Set{\rho=\tau}\cap\Set{\tau\leq t} = \bigcup_{n=0}^t \Set{\rho = n,\tau = k}
$$

Each event in the union is in $\mathcal{F}_t$
3. This follows from symmetry by reversing $\Set{\rho<\tau}$
4. Note that $\Set{\rho\leq\tau} = \Set{\rho<\tau}\cup\Set{\rho=\tau}\in\mathcal{F}_\tau$
5. Note that $\Set{\rho\geq\tau} = \Set{\rho>\tau}\cup\Set{\rho=\tau}\in\mathcal{F}_\tau$
</details>
</MathBox>

<MathBox title='Stopped filtration' boxType='proposition'>
Suppose $\mathscr{F}=\Set{\mathcal{F}_t }_{t\in T}$ is a filtration on $(\Omega,\mathcal{F})$ and that $\tau$ is a stopping time for $\mathscr{F}$. For $t\in T$ define $\mathcal{F}_t^\tau = \mathcal{F}_{t\wedge\tau}$. Then $\mathscr{F}^\tau = \Set{\mathcal{F}_t^\tau}_{t\in T}$ is a filtration coarser than $\mathscr{F}$.
<details>
<summary>Proof</summary>

The random time $t\wedge\tau$ is a stopping time for each $t\in T$ by previous results, so $\mathcal{F}_t^\tau$ is a sub $\sigma$-algebra of $\mathcal{F}$. If $t\in T$, then by definition $A\in\mathcal{F}_t^\tau$ if and only if $A\cap\Set{t\wedge\tau\leq r}\in\mathcal{F}_r$ for every $r\in T$. However, for $r\in T$, then $\Set{t\wedge\tau\leq r} = \Omega$ if $r\geq t$ and $\Set{t\wedge\tau\leq r}=\Set{\tau\leq r}$ if $r<t$. Hence $A\in\mathcal{F}_t^\tau$ if and only if $A\cap\Set{\tau< r}\in\mathcal{F}_r$ for $r<t$ and $A\in\mathcal{F}_t$. So in particular, $\mathscr{F}^\tau$ is coarser than $\mathscr{F}$. Further, suppose $s,t\in T$ with $s\leq t$ and that $A\in\mathcal{F}_s^\tau$. Let $r\in T$. If $r<s$ then $A\cap\Set{\tau\leq r}\in\mathcal{F}_r$. If $s\leq r< t$, then $A\in\mathcal{F}_s\subseteq\mathcal{F}_r$ and $\Set{\tau\leq r}\in\mathcal{F}_r$ so again $A\cap\Set{\tau\leq r}\in\mathcal{F}_r$. Finally if $r\geq t$ then $A\in\mathcal{F}_s\subseteq\mathcal{F}_t$. Hence $A\in\mathcal{F}_t^\tau$.
</details>
</MathBox>

#### Construction

<MathBox title='Trivial stopping time' boxType='proposition'>
Suppose $s\in T_\infty$ and that $\tau(\omega) = s$ for all $\omega\in\Omega$. Then $\tau$ is a stopping time relative to any filtration on $(\Omega,\mathcal{F})$. 
<details>
<summary>Proof</summary>

For $t\in T$ note that $\Set{\tau\leq t} = \Omega$ if $s\leq t$ and $\Set{\tau<t} = \emptyset$ if $s>t$. 
</details>
</MathBox>

<MathBox title='Combinations of stopping time' boxType='proposition'>
Suppose $\mathscr{F} = \Set{\mathcal{F}_t}_{t\in T}$ is a filtration on $(\Omega,\mathcal{F})$ and that $\tau_1$ and $\tau_2$ are stopping times relative to $\mathscr{F}$. Then each of the following is also a stopping time relative to $\mathscr{F}$
1. $\tau_1\vee\tau_2 = \max\Set{\tau_1,\tau_2}$
2. $\tau_1\wedge\tau_2 = \min\Set{\tau_1,\tau_2}$
3. $\tau_1+\tau_2$

It follows that if $(\tau_i)_{i=1}^{n\in\N}$ is a finite sequence of stopping times relative to $\mathscr{F}$, then each of the following is also a stopping time relative to $\mathscr{F}$:
1. $\bigvee_{i=1}^n \tau_i$
2. $\bigwedge_{i=1}^n \tau_i$
3. $\sum_{i=1}^n \tau_i$
<details>
<summary>Proof</summary>

1. Note that $\Set{\tau_1 \vee \tau_2 \leq t } = \Set{\tau_1\leq t}\cap\Set{\tau_2\leq t}\in\mathcal{F}_t$ for $t\in T$.
2. Note that $\Set{\tau_1 \wedge \tau_2 \leq t } = \Set{\tau_1 > t}\cap\Set{\tau_2 > t}\in\mathcal{F}_t$ for $t\in T$.
3. Suppose first that $T=\N$. Then $\Set{\tau_1 + \tau_2 \leq t} = \bigcup_{n=0}^t \Set{\tau_1 = n}\cap\Set{\tau_2 \leq t-n}$. However, for $n\leq t$, then $\Set{\tau_1 = n}\in\mathcal{F}_n\subseteq\mathcal{F}_t$ and $\Set{\tau_2\leq t-n}\in\mathcal{F}_{t-n}\subseteq\mathcal{F}_t$. Hence $\Set{\tau_1 + \tau_2\leq t}\in\mathcal{F}_t$. Suppose instead that $T=[0,\infty)$ and $t\in T$. Then $\tau_1 + \tau_2 > t$ if and only if either $\tau_1\leq t$ and $\tau_2 > t - \tau_2$ or $\tau_1 > t$. Clearly $\Set{\tau_1>t}\in\mathcal{F}_t$, so it remains show that the first event is also in $\mathcal{F}_t$. Note that $\tau_1\leq t$ and $\tau_2 > t-\tau_1$ if and only if there exists a rational $q\in[0, t]$ such that $q\leq\tau_1\leq t$ and $\tau_2\geq t-q$. Each of these events is in $\mathcal{F}_t$ and hence so is the union of events over the countable collection of rationals $q\in[0, t]$.
</details>
</MathBox>

<MathBox title='Countable combinations of stopping time' boxType='proposition'>
Suppose $\mathscr{F} = \Set{\mathcal{F}_t}_{t\in T}$ is a filtration on $(\Omega,\mathcal{F})$ and that $(\tau_n)_{n\in\N}$ is a sequence of stopping times relative to $\mathscr{F}$. Then $\sup\Set{\tau_n}_{n\in\N}$ is also a stopping time relative to $\mathscr{F}$.

If $(\tau_n)_{n\in\N}$ is an increasing sequence of stopping times relative to $\mathscr{F}$, then $\lim_{n\to\infty}\tau_n = \sup\Set{\tau_n}_{n\in\N}$ is a stopping time relative to $\mathscr{F}$.
<details>
<summary>Proof</summary>

Let $\tau = \sup\Set{\tau_n}_{n\in\N}$. Note that $\tau\in T_\infty$ is a random time. For $t\in T$ then $\Set{\tau\leq t} = \bigcap_{n\in\N}\Set{\tau_n\leq t}$. However, $\Set{\tau_n\leq t}\in\mathcal{F}_t$ for each $n\in\N$ and hence the intersection is also in $\mathcal{F}_t$. 
</details>
</MathBox>

<MathBox title='Stopping times of right-continuous filtrations' boxType='proposition'>
Let $T=[0,\infty)$ and suppose $\mathscr{F} = \Set{\mathcal{F}_t | t\in[0,\infty)}$ is a filtration on $(\Omega,\mathcal{F})$. If $(\tau_n)_{n\in\N}$ is a sequence of stopping times relative to $\mathscr{F}$, then each of the following is a stopping time relative to $\mathscr{F}_+$
1. $\inf\Set{\tau_n}_{n\in\N}$
2. $\liminf_{n\to\infty}\tau_n$
3. $\limsup_{n\to\infty}\tau_n$
<details>
<summary>Proof</summary>

1. Let $\tau = \inf\Set{\tau_n}_{n\in\N}$. Then $\Set{\tau\geq t} = \bigcap_{n\in\N} \Set{\tau_n\geq t}\in\mathcal{F}_t$ for $t\in T$. Hence by previous results, $\tau$ is a stopping time relative to $\mathscr{F}_+$.
2. Note that $\liminf_{n\to\infty}\tau_n = \sup\Set{\inf\Set{\tau_k | k\geq n}\mid n\in\N}$ and so this is a stopping time relative to $\mathscr{F}_+$ by part ($1$) and the result above on supremums.
3. Note that $\limsup_{n\to\infty}\tau_n = \inf\Set{\sup\Set{\tau_k \;\; k\geq n}\;\; n\in\N}$, and so this is a stopping time relative to $\mathscr{F}_+$.
</details>
</MathBox>

## Events

<MathBox title='Event' boxType='definition'>
Let $(\Omega, \mathcal{A}, \mathbb{P})$ be a probability space. An event is a subset $A\subseteq\Omega$. If $\omega\in\Omega$ is the outcome of a random experiment, then $A$ is said to occur if $\omega\in A$. Otherwise, if $\omega\notin A$, then $A$ is said to not occur. In terms of the indicator function $\mathbf{1}_A:\Omega\to\Set{0, 1}$, this is written

$$
  \mathbf{1}_A(\omega) = \begin{cases} 1,\quad \omega\in A \\ 0,\quad \omega\notin A \end{cases}
$$
</MathBox>

<MathBox title='Algebra of events' boxType='proposition'>
Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space, and consider the events $A, B\in\mathcal{F}$. Then
1. $A\subseteq B$ if and only if the occurence of $A$ implies the occurence of $B$, i.e. $A\subseteq B\iff \mathbf{1}_A\leq \mathbf{1}_B$.
2. $A\cup B$ is the event that occurs if and only if $A$ occurs or $B$ occurs, i.e. $\mathbf{1}_{A\cup B} = 1 - (1 - \mathbf{1}_A)(1 - \mathbf{1}_B) = \max\Set{\mathbf{1}_A, \mathbf{1}_B}$.
3. $A\cap B$ is the event that occurs if and only if $A$ occurs and $B$ occurs, i.e. $\mathbf{1}_{A\cap B} = \mathbf{1}_A\mathbf{1}_B = \min\Set{\mathbf{1}_A, \mathbf{1}_B}$
4. $A$ and $B$ are disjoint if and only if they are mutually exclusive; they cannot both occur on the same run of experiment.
5. $A^c$ is the event that occurs if and only if $A$ does not occur, i.e $\mathbf{1}_{A^c} = 1 - \mathbf{1}_A$.
6. $A\setminus B$ is the event that occurs if and only if $A$ occurs and $B$ does not occur, i.e. $\mathbf{1}_{A\setminus B} = \mathbf{1}_A(1 - \mathbf{1}_B)$
7. $(A\cap B^c)\cup(B\cap A^c)$ is the event that occurs if and only if one but not both of the given events occur.
8. $(A\cap B)\cup(A^c\cap B^c)$ is the event that occurs if and only if both or neither of the given events occur.
</MathBox>

### Equivalence

<MathBox title='Null and almost sure events' boxType='definition'>
Let $(\Omega, \mathcal{F}, \mathbb{P})$  be a probability space. Define the following collections of events
- $\mathcal{N} := \Set{A\in\mathcal{F}| \mathbb{P}(A) = 0}$, the collection of null events
- $\mathcal{M} := \Set{A\in\mathcal{F}| \mathbb{P}(A) = 1}$, the collection of almost sure events
- $\mathcal{D} := \mathcal{N}\cup\mathcal{M} = \Set{A\in\mathcal{F}| \mathbb{P}(A) = 0 \lor \mathbb{P}(A) = 1}$, the collection of essentially deterministic events

The collection of essentially deterministic events is a sub $\sigma$-algebra of $\mathcal{F}$.
</MathBox>

<MathBox title='Equivalence of events' boxType='definition'>
Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space. Two events $A, B\in\mathcal{F}$ are equivalent if 

$$
\begin{gather*}
  A\Delta B = (A\setminus B)\cup(B\setminus A)\in\mathcal{N} \\
  \iff \mathbb{P}(A\Delta B) = 0 \iff \mathbb{P}(A\setminus B) = \mathbb{P}(A\setminus B) = 0
\end{gather*}
$$

The relation $\equiv$ is a an equivalence relation on $\mathcal{F}$ satisfying for $A,B,C\in\mathcal{F}$
- reflexivity: $A\equiv A$
- symmetry: $A\equiv B\implies B\equiv A$
- transitivity: $A\equiv B\land B\equiv C\implies A\equiv C$
</MathBox>

<MathBox title='Properties of equivalent events' boxType='proposition'>
Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space. The following properties hold for equivalent events
1. If $A, B\in\mathcal{F}$ and $A\equiv B$ then $\mathbb{P}(A) = \mathbb{P}(B)$
2. If $A\equiv B$ then $A^c \equiv B^c$
3. Suppose $A_i, B_i\in\mathcal{F}$ for $i$ in a countable set $I$. If $A_i\equiv B_i$ for $i\in I$ then
    1. $\bigcup_{i\in I} A_i\equiv \bigcup_{i\in I} B_i$
    2. $\bigcap_{i\in I} A_i\equiv \bigcap_{i\in I} B_i$
4. If $A\in\mathcal{N}$ then $A\equiv B$ if and only if $B\in\mathcal{N}$
5. If $A\in\mathcal{M}$ then $A\equiv B$ if and only if $B\in\mathcal{M}$
</MathBox>

## Completion
<MathBox title='Complete probability space' boxType='definition'>
A probability space $(\Omega,\mathcal{F},\mathbb{P})$ is complete if $A\in\mathcal{N}$ and $B\subseteq A$ implies $B\in\mathcal{F}$ (and hence $B\in\mathcal{N}$).
</MathBox>

<MathBox title='Extended equivalence' boxType='definition'>
Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space. For $A, B\subseteq\Omega$, define $A\equiv B$ if and only if there exists $N\in\mathcal{N}$ such that $A\Delta B\subseteq N$. The relation $\equiv$ is an equivalence relation on $\mathcal{P}(\Omega)$. 
</MathBox>

<MathBox title='Completion of a probability space' boxType='proposition'>
Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space. Consider the collection $\mathcal{F}_0 = \Set{A\subseteq\Omega | A\equiv B\textrm{ for some }B\in\mathcal{F}}$. For $A\in\mathcal{F}_0$, define $\mathbb{P}_0(A) = \mathbb{P}(B)$ where $B\in\mathcal{F}$ and $A\equiv B$. Then
- $\mathcal{F}_0$ is a $\sigma$-algebra of subsets of $\Omega$ and $\mathcal{F}\subseteq\mathcal{F}_0$
- $\mathbb{P}_0$ is a probability measure on $(\Omega, \mathcal{F}_0)$
- the probability space $(\Omega,\mathcal{F}_0,\mathbb{P}_0)$ is the completion of $(\Omega,\mathcal{F},\mathbb{P})$
</MathBox>

## Random variables

<MathBox title='Random variable' boxType='definition'>
Let $(\Omega, \mathcal{F})$ and $(S, \mathcal{S})$ be event spaces. A measurable function $X: \Omega\to S$ is called a random variable if $\Set{ \omega\in\Omega | X(\omega)\in A }\in\mathcal{F}$ for all $A\in\mathcal{S}$. For a probability measure $\mathbb{P}:\mathcal{F}\to [0, 1]$ we can define

$$
  \mathbb{P}( X\in A) := \mathbb{P}\left( X^{-1}[A]\right) = \mathbb{P}(\Set{\omega \in\Omega | X(\omega)\in A})
$$

The collection of events $\Set{\Set{X\in A}| A\in\mathcal{S}}$ is a sub $\sigma$-algebra of $\mathcal{F}$, and is the $\sigma$-algebra generated by $X$, denoted $\sigma(X)$.

Usually we set $\left(S, \mathcal{S}\right) = (\R, \mathcal{B}(\R))$.
</MathBox>

<MathBox title='Properties of random variables' boxType='proposition'>
Consider the random variable $X: \Omega \to \tilde{\Omega}$, then for events $A, B\subseteq \tilde{\Omega}$
1. $\Set{X\in A\cup B} = \Set{X\in A}\cup\Set{X\in B}$
2. $\Set{X\in A\cap B} = \Set{X\in A}\cap\Set{X\in B}$
3. $\Set{X\in A\setminus B} = \Set{X\in A}\setminus\Set{X\in B}$
4. $A\subseteq B\implies \Set{X\in A}\subseteq\Set{X\in B}$
5. $A\cap B = \emptyset \implies \Set{X\in A}\cap\Set{X\in B}=\emptyset$ (disjoint)
</MathBox>

### Convergence

In the following discussion, Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space induced from a metric space $(\Omega, d)$. In this setting, $\mathcal{F}$ is the Borel $\sigma$-algebra generated by the topology of $\Omega$.

<MathBox title='Convergence of random variables (almost surely)' boxType='definition'>
Suppose $(X_n)_{n\in\N}$ is a sequence of random variables with domain in $\Omega$. We say that $X_n \xrightarrow{n\to\infty} X$ with *probability* $1$ if the event that corresponding to this convergence has probability $1$ (almost surely), i.e.

$$
  \mathbb{P}\Set{\omega\in\Omega| X_n(\omega)\xrightarrow{n\to\infty} X(\omega)} = 1
$$

Convergence with probability $1$ (almost surely) is called strong convergence. A weaker convergence criteria is that $X_n\xrightarrow[\textrm{i.p.}]{n\to\infty} X$ *in probability* if

$$
  \forall\epsilon>0: \mathbb{P}\left[d(X_n,X)>\epsilon\right]\xrightarrow{n\to\infty} 0
$$
<details>
<summary>Details</summary>

We need to verify that the convergence $X_n\xrightarrow{n\to\infty} X$ defines a valid event. The easiest way is to check if the complement is a valid event. Note that $X_n$ does not converge to $X$ as $n\to\infty$ if and only if the metric $\lim_{n\to\infty} d(X_n, X)>\epsilon$ for some $\epsilon>0$. Moreover, to construct a countable set we may choose rational $\epsilon\in\mathbb{Q}_+$ giving

$$
  \Set{X_n\xrightarrow{n\to\infty} X}^c = \bigcup_{\epsilon\in\mathbb{Q}_+}\limsup_{n\to\infty}\Set{d(X_n, X)>\epsilon }
$$

Since $X_n$ and $X$ are random variables, the set $\Set{d(X_n, X)>\epsilon}$ is an event for each $\epsilon\in\mathbb{Q}_+$ and $n\in\N$. Next, the limit superior of a sequence of events is an event. Finally, a countable union of events is an event.

Equivalently, the convergence statement can be rewritten as $\mathbb{P}(X_n\xrightarrow{n\to\infty} X) = 1$ if and only if

$$
  \mathbb\left(\bigcup_{\epsilon\in\mathbb{Q}_+} \Set{d(X_n, X)>\epsilon\;\forall n\in\N} \right) = 0
$$
</details>
</MathBox>

<MathBox title='Criterion for almost surely convergence' boxType='proposition'>
Suppose $(X_n)_{n\in\N}$ is a sequence of random variables with domain in $\Omega$. If for every $\epsilon > 0$

$$
  \sum_{n\in\N}\mathbb{P}\left[d(X_n, X)>\epsilon \right]
$$

then $X_n\xrightarrow{n\to\infty}X$ almost surely.
<details>
<summary>Details</summary>

By the first Borel-Cantelli lemma, if

$$
  \sum_{n\in\N}\mathbb{P}\left[d(X_n, X)>\epsilon \right]
$$

then

$$
  \mathbb{P}\left[d(X_n, X)>\epsilon\;\forall n\in\N\right] = 0
$$

this is equivalent with that

$$
  \mathbb\left(\bigcup_{\epsilon\in\mathbb{Q}_+} \Set{d(X_n, X)>\epsilon\;\forall n\in\N} \right) = 0
$$

By Boole's inequality, a countable union of events has probability $0$ if and only if every event in the union has probability $0$.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Suppose $(X_n)_{n\in\N}$ is a sequence of random variables with domain in $\Omega$. If $X_n\xrightarrow{n\to\infty}X$ in probability, then there exists a subsequence $(n_k)_{k\in\N}$ of $\N$ such that $X_{n_k}\xrightarrow{k\to\infty}X$ almost surely.

<details>
<summary>Details</summary>

Suppose $X_n\xrightarrow{n\to\infty}X$ in probability. Then for each $k\in\N$ there exists $n_k\in\N$ such that 

$$
  \mathbb{P}\left[ d(X_{n_k}, X)>\frac{1}{k} \right]<\frac{1}{k^2}
$$

We can make the choices so that $n_k < n_{k+1}$ for each $k$. It follows that $\sum_{k=1}^\infty\mathbb{P}\left[d(X_n, X)>0 \right]<\infty$ for every $\epsilon>0$ and by the criterion for almost surely convergence we conclude that $X_{n_k}\xrightarrow{n\to\infty}X$ with probability $1$.
</details>
</MathBox>

### Equivalence

<MathBox title='Equivalence of random variables' boxType='definition'>
Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space and $(S,\mathcal{S})$ be a sample space. Two random variables $X, Y:S\to\tilde T$ are equivalent if $\mathbb{P}(X=Y)=1$, denoted $X\equiv Y$. The relation $\equiv$ is a an equivalence relation on $\mathcal{F}$ satisfying for $X,Y,Z:S\to T$
- reflexivity: $X\equiv X$
- symmetry: $X\equiv Y\implies Y\equiv X$
- transitivity: $X\equiv Y\land Y\equiv Z\implies X\equiv Z$
</MathBox>

<MathBox title='Properties of equivalent random variables' boxType='proposition'>
Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space and $(S,\mathcal{S})$ be a sample space. Suppose $X, Y:\Omega\to S$ are equivalent, i.e. $X\equiv Y$. Then
- $\Set{X\in A}\equiv\Set{Y\in A}$ for every $A\in \mathcal{S}$
- $X$ and $Y$ have the same probability distribution on $(S, \mathcal{S}$

Consider a third sample space $(U,\mathcal{U})$ and suppose $g:T\to U$ is measurable. If $X\equiv Y$ then $g(X)\equiv g(Y)$.
</MathBox>

# Stochastic processes

In the following discussion, let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability, $(S,\mathcal{S})$ a state space and $(T,\mathcal{T})$ an index space.

<MathBox title='Stochastic process' boxType='definition'>
A stochastic (random) process on $(\Omega,\mathcal{F},\mathbb{P})$ with state space $(S,\mathcal{S})$ and index space $(T,\mathcal{T})$ is a collection of random variables $\mathbf{X}:=\Set{X_t: \Omega\to S}_{t\in T}$, forming a function $\mathbf{X}:\Omega\times T\to S$. A stochastic process $\mathbf{X}$ is measurable if the function $\mathbf{X}:\Omega\times T$ given by $(\omega, t)\mapsto X_t(\omega)$ is measurable with respect to $\mathcal{F}\otimes\mathcal{T}$ and $\mathcal{S}$. If $T$ is countable, then $\mathcal{T} = \mathcal{P}(T)$, and it follows that $\mathbf{X}$ is measurable. Often $X_t$ is written $X(t)$ for convenience.

For the state space, $S$ is usually a topological space and $\mathcal{S}$ the Borel $\sigma$-algebra generated by the topology of $S$. A standard set of assumptions is that the topology is locally compact, Hausdorff, and has a countable base. The index set $T$ usually represents time. For a discrete-time process, $T\subseteq\N$ and $\mathcal{T} = \mathcal{P}(T)$. For a continuous-time process, $T\subseteq[0,\infty)$ and $\mathcal{T}\subseteq\mathcal{B}(\R)$, the Borel $\sigma$-algebra generated by the standard Euclidean topology.

The filtration $\mathscr{F}^0:=\Set{X_s | s\in T,s\leq t} }_{t\in T}$ is the natural filtration associated with $\mathbf{X}$.
</MathBox>

<MathBox title='Adapted process' boxType='definition'>
A stochastic process $\mathbf{X} = \Set{\mathcal{F}_t}_{t\in T}$ on $(\Omega,\mathcal{F})$ if $X_t$ is measurable with respect to $\mathcal{F}_t$ for each $t\in T$. Equivalently, $\mathbf{X}$ is adapted to $\mathscr{F}$ if $\mathscr{F}$ is finer than $\mathscr{F}^0 $, the natural filtration associated with $\mathbf{X}$. That is, $\sigma\Set{X_s| s\in T, s\leq t} \subseteq \mathcal{F}_t$ for each $t\in T$. The natural filtration $\mathscr{F}^0$ is the therefore the coarsest filtration to which $\mathbf{X}$ is adapted.
</MathBox>

<MathBox title='Predictable process' boxType='definition'>
Let $T=\N$. A discrete stochastic process $\mathbf{X} = \Set{X_n}_{n\in\N}$ on $(\Omega,\mathcal{F})$ is predictable by the filtration $\mathscr{F} = \Set{\mathcal{F}_n}_{n\in\N}$ if $X_{n+1}$ is measurable with respect to $\mathcal{F}_n$ for all $n\in\N$. If $\mathbf{X}$ is predictable by $\mathscr{F}$ then $\mathbf{X}$ is also adapted to $\mathscr{F}$.
</MathBox>

<MathBox title='Progressively measurable process' boxType='definition'>
Let $T_t = \Set{s\in T| s\leq t}$ for $t\in T$ and let $\mathcal{T}_t = \Set{s\in T| A\in\mathcal{T}}$ be the corresponding induced $\sigma$-algebra. Suppose $\mathbf{X} = \Set{X_t}_{t\in T}$ is a stochastic process on $(\Omega,\mathcal{F})$, and that $\mathscr{F} = \Set{\mathcal{F}_t}_{t\in T}$ is a filtration. Then $\mathbf{X}$ is progressively measurable relative to $\mathscr{F}$ if $\mathbf{X}:\Omega\times T_t \to S$ is measurable with respect to $\mathcal{F}_t\otimes\mathcal{T}_t$ and $\mathcal{S}$ for each $t\in T$.
</MathBox>

<MathBox title='Properties of pregressively measurable processes' boxType='proposition'>
Let $\mathbf{X} = \Set{X_t}_{t\in T}$ be a stochastic process with sample space $(\Omega,\mathcal{F})$ and state space $(S,\mathcal{S})$, and let $\mathscr{F}=\Set{\mathcal{F}_t}_{t\in T}$ be a filtration. If $\mathbf{X}$ is progressively measurable relative to $\mathscr{F}$ then
1. $\mathbf{X}$ is measurable
2. $\mathbf{X}$ is adapted to $\mathscr{F}$
<details>
<summary>Details</summary>

1. If $A\in\mathcal{S}$ then

$$
  \mathbf{X}^{-1}(A) = \Set{(\omega, t)\in\Omega\times T | X_t(\omega)\in A } = \bigcup_{n\in\N}\Set{(\omega, t)\in\Omega\times T | X_t(\omega)\in A }
$$

By assumption, the $n$-th term in the union is in $\mathcal{F}\otimes\mathcal{T}_n\subseteq\mathcal{F}\otimes\mathcal{T}$, so the union is in $\mathcal{F}\otimes\mathcal{T}$.
2. Suppose $t\in T$. Then $\mathbf{X}:\Omega\times T_t\to S$ is measurable with respect to $\mathcal{F}_t\otimes\mathcal{T}_t$ and $\mathcal{S}$. However, $X_t:\Omega\to S$ is just the cross section of this function at $t$ and thus is measurable with respect to $\mathcal{F}_t$ and $\mathcal{S}$.
</details>
</MathBox>

<MathBox title='Right continuous processes are progressively measurable to the natural filtration' boxType='proposition'>
Suppose the sample space $(S,\mathcal{S})$ is a Hausdorff topological space that is locally compact and has a countable base. Consider a right continuous stochastic process $\mathbf{X} = \Set{X_t | t\in[0,\infty)}$. Then $\mathbf{X}$ is progressively measurable to the natural filtration $\mathscr{F}^0$.
</MathBox>

## Equivalence

<MathBox title='Equivalence in distribution' boxType='definition'>
Two stochastic processes $\mathbf{X} = \Set{X_t}_{t\in T}$ and $\mathbf{Y} = \Set{Y_t}_{t\in T}$ with state space $(S,\mathcal{S})$ and index set $T$ are equivalent *in distribution* if they have the same finite dimensional distributions. This defines an equivalence relation on the collection of stochastic processes with this state space and index set. That is, if $\mathbf{X}$, $\mathbf{Y}$ and $\mathbf{Z}$ are such processes then
- reflexivity: $\mathbf{X}$ is equivalent in distribution with $\mathbf{X}$
- symmetry: if $\mathbf{X}$ is equivalent in distribution with $\mathbf{Y}$, then $\mathbf{Y}$ is equivalent in distribution with $\mathbf{X}$
- transitivity: if $\mathbf{X}$ is equivalent in distribution with $\mathbf{Y}$, and $\mathbf{Y}$ is equivalent in distribution with $\mathbf{Z}$, then $\mathbf{X}$ is equivalent in distribution with $\mathbf{Z}$

Stochastic processes equivalent in distribution share the same sample space and index set, and need not be defined on the same probability space.
</MathBox>

<MathBox title='Version of equivalence' boxType='definition'>
Suppose $\mathbf{X} = \Set{X_t}_{t\in T}$ and $\mathbf{Y} = \Set{Y_t}_{t\in T}$ are stochastic processes defined on the same probability space $(\Omega,\mathcal{F},\mathbb{P})$ and both with state space $(S,\mathcal{S})$ and index set $T$. Then $\mathbf{Y}$ is a *version of* $\mathbf{X}$ if $Y_t$ is equivalent to $X_t$, i.e. $\mathbb{P}(X_t = Y_t = 1)$ for every $t\in T$. This defines an equivalence relation on the collection of stochastic processes on the same probability space and with this state space and index set. That is, if $\mathbf{X}$, $\mathbf{Y}$ and $\mathbf{Z}$ are such processes then
- reflexivity: $\mathbf{X}$ is a version of $\mathbf{X}$
- symmetry: if $\mathbf{X}$ is a version of $\mathbf{Y}$, then $\mathbf{Y}$ is a version of $\mathbf{X}$
- transitivity: if $\mathbf{X}$ is a version of $\mathbf{Y}$, and $\mathbf{Y}$ is a version of $\mathbf{Z}$, then $\mathbf{X}$ is a version of $\mathbf{Z}$
</MathBox>

<MathBox title='Indistinguishable from equivalence' boxType='definition'>
Suppose $\mathbf{X} = \Set{X_t}_{t\in T}$ and $\mathbf{Y} = \Set{Y_t}_{t\in T}$ are stochastic processes defined on the same probability space $(\Omega,\mathcal{F},\mathbb{P})$ and both with state space $(S,\mathcal{S})$ and index space $(T, \mathcal{T})$. Then $\mathbf{X}$ is *Indistinguishable from* $\mathbf{X}$ if $\mathbb{P}(X_t = Y_t = 1\;\forall t\in T)$. This defines an equivalence relation on the collection of stochastic processes on the same probability space and with this state space and index set. That is, if $\mathbf{X}$, $\mathbf{Y}$ and $\mathbf{Z}$ are such processes then
- reflexivity: $\mathbf{X}$ is indistinguishable from $\mathbf{X}$
- symmetry: if $\mathbf{X}$ is indistinguishable from $\mathbf{Y}$, then $\mathbf{Y}$ is indistinguishable from $\mathbf{X}$
- transitivity: if $\mathbf{X}$ is indistinguishable from $\mathbf{Y}$, and $\mathbf{Y}$ is indistinguishable from $\mathbf{Z}$, then $\mathbf{X}$ is indistinguishable from $\mathbf{Z}$

Trivially, if $\mathbf{X}$ is indistinguishable from $\mathbf{Y}$, then $\mathbf{X}$ is a version of $\mathbf{Y}$. The converse is also true if $T$ is countable.
</MathBox>

## Kolmogorov existence theorem
 
<MathBox title='Consistency conditions for probability measures' boxType='definition'>
Suppose $(S,\mathcal{S})$ is a measurable space and $T$ is an index set. For $n\in\N$, let $T^{(n)}\subseteq T^n$ denote the set of $n$-tuples of distinct elements of an index set $T$, and let $\mathbf{T}=\bigcup_{n\in\N}T^{(n)}$ denote the set of all finite sequences of distinct elements of $T$. If $n\in\N$, $\mathbf{t} = (t_i)_{i=1}^n \in T^{(n)}$ and $\pi$ is a permutation of $\Set{i}_{i=1}^n$, let $\mathbf{t}\pi$ denote the elements of $T^{(n)}$ with coordinates $(\mathbf{t}\pi)_i = t_{\pi(i)}$. That is, we permute the coordinates of $\mathbf{t}$ according to $\pi$. If $C\in\mathcal{S}^n$ let

$$
  \pi C = \Set{(x_i)_{i=1}^n \in S^n | \left(x_{\pi(i)} \right)_{i=1}^n \in C }\in\mathcal{S}^n
$$

If $n>1$, let $\mathbf{t}_-$ denote the vector $(t_i)_{i=1}^{n-1}\in T^{(n-1)}$. 

Suppose $P_\mathbf{t}$ is a probability measure on $(S^n, \mathcal{S}^n)$ for each $n\in\N$ and $\mathbf{t}\in T^{(n)}$. The collection $\mathcal{P} = \Set{P_\mathbf{t}}_{\mathbf{t}\in\mathbf{T}}$ is consistent if
1. $P_{\mathbf{t}\pi}(C) = P_{\mathbf{t}}(\pi C)$ for every $n\in\N$, $\mathbf{t}\in T^{(n)}$, permutation $\pi$ of $\Set{i}_{i=1}^n$ and measurable $C\in\mathcal{S}^n$ 
2. $P_{\mathbf{t}_-}(C) = P_\mathbf{t}(C\times S)$ for every $n>1$, $\mathbf{t}\in T^{(n)}$ and measurable $C\in\mathcal{S}^{n-1}$
</MathBox>

<MathBox title='Kolmogorov existence theorem' boxType='theorem'>
If $\mathcal{P}$ is a consistent collection of probability distributions relative to an index set $T$ and a state space $(S,\mathcal{S})$, then there exists a probability space $(\Omega,\mathcal{F},\mathbb{P})$ and a stochastic process $\mathbf{X} = \Set{X_t}_{t\in T}$ on this probability space such that $\mathcal{P}$ is the collection of finite dimensional distribution of $\mathbf{X}$.
<details>
<summary>Proof sketch</summary>

Let $\Omega = S^T$ be the set of functions from $T$ to $S$, i.e. the outcomes of $\mathbf{X}$. Let $\mathcal{F} = \mathcal{S}^T$, the product $\sigma$-algebra, generated by sets of the form

$$
  B = \Set{\omega\in\Omega\mid \omega(t)\in A_t \; \forall t\in T}
$$

where $A_t\in\mathcal{S}$ for all $t\in T$ and $A_t = S$ for all but finitely many $t\in T$. Suppose $A_t = S$ except for $\mathbf{t} = (t_i)_{i=1}^n\in T^{(n)}$. Then we require

$$
  \mathbb{P}(B) = P_\mathbf{t}\left(\prod_{i=1}^n A_{t_i} \right)
$$

Applying Caratheodory's existence theorem and the consistency $\mathcal{P}$ guarantees that $\mathbb{P}$ can be extended to probability measure on all of $\mathcal{F}$. Finally, for $t\in T$ define $X_t:\Omega\to S$ by $X_t(\omega) = \omega(t)$ for $\omega\in\Omega$, so that $X_t$ is simply the coordinate function of index $t$. Hence, we have a stochastic process $\mathbf{X} = \Set{X_t}_{t\in T}$ with state space $(S,\mathcal{S})$ defined on the probability space $(\Omega,\mathcal{F},\mathbb{P})$ with $\mathcal{P}$ as the collection of finite dimensional distributions.
</details>
</MathBox>

## Stopping processes

Note that a random time state $X_\tau$ is a random state at random time, and therefore depends on an outcome $\omega\in\Omega$ in two ways, i.e. $X_{\tau(\omega)}(\omega)$.

<MathBox title='Random time states are measurable' boxType='proposition'>
Suppose $\mathbf{X} = \Set{X_t}_{t\in T}$ is a measurable stochastic process on the sample space $(\Omega,\mathcal{F})$ with state space $(S,\mathcal{S})$. If $\tau$ is a finite random time, then $X_\tau$ is measurable. 
<details>
<summary>Proof</summary>

Note that $X_\tau:\Omega\to S$ is the composition of the function $\omega\mapsto(\omega,\tau(\omega))$ from $\Omega$ to $\Omega\times T$ with the function $(\omega, t)\mapsto X_t(\omega)$ from $\Omega\times S$ to $S$. The firs function is measurable because the two coordinate functions are measurable. The second function is measurable by assumption.
</details>
</MathBox>

<MathBox title='Stopped process' boxType='proposition'>
Suppose $\mathbf{X} = \Set{X_t}_{t\in T}$ is a measurable stochastic process on the sample space $(\Omega,\mathcal{F})$ with state space $(S,\mathcal{S})$. If $\tau$ is a random time, then the process $\mathbf{X}^\tau = \Set{X_t^\tau}_{t\in T}$ defined by $X_t^\tau = X_{t\wedge\tau}$ for $t\in T$ is the process $\mathbf{X}$ stopped at $\tau$. 
<details>
<summary>Proof</summary>

For each $t\in T$, note that $t\wedge \tau$ is a finite random time, and hence $X_{t\wedge\tau}$ is measurable by the previous result. Thus $\mathbf{X}^\tau$ is a well-defined stochastic process on $(\Omega,\mathcal{F})$ with state space $(S,\mathcal{S})$.
</details>
</MathBox>

<MathBox title='Stopped processes inherit progressive measurability' boxType='proposition'>
Suppose $\mathbf{X} = \Set{X_t}_{t\in T}$ is a stochastic process on the sample space $(\Omega,\mathcal{F})$ with state space $(S,\mathcal{S})$, and that $\mathbf{X}$ is progressively measurable with respect to a filtration $\mathscr{F} = \Set{\mathcal{F}_t}_{t\in T}$ on $(\Omega,\mathcal{F})$. If $\tau$ is a finite stopping time relative to $\mathscr{F}$, then the stopped process $\mathbf{X}^\tau =\Set{X_t^\tau}_{t\in T}$ is progressively measurable with respect to the stopped filtration $\mathscr{F}^\tau$. Consequently, $X_\tau$ is measurable with respect to $\mathcal{F}_\tau$.
</MathBox>

<MathBox title='Entry and hitting time' boxType='definition'>
Suppose $\mathbf{X} = \Set{t\in T| t>0}$ For $A\in\mathcal{S}$, define
1. $\rho_A = \inf\Set{t\in T| X_t\in A}$, the first entry time to $A$
2. $\tau_A = \inf\Set{t\in T_+ | X_t\in A}$, the first hitting time to $A$
</MathBox>

<MathBox title='Stopping times for discrete processes' boxType='proposition'>
Suppose $\mathbf{X} = \Set{X_n}_{n\in\N}$ is a discrete stochastic process on the sample space $(\Omega,\mathcal{F})$ with state space $(S,\mathcal{S})$. If $A\in\mathcal{S}$, then $\tau_A$ and $\rho_A$ are stopping times relative to the natural filtration $\mathscr{F}^0$.
<details>
<summary>Proof</summary>

Let $n\in\N$. Note that $\Set{\rho_A > n} = \Set{X_i \notin A}_{i=0}^n\in\sigma\Set{X_i}_{i=1}^n$. Similarly, $\Set{\tau_A>n} = \Set{X_i \notin A}_{i=0}^n\subseteq\sigma\Set{X_i}_{i=1}^n$
</details>
</MathBox>

<MathBox title='Stopping times for continuous processes' boxType='proposition'>
Suppose $\mathbf{X} = \Set{X_t | t\in[0,\infty)}$ is a right-continuous stochastic process on the sample space $(\Omega,\mathcal{F})$ with state space $(S,\mathcal{S})$. Suppose $S$ is a Hausdorff topological space that is locally compact and has countable bases, and that $\mathcal{S}$ is the $\sigma$-algebra of Borel sets. Then $\tau_A$ and $\rho_A$ are stopping times relative to the natural filtration $\mathscr{F}_+^0$ for every open $A\in\mathcal{S}$.

Similarly, if $\mathbf{X}$ is progressively measurable relative to a complete, right-continuous filtration $\mathscr{F} = \Set{\mathcal{F}_t| t\in[0,\infty)}$. If $A\in\mathcal{S}$ then $\rho_A$ and $\tau_A$ are stopping times relative to $\mathscr{F}$.
</MathBox>

# Probability distribution

<MathBox title='Probability distribution' boxType='definition'>
Let $(\Omega, \mathcal{F}, \mathbb{P})$ and $\left(\R, \mathcal{B}(\R), \mathbb{P}_X \right)$ be probability spaces and $X: \Omega \to \R$ a random variable. Then the probability measure $\mathbb{P}_X: \mathcal{B}(\R) \to [0, 1]$ defined by

$$
  \mathbb{P}_X (B) := \mathbb{P}\left( X^{-1}(B) \right) = \mathbb{P}(X \in B)
$$

is called a probability distribution of $X$.
<details>
<summary>Proof</summary>

To prove that $\mathbb{P}_X$ is a probability measure we first verify

$$
\begin{gather*}
  X^{-1}(\R) = \Omega \implies \mathbb{P}_X (\R) = \mathbb{P}\left( X^{-1}(\R) \right) = \mathbb{P}(\Omega) = 1 \\
  X^{-1}(\emptyset) = \emptyset \implies \mathbb{P}_X (\emptyset) = \mathbb{P}\left( X^{-1}(\emptyset) \right) = \mathbb{P}(\emptyset) = 0
\end{gather*}
$$

To verify $\sigma$-additivity choose $(B_n)_{n\in\N} \in\mathcal{B}(\R)$ pairwise disjoint, then

$$
  i \neq j \implies X^{-1}\left( B_i \right) \cap X^{-1}\left( B_j \right) = X^{-1}\left( B_i \cap B_j \right) = X^{-1}(\emptyset) = \emptyset
$$

so $X^{-1}\left( B_i \right) \in \mathcal{F}$ is pairwise joint, and moreover

$$
\begin{align*}
  \mathbb{P}_X \left( \bigcup_{j=1}^\infty B_j \right) &= \mathbb{P}\left( X^{-1}\left( \bigcup_{j=1}^\infty B_j \right) \right) = \mathbb{P}\left( \bigcup_{j=1}^\infty X^{-1}\left(  B_j \right) \right) \\
  &= \sum_{j=1}^\infty \mathbb{P}\left( X^{-1}\left(B_j \right) \right) = \sum_{j=1}^\infty \mathbb{P}_X \left( B_j \right)
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Rules of probability measures' boxType='proposition'>
Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space. The probability measure $\mathbb{P}$ satisfy the following rules for the events $A, B\in\mathcal{F}$:
1. Complement rule: $\mathbb{P}(A^c) = 1 - \mathbb{P}(A)$
  - $\mathbb{P}(\emptyset) = 1 - \mathbb{P}(\Omega) = 0$
2. Difference rule: $\mathbb{P}(B\setminus A) = \mathbb{P}(B) - \mathbb{P}(A\cap B)$
  - If $A\subseteq B$, i.e. $A\cap B = A$, then 
      i. $\mathbb{P}(B\setminus A) = \mathbb{P}(B) - \mathbb{P}(A)$ and $\mathbb{P}(A)\leq\mathbb{P}(B)$
      ii. If $\mathbb{P}(B) = 0$ then $\mathbb{P}(A) = 0$. 
      iii. If $\mathbb{P}(A) = 1$ then $\mathbb{P}(B) = 1$
3. Boole's inequality: if $\Set{A_i}_{i\in I}$ is countable collection of events then $\mathbb{P}\left(\bigcup_{i\in I} A_i\right)\leq\sum_{i\in I}\mathbb{P}(A_i)$
  - If $\mathbb{P}(A_i) = 0$ for each $i\in I$ then $\mathbb{P}\left(\bigcup_{i\in I} A_i\right) = 0$
4. Bonferroni's inequality: if $\Set{A_i}_{i\in I}$ is countable collection of events then $\mathbb{P}\left(\bigcap_{i\in I} A_i\right)\geq 1 - \sum_{i\in I}\left[1 - \mathbb{P}(A_i)\right]$
  - If  $\mathbb{P}(A_i) = 1$ for each $i\in I$ then $\mathbb{P}\left(\bigcap_{i\in I} A_i\right) = 1$
5. Partition rule: if $\Set{A_i}_{i\in I}$ is countable collection of events that partition $\Omega$, then $\mathbb{P}(B) = \sum_{i\in I}\mathbb{P}(A_i\cap B)$
6. Inclusion-exclusion rule: $\mathbb{P}(A\cup B) = \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A\cap B) \iff \mathbb{P}(A\cap B) = \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A\cup B)$
  - If $\Set{A_i }_{i\in I}\subseteq\mathcal{F}$ for an index set $I$ with $|I| = n\in\N$ then

$$
\begin{align*}
  \mathbb{P}\left(\bigcup_{i\in I} A_i \right) &= \sum_{k=1}^n (-1)^{k-1} \sum_{J\subseteq I,\; |J|=k} \mathbb{P}\left( \bigcap_{j\in J} A_j \right) \\
  \mathbb{P}\left(\bigcap_{i\in I} A_i \right) &= \sum_{k=1}^n (-1)^{k-1} \sum_{J\subseteq I,\; |J|=k} \mathbb{P}\left( \bigcup_{j\in J} A_j \right)
\end{align*}
$$
</MathBox>

## Construction of probability measures
<MathBox title='Rescaling of probability measures' boxType='proposition'>
Let $(\Omega, \mathcal{F})$ be a sample space. If $\mu: \Omega\to [0, \infty)$ is a positive measure on $\Omega$ with $0<\mu(\Omega)<\infty$, then a probability measure on $(\Omega, \mathcal{F})$ can be defined as

$$
  \mathbb{P}(A) := \frac{\mu(A)}{\mu(\Omega)},\quad A\in\mathcal{F}
$$

<details>
<summary>Proof</summary>

To prove that $\mathbb{P}$ is a probability measure, we varify the Kolmogorov axioms for $A\subseteq\Omega$
1. $\mathbb{P}(A) \geq 0$ since $\mu(A)\geq 0$ and $0<\mu(\Omega)<\infty$.
2. $\mathbb{P}(\Omega) = \frac{\mu(\Omega)}{\mu(\Omega)} = 1$
3. If $\Set{A_i}_{i\in I}\subseteq\mathcal{F}$ is a countable collection of disjoint events then

$$
  \mathbb{P}\left(\bigcup_{i\in I} A_i \right) = \frac{1}{\mu(\Omega)}\mu\left( \bigcup_{i\in I} A_i \right) = \frac{1}{\mu(\Omega)}\sum_{i\in I}\mu(A_i) = \sum_{i\in I}\frac{\mu(A_i)}{\mu(\Omega)} = \sum_{i\in I}\mathbb{P}(A_i)
$$
</details>
</MathBox>

<MathBox title='Construction of discrete probability measures' boxType='proposition'>
Let $(\Omega, \mathcal{F})$ be a sample space. Consider a function $g: \Omega\to [0, \infty)$, then $\mu$ defined by $\mu(A) = \sum_{x\in A} g(x)$ for $A\in\Omega$ is a positive measure on $\Omega$. If $0<\mu(\Omega)<\infty$ then a probability measure on $(\Omega, \mathcal{F})$ can be defined as

$$
  \mathbb{P}(A) := \frac{\mu(A)}{\mu(\Omega)} = \frac{\sum_{x\in A} g(x)}{\sum_{x\in \Omega} g(x)},\quad A\subseteq\Omega
$$
</MathBox>
<details>
<summary>Proof</summary>

To prove that $\mathbb{P}$ is a probability measure, we varify the Kolmogorov axioms for $A\subseteq\Omega$
1. $\mathbb{P}(A) \geq 0$ since $g$ is nonnegative, $\mu(A)\geq 0$ and $0<\mu(\Omega)<\infty$.
2. $\mathbb{P}(\Omega) = \frac{\sum_{x\in \Omega} g(x)}{\sum_{x\in \Omega} g(x)} = 1$
3. If $\Set{A_i}_{i\in I}\subseteq\mathcal{F}$ is a countable collection of disjoint events then

$$
  \mathbb{P}\left(\bigcup_{i\in I} A_i \right) = \frac{1}{\mu(\Omega)}\mu\left( \bigcup_{i\in I} A_i \right) = \frac{1}{\mu(\Omega)}\sum_{i\in I}\sum_{x\in A_i} g(x) = \sum_{i\in I}\frac{\mu(A_i)}{\mu(\Omega)} = \sum_{i\in I}\mathbb{P}(A_i)
$$
</details>

<MathBox title='Continuous probability measure' boxType='definition'>
A probability measure on a sample space $(\Omega,\mathcal{F})$ is continuous if $\mathbb{P}(\Set{\omega}) = 0$ for all $\omega\in\Omega$.
</MathBox>

<MathBox title='Construction of continuous probability measure' boxType='proposition'>
Let $(\Omega, \mathcal{F})$ be a sample space with $\Omega\subseteq\R^n$. Consider a function $g: \Omega\to [0, \infty)$, then $\mu$ defined by $\mu(A) = \int_A g(x)\d x$ for $A\in\Omega$ is a positive measure on $\Omega$. If $0<\mu(\Omega)<\infty$ then a probability measure on $(\Omega, \mathcal{F})$ can be defined as

$$
  \mathbb{P}(A) := \frac{\mu(A)}{\mu(\Omega)} = \frac{\int_{A} g(x)\d x}{\sum_{\Omega}g(x)\d x},\quad A\subseteq\Omega
$$
<details>
<summary>Proof</summary>

To prove that $\mathbb{P}$ is a probability measure, we varify the Kolmogorov axioms for $A\subseteq\Omega$
1. $\mathbb{P}(A) \geq 0$ since $g$ is nonnegative, $\mu(A)\geq 0$ and $0<\mu(\Omega)<\infty$.
2. $\mathbb{P}(\Omega) = \frac{\int_{\Omega} g(x)\d x}{\int_{\Omega} g(x)\d x} = 1$
3. If $\Set{A_i}_{i\in I}\subseteq\mathcal{F}$ is a countable collection of disjoint events then

$$
  \mathbb{P}\left(\bigcup_{i\in I} A_i \right) = \frac{1}{\mu(\Omega)}\mu\left( \bigcup_{i\in I} A_i \right) = \frac{1}{\mu(\Omega)}\sum_{i\in I}\int_{A_i} g(x)\d x = \sum_{i\in I}\frac{\mu(A_i)}{\mu(\Omega)} = \sum_{i\in I}\mathbb{P}(A_i)
$$
</details>
</MathBox>

## Cumulative distribution function (CDF)

<MathBox title='Cumulative distribution function' boxType='proposition'>
Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space, and $X: \Omega\to\R$ a random variable. The cumulative distribution function for $X$ is the function $F_X:\R\to [0, 1]$ defined as

$$
  F_X(x) := \mathbb{P}_X \left((-\infty, x]\right) = P(X \leq x)
$$

Let $f_X$ denote the probability density function of $X$. If $X$ has a discrete distribution on a countable subset $S\subseteq\R$, then
1. $F_X(x) = \sum_{t\in S, t\leq x} f_X(x)$ for $x\in\R$
2. $f_X(x) = F_X(x) - F_X(x^-)$ for $x\in S$

If $X$ has a continuous distribution on $\R$, then
1. $F_X(x) = \int_{-\infty}^x f_X(t),\d t$
2. $f_X(x) = F_X'(x)$ if $f_X$ is continuous at $x$
</MathBox>

<MathBox title='Properties of cumulative distribution functions' boxType='proposition'>
Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space, and $X: \Omega \to \R$ a random variable. The cumulative distribution function $F_X$ has the following properties:
- Monotonically increasing: if $x \leq y$ then $F_X(x) \leq F_X(y)$
- Right continuity: $\lim_{t \downarrow x} F_X (t) = F_X{x}$.
  - $\lim_{t \uparrow x} F_X (t) = \mathbb{P}_X \left((-\infty, x)\right) = P(X < x)$
- Limits
$$
\begin{gather*}
  \lim_{x\to -\infty} F_X(x) = 0 \\
  \lim_{x\to \infty} F_X(x) = 1
\end{gather*}
$$
</MathBox>

The right and left limits of a cumulative distribution function $F$ at $x\in\R$ are usually abbreviated as

$$
\begin{align*}
  F(x^+) &:= \lim_{t\downarrow x} F(t) \\
  F(x^-) &:= \lim_{t\uparrow x} F(t)
\end{align*}
$$

<MathBox title='' boxType='proposition'>
Suppose $F_X$ is the cumulative distribution function of a real-valued random variable $X$. If $a,b\in\R$ with $a < b$ then
1. $\mathbb{P}(X = a) = F_X (a) - F_X(a^-)$
2. $\mathbb{P}(a < X \leq b) = F_X (b) - F_X(a)$
3. $\mathbb{P}(a < X < b) = F_X (b^-) - F_X(a)$
4. $\mathbb{P}(a \leq X \leq b) = F_X (b) - F_X(a^-)$
5. $\mathbb{P}(a \leq X < b) = F_X (b^-) - F_X(a^-)$

<details>
<summary>Proof</summary>

1. $\Set{X = a} = \Set{ X\leq a }\setminus \Set{ X < a }$, so $\mathbb{P}(X = a) = \mathbb{P}(X\leq a) - \mathbb{P}(X < a) = F_X(a) - F_X(a^-)$
2. $\Set{a < X \leq b} = \Set{ X\leq b }\setminus \Set{ X \leq a }$, so $\mathbb{P}(a < X \leq b) = \mathbb{P}(X\leq b) - \mathbb{P}(X \leq a) = F_X(b) - F_X(a)$
3. $\Set{a < X < b} = \Set{ X < b }\setminus \Set{ X \leq a }$, so $\mathbb{P}(a < X < b) = \mathbb{P}(X < b) - \mathbb{P}(X \leq a) = F_X(b^-) - F_X(a)$
4. $\Set{a \leq X \leq b} = \Set{ X\leq b }\setminus \Set{ X < a }$, so $\mathbb{P}(a \leq X \leq b) = \mathbb{P}(X\leq b) - \mathbb{P}(X < a) = F_X(b) - F_X(a^-)$
5. $\Set{a \leq X < b} = \Set{ X < b }\setminus \Set{ X < a }$, so $\mathbb{P}(a \leq X < b) = \mathbb{P}(X< b) - \mathbb{P}(X < a) = F_X(b^-) - F_X(a^-)$
</details>
</MathBox>

<MathBox title='Continuous distributions have continuous density functions' boxType='proposition'>
If $X$ has a continuous distribution the distribution function $F_X$ is continuous.

<details>
<summary>Proof</summary>

If $X$ has a continuous distribution, then by definition $\mathbb{P}(X = x) = 0$ and thus $\mathbb{P}(X < x) = \mathbb{P}(X \leq x)$. Hence the left and right limits of $F_X$ are equal, i.e. $F_X(x^-) = F_X(x^+) = F_X(x)$
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
If $X$ has a continuous distribution on $\R$ that is symmetric about a point $a$, the distribution  function $F_X$ satisfies

$$
  F(a - t) = 1 - F(a + t),\quad t\in\R
$$

<details>
<summary>Proof</summary>

Recall that the distribution of a real-valued random variable $X$ is symmetric about $a\in\R$ if the distribution of $X-a$ is the same as the distribution of $a - X$. Since $X - a$ and $a - X$ have the same distribution

$$
\begin{align*}
  F(a-t) &= \mathbb{P}(X\leq a - t) = \mathbb{P}(X - a \leq -t) \\
  &= \mathbb{P}(a - X \leq -t) = \mathbb{P}(X \geq a + t) \\
  &= 1 - F(a + t)
\end{align*}
$$
</details>
</MathBox>

### Quantile functions

<MathBox title='Quantile' boxType='definition'>
Suppose $X$ is a real-valued function with cumulative distribution function $F_X$. For $p\in(0, 1)$, a value of $x$ such that $F(x^-) = \mathbb{P}(X < x)\leq p$ and $F(x) = \mathbb{P}(X\leq x)\geq p$ is called a *quantile* of order $p$ for the distribution.

Certain quantiles have their special names
1. A quantile of order $1/4$ is a first quartile of the distribution.
2. A quantile of order $1/2$ is a median of second quartile of the distribution.
3. A quantile of order $3/4$ is a third quartile of the distribution.
</MathBox>

<MathBox title='Quantiles of symmetric distributions' boxType='proposition'>
Suppose $X$ has a continuous distribution that is symmetric about a point $a\in\R$. If $a + t$ is a quantile of order $p\in(0, 1)$, then $a - t$ is a quantile of order $1 - p$.

<details>
<summary>Proof</summary>

If $a + t$ is a quantile of order $p$, then (since $X$ has a continuous distribution) $F(a + t) = p$. Since $X$ has a symmetric distribution, $F(a - t) = 1 - F(a + t) = 1 - p$, so $a - t$ is a quantile of order $1 - p$.
</details>
</MathBox>

<MathBox title='Quantile function' boxType='definition'>
The quantile function $F_X^{-1}:[0,1]\to\R$ of a real-valued random variable $X$ with distribution function $F_X:\R\to[0,1]$ is defined by

$$
  F_X^{-1}(p) = \min\Set{ x\in\R | F_X(x) \geq p },\quad p\in(0, 1)
$$

<details>
<summary>Details</summary>

Since $F_X$ is right continuous and increasing, $\Set{ x\in\R | F_X(x) }$ is an interval of the form $[a, \infty)$. Thus, the mininum of the set is $a$.
</details>
</MathBox>

<MathBox title='Properties of quantile function' boxType='proposition'>
The quantile function $F_X^{-1}$ of a real-valued random variable $X$ has the following properties for $p\in(0,1)$

1. $F_X^{-1}$ is increasing on $(0, 1)$
2. $[F_X^{-1}[F(x)]\leq x$ for any $x\in\R$ with $F_X(x) < 1$
    1. $F_X^{-1}(p)$ is a quantile of order $p$
    2. If $x$ is a quantile of order $p$, then $F^{-1}(p)\leq x$
3. $F_X[F_X^{-1}(p)]\geq p$
4. Right-continuity: $F_X^{-1}(p^-) = F^{-1}(p)$
    1. $F_X^{-1}(p^+) = \inf\Set{ x\in\R | F_X(x) > p }$
5. $F_X^{-1}(p) \leq x$ if and only if $p\leq F(x)$.

<details>
<summary>Details</summary>

1. If $p,q\in(0,1)$ with $p\leq q$, then $\Set{ x\in\R | F_X(x) \geq q}\subseteq \Set{ x\in\R | F_X(x) \geq p }$
2. By definition $F_X^{-1}[F_X(x)]$ is the smallest $y\in\R$ with $F_X(y)\geq F_X(x)$
    1. If $x < F_X^{-1}(p)$, then $F_X[x] < p$ implying $F_X [F_X^{-1}(p)^-]$. Hence, $F_X^{-1}(p)$ is a quantile of order $p$.
    2. Suppose $x$ is a quantile of order $p$. Then $F_X(x) \geq p$ so by definition $F_X^{-1}(p) \leq p$.
3. By definition $F_X^{-1}(p)$ is a value $y\in\R$ satisfying $F_X(y)\geq p$
4. This follows from the fact that $F_X$ is right continuous.
    1. This follows from the fact that $F_X$ has limits from the left.
5. Suppose $F_X^{-1}(p) < x$. Since $F_X$ is increasing $F_X[F_X^{-1}(p)]\leq F(x)$. However, $p\leq F_X[F_X^{-1}(p)]$ so $p\leq F_X(x)$. Conversely, suppose $p\leq F(x)$. Since $F_X^{-1}$ is increasing $F_X^{-1}(p)\leq F_X^{-1}[F(x)]$. However, $F_X^{-1}[F_X(x)]\leq x$, so $F_X^{-1}(p)\leq x$.
</details>
</MathBox>

### Reliability

<MathBox title='Right-tail distribution function' boxType='proposition'>
Suppose $X$ is a real-valued random variable with cumulative distribution function $F$. The function $F_c:\R\to[0, 1]$ defined by

$$
  F^c (x) = 1 - F(x) = \mathbb{P}(X > x)
$$

is the *right-tail distribution function* of $X$. This function has the following properties

1. $F^c$ is decreasing
2. Right continuous $\lim_{t\downarrow x} F^c (t) = F^c (x)$
    1. $\lim_{t\uparrow x} F^c (t) = \mathbb{P}(X\geq x)$
3. $\lim_{x\to\infty}F^c (x) = 0$
4. $\lim_{x\to -\infty}F^c (x) = 1$
</MathBox>

<MathBox title='Reliability function' boxType='definition'>
Suppose $T$ represents the lifetime of device

1. The right-tail distribution function $F^c$ is the reliability function of $T$.
2. The function $h$ defined by $h(t) = \frac{f(t)}{F^c(t)}$ for $t\geq 0$ is the failure rate function of $T$

The reliability function can be expressed in terms of the failure rate function by 

$$
  F^c(t) = \exp\left(-\int_0^t h(s)\;\d s \right)
$$

<details>
<summary>Proof</summary>

At the points of continuity of $f$ we have $\frac{\d}{\d t}F^c(t) = -f(t)$. Hence

$$
\begin{align*}
  \int_0^t h(s)\;\d s = \int_0^t \frac{f(s)}{F^c(s)} \\
  &= \int_0^t -\frac{\frac{\d}{\d t}F^c(t)}{F^c(s)},\d s = -\ln[F^c (t)]
\end{align*}
$$
</details>
</MathBox>

Note that $F^c(t) = \mathbb{P}(T > t)$ is the probability that the device lasts at least $t$ time units. For small time interval $\d t$

$$
\begin{align*}
  \mathbb{P}(t < T < t + \d t | T > t) = \frac{\mathbb{P}(t < T < t +\d t)}{\mathbb{P}(T > t)} \\
  &\approx \frac{f(t)\d t}{F^c (t)} = h(t)\;\d t
\end{align*}
$$

Thus $h(t)\;\d t$ is the approximate probability that the device will fail in the interval $(t, t+\d t)$, given survival up to time $t$. 

### Simulations

Recall that for $n\in\N_+$, the standard measure of the size of set $A\subseteq\R^n$ is given by Lebesgue measure

$$
  \lambda_n (A) = \int_A 1\;\d x
$$

If $S\subseteq\R^n$ with $0< \lambda_n(S) < \infty$, the uniform distribution on $S$ is the continuous distribution with constant probability density function $f$ defined by $f(x) = \frac{1}{\lambda_n(S)}$.

The standard uniform distribution can be transformed into almost any other distribution on $\R$. This is particularly important for simulations. Conversely, any continuous distribution supported on an interval of $\R$ can be transformed into the standard uniform distribution.

<MathBox title='Random quantile function' boxType='proposition'>
Let $F$ be a distribution function for a distribution on $\R$, and let $F^{-1}$ denote the quantile function. Suppose $U$ has the standard uniform distribution. Then $X = F^{-1}(U)$ has distribution function $F$.

<details>
<summary>Proof</summary>

The critical property satisfied by the quantile function (regardless of the type of distribution) is $F^{-1}(p) \leq x$ if and only if $p\leq F(x)$ for $p\in(0,1)$ and $x\in\R$. Hence

$$
  \mathbb{P}(X\leq x) = \mathbb{P}\left[F^{-1}(U)leq x\right] = \mathbb{P}[U\leq F(x)] = F(x)
$$
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $F$ be a distribution function for a distribution on $\R$. Suppose $X$ has a continuous distribution on an interval $S\subseteq\R$. Then $U = F(X)$ has the standard uniform distribution.

<details>
<summary>Proof</summary>

For $u\in(0,1)$ recall that $F^{-1}(u)$ is a quantile of order $u$. Since $X$ has a continuous distribution

$$
  \mathbb{P}(U \geq u) = \mathbb{P}[F(X)\geq u] = \mathbb{P}\left[X\geq F^{-1}(u) \right] = 1 - F[F^{-1}(u)] = 1 - u
$$
</details>
</MathBox>

## Transformation of random variables

<MathBox title='Transformed random variable' boxType='definition'>
Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space, and let $X:\Omega\to S$ be a random variable. If $r:S\to T$ is a function from $(S,\mathcal{S})$ to $(T,\mathcal{T})$, then $Y = r(X)$ is a new random variable taking values in $T$. If $B\in\mathcal{T}$, then

$$
  \mathbb{P}(Y\in B) = \mathbb{P}[r(X)\in B] = \mathbb{P}[X\in r^{-1}(B)]
$$

where $r^{-1}(B) = \Set{ x\in S | r(x)\in B}$.
</MathBox>

<MathBox title='CDF of transformed random variables' boxType='proposition'>
Let $X$ be a real-valued random variable with probability density function $f$. Suppose $Y = r(X)$ is a transformed real-valued random variable. The cumulative distribution function of $Y$ is given by

$$
  G(y) = \int_{r^{-1}(-\infty, y]} f(x)\;\d x
$$

<details>
<summary>Proof</summary>

This follows from the definition of $f$ as a probability density function of $X$. For $y\in\R$

$$
\begin{align*}
  G(y) &= \mathbb{P}(Y\leq y) = \mathbb{P}[r(X)\in (-\infty,y]] \\
  &= \mathbb{P}\left[X\in r^{-1}(-\infty, y] \right] \\
  &= \int_{r^{-1}(-\infty, y]} f(x)\;\d x
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Change of variable formula' boxType='theorem'>
Suppose the random variable $\mathbf{X}$ has a continuous distribution in $S\subseteq\R^n$ given by a probability density function $f$. Let $\mathbf{Y} = r(\mathbf{X})$ where $r:S\to T\subseteq\R^n$ is a differentiable function from $S$ to $T$. The probability density function $g$ of $\mathbf{Y}$ is given by

$$
  g(\mathbf{y}) = f(\mathbf{x})\left|\det\left(\frac{\d\mathbf{x}}{\d\mathbf{y}} \right) \right|,\quad \mathbf{y}\in T
$$

where $\frac{\d\mathbf{x}}{\d\mathbf{y}}$ is the $n\times n$ matrix of first partial derivatives of $\mathbf{x} = r^{-1}(\mathbf{y})$, i.e.

$$
  \left( \frac{\d\mathbf{x}}{\d\mathbf{y}} \right)_{ij} = \frac{\partial x_i}{\partial y_j}
$$

In the single variate case $n=1$, this reduces to for $x = r^{-1}(y)$

$$
  g(y) = f\left[r^{-1}(y)\right] \left| \frac{\d}{\d y} r^{-1} \right| = f(x)\left| \frac{\d x}{\d y} \right|
$$

<details>
<summary>Proof</summary>

If $B\in\mathcal{T}$ then

$$
\begin{align*}
  \mathbb{P}(\mathcal{Y}\in B) &= \mathbb{P}[r(\mathbf{X})\in B] = \mathbb{P}\left[ \mathbf{X}\in r^{-1}(B) \right] \\
  &= \int_{r^{-1}(B)} f(\mathbf{X})\;\d\mathbf{x}
\end{align*}
$$

Changing variables $\mathbf{x} = r^{-1}(\mathbf{y})$, and using the the chain rule $\d\mathbf{x} =\left|\det\left(\frac{\d\mathbf{x}}{\d\mathbf{y}} \right)\right|$ gives

$$
  \mathbf{P}(\mathbf{Y}\in B) = \int_B f[r^{-1}(\mathbf{y})]\left|\det\left(\frac{\d\mathbf{x}}{\d\mathbf{y}} \right)\right|\;\d\mathbf{y} = \int_B g(\mathbf{y})\;\d\mathbf{y}
$$

showing that $g(\mathbf{y})$ is a probability density function for $\mathbf{Y}$.
</details>
</MathBox>

<MathBox title='Linear tranformation' boxType='proposition'>
Suppose the random variable $\mathbf{X}$ has a continuous distribution in $S\subseteq\R^n$ given by a probability density function $f$. Let $\mathbf{Y} = \mathbf{a} + \mathbf{B}\mathbf{Y}$ be a random variable taking values in $T = \Set{\mathbf{a} + \mathbf{B}\mathbf{x} | \mathbf{x}\in S}\subseteq\R^n$ where $\mathbf{a}\in\R^n$ and $\mathbf{B}$ is an invertible $n\times n$ matrix. The probability density function $g$ of $\mathbf{Y}$ is given by

$$
  g(\mathbf{y}) = \frac{1}{|\det(\mathbf{B})|}f\left[ B^{-1}(\mathbf{y} - \mathbf{a})\right],\quad \mathbf{y}\in T
$$

In the single variate case $n=1$, this reduces to

$$
  g(y) = \frac{1}{|b|}f\left( \frac{y - a}{b} \right),\quad y\in T\subseteq\R
$$

<details>
<summary>Proof</summary>

The linear transformation $\mathbf{y} = \mathbf{a} + \mathbf{B}\mathbf{x}$ is bijective in $\R$ with the inverse $\mathbf{x} = \mathbf{B}^{-1}(\mathbf{y} - \mathbf{a})$. The Jacobian of the inverse transformation is the constant function $\det{\mathbf{B}^{-1}} = \frac{1}{\det(\mathbf{B})}$.
</details>
</MathBox>

### Sum transformations

<MathBox title='PDF of sum of random variables' boxType='proposition'>
Let $X$ and $Y$ be random variables taking values in $R\subseteq\R$ and $S\subseteq\R$, respectively, so that $(X, Y)$ takes values in a subset of $R\times S$. Suppose $(X, Y)$ has a probability density function $f$ and consider the random variable $Z = X + Y$ taking values $T = \Set{z\in\R | z = x + y, (x,y)\in R\times S }$. For $z\in T$, let $D_z = \Set{z\in\R | z - x\in S}$.

1. If $(X, Y)$ has a discrete distribution, then $Z = X + Y$ has a discrete distribution with probability density function $u$ given by
$$
  u(z) = \sum_{x\in D_z} f(x, z - x)
$$
2. If $(X, Y$ has a continuous distribution then $Z = X + Y$ has a continuous distribution with probability density function $u$ given by
$$
  u(z) = \int_{D_z} f(x, z - x)\;\d x
$$

<details>
<summary>Proof</summary>

1. $\mathbb{P}(Z = z) = \mathbb{P}(X = x, Y = z - x \textrm{ for some }x\in D_z} = \sum_{x\in D_z} f(x, z - x)$
2. For $A\subseteq T$, let $C = \Set{(u,v)\in R\times S | u + v \in A }$ then

$$
  \mathbb{P}(Z\in A) = \mathbb{P}(X + Y\in A) = \int_C f(u, v)\;\d(u, v)
$$

Use the change of variables $x = u$, $z = u + v$. Then the inverse transformation is $u = x$, $v = z - x$ and the Jacobian is $1$. Using the change of variables theorem gives

$$
  \mathbb{P}(Z\in A) = \int_{D_z \times A} f(x, z - x)\;\d(x, z) = \int_A \int_{D_z} f(x, z - x)\;\d x,\d y
$$

It follows that $Z$ has probability density function $z\mapsto \int_{D_z} f(x, z - x)\;\d x$.
</details>
</MathBox>

<MathBox title='Convolution of random variables' boxType='proposition'>
Let $X$ and $Y$ be independent real-valued random variables with probability density functions $g$ and $h$, respectively

1. If $X$ and $Y$ have discrete distributions then $Z = X + Y$ has a discrete distribution with probability density function $g*h$ given by
$$
  (g*h)(z) = \sum_{x\in D_z} g(x)h(z - x),\quad z\in T
$$

If $X$ and $Y$ take values in $\N$, this simplifies to
$$
  (g*h)(z) = \sum_{x=0}^z g(x)h(z - x),\quad z\in\N
$$

2. If $X$ and $Y$ have continuous distributions then $Z = X + Y$ has a continuous distribution with probability density function $g*h$ given by
$$
  (g*h)(z) = \int_{D_z} g(x)h(z - x)\;\d x,\quad z\in T
$$

If $X$ and $Y$ take values in $[0,\infty)$, this simplifies to
$$
  (g*h)(z) = \int_0^z g(x)h(z - x)\;\d x,\quad z\in T
$$

In both cases, the probability density function $g*h$ is called the convolution of $g$ and $h$, which satisfies
- Commutativity: $g * h = h * g$, which follows from commutativity of addition of random variables, i.e. $X + Y = Y + X$
- Associativity: $(f*g)*h = f*(g*h)$, which follows from associativity of addition of random variables, i.e. $(X + Y) + Z = X + (Y + Z)$

<details>
<summary>Proof</summary>

Since $X$ and $Y$ are independent, $(X, Y)$ has probability density function $f(x, y) = g(x)h(y)$.
1. If $X$ and $Y$ take values in $\N$, then $D_z = \Set{i }_{i=0}^{z\in\N}$.
2. If $X$ and $Y$ take values in $[0,\infty)$, then $D_z = [0, z]$ for $z\in [0, \infty)$.
</details>
</MathBox>

If $\mathbf{X} = (X_i)_{i=1}^{n\in\N}$ is a sequence of independent and identically distributed real-valued random variables, with common probability density function $f$, then $Y_n = \sum_{i=1}^n X_i$ has probability density function $f^{*n}$, the $n$-fold convolution power of $f$. In statistical terms, $\mathbf{X}$ corresponds to sampling from the common distribution. By convention, $Y_0 = 0$, so naturally we take $f^{*0} = \delta$. When appropriately scaled and centered, then by the central limit theorem $Y_n$ converges to the standard normal distribution as $n\to\infty$.

### Product and quotient transformations

<MathBox title='PDF of products and quotients of random variables' boxType='proposition'>
Suppose $(X, Y)$ has a continuous distribution on $\R^2$ with probability density function $f$.

1. The random variable $V = XY$ has probability density function
$$
  v\mapsto\int_{-\infty}^\infty f(x, v/x)\frac{1}{|x|}\;\d x
$$

2. The random variable $W = \frac{Y}{X}$ has probability density function
$$
  w\mapsto\int_{-\infty}^\infty f(x,wx)|x|\;\d x
$$

In the special case when $X$ and $Y$ are independent, having probability density functions $g$ and $h$, respectively then
3. The random variable $V = XY$ has probability density function
$$
  v\mapsto\int_{-\infty}^\infty g(x)h(v/x)\frac{1}{|x|}\;\d x
$$
4. The random variable $W = \frac{Y}{X}$ has probability density function
$$
  w\mapsto\int_{-\infty}^\infty g(x)h(wx)|x|\;\d x
$$

<details>
<summary>Proof</summary>

We introduce the auxiliary variable $U = X$ so that we have bivariate transformations.

1. Applying the transformations $u = x$, $v = xy$ with inverses $x = u$, $y = \frac{v}{u}$ gives first derivative matrix

$$
  \frac{\partial(x,y)}{\partial(u,v)} = \begin{bmatrix} 
    1 & 0 \\
    -v/u^2 & 1/u
 \end{bmatrix}
$$

with Jacobian $\frac{1}{u}$. Using the change of variables theorem, the joint probability density function of $(U, V)$ is $(u, v)\mapsto f(u, v/u)\frac{1}{|v|}$. Hence the probability density function of $V$ is

$$
  v\mapsto\int_{-\infty}^\infty f(x, v/x)\frac{1}{|x|}\;\d x
$$

2. Applying the transformations $u = x$, $w = \frac{y}{x}$ with inverses $x = u$, $y = uw$ gives the first derivative matrix

$$
  \frac{\partial(x,y)}{\partial(u,v)} = \begin{bmatrix} 
    1 & 0 \\
    w & u
 \end{bmatrix}
$$

with Jacobian $u$. Using the change of variables theorem, the joint probability density function of $(U, W)$ is $(u, w)\mapsto f(u, uw)|u|$. Hence the probability density function of $W$ is

$$
  w\mapsto\int_{-\infty}^\infty g(x)h(wx)|x|\;\d x
$$

In the special case when $X$ and $Y$ are independent, the joint probability density function of $(X, Y)$ is $f(x,y) = g(x)h(y)$.
</details>
</MathBox>

### Minimum and maximum tranformations

<MathBox title='CDF of minimum and maximum of random variables' boxType='proposition'>
Suppose $(X_i)_{i=1}^{n\in\N}$ is a sequence of independent real-valued random variables and that $X_i$ has cumulative distribution function $F_i$.

1. The maximum transformation $V = \max\Set{ X_i}_{i=1}^n$ has distribution function $H$ given by $H(x) = \prod_{i=1}^n F_i (x)$ for $x\in\R$.
2. The maximum transformation $U = \min\Set{X_i}_{i=1}^n$ has distribution function $G$ given by $G(x) = 1 - \prod_{i=1}^n [1 - F_i(x)]$ for $x\in\R$.

In the special case when the $X_i$ are identically distributed with common density function $F$, then
3. $V = \max\Set{X_i}_{i=1}^n$ has distribution function $H$ given by $H(x) = F^n (x)$
4. $U = \min\Set{X_i}_{i=1}^n$ has distribution function $G$ given by $G(x) = 1 - [1 - F(x)]^n$

If the $X_i$ have a common continuous distribution with a probability density function $f$, then
5. $V = \max\Set{X_i}_{i=1}^n$ has probability density function $h$ given by $h(x) = nF^{n-1}f(x)$
6. $U = \min\Set{X_i}_{i=1}^n$ has probability density function $g$ given by $g(x) = n[1 - F(x)]^{n-1} f(x)$

<details>
<summary>Proof</summary>

1. Since $V$ is the maximum of the variables, $\Set{V\leq x} = \Set{ X_i \leq x }_{i=1}^n$. Hence, by independence

$$
  H(x) = \mathbb{P}(V\leq x) = \prod_{i=1}^n \mathbb{P}(X_i\leq x) = \prod_{i=1}^n F_i(x)
$$

2. Since $U$ is the minimum of the variables, $\Set{U < x } = \Set{ X_i > x}_{i=1}^n$. Hence, by independence

$$
\begin{align*}
  G(x) &= \mathbb{P}(U \leq x) = 1 - \mathbb{P}(U > x) = 1 - \prod_{i=1}^n \mathbb{P}(X_i > x)
  &= 1 - \prod_{i=1}^n [1 - F_i (x)] 
\end{align*}
$$
</details>
</MathBox>

### Sign and absolute value transformations

<MathBox title='Absolute value of random variable' boxType='proposition'>
Suppose $X$ has a continuous distribution on $\R$ with distribution function $F$ and probability density function $f$. Let $|X|:\R\to[0,\infty)$ be the absolute value transformation of $X$. Then for $y\in[0,\infty)$

1. $|X|$ has distribution function $G$ given by $G(y) = F(y) - F(-y)$.
2. $|X|$ has probability density function $g$ given by $g(y) = f(y) + f(-y)$

<details>
<summary>Proof</summary>

1. $\mathbb{P}(|X| \leq y) = \mathbb{P}(-y \leq X \leq y) = F(y) - F(-y)$
2. $g(y) = G'(y) = [F(y) - F(-y)]' = f(y) + f(-y)$
</details>
</MathBox>

<MathBox title='Sign and absolute value of symmetrically distributed random variable' boxType='proposition'>
Suppose $X$ has a continuous distribution on $\R$ that is symmetric about $0$, and suppose $X$ has distribution function $F$ and probability density function $f$. Then

1. $|X|$ has distribution function $G$ given by $G(y) = 2F(y) - 1$ for $y\in[0,\infty)$.
2. $|X|$ has probability density function $g$ given by $g(y) = 2f(y)$ for $y\in[0,\infty)$
3. $\operatorname{sgn}(X)$ is uniformly distributed on $\Set{-1, 1}$.
4. $|X|$ and $\operatorname{sgn}(X)$ are independent.

Recall that the sign function on $\R$ is defined as

$$
  \operatorname{sgn}(x) = \begin{cases}
    -1,\quad &x< 0\\
    0,\quad &x=0 \\
    1,\quad x> 0
  \end{cases}
$$
<details>
<summary>Proof</summary>

1. By symmetry $F(-y) = 1 - F(y)$ for $y > 0$. Hence $G(y) = F(y) - F(-y) = 2F(y) - 1$
2. $g(y) = G'(y) = [2F(y) - 1]' = 2f(y)$
3. Note that $\mathbb{P}[\operatorname{sgn}(X) = 1] = \mathbb{P}(X > 0) = \frac{1}{2}$ and $\mathbb{P}[\operatorname{sgn}(X) = -1] = \frac{1}{2}$
4. If $A\subseteq (0,\infty)$ then

$$
\begin{align*}
  \mathbb{P}\left[ |X|\in A,\operatorname{sgn}(X) = 1\right] &= \mathbb{P}(X\in A) \\
  &= \int_A f(x)\;\d x = \frac{1}{2}\int_A 2f(x)\;\d x \\
  &= \mathbb{P}[\operatorname{sgn}(X) = 1]\mathbb{P}(|X|\in A)
\end{align*}
$$
</details>
</MathBox>

## Conditional probability

<MathBox title='Conditional probability' boxType='definition'>
Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space and consider an event $B\in\mathcal{F}$ with $\mathbb{P}(B)\neq 0$. The conditional probability of an event $A\in\mathcal{F}$ given $B$ is defined as

$$
  \mathbb{P}(A\mid B) := \frac{\mathbb{P}(A\cap B)}{\mathbb{P}(B)}
$$

where $\mathcal{P}(\cdot|B):\mathcal{A}\to[0, 1]$ is the conditional probability measure given $B$, which satisfies
- $\mathcal{P}(B\mid B) = 1$
<details>
<summary>Proof</summary>

To prove that $A\mapsto\mathbb{P}(A\mid B)$ is probability measure, we verify the Kolmogorov axioms
1. Trivially $\mathbb{P}(A\mid B)\geq 0$ for every $A\in\mathcal{A}$
2. Trivially $\mathbb{P}(\Omega|B) = \frac{\mathbb{P}(\Omega\cap B)}{\mathbb{P}(B)} = \frac{\mathbb{P}(B)}{\mathbb{P}(B)} = 1$
3. If $\Set{A_i}_{i\in I}\subseteq\mathcal{F}$ is a countable collection of pairwise disjoint events then

$$
\begin{align*}
  P\left( \bigcup_{i\in I} A_i | B \right) &= \frac{1}{\mathbb{P}(B)}\mathbb{P}\left[\left(\bigcup_{i\in I} A_i \right)\cap B\right] = \frac{1}{\mathbb{P}(B)}\mathbb{P}\left[\bigcup_{i\in I}(A_i \cap B) \right] \\
  &= \frac{1}{\mathbb{P}(B)}\sum_{i\in I}\mathbb{P}(A_i\cap B) = \sum_{i\in I}\frac{\mathbb{P}(A_i\cap B)}{\mathbb{P}(B)} = \sum_{i\in I}\mathbb{P}(A_i\mid B)
\end{align*}
$$

Where we have used the fact that the collection of events $\Set{A_i\cap B}_{i\in I}$ is also pairwise disjoint.
</details>
</MathBox>

<MathBox title='Correlation of events' boxType='definition'>
Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space. Two events $A, B\in\mathcal{F}$ with positive probability can be related in the following ways
1. Positively correlated: $\mathbb{P}(A\mid B)>\mathbb{P}(A)\iff\mathbb{P}(A\mid A)>\mathbb{P}(B)\iff\mathbb{P}(A\cap B)>\mathbb{P}(A)\mathbb{P}B$
2. Negatively correlated: $\mathbb{P}(A\mid B)<\mathbb{P}(A)\iff\mathbb{P}(A\mid A)<\mathbb{P}(B)\iff\mathbb{P}(A\cap B)<\mathbb{P}(A)\mathbb{P}B$
3. Uncorrelated or independent: $\mathbb{P}(A\mid B)=\mathbb{P}(A)\iff\mathbb{P}(A\mid A)=\mathbb{P}(B)\iff\mathbb{P}(A\cap B)=\mathbb{P}(A)\mathbb{P}(B)$
</MathBox>

<MathBox title='Properties of correlation' boxType='proposition'>
Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space and consider two events $A, B\in\mathcal{F}$. Then
1. $A$ and $B$ have the same correlation as $A^c$ and $B^c$.
2. $A$ and $B$ have the opposite correlation as $A$ and $B^c$
<details>
<summary>Proof</summary>

1. Applying DeMorgan's theorem, the complement rule and the inclusion-exclusion rule gives

$$
\begin{align*}
  \mathbb{P}(A^c \cap B^c) - \mathbb{P}(A^c)\mathbb{P}(B^c) &= \mathbb{P}\left[ (A\cap B)^c \right] - \mathbb{P}(A^c)\mathbb{P}(B^c) \\
  &= \left[1 - \mathbb{P}(A\cap B) \right] - \left[1 - \mathbb{P}(A)\right]\left[\mathbb{P}(B)] \\
  &= 1 - \left[\mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A\cap B) \right] - \left[1 - \mathbb{P}(A) - \mathbb{P}(B) - \mathbb{P}(A)\mathbb{P}(B) \right] \\
  &= \mathbb{P}(A\cap B) - \mathbb{P}(A)\mathbb{P}(B)
\end{align*}
$$

2. Applying the difference rule and the complement rule

$$
\begin{align*}
  \mathbb{P}(A \cap B^c) - \mathbb{P}(A)\mathbb{P}(B^c) &= \mathbb{P}(A) - \mathbb{P}(A\cap B) - \mathbb{P}(A)[1 - \mathbb{P}(B)] \\
  &= \left[1 - \mathbb{P}(A\cap B) \right] - \left[1 - \mathbb{P}(A)\right]\left[\mathbb{P}(B)] \\
  &= -\left[\mathbb{P}(A\cap B) - \mathbb{P}(A)\mathbb{P}(B) \right]
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Multiplication rule' boxType='theorem'>
Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space, and consider two events $A, B\in\mathcal{F}$ with positive probability, then

$$
  \mathbb{P}(A\cap B) = \mathbb{P}(A)\mathbb{P}(B\mid A) = \mathbb{P}(B)\mathbb{P}(B\mid A)
$$

In the general case, suppose $(A_i \in\mathcal{F})_{i=1}^{n\in\N}$ is a sequence of events. then

$$
  \mathbb{P}\left(\bigcap_{i=1}^n A_i \right) = \mathbb{P}(A_1)\mathbb{P}(A_2\mid A_1)\mathbb{P}(A_2\mid A_1\cap A_2)\cdots\mathbb{P}\left(A_2\mid  \bigcup_{i=1}^{n-1} A_i\right)
$$
</MathBox>

<MathBox title='Law of total probability' boxType='theorem'>
Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space, and consider two events $A, B\in\mathcal{F}$ with $\mathbb{P}(B) > 0$. Then

$$
  \mathbb{P}(A) = \mathbb{P}(B)\mathbb{P}(A\mid B) + \mathbb{P}(B^c)\mathbb{P}(A\mid B^c)
$$

In the general case, suppose $\Set{B_i }_{i\in I}\subseteq\mathcal{F}$ is a countable collection of events that partition $\Omega$ with $\mathbb{P}(B_i) > 0$ for each $i\in I$. For an event $A\in\mathcal{F}$ then

$$
  \mathbb{P}(A) = \sum_{i\in I}\mathbb{P}(B_i)\mathbb{P}(A\mid B_i)
$$
<details>
<summary>Proof</summary>

Note that $A = (A\cap B)\cup (A\cap B^c)$ is a disjoint union, so

$$
\begin{align*}
  \mathbb{P}(A) &= \mathbb{P}\left[(A\cap B)\cup (A\cap B^c) \right] = \mathbb{P}(A\cap B) + \mathbb{P}(A\cap B^c) \\
  &= \mathbb{P}(B)\mathbb{P}(A\mid B) + \mathbb{P}(B^c)\mathbb{P}(A\mid B^c)
\end{align*}
$$
</details>
</MathBox>

<MathBox title="Bayes' theorem" boxType='theorem'>
Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space, and consider two events $A, B\in\mathcal{F}$ with $\mathbb{P}(B) > 0$. Then Bayes' theorem states that

$$
  \mathbb{P}(A\mid B) = \frac{\mathbb{P}(A)\mathbb{P}(A\mid A)}{\mathbb{P}(B)} = \frac{\mathbb{P}(A)\mathbb{P}(A\mid A)}{\mathbb{P}(A)\mathbb{P}(A\mid A) + \mathbb{P}(A^c)\mathbb{P}(A\mid A^c)}
$$

where
- $\mathbb{P}(A)$ is the prior probability of $A$
- $\mathbb{P}(A\mid B)$ is the posterior probability of $A$

In the general case, suppose $\Set{A_i }_{i\in I}\subseteq\mathcal{F}$ is a countable collection of events that partition $\Omega$ with $\mathbb{P}(A_i) > 0$ for each $i\in I$. Then Bayes' theorem takes the general form for $j\in I$

$$
  \mathbb{P}(A_j \mid B) = \frac{\mathbb{P}(A_j)\mathbb{P}(A_j \mid A_j)}{\sum_{i\in I}\mathbb{P}(A_i)\mathbb{P}(A_j \mid A_i)}
$$
</MathBox>

<MathBox title='Conditional probability kernel' boxType='definition'>
Suppose $(\Omega,\mathcal{F},\mathbb{P})$ is a probability space, and that $(S,\mathcal{S})$ and $(T,\mathcal{T})$ are measurable spaces. Consider the random variables $X:\Omega\to S$ and $Y:\Omega\to T$. The function $P$ defined as follows is a probability kernel from $(S,\mathcal{S})$ to $(T,\mathcal{T})$, known as the conditional probability kernel of $Y$ given $X$.

$$
  P(x, A) = \mathbb{P}(Y\in A | X = x),\quad x\in S, A\in\mathcal{T}
$$

The right and left operators associated with this kernel correspond to the conditional expectation and probability destribution, respectively.
1. If $f:T\to\R$ is measurable, then $Pf(x) = \mathbb{E}[f(Y)|X = x]$ for $x\in S$
2. If $\mu$ is the probability distribution of $X$ then $\mu P$ is the probability distribution of $Y$ 

<details>
<summary>Proof</summary>

Recall that for $A\in\mathcal{T}$ the conditional probability $\mathbb{P}(Y\in A| X)$ is itself a random variable, and is measurable with respect to $\sigma(X)$. That is, $\mathbb{P}(Y\in A|X) = P(X, A)$ for some measurable function $S\ni x \mapsto P(x, A)\in [0,1]$. Then by definition $\mathbb{P}(Y\in A|X=x)=P(x,A)$. Trivially, $A\mapsto P(x, A)$ is a probability measure on $(T,\mathcal{T})$ for $x\in S$.

1. Since $A\mapsto P(x, A)$ is the conditional distribution of $Y$ given $X = x$
$$
  \mathbb{E}[f(Y)| X=x] = \int_S P(x,\d y)f(y) = Pf(x)
$$
2. Let $A\in\mathcal{T}$. Conditioning on $X$ gives
$$
\begin{align*}
  \mathbb{P}(Y\in A) &= \mathbb{E}[\mathbb{P}(Y\in A| X)] \\
  &= \int_S P(Y\in A| X=x)\;\d\mu(x) \\
  &= \int_S P(x, A)\;\d\mu(x) = \mu P(A)
\end{align*}
$$
</details>
</MathBox>

## Independence

<MathBox title='Independence of events' boxType='definition'>
Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space. Two events $A, B\in\mathcal{F}$ are independent (uncorrelated) if $\mathbb{P}(A\cap B) = \mathbb{P}(A)\mathbb{P}(B)$. In the general case, a countable collection $\mathcal{A} = \Set{A_i }_{i\in I}\subseteq\mathcal{F}$ of events is independent if for every finite $J\subseteq I$

$$
  \mathbb{P}\left(\bigcap_{j\in J} A_j \right) = \prod_{j\in J}\mathbb{P}(A_j)
$$

From the definition it follows that $\mathcal{A}$ has the following inheritance properties
1. If $\mathcal{A}$ is independent, then $\mathcal{B}$ is independent for every $\mathcal{B}\subseteq\mathcal{A}$.
2. If $\mathcal{B}$ is independent for every finite $\mathcal{B}\subseteq\mathcal{A}$ then $\mathcal{A}$ is independent.

The notion of independence can be extended to collections of collections of events. Consider an index set $I$ and suppose $\mathcal{A}_i$ is a collection of events for each $i\in I$. Then $\mathscr{A} = \Set{\mathcal{A}}_{i\in I}$ is independent if and only if for every choice of $A_i\in\mathcal{A}_i$ for $i\in I$, the collection of events $\Set{A_i}_{i\in I}$ is independent as defined above. The independence of $\Set{A_i}_{i\in I}$ is equivalent to the independence of $\Set{\mathcal{A}_i}_{i\in I}$ where $\mathcal{A}_i = \sigma\Set{A_i} = \Set{\Omega,\emptyset, A_i, A_i^c}$ for each $i\in I$. From the definition it follows that $\mathscr{A}$ has the following inheritance properties
1. If $\mathscr{A}$ is independent, then $\mathscr{B}$ is independent for every $\mathscr{B}\subseteq\mathscr{A}$.
2. If $\mathscr{B}$ is independent for every finite $\mathscr{B}\subseteq\mathscr{A}$ then $\mathscr{A}$ is independent.
</MathBox>

Note that independence and disjointness of events are different concepts. Two disjoint events can never be independent.
<MathBox title='Disjoint events are negatively correlated' boxType='proposition'>
Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space and consider two disjoint events $A, B\in\mathcal{F}$ with positive probability. Then $A$ and $B$ are dependent, and are in fact negatively correlated.
<details>
<summary>Proof</summary>

Note that $\mathbb{P}(A\cap B) = \mathbb{P}(\emptyset) = 0$ but $\mathbb{P}(A)\mathbb{P}(B) > 0$.
</details>
</MathBox>

<MathBox title='Properties of independence' boxType='proposition'>
Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space. If $A, B\in\mathcal{F}$ are independent events, then the following pairs of events are also independent:
1. $A^c$ and $B$
2. $B$ and $A^c$
3. $A^c$ and $B^c$

In the general case, suppose $\mathcal{A} = \Set{A_i}_{i\in I}\mathcal{F}$ and $\mathcal{B} = \Set{B_i}_{i\in I}\subseteq\mathcal{F}$ are two countable collections of events with the property that for each $i\in I$, either $B_i = A_i$ or $B_i = A_i^c$. Then $\mathcal{A}$ is independent if and only if $\mathcal{B}$ is independent.
<details>
<summary>Proof</summary>

1. Note that $\mathbb{P}(A^c \cap B) = \mathbb{P}(B\setminus A)$. Applying the difference rule and the complement rule

$$
\begin{align*}
  \mathbb{P}(A^c \cap B) &= \mathbb{P}(B) - \mathbb{P}(A\cap B) \\
  &= \mathbb{P}(B) - \mathbb{P}(A)\mathbb{P}(B) = \mathbb{P}(B)[1 - \mathbb{P}(A)] \\
  &= \mathbb{P}(B)\mathbb{P}(A^c)
\end{align*}
$$

2. Note that $\mathbb{P}(A \cap B^c) = \mathbb{P}(A\setminus B)$. Applying the difference rule and the complement rule

$$
\begin{align*}
  \mathbb{P}(A \cap B^c) &= \mathbb{P}(A) - \mathbb{P}(A\cap B) \\
  &= \mathbb{P}(A) - \mathbb{P}(A)\mathbb{P}(B) = \mathbb{P}(A)[1 - \mathbb{P}(B)] \\
  &= \mathbb{P}(A)\mathbb{P}(B^c)
\end{align*}
$$

3. Applying DeMorgan's theorem, the complement rule and the inclusion-exclusion rule

$$
\begin{align*}
  \mathbb{P}(A^c \cap B^c) &= \mathbb{P}[(A\cup B)^c] = 1 - \mathbb{P}(A\cup B) \\
  &= 1 - [\mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A\cap B)] \\
  &= 1 - [1 - \mathbb{P}(A^c)] - [1 - \mathbb{P}(B^c)] + [1 - \mathbb{P(A^c)}][1 - \mathbb{P}(B^c)] \\
  &= \mathbb{P}(A^c)\mathbb{P}(B^c)
\end{align*}
$$

For the general case, it suffices to show $\mathcal{A}$ independent implies $\mathcal{B}$ independent due to the symmetry of the relation between $\mathcal{A}$ and $\mathcal{B}$. By the inheritance property, it also suffices to consider where the index set $I$ is finite. Fix $k\in I$ and define $B_k = A_k^c$ and $B_i = A_i$ for $i\in I\setminus\Set{k}$. Suppose $J\subseteq I$. If $k\notin J$, then trivially $\mathbb{P}\left(\bigcup_{j\in J} B_j \right) = \prod_{j\in J}\mathbb{P}(B_j)$. If $k\in J$, then by the difference rule

$$
\begin{align*}
  \mathbb{P}\left(\bigcup_{j\in J}B_j\right) &= \mathbb{P}\left(\bigcap_{j\in J\setminus\Set{k}} A_j \right) - \mathbb{P}\left(\bigcup_{j\in J} A_j \right) \\
  &= \prod_{j\in J\setminus\Set{k}}\mathbb{P}(A_j) - \prod_{j\in J}\mathbb{P}(A_j) \\
  &= \left[\prod_{j\in J\setminus\Set{k}}\mathbb{P}(A_j) \right][1 - \mathbb{P}(A_k)] \\
  &= \prod_{j\in J}\mathbb{P}(B_j)
\end{align*}
$$

Suppose $\mathcal{B} = \Set{B_i }_{i\in I}\subseteq\mathcal{F}$ is a general finite collection of events where $B_i = A_i$ or $B_i = A_i^c$ for each $i\in I$. Then same procedure can be applied by a finite sequence of complement changes. Hence $\mathcal{B}$ is a collection of independent events.
</details>
</MathBox>

<MathBox title='Independence of collection of random variables' boxType='definition'>
Let $(S,\mathcal{S},\mathbb{P})$ be a probability space and $(T_i,\mathcal{T}_i)$ be event spaces for $i$ in an index set $I$. A collection of random variables $\mathcal{X} = \Set{\Set{X_i\in B_i}| i\in I}$ is independent for every choice of $B_i\in\mathcal{T}_i$ for $i\in I$. Equivalently, $\mathcal{X}$ is independent if for every finite $J\subseteq I$ and for every choice of $B_j\in \mathcal{T}_j$ for $j\in J$

$$
  \mathbb{P}\left(\bigcap_{j\in J}\Set{X_j \in B_j }\right) = \prod_{j\in J}\mathbb{P}(X_j\in B_j)
$$

From the definition it follows that $\mathcal{X}$ has the following inheritance properties
1. If $\mathcal{X}$ is independent, then $\mathcal{Y}$ is independent for every $\mathcal{Y}\subseteq\mathcal{X}$.
2. If $\mathcal{Y}$ is independent for every finite $\mathcal{Y}\subseteq\mathcal{X}$ then $\mathcal{X}$ is independent.
</MathBox>

<MathBox title='Independence is preserved under compositions' boxType='proposition'>
Let $(S,\mathcal{S},\mathbb{P})$ be a probability space and $(T_i,\mathcal{T}_i)$ and $(U_i,\mathcal{U}_i)$ be event spaces for $i$ in an index set $I$. Suppose $g_i:T_i\to U_i$ are measurable functions. If the collection $\mathcal{X} = \Set{X_i:S\to T_i}_{i\in I}$ is independent then $\Set{g_i(X_i)}_{i\in I}$ is also independent.
<details>
<summary>Proof</summary>

Suppose $C_i\in\mathcal{U}_i$ for each $i\in I$ such that $g_i^{-1}[C_i]\in\mathcal{T}_i$. Then $\Set{X_i\in g_i^{-1}[C_i]}\in\mathcal{S}$ for $i\in I$. By the independence of $\Set{X_i}_{i\in I}$, the collection of events $\Set{\Set{X_i\in g_i^{-1}[C_i]}| i\in I}$ is independent.
</details>
</MathBox>

<MathBox title='Condition for independence of collections of $\sigma$-algebras' boxType='proposition'>
Let $(X,\mathcal{F},\mathbb{P})$ be a probability space. Suppose $\mathcal{A}_i$ is a collection of events, and a $\pi$-system for each $i$ in an index set $I$. If $\Set{\mathcal{A}_i}_{i\in I}$ is independent, then the collection of $\sigma$-algebras $\Set{\sigma(\mathcal{A}_i)}_{i\in I}$ is independent.
<details>
<summary>Proof</summary>

Due to inheritance of independence it suffices to consider a finite set of collections. Suppose $\Set{1,2,\dots,n}$ and let $E=\bigcap_{i=2}^n A_i$, and $\mathcal{L}=\Set{B\in\mathcal{F} | \mathbb{P}(B\cap E) = \mathbb{P}(B)\mathbb{P}(E)}$. We will show that $\mathcal{L}$ is a $\lambda$-system and apply the $\pi$-$\lambda$ theorem to find that $\sigma(\mathcal{A}_1)\subseteq\mathcal{L}$. Trivially $X\in\mathcal{L}$ since $\mathbb{P}(X\cap E) = \mathbb{P}(E) = \mathbb{P}(X)\mathbb{P}(E)$. Next, suppose $A\in\mathcal{L}$, then

$$
\begin{align*}
  \mathbb{P}(A^c\cap E) &= \mathbb{P}(E) - \mathbb{P}(A\cap E) = \mathbb{P}(E) - \mathbb{P}(A)\mathbb{P}(E) \\
  &= [1 - \mathbb{P}(A)]\mathbb{P}(E) = \mathbb{P}(A^c)\mathbb{P}(E)
\end{align*}
$$

showing that $A^c\in\mathcal{L}$. Finally, suppose $\Set{A_j}_{j\in J}$ is a countable collection of disjoint sets in $\mathcal{L}$, then

$$
\begin{align*}
  \mathcal{P}\left[\left(\bigcup_{j\in J}A_j \right)\cap E \right] &= \mathbb{P}\left[\bigcup_{j\in J}(A_j\cap E)\right] = \sum_{j\in J}\mathbb{P}(A_j\cap E) \\
  &= \sum_{j\in J}\mathbb{P}(A_j)\mathbb{P}(E) = \mathbb{P}(E)\sum_{j\in J}\mathbb{P}(A_j) \\
  &= \mathbb{P}(E)\mathbb{P}\left(\bigcup_{j\in J}A_j \right)
\end{align*}
$$

showing that $\bigcup_{j\in J}A_j\in\mathcal{L}$, and thus $\mathcal{L}$ is a $\lambda$-system. Trivially $\mathcal{A}_1\subseteq \mathcal{L}$ by the independence assumption, so by the $\pi$-$\lambda$ theorem, $\sigma(\mathcal{A}_1)\subseteq\mathcal{L}$. Thus, we have that for every $A_1\in\sigma(\mathcal{A}_1)$ and $A_i\in\mathcal{A}_i$ for $i\in\Set{2,3,\dots,n}$

$$
  \mathbb{P}\left(\bigcap_{i=1}^n A_i\right) = \prod_{i=1}^n\mathbb{P}(A_i)
$$

Hence we have shown that $\Set{\sigma(\mathcal{A}_1), \mathcal{A}_2,\dots,\mathcal{A}_n}$ is independent. Repeating the argument $n-1$ times, we get that $\Set{\sigma(\mathcal{A}_i)}_{i=1}^n$ is independent.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Suppose $\mathcal{A}$ is an independent collection of events, and that $\Set{\mathcal{B}_j}_{j\in J}$ is a partition of $\mathcal{A}$. Then $\Set{\sigma(\mathcal{B}_j)}_{j\in J}$ is independent. 

<details>
<summary>Proof</summary>

Let $\mathcal{B}*_j$ denote the set of all finite intersections of sets in $\mathcal{B}_j$, for each $j\in J$. Then clearly $\mathcal{B}*_j$ is a $\pi$-system for each $j$, and $\Set{\mathcal{B}*_j}_{j\in J}$ is independent. By the previous result $\Set{\sigma(\mathcal{B}*_j)}_{j\in J}$ is independent. However, clearly $\sigma(\mathcal{B}*_j) = \sigma(\mathcal{B})$ for $j\in J$. 
</details>
</MathBox>

## Exchangeability

<MathBox title='Exchangeable collection of events' boxType='definition'>
Let $I$ be an index set. A collection of events $\mathcal{A} = \Set{A}_{i\in I}$ is exchangeable if the probability of a finite intersection of events depends only on the number of events. That is, for finite $J, K\subseteq I$ with $|J|=|K|$ then

$$
  \mathbb{P}\left(\bigcap_{j\in J} A_j \right) = \mathbb{P}\left(\bigcap_{k\in K} A_k \right)
$$

From the definition it follows that $\mathcal{A}$ has the following inheritance properties
1. If $\mathcal{A}$ is exchangeable, then $\mathcal{B}$ is exchangeable for every $\mathcal{B}\subseteq\mathcal{A}$.
2. If $\mathcal{B}$ is exchangeable for every finite $\mathcal{B}\subseteq\mathcal{A}$ then $\mathcal{A}$ is exchangeable.
</MathBox>

<MathBox title='Inclusion-exclusion formula for exhangeable events' boxType='corollary'>
Let $\mathcal{A} = \Set{A_i}_{i=1}^{n\in\N}$ be an exchangeable collection of events. For $J\subseteq \Set{1,2,\dots,n}$ with $|J| = k$, let $p_k = \mathbb{P}\left(\bigcap_{j\in J}A_j\right)$. Then the inclusion-exclusion formula reduces to

$$
  \mathbb{P}\left(\bigcup_{i=1}^n A_i \right) = \sum_{k=1}^n (-1)^{k-1}\binom{n}{k}p_k
$$
</MathBox>

## Continuity

<MathBox title='Continuity theorem for probability' boxType='theorem'>
Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space and consider a sequence of events $(A_n\in\mathcal{F})_{n\in\N}$.
1. If the sequence is increasing then $\lim_{n\to\infty}\mathbb{P}(A_n)=\mathbb{P}\left(\lim_{n\to\infty} A_n \right)=\lim_{n\to\infty}\mathbb{P}\left(\bigcup_{i=1}^n A_i \right)=\mathbb{P}\left(\bigcup_{n\in\N} A_n \right)$
2. If the sequence is decreasing then $\lim_{n\to\infty}\mathbb{P}(A_n)=\mathbb{P}\left(\lim_{n\to\infty} A_n \right)=\lim_{n\to\infty}\mathbb{P}\left(\bigcap_{i=1}^n A_i \right)=\mathbb{P}\left(\bigcap_{n\in\N} A_n \right)$ 
<details>
<summary>Proof</summary>

1. Define the collection of events $\mathcal{B} = \Set{B_n}_{n\in\N}\subseteq\mathcal{F}$ where $B_1 = A_1$ and $B_n = A_n\setminus A_{n-1}$ for $n\in\N\setminus\Set{1}$. Note that $\mathcal{B}$ is pairwise disjoint and has the same union as $\Set{A_n}_{n\in\N}$. From countable additivity

$$
  \mathbb{P}\left(\bigcup_{n\in\N} A_n \right) = \mathbb{P}\left(\bigcup_{n\in\N} B_n \right) = \sum_{n\in\N}\mathbb{P}(B_n) = \lim_{n\to\infty} \sum_{i=1}^n \mathbb{P}(B_i)
$$

However, $\mathbb{P}(B_1) = \mathbb{P}(A_1)$ and $\mathbb{P}(B_i) = \mathbb{P}(A_i) - \mathbb{P}(A_{i-1})$ for $i\in\N\setminus\Set{1}$. Thus $\sum_{i\in n}\mathbb{P}(B_i) = \mathbb{P}(A_n)$ resulting in $\mathbb{P}\left(\bigcup_{n\in\N}A_n\right) = \lim_{n\to\infty}\mathbb{P}(A_n)$.

2. Note that the sequence of complements $\left(A_n^c\right)_{n\in\N}$ is increasing. Applying DeMorgan's rule and the complement rule on the result above gives

$$
  \mathbb{P}\left(\bigcap_{n\in\N} A_n \right) = 1 - \mathbb{P}\left(\bigcup_{n\in\N} A_n^c \right) = 1 - \lim_{n\to\infty}\mathbb{P}\left(A_n^c\right) = \lim_{n\to\infty}\left[1 - \mathbb{P}\left(A_n^c\right) \right] = \lim_{n\to\infty}\mathbb{P}(A_n)
$$
</details>
</MathBox>

<MathBox title='Bounded limits of probability' boxType='proposition'>
Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space. A sequence of events $(A_n\in\mathcal{F})_{n\in\N}$ has the following the following bounded limits.
1. $\mathbb{P}\left(\limsup_{n\to\infty} A_n \right) = \lim_{n\to\infty}\mathbb{P}\left(\bigcup_{i=n}^\infty A_i \right)$
2. $\mathbb{P}\left(\liminf_{n\to\infty} A_n \right) = \lim_{n\to\infty}\mathbb{P}\left(\bigcap_{i=n}^\infty A_i \right)$
</MathBox>

<MathBox title='Borel-Cantelli lemmas' boxType='lemma'>
Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space. 
1. **First Borel-Cantelli lemma:** Suppose $(A_n \in\mathcal{F})_{n\in\N}$ is a sequence of events. If $\sum_{n\in\N}\mathbb{P}(A_n)<\infty$ then $\mathbb{P}\left(\limsup_{n\to\infty}A_n\right) = 0$.
2. **Second Borel-Cantelli lemma:** Suppose $(A_n \in\mathcal{F})_{n\in\N}$ is a sequence of independent events. If $\sum_{n\in\N}\mathbb{P}(A_n) = \infty$ then $\mathbb{P}\left(\limsup_{n\to\infty}A_n\right) = 1$.
<details>
<summary>Proof</summary>

1. Recall that $\mathbb{P}\left(\limsup_{n\to\infty} A_n\right) = \lim_{n\to\infty}\mathbb{P}\left(\bigcup_{i=n}^\infty A_i\right)$. Boole's inequality gives 

$$
  \mathbb{P}\left(\bigcup_{i=n}^\infty A_i)\leq\sum_{i=n}^\infty \mathbb{P}(A_i)
$$

Since $\sum_{i=n}^\infty \mathbb{P}(A_i)$, it follows that $\sum_{i=n}^\infty \mathbb{P}(A_i) \xrightarrow{n\to\infty} 0$.

2. Note that $\mathbb{P}\left(\limsup_{n\to\infty} A_n\right) = 1 - \mathbb{P}\left[\left(\limsup_{n\to\infty} A_n\right)^c\right]$ and recall that $\left(\limsup_{n\to\infty}A_n\right)^c = \liminf_{n\to\infty} A_n^c$.

$$
  \mathbb{P}\left[\left(\limsup_{n\to\infty} A_n\right)^c\right] = \mathbb{P}\left(\liminf_{n\to\infty} A_n^c) = \lim_{n\to\infty}\mathbb{P}\left(\bigcap_{i=n}^\infty A_i^c)
$$

Recall that $1 - x \leq e^{-x}$ for every $x\in\R$, so that $1 - \mathbb{P}(A_i)\leq\exp\left[-\mathbb{P}(A_i)\right]$ for each $i\in\N$.

$$
  \mathbb{P}\left(\bigcap_{i=n}^\infty A_i^c) = \prod_{i=n}^\infty \mathbb{P}(A_i^c) = \prod_{i=1}^\infty\left[1 - \mathbb{P}(A_i) \right] \leq \prod_{i=n}^\infty\exp\left[-\mathbb{P}(A_i)\right] = \exp\left( -\sum_{i=n}^\infty\mathbb{P}(A_i)\right) = 0 \right) 
$$
</details>
</MathBox>

<MathBox title='Tail $\sigma$-algebra' boxType='definition'>
Let $(X_n)_{n\in\N}$ be a sequence of random variables. The tail $\sigma$-algebra of the sequence is 

$$
  \mathcal{T} = \bigcap_{n\in\N}\sigma\Set{X_n,X_{n+1},\dots}
$$

An event $B\in\mathcal{T}$ is a tail event for the sequence. A random variable $Y$ that is measurable with respect to $\mathcal{T}$ is a tail random variable for the sequence.
</MathBox>

<MathBox title='Kolmogorov zero-one theorem' boxType='theorem'>
Suppose $\mathbf{X} = (X_n)_{n\in\N}$ is an independent sequence of random variables.
1. If $B$ is a tail event for $\mathbf{X}$ then $\mathbb{P}(B) = 0$ or $\mathbb{P}(B) = 1$.
2. If $Y$ is a real-valued tail random variable for $\mathbf{X}$ then $Y$ is constant with probability $1$.
<details>
<summary>Proof</summary>

1. By definition $B\in\sigma\Set{X_{n+i}}_{i\in\N}$ for each $n\in\N}$, so $\Set{X_i}_{i=1}^n \cup \Set{\mathbf{1}_B}$ is an independent set of random variables. Thus $\Set{X_n}_{n\in\N}\cup\Set{\mathbf{1}_B}$. However, $B\in\sigma\Set{X_n}_{n\in\N}$ so it follows that $B$ is independent itself. Therefore $\mathbb{P}(0)$ or $\mathbb{P}(1)$.
2. The function $y\mapsto \mathbb{P}(Y\leq y)$ on $\R$ is the cumulative distribution function of $Y$. This function is clearly increasing, and it can be shown that it is right continuous and that $\mathbb{P}(Y\leq y)\xrightarrow{y\to-\infty}0$ and $\mathbb{P}(Y\leq y)\xrightarrow{y\to\infty}1$. However, since $Y$ is a tail random variable, $\Set{Y\leq y}$ is a tail event and hence $\mathbb{P}(Y\leq y)\in\Set{0,1}$ for each $y\in\R$. It follows that there exists $c\in\R$ such that 

$$
  \mathbb{P}(Y\leq y) = \begin{cases} 0, &\quad y<c \\ 1, &\quad y\geq c \end{cases}
$$

Hence $\mathbb{P}(Y=c) = 1$.
</details>
</MathBox>

## Convergence in distribution

This section uses the notation $\N_+^* = \N_+ \cup \Set{\infty}$.

<MathBox title='Convergence in distribution' boxType='definition'>
1. Suppose $P_n$ is a probability measure on the measurable space $(S,\mathcal{S})$ for each $n\in\N_+^*$. Then $P_n$ converges (weakly) to $P_\infty$ as $n\to\infty$ if $P_n (A) \xrightarrow{n\to\infty} P_\infty (A)$ for every $A\in\mathcal{S}$ with $P_\infty (\partial A) = 0$. 
2. Suppose $X_n$ is a random variable with distribution $P_n$ on $(S,\mathcal{S})$ for each $n\in\N_+^*$. Then $X_n$ converges in distribution to $X_\infty$ as $\n\to\infty$ if $P_n \xrightarrow{n\to\infty} P_\infty$.

<details>
<summary>Notes</summary>

1. This definition makes sense since $A\in\mathcal{S}$ implies $\partial A = \mathrm{cl}(A)\setminus\mathrm{int}(A)\in\mathcal{S}$. Specifically, $\mathrm{cl}(A)\in\mathcal{S}$ because $\mathrm{cl}(A)$ is closed, and $\mathrm{int}(A)\in\mathcal{S}$ because $\mathrm{int}(A)$ is open.
</details>
</MathBox>

<MathBox title='Convergence in probability implies convergence in distribution' boxType='proposition'>
Suppose $X_n$ is a random variable with values on $S$ for each $n\in\N_+$ defined on a probability space $(\Omega,\mathcal{F},\mathbb{P})$. If $X_n \xrightarrow{n\to\infty} X_\infty$ in probability then $X_n \xrightarrow{n\to\infty} X_\infty$ in distribution.

<details>
<summary>Proof</summary>

Recall that convergence in probability means that $\mathrm{P}[d(X_n, X_\infty) > \varepsilon]\xrightarrow{n\to\infty}0$ for every $\varepsilon > 0$.
</details>
</MathBox>

<MathBox title="Skorohod's representation theorem" boxType='theorem'>
Suppose $P_n$ is a probability measure on the measurable space $(S,\mathcal{S})$ for each $n\in\N_+^*$ and that $P_n \xrightarrow{n\to\infty}$. Then there exists a random variable $X_n$ with values in $S$ for each $n\in\N_+^*$, defined on a common probability space $(\Omega,\mathcal{F},\mathbb{P})$, such that

1. $X_n$ has distribution $P_n$ for $n\in\N_+^*$
2. $X_n\xrightarrow{n\to\infty} X_\infty$ with probability $1$.

<details>
<summary>Proof</summary>

The theorem is proved in the case $(S,\mathcal{S}) = (\R, \mathcal{B}(\R))$. Let $U$ be a random variable defined on $(\Omega,\mathcal{F},\mathbb{P})$ that is uniformly distributed on the interval $(0, 1)$. For a specific construction, we could take $\Omega = (0, 1)$ , $\mathcal{F}$ the $\sigma$-algebra of Borel measurable subsets of $(0, 1)$ and $\mathbb{P}$ the Lebesgue measure on $(\Omega, \mathcal{F})$. The let $U$ be the identity function on $\Omega$, i.e. $U(\omega) = \omega\in\Omega$, so that $U$ has the probability distribution $\mathbb{P}$.

1. For $n\in\N_+^*$, let $F_n$ denote the distribution function of $P_n$ and define $X_n = F_n^{-1}(U)$ where $F_n^{-1}$ is the quantile function of $F_n$. Recall that $X_n$ has distribution function $F_n$ and therefore $X_n$ has distribution $P_n$ for $n\in\N_+^*-$.
2. Let $\varepsilon > 0$ and let $u\in(0,1)$. Pick a continuity point $x$ of $F_\infty$ such that $F_\infty^{-1}(u)-\varepsilon < x < F_\infty^{-1}(u)$. Then $F_\infty(x) < u$ and hence $F_n(x) < u$ for all but finitely many $n\in\N_+$. It follows that $F_\infty^{-1}(u) - \varepsilon < x < F_n^{-1}(u)$ for all but finitely many $n\in\N$. Let $n\to\infty$ and $u\downarrow 0$ to conclude that $F_\infty^{-1}(u) \leq \liminf_{n\to\infty} F_n^{-1}(u)$. Next, let $v$ satisfy $0 < u < v < 1$ and let $\varepsilon > 0$. Pick a continuity point $x$ of $F_\infty$ such that $F_\infty^{-1}(v) < x < F_\infty^{-1}(v) + \varepsilon$. Then $u < v < F_\infty(x)$ and hence $u < F_n(x)$ for all but finitely many $n\in\N_+$. It follows that $F_n^{-1}(u) \leq x < F_\infty^{-1}(v) + \varepsilon$ for all but finitely many $n\in\N_+$. Let $n\to\infty$ and $\varepsilon\downarrow 0$ to conclude that $\limsup_{n\to\infty} F_n^{-1}(u) \leq F_\infty^{-1} (v)$. Letting $v\downarrow u$ it follows that $\limsup_{n\to\infty} F_n^{-1}(u) \leq F_\infty^{-1} (u)$ if $u$ is a point of continuity of $F_\infty^{-1}$. Therefore $F_n^{-1}(u)\xrightarrow{n\to\infty} F_\infty^{-1}(u)$ if $u$ is a point of continuity of $F_\infty^{-1}$. Since $F_\infty^{-1}(u)$ is increasing, the set $D\subseteq(0, 1)$ of discontinuities of $F_\infty^{-1}$ is countable. Since $U$ has a continuous distribution, $\mathbb{P}(U\in D) = 0$. Finally, it follows that $\mathbb{P}\left(X_n \xrightarrow{n\to\infty} X_\infty\right) = 1$. 
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Suppose $X_n$ is a is a random value with values in $S$ for each $n\in\N_+^*$ (not necessarily defined on the same probability space). Suppose also that $g: S\to T$ is measurable, and let $D_g$ denote the set of discontinuities of $g$ and $P_\infty$, and $P_\infty$ the distribution of $X_\infty$. If $X_n\xrightarrow{n\to\infty}$ in distribution and $P_\infty(D_g) = 0$, then $g(X_n)\xrightarrow{n\to\infty} g(X_\infty)$ in distribution.

<details>
<summary>Proof</summary>

By Skorohod's theorem, there exists random variables $Y_n$ with values in $S$ for $n\in\N_+^*$, defined on the same probability space $(\Omega,\mathcal{F},\mathbb{P})$ such that $Y_n$ has the same distribution as $X_n$ for $n\in\N_+^*$, and $Y_n \xrightarrow{n\to\infty} Y_\infty$ with probability $1$. Since $\mathbb{P}(Y_\infty \in D_g) = P_\infty(D_g) = 0$ it follows that $g(Y_n) \xrightarrow{n\to\infty} g(Y_\infty)$ with probability $1$. Thus $g(Y_n) \xrightarrow{n\to\infty} g(Y_\infty)$ in distribution. However, $g(Y_n)$ has the same distribution as $g(X_n)$ for each $n\in\N_+^*$.
</details>
</MathBox>

<MathBox title="Scheffe's theorem" boxType='theorem'>
Suppose $(S,\mathcal{S},\mu)$ is a measure space, $P_n$ is a probability measure on $(S,\mathcal{S})$ with density function $f_n$ with respect to $\mu$ for each $n\in\N_+$, and $P$ is a probability measure on $(S,\mathcal{S})$ that has density function $f$ with respect to $\mu$. If $f_n(x) \xrightarrow{n\to\infty} f(x)$ for almost all $x\in S$ with respect to $\mu$, then $P_n (A)\xrightarrow{n\to\infty} P(A)$ uniformly in $A\in\mathcal{S}$.

<details>
<summary>Proof</summary>

From basic properties of the integral

$$
\begin{align*}
  |P(A) - P_n(A)| &= \left| \int_A f\;\d\mu - \int_A f_n\;\d\mu \right| \\
  &= \left| \int_A (f - f_n)\;\d\mu \right| \\
  &\leq \int_A |f - f_n|\;\d\mu \leq \int_S |f - f_n|\;\d\mu
\end{align*}
$$

Let $g_n = f - f_n = g_n^+ - g_n^-$, where $g_n^+$ and $g_n^-$ are the usual positive and negative parts of $g_n$. Note that $g_n^+ \leq f$ and $g_n^+ \xrightarrow{n\to\infty} 0$ almost everywhere on $S$. Since $f$ is a probability density function, it is trivially integrable, so by the dominated convergence theorem, $\int_S g_n^+\;\d\mu\xrightarrow{n\to\infty} 0$. Since $\int_{\R} g_n\;\d\mu = 0$, it follows that $\int_{\R} g_n^+\;\d\mu = \int_{\R} g_n^-\;\d\mu$. Therefore $\int_S |g_n|\;\d\mu = 2\int_S g_n^+\;\d\mu \xrightarrow{n\to\infty}0$. Hence $P_n(A)\xrightarrow{n\to\infty}P(A)$ is uniformly in $A\in\mathcal{S}$.  
</details>
</MathBox>

# Moments of random variables

<MathBox title='Moment' boxType='definition'>
If $a\in\R$ and $n\in\N$, the moment of $X$ about $a$ of order $n$ is defined as

$$
  \mathbb{E}\left[(X - a)^n \right]
$$
</MathBox>

## Expected value

The expected value (mean) of a real-valued random variable gives a measures of the center of the variables' distribution. In the following discussion, let $(\Omega, \mathcal{F}, \mathbb{P})$ and $(S,\mathcal{S},\mathbb{P})$ be probability spaces, and $X: \Omega\to S$ a random variable.

<MathBox title='Expected value' boxType='definition'>
The expected of $X$ is defined as the Lebesgue integral

$$
  \mathrm{E}[X] \equiv \mathbb{E}(X) := \int_{\Omega} X \;\d\mathbb{P}
$$

Depending on $S$ the expected value can take the following forms:
1. If $S$ is finite, then the expectation reduces to the simple function $\mathbb{E}(X) = \sum_{x\in S} x\mathbb{P}(X = x)$
2. If $S\subseteq[0,\infty)$, then $\mathbb{E}(X) = \sup\Set{\mathbb{E}(Y) : Y\textrm{ has finite range and }0\leq Y \leq X }$
3. For general $S\subseteq\R$, then $\mathbb{E}(X) = \mathbb{E}(X^+) - \mathbb{E}(X^-)$ as long as the right side is not of the form $\infty - \infty$. Here $X_+ := \max(X, 0)$ and $X_- := \max(-X, 0)$ denote the positive and negative part of $X$, respectively.
4. If $A\in\subseteq\mathcal{F}$ then $\mathbb{E}(X;A) = \mathbb{E}(X\mathbf{1}_A)$ assuming that the expected value on the right exists. 
</MathBox>

<MathBox title='Properties of expectation' boxType='proposition'>
Let $X, Y : \Omega\to\R$ be random variables with $\mathbb{E}(|X|), \mathbb{E}(|Y|)\leq\infty$.
- Linearity: $\mathbb{E}(aX + bY) = a\mathbb{E}(X) + b\mathbb{E}(Y)$ for all $a, b \in \R$.
- Positivity: If $\mathbb{P}(X\geq 0) 1$ then $\mathbb{E}(X) \geq 0$ and $\mathbb{E}(X) = 0$ if and only if $\mathbb{P}(X=0) = 1$.
- Monotonicity: If $X \overset{a.s.}{\leq} Y$, i.e. $\mathbb{P}\left( X\leq Y \right) = 1$, then $\mathbb{E}(X) \leq \mathbb{E}(Y)$.
- If $\mathbb{P}(X = Y)$, then $\mathbb{E}(X) = \mathbb{E}(Y)$.
- If $X, Y$ are independent, then $\mathbb{E}(XY) = \mathbb{E}(X)\mathbb{E}(Y)$.
- If $X$ is symmetric about $a\in\R$, then $\mathbb{E}(X) = a$.
- Absolute value inequality: $|\mathbb{E}(X)| \leq \mathbb{E}(|X|)$.
  - If $\mathbb{E}(X)$ is finite, the equality holds if and only if $\mathbb{P}(X\geq 0) = 1$ or $\mathbb{P}(X\leq 0) = 1$.
- Hölder's inequality: If $p, q\in[1, \infty]$ with $\frac{1}{p} + \frac{1}{q}$, then $\mathbb{E}(|XY|) \leq \norm{ X\|_p \cdot} Y\|_q = \left[\mathbb{E}(|X|^p)\right]^{1/p} \left[\mathbb{E}(|X|^q)\right]^{1/q}$.

<details>
<summary>Proof</summary>

**Symmetry property:** By assumption, the distribution of $X - a$ is the same as the distribution of $a - X$ giving 

$$
\begin{align*}
  \mathbb{E}(a - X) &= \mathbb{E}(X - a) \\
  a - \mathbb{E}(X) &= \mathbb{E}(X) - a \\
  2\mathbb{E}(X) &= 2a \\
  \mathbb{E}(X) &= a
\end{align*}
$$
</details>
</MathBox> 

<MathBox title='Change of variables theorem' boxType='theorem'>
Suppose $P$ is the probability distribution of $X$. If $g:S\to\R$ is measurable then, assuming that the expected value exists

$$
  \mathbb{E}[g(X)] = \int_\Omega g[X(\omega)]\;\d\mathbb{P}(\omega) = \int_S g(x)\;\d P(x)
$$
</MathBox> 

<MathBox title='Law of the unconscious statistician' boxType='theorem'>

Suppose $\mu$ is a positive measure on $(S,\mathcal{S})$, and that the distribution of $X:\Omega\to S$ is absolutely continuous with respect to $\mu$, i.e. $\mu(A) = 0 \implies P(A) = \mathbb{P}(X\in A) = 0$ for $A\in\mathcal{S}$. By the Radon-Nikodym theorem, $X$ has a probability density function $f:X(A)\to\R$ such that

$$
  P(A) = \mathbb{P}(X\in A) = \int_A f\;\d\mu,\quad A\in\mathcal{S}
$$

If $g:S\to\R$ is measurable, then by changing variables

$$
  \mathbb{E}[g(X)] = \int_\Omega g[X(\omega)]\;\d\mathbb{P}(\omega) = \int_S g(x)\;\d P(x) = \int_S g(x)f(x)\;\d\mu(x)
$$

**Discrete distributions:** 
If $(S,\mathcal{P}(S), \#)$ is a discrete measure space, $X$ has a discrete distribution on $S$. Since $\#(A) = 0 \iff A=\emptyset$ and $\mathbb{P}(X\in\emptyset) = 0$, then $X$ is always absolutely continuous with respect to $\#$. The probability density function $f$ of $X$ with respect to $\#$ is simply $f(x) = \mathbb{P}(X = x)$ for $x\in S$. In this case

$$
  \mathbb{E}[g(X)] = \sum_{x\in S} g(x)f(x)
$$

If $X$ is real-valued and $g = 1$, this reduces to the original definition of expected value in the discrete case

$$
  \mathbb{E}(X) = \sum_{x\in S}xf(x) 
$$

**Continuous distributions:**
Suppose $(\R^n,\mathcal(\R^n), \lambda_n)$ is a Euclidean measure space. The distribution of $X$ is absolutely continuous with respect to the Lebesgue measure $\lambda_n$ if $\lambda_n(A) = 0 \implies \mathbb{P}(X\in A) = A$. In this case,

$$
  \mathbb{E}[g(X)] = \int_S g(x)f(x)\;\d\lambda_n(x)
$$
</MathBox>

<MathBox title="Markov's inequality" boxType='theorem'>
If $X$ is a nonnegative random variable then

$$
  \mathbb{P}(X\geq x) \leq\frac{\mathbb{E}(X)}{x},\quad x>0
$$

As a direct corollary, if $X$ is a real-valued random variable and $k\in(0,\infty)$, then

$$
  \mathbb{P}(|X|\geq x) \leq\frac{\mathbb{E}\left(|X|^k \right)}{x^k},\quad x>0
$$

<details>
<summary>Proof</summary>

For $x > 0$, note that $x\cdot\mathbf{1}(X\geq x)\leq X$. Taking the expected value gives $x\mathbb{P}(X\geq x)\leq\mathbb{E}(X)$.

For $k\geq 0$, the function $x\mapsto x^k$ is strictly increasing on $[0,\infty)$. Applying Markov's inequality gives

$$
  \mathbb{P}(|X|\geq x) = \mathbb{P}\left(|X|^k \geq x^k) \leq\frac{\mathbb{E}\left(|X|^k \right)}{x^k}
$$

</details>
</MathBox>

<MathBox title="Jensen's inequality" boxType='proposition'>
Let $X$ be an integrable random variable with $\mathbb{E}(|X|) < \infty. $If $\phi: \R\to\R$ is a convex function and $\mathbb{E}\left[|\phi(X)|\right] < \infty$, then 

$$
  \mathbb{E}\left[\phi(X)\right]\overset{\textrm{a.s.}}{\geq}\phi\left[\mathbb{E}(X)\right]
$$

Two useful special cases are $\left|\mathbb{E}(X)\right| < \mathbb{E}(|X|)$ and $\left[\mathbb{E}(X)\right]^2 \leq \mathbb{E}\left(X^2\right)$

<details>
<summary>Proof</summary>

Recall that a convex function is the supremum of countably many affine functions, i.e. for $x\in\R$

$$
  \phi(x) = \sup_{n\in\N} (a_n x + b_n),\quad a_n, b_n\in\R
$$

Thus, for all $n\in\N$ we have $\mathbb{E}[\phi(X)] \overset{\textrm{a.s.}{\geq} a_n \mathbb{E}(X) + b_n$. Using the fact that the supremum is over a countable set we get

$$
  \mathbb{E}[\phi(X)] \geq\sup_{n\in\N}\left[a_n\mathbb{E}(X) + b_n\right] = \phi[\mathbb{E}(X)]
$$
</details>
</MathBox>

### Convergence theorems

<MathBox title='Monotone convergence theorem' boxType='theorem'>
Let $(X_n)_{n\in\N}$ be an increasing sequence of random variables with $0\leq X_n \xrightarrow{n\to\infty} X$. Then

$$
  \lim_{n\to\infty} \mathbb{E}(X_n)\overset{\textrm{a.s.}}{=}\mathbb{E}(X)
$$
</MathBox>

<MathBox title="Fatous's lemma" boxType='lemma'>
Let $(X_n)_{n\in\N}$ be an increasing sequence of random variables with $X_n \geq 0$ for all $n$. Then

$$
  \mathbb{E}(\liminf_{n\to\infty} X_n)\overset{\textrm{a.s.}}{\leq}\liminf_{n\to\infty} \mathbb{E}(X)
$$
</MathBox>

<MathBox title='Dominated convergence theorem' boxType='theorem'>
Let $Y$ be a nonnegative random variable with $\mathbb{E}(Y)<\infty$ and suppose $(X_n)_{n\in\N}$ is a sequence of random variables with $X_n \underset{n\to\infty}{\uparrow} X$. If $|X_n| \leq Y \leq\infty$ Then

$$
\begin{gather*}
  \mathbb{E}(|X|)\leq\mathbb{E}(Y) \leq\infty \\
  \lim_{n\to\infty} \mathbb{E}(X) \overset{\textrm{a.s.}}{=} \mathbb{E}(X)
\end{gather*}
$$
</MathBox>

## Variance

<MathBox title='Variance and standard deviation' boxType='definition'>
Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space and $X: \Omega\to\R$ a random variable with expected value $\mathrm{E}(X)$. The variance of $X$ is defined as 

$$
\begin{align*}
  \mathrm{var}(X) :=& \mathbb{E}\left(\left[ X - \mathbb{E}(X) \right]^2 \right) \\
  =& \mathbb{E}(X^2) - \mathbb{E}(X)^2
\end{align*}
$$

The standard deviation of $X$ is defined as

$$
  \sigma_X = \mathrm{sd}(X) := \sqrt{\mathrm{var}(X)}
$$

<details>
<summary>Details</summary>

$$
\begin{align*}
  \mathrm{var}(X) &:= \mathbb{E}\left[\left( X - \mathbb{E}(X) \right)^2 \right] \\
  &= \mathbb{E}\left[ X^2 - 2\mathbb{E}(X)\cdot X + \mathbb{E}(X)^2  \right] \\
  &= \mathbb{E}\left(X^2 \right) - \mathbb{E}\left[ 2 \mathbb{E}(X)\cdot X \right] + \mathbb{E}\left[\mathbb{E}(X)^2 \right] \\
  &= \mathbb{E}\left(X^2 \right) - 2 \mathbb{E}(X) \mathbb{E}(X) + \mathbb{E}\left[\mathbb{E}(X)^2 \right] \\
  &= \mathbb{E}\left(X^2 \right) - 2 \mathbb{E}(X)^2 + \mathbb{E}(X)^2 \underbrace{\int_\Omega 1 \;\d P}_{=P(\Omega) = 1} \\
  &= \mathbb{E}\left(X^2 \right) - \mathbb{E}(X)^2 
\end{align*}
$$
</details>
</MathBox> 

Recall that the second moment of $X$ about $a\in\R$ is $\mathbb{E}\left[ (X-a)^2 \right]$. Thus, the variance is the second moment of $X$ about the mean $\mu_X = \mathbb{E}(X)$, or equivalently, the second central moment of $X$. In general, the second moment of $X$ about $a\in\R$ represents the mean square error if $a$ is an estimate of $X$. 

<MathBox title='Properties of variance' boxType='proposition'>
1. Positive definiteness: $\mathrm{var}(X) \geq 0$ and $\mathrm{var}(X) = 0$ if and only if $\mathbb{P}(X = c) = 1$ for some constant $c = \mathbb{E}(X)$.
2. Variance of linear combinations: If $a,b\in\R$
    1. $\mathrm{var}(a + bX) = b^2\mathrm{var}(X)$
    2. $\mathrm{sd}(a + bX) = |b|\mathrm{var}(X)$

<details>
<summary>Proof</summary>

Let $\mu = \mathbb{E}(X)$

1. These results follows from the positive property of expected value. Evidently, $(X - \mu)^2 \geq 0$ with probability $1$ so $\mathbb{E}\left[(X -\mu)^2\right]\geq 0$. Additionally, $\mathbb{E}\left[(X -\mu)^2\right] = 0$ if and only if $\mathbb{P}(X = \mu) = 1$.
2. By linearity, $\mathbb{E}(a + bX) = a + b\mu$, giving
$$
\begin{align*}
  \mathrm{var}(a + bX) &= \mathbb{E}\left([(a + bX) - (a + b\mu)]^2 \right) \\
  &= \mathbb{E}\left[b^2(X - \mu)^2\right] = b^2\mathrm{var}(X)
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Standard score' boxType='definition'>
Suppose $X$ is a random variable with mean $\mu$ and variance $\sigma^2$. The *standard score* of $X$ is defined as the random variable

$$
  Z := \frac{X - \mu}{\sigma}
$$

with $\mathbb{E}(Z) = \frac{1}{\sigma}[\mathbb{E}(X) - \mu] = 0$ and $\mathrm{var}(Z) = \frac{1}{\sigma^2}\mathrm{var}(X) = 1$.
</MathBox> 

The standard score of $X$ measures the distance from $\mathbb{E}(X)$ to $X$ in terms of standard deviations.

<MathBox title='' boxType='proposition'>
Let $Z$ denote the standard score of $X$, and suppose $Y = a + bX$ where $a,b\in\R$ and $b\neq 0$. Then
1. If $b > 0$, the standard score of $Y$ is $Z$
2. If $b < 0$, the standard score of $Y$ is $-Z$

<details>
<summary>Proof</summary>

Since $\mathbb{E} = a + b\mathbb{E}(X)$ and $\mathrm{sd}(Y) = |b|\mathrm{sd}(X)$, then

$$
  \frac{Y - \mathbb{E}}{\mathrm{sd}(Y)} = \frac{b}{|b|}\frac{X - \mathbb{E}(X)}{\mathrm{sd}(X)} 
$$
</details>
</MathBox> 

<MathBox title='Coefficient of variation' boxType='definition'>
Suppose $X$ is a random variable with $\mathrm{E}(X) \neq 0$. The coefficient of variation is the ratio of the standard deviation to the mean

$$
  \mathrm{cv}(X) := \frac{\mathrm{sd}}{\mathbb{E}(X)}
$$
</MathBox> 

<MathBox title="Chebyshev's inequalities" boxType='proposition'>
Suppose $X$ is a real-valued random variable with mean $\mu_X = \mathbb{E}(X)\in\R$ and standard deviation $\sigma_X = \mathrm{sd}(X)\in(0,\infty)$

**Chebyshev's first inequality**

$$
  \mathbb{P}(|X - \mu_X|\geq t) \leq \frac{\sigma_X^2}{t^2},\quad t>0
$$

**Chebyshev's second inequality**

$$
  \mathbb{P}(|X - \mu_X|\geq k\sigma) \leq \frac{1}{k^2},\quad k>0
$$

<details>
<summary>Proof</summary>

Chebyshev's first inequality can be proved by applying Markov's inequality to the random variable $Y = (X - \mu_X)^2$ with $\mathbb{E}(Y) = \left[(X - \mu_X)^2 \right] = \sigma_X^2$

$$
  \mathbb{P}(|X - \mu|\geq t) = \frac{\mathbb{E}\left[ (X - \mu_X)^2 \right]}{t^2} = \frac{\sigma_X^2}{t^2}
$$

Chebyshev's second inequality follows by substituting $t = k\sigma$.
</details>
</MathBox>

## Skewness

<MathBox title='Skewness' boxType='definition'>
Let $X$ be a random variable with mean $\mu$ and variance $\sigma^2$. The skewness of $X$ is the third moment of the standard score of $X$

$$
\begin{align*}
  \mathrm{skew}(X) :=& \mathbb{E}\left[\left( \frac{X - \mu}{\sigma} \right)^3 \right] \\
  =& \frac{\mathbb{E}(X^3) - 3\mu\sigma^2 - \mu^3}{\sigma^3}
\end{align*}
$$

Depending on the sign of its skewness, a distribution $X$ is called
1. Positively skewed if $\mathrm{skew}(X)> 0$.
2. Negatively skewed if $\mathrm{skew}(X)< 0$.
3. Unskewed if $\mathrm{skew}(X) = 0$.

<details>
<summary>Details</summary>

Note that $(X - \mu)^3 = X^3 - 3X^2\mu + 3X\mu^2 - \mu^3$ and $\mathbb{E}(X^2) = \sigma^2 + \mu^2$. From the linearity of expected value we have

$$
\begin{align*}
  \mathbb{E}\left[(X - \mu)^3 \right] &= \mathbb{E}(X^3) - 3\mu\mathbb{E}(X^2) + 3\mu^2\mathbb{E}(X) - \mu^3 \\
  &= \mathbb{E}(X^3) - 3\mu\mathbb{E}(X^2) + 2\mu^3 \\
  &= \mathbb{E}(X^3) - 3\mu\sigma^2 - \mu^3
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Properties of skewness' boxType='proposition'>
Let $X$ be a real-valued random variable with $\mathbb{E}(X) = \mu$ and $\mathrm{var}(X) = \sigma^2$.
1. If the distribution of $X$ is symmetric $\mathrm{skew}(X) = 0$.
2. If $a\in\R$ and $b\in\R\setminus\Set{0}$
    1. $\mathrm{skew}(a + bX) = \mathrm{skew}(X)$ if $b > 0$
    2. $\mathrm{skew}(a + bX) = -\mathrm{skew}(X)$ if $b < 0$

<details>
<summary>Proof</summary>

1. Recall that if $X$ is symmetric about $a\in\R$ then $\mathbb{E}(X) = a$, such that $\mathrm{skew}(X) = \frac{\mathbb{E}\left[(X - a)^3 \right]}{\sigma^3}$. By symmetry and linearity

$$
  \mathbb{E}[(X - a)^3] = \mathbb{E}[(a - X)^3] = -\mathbb{E}[(X - a)^3]
$$

It follows that $\mathbb{E}[(X - a)^3] = 0$.

2. Let $Z = \frac{X - \mu}{\sigma}$, the standard score of $X$. Recall that the standard score of $a + bX$ is
    1. $Z$ if $b > 0$
    2. $-Z$ if $b < 0$

The result follows since skewness is defined in terms of an odd power of $Z$.
</details>
</MathBox>

## Kurtosis

Kurtosis is a measure of the fatness in the tails of a distribution.

<MathBox title='Kurtosis' boxType='definition'>
Let $X$ be a random variable with mean $\mu$ and variance $\sigma^2$. The kurtosis of $X$ is the fourth moment of the standard score of $X$

$$
\begin{align*}
  \mathrm{kurt}(X) :=& \mathbb{E}\left[\left( \frac{X - \mu}{\sigma} \right)^4 \right] \\
  =& \frac{\mathbb{E}(X^4) - 4\mu\mathbb{E}(X^3) + 6\mu^2\sigma^2 + 3\mu^4}{\sigma^4}
\end{align*}
$$

<details>
<summary>Details</summary>

Note that $(X - \mu)^4 = X^4 - 4X^3\mu + 6X^2\mu^2 - 4X\mu^3 + \mu^4$ and $\mathbb{E}(X^2) = \sigma^2 + \mu^2$. From the linearity of expected value we have

$$
\begin{align*}
  \mathbb{E}\left[(X - \mu)^3 \right] &= \mathbb{E}(X^4) - 4\mu\mathbb{E}(X^3) + 6\mu^2\mathbb{E}(X^2) - 4\mu^3\mathbb{E}(X) + \mu^4 \\
  &= \mathbb{E}(X^4) - 4\mu\mathbb{E}(X^3) + 6\mu\mathbb{E}(X^2) - 3\mu^4 \\
  &= \mathbb{E}(X^4) - 4\mu\mathbb{E}(X^3) + 6\mu^2\sigma^2 + 3\mu^4
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Invariance under linear transformations' boxType='proposition'>
If $a\in\R$ and $b\in\R\setminus\Set{0}$, then $\mathrm{kurt}(a + bX) = \mathrm{kurt}(X)$

<details>
<summary>Proof</summary>

Let $Z = \frac{X - \mu}{\sigma}$, the standard score of $X$. Recall that the standard score of $a + bX$ is
1. $Z$ if $b > 0$
2. $-Z$ if $b < 0$

The result follows since kurtosis is defined in terms of an even power of $Z$.
</details>
</MathBox>

## Covariance

Suppose $X$ and $Y$ are real-valued random variables with means $\mathbb{E}(X), \mathbb{E}(Y)$ and variances $\mathrm{var}(X),\mathrm{var}(Y)$, respectively.

<MathBox title='Covariance and correlation' boxType='definition'>
The covariance of $(X, Y)$ is defined by

$$
\begin{align*}
  \mathrm{cov}(X,Y) &:= \mathbb{E}([X-\mathbb{E}(X)][Y-\mathbb{E}(Y)]) \\
  &= \mathbb{E}(XY) - \mathbb{E}(X)\mathbb{Y}
\end{align*}
$$

and, assuming the variances are positive, the correlation of $(X, Y)$ is defined by

$$
\begin{align*}
  \rho_{X,Y} &= \mathrm{corr}(X,Y) := \frac{\mathrm{cov}(X,Y)}{\sigma_X \sigma_Y} \\
  &= \frac{\mathbb{E}(XY) - \mathbb{E}(X)\mathbb{E}(Y)}{\sqrt{\mathbb{E}(X^2) - \mathbb{E}(X)^2}\cdot\sqrt{\mathbb{E}(X^2) - \mathbb{E}(X)^2}}
\end{align*}
$$

Depending on the sign of their coveriance, $X$ and $Y$ are called:
1. Positively correlated if $\mathrm{cov}(X, Y) > 0$.
2. Negatively correlated if $\mathrm{cov}(X, Y) < 0$.
3. Uncorrelated if $\mathrm{cov}(X, Y) = 0$.

<details>
<summary>Details</summary>

$$
\begin{align*}
  \mathrm{cov}(X,Y) &= \mathbb{E}([X-\mathbb{E}(X)][Y-\mathbb{E}(Y)]) \\
  &= \mathbb{E}(XY - X\mathbb{E}[Y] - \mathbb{E}[X]Y + \mathbb{E}[X]\mathbb{E}[Y]) \\
  &= \mathbb{E}(XY) - \mathbb{E}(X)\mathbb{E}(Y) - \mathbb{E}(X)\mathbb{E}(Y) \\
  &= \mathbb{E}(XY) - \mathbb{E}(X)\mathbb{Y}
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Properties of covariance' boxType='proposition'>
1. Symmetry: $\mathrm{cov}(X,Y) = \mathrm{cov}(Y,X)$
2. Covariance generalizes variance: $\mathrm{cov}(X,X) = \mathbb{E}([X-\mathbb{E}(X)][X-\mathbb{E}(X)]) = \mathbb{E}([X - \mathbb{E}]^2) = \mathrm{var}X$
3. If $X$ and $Y$ are independent, then they are uncorrelated, i.e. $\mathrm{cov}(X,Y) = 0$.
4. Bilinearity: If $X, Y, Z$ are random variables and $a,b$ are real constants then
    1. $\mathrm{cov}(aX + bY, Z) = a\,\mathrm{cov}(X, Z) + b\,\mathrm{cov}(Y, Z)$
    2. $\mathrm{cov}(X, aY + bZ) = a\,\mathrm{cov}(X, Y) + b\,\mathrm{cov}(X, Z)$
    3. In the general case, suppose $(X_i)_{i=1}^{n\in\N}$ and $(Y_i)_{i=1}^{m\in\N}$ are sequences of random variables, and that $(a_i)_{i=1}^{n\in\N}$ and $(b_i)_{i=1}^{m\in\N}$ are constants. Then
$$
  \mathrm{cov}\left( \sum_{i=1}^n a_i X_i, \sum_{j=1}^m b_j Y_j \right) = \sum_{i=1}^n\sum_{j=1}^m a_i b_j\,\mathrm{cov}(X_i, X_j)
$$

<details>
<summary>Proof</summary>

1. This follows trivially from the definition.
2. Let $\mu = \mathbb{E}(X)$. Then $\mathrm{cov}(X, X) = \mathbb{E}\left[(X - \mu)^2 \right] = \mathrm{var}(X)$.
3. If $X$ and $Y$ are independent then $\mathbb{E}(XY) = \mathbb{E}(X)\mathbb{E}(Y)$
4.
    .1
$$
\begin{align*}
  \mathrm{cov}(aX + bY, Z) &= \mathbb{E}[(aX + bY)Z] - \mathbb{E}(aX + bY)\mathbb{E}(Z) \\
  &= \mathbb{E}(aXZ + bYZ) - [a\mathbb{E}(X) + b\mathbb{E}(Y)]\mathbb{E}(Z) \\
  &= a[\mathbb{E}(XZ) - \mathbb{E}(X)\mathbb{E}(Z)] + b[\mathbb{E}(YZ) - \mathbb{E}(Y)\mathbb{E}(Z)] \\
  &= a\mathrm{cov}(X, Z) + b\mathrm{cov}(Y, Z)
\end{align*}
$$
    2.
$$
\begin{align*}
  \mathrm{cov}(X, aY + bZ) &= \mathbb{E}[X(aY + bZ)] - \mathbb{E}(X)\mathbb{E}(aY + bZ) \\
  &= \mathbb{E}(aXY + bXZ) - \mathbb{E}(X)[a\mathbb{E}(Y) + b\mathbb{E}(Z)] \\
  &= a[\mathbb{E}(XY) - \mathbb{E}(X)\mathbb{E}(Y)] + b[\mathbb{E}(XZ) - \mathbb{E}(X)\mathbb{E}(Z)] \\
  &= a\mathrm{cov}(X, Y) + b\mathrm{cov}(X, Z)
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Properties of correlation' boxType='proposition'>
1. Symmetry: $\mathrm{cor}(X,Y) = \mathrm{cor}(Y,X)$
2. The correlation between $X$ and $Y$ is the covariance of corresponding standard scores
$$
\begin{align*}
  \mathrm{cor}(X, Y) &= \mathrm{cov}\left(\frac{X - \mathbb{E}(X)}{\mathrm{sd}(X)}, \frac{Y - \mathbb{E}(Y)}{\mathrm{sd}(Y)} \right) \\
  &= \mathbb{E}\left( \frac{X - \mathbb{E}(X)}{\mathrm{sd}(X)}\frac{Y - \mathbb{E}(Y)}{\mathrm{sd}(Y)} \right)
\end{align*}
$$
3. If $a, b\in\R$ and $b\neq 0$ then:
    1. $\mathrm{cor}(a + bX, Y) = \mathrm{cor}(X, Y)$ if $b > 0$
    2. $\mathrm{cor}(a + bX, Y) = -\mathrm{cor}(X, Y)$ if $b < 0$

<details>
<summary>Proof</summary>

1. This follows trivially from the definition.
2. From the definitions and linearity of expected value

$$
\begin{align*}
  \mathrm{cor}(X, Y) &= \frac{\mathrm{cov}(X, Y)}{\mathrm{sd}(X)\mathrm{sd}(Y)} \\
  &= \frac{\mathbb{E}([X - \mathbb{E}(X)][Y - \mathbb{E}(Y)])}{\mathrm{sd}(X)\mathrm{sd}(Y)} \\
  &= \mathbb{E}\left( \frac{X - \mathbb{E}(X)}{\mathrm{sd}(X)}\frac{Y - \mathbb{E}(Y)}{\mathrm{sd}(Y)} \right)
\end{align*}
$$

3. Let $Z$ denote the standard score of $E$. If $b > 0$, the standard score of $a + bX$ is also $Z$. If $b < 0$, the standard score of $a + bX$ is $-Z$. The result follows from $(2)$. 
</details>
</MathBox>

<MathBox title='Variance of a sum' boxType='proposition'>
If $(X_i)_{i=1}^{n\in\N}$ is a sequence of real-valued random variables then

$$
\begin{align*}
  \mathrm{var}\left(\sum_{i=1}^n X_i \right) &= \sum_{i=1}^n \sum_{j=1}^n \mathrm{cov}(X_i, X_j) \\
  &= \sum_{i=1}^n \mathrm{var}(X_i) + \sum_{\Set{(i, j): i<j}} \mathrm{cov}(X_i, X_j)
\end{align*}
$$

If all $X_i$ are pairwise uncorrelated for each $i$, then this reduces to

$$
  \mathrm{var}\left(\sum_{i=1}^n X_i\right) = \sum_{i=1}^n \mathrm{var}(X_i)
$$

<details>
<summary>Proof</summary>

$$
\begin{align*}
  \mathrm{var}\left( \sum_{i=1}^n X_i) &= \mathrm{cov}\left(\sum_{i=1}^n X_i, \sum_{j=1}^n X_j \right) \\
  &= \sum_{i=1}^j \sum_{j=1}^n \mathrm{cov}(X_i, X_j)
\end{align*}
$$

The result follows since $\mathrm{cov}(X_i, X_i) = \mathrm{var}(X_i)$ for each $i$ and $\mathrm{cov}(X_i, X_j) = \mathrm{cov}(X_j, X_i)$ for $i\neq j$.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
If $X$ and $Y$ are real-valued random variables then 

1. $\mathrm{var}(X + Y) + \mathrm{var}(X - Y) = 2[\mathrm{var}(X) + \mathrm{var}(Y)]$
2. If $\mathrm{var}(X) = \mathrm{var}(Y)$ then $X + Y$ and $X - Y$ are uncorrelated.

<details>
<summary>Proof</summary>

1. From the variance of a sum with $n=2$ we get

$$
\begin{align*}
  \mathrm{var}(X + Y) &= \mathrm{var}(X) + \mathrm{var}(Y) + 2\mathrm{cov}(X, Y) \\
  \mathrm{var}(X - Y) &= \mathrm{var}(X) + \mathrm{var}(-Y) + 2\mathrm{cov}(X, -Y) \\
  &= \mathrm{var}(X) + \mathrm{var}(Y) - 2\mathrm{cov}(X, Y)
\end{align*}
$$

Adding the variances gives the result.

2. From linearity and symmetry of covariance we get

$$
\begin{align*}
  \mathrm{cov}(X + Y, X - Y) &= \mathrm{var}(X, X) - \mathrm{cov}(X, Y) + \mathrm{cov}(Y, X) - \mathrm{cov}(Y, Y) \\
  &= \mathrm{var}(X) - \mathrm{var}(Y)
\end{align*}
$$

If $X + Y$ and $X - Y$ are uncorrelated then $\mathrm{var}(X) = \mathrm{var}(Y)$. 
</details>
</MathBox>

### The best linear predictor

Consider two random variables $X$ and $Y$, where $X$ is observable while $Y$ is not. One way to estimate $Y$ is to find the linear function from observed values of $X$ that is closest to $Y$ in mean square sense. The best linear predictor for $Y$ is a linear function of $X$ that shares the same expected value as $Y$, and whose covariance with $X$ is same as that of $Y$.

<MathBox title='Criteria of the linear predictor' boxType='proposition'>
Let $X$ and $Y$ be real-valued random variables. The random variable

$$
  L(Y\mid X) := \mathbb{E}(Y) + \frac{\mathrm{cov}(X, Y)}{\mathrm{var}(X)}[X - \mathbb{E}(X)]
$$

is the only linear function of $X$ that satisfies

1. $\mathbb{E}[L(X\mid Y)] = \mathbb{E}(Y)$
2. $\mathrm{cov}[X, L(Y\mid X)] = \mathrm{cov}(X, Y)$
    1. Equivalently, $\mathrm{cov}[Y - L(Y\mid X), U] = 0$ for every linear function $U$ of $X$

<details>
<summary>Proof</summary>

1. By linearity of the expected value
$$
\begin{align*}
  \mathbb{E}[L(Y\mid X)] &= \mathbb{E}(Y) + \frac{\mathrm{var}(X, Y)}{\mathrm{var}(X)}[\mathbb{E}(X) - \mathbb{E}(X)]
  &= \mathbb{E}(Y)
\end{align*}
$$

2. By linearity of covariance
$$
\begin{align*}
  \mathrm{cov}[X, L(Y\mid X)] &= \frac{\mathrm{cov}(X, Y)}{\mathrm{var}(X)}\mathrm{cov}(X, X) \\
  &= \frac{\mathrm{cov}(X, Y)}{\mathrm{var}(X)}\mathrm{var}(X) = \mathrm{cov}(X, Y)
\end{align*}
$$
    1. Suppose $U = a + bX$, where $a, b\in\R$, then
$$
\begin{align*}
  \mathrm{cov}[Y - L(Y\mid X), U] &= b\mathrm{cov}[Y - L(Y\mid X), X] \\
  &= b\left(\mathrm{cov}(Y, X) - \mathrm{cov}[L(Y\mid X), X]\right) = 0
\end{align*}
$$

Conversely, to show the uniquenes of $L(Y\mid X)$, suppose $U = a + bX$ satisfies 
1. $\mathbb{E}(U) = \mathbb{E}(Y)$
2. $\mathrm{cov}(X, U) = \mathrm{cov}(X, Y)$
    1. Equivalently, $\mathrm{cov}(Y - U, V) = 0$ for every linear function $V$ of $X$ 

The second equality gives 

$$
\begin{align*}
  \mathrm{cov}(X, Y) &= \mathrm{cov}(X, a + bX)= b\mathrm{cov}(X, X) = b\mathrm{var}(X) \\
  b &= \frac{\mathrm{cov}(X, Y)}{\mathrm{var}(X)}
\end{align*}
$$

The first equation gives

$$
\begin{align*}
  \mathbb{E}(Y) &= \mathbb{E}(a + bX) = a + b\mathbb{E}(X) =  \\
  a &= \mathbb{E}(Y) - b\mathbb{E}(X) = \mathbb{E}(Y) - \frac{\mathrm{cov}(X, Y)}{\mathrm{var}(X)}\mathbb{E}(X)
\end{align*}
$$

Hence $U = \mathbb{E}(Y) + \frac{\mathrm{cov}(X, Y)}{\mathrm{var}(X)}[X - \mathbb{E}(X)] = L(Y\mid X)$. Equivalently, by letting $V =  X$, then from equation $(2a)$ we get

$$
\begin{align*}
  \mathrm{cov}(Y - U, X) &= 0 \\
  \mathrm{cov}(U, X) = \mathrm{cov}(Y, X)
\end{align*}
$$

Hence $U = L(Y\mid X)$.
</details>
</MathBox>

<MathBox title='Properties of the linear predictor' boxType='proposition'>
The linear predictor $L(Y\mid X)$ has the following properties for random variables $X, Y, Z$ and constants $a, b$

1. $\mathrm{var}[L(Y\mid X)] = \frac{\mathrm{cov}^2(X, Y)}{\mathrm{var}(X)}$
2. $\mathrm{cov}[L(Y\mid X), Y] = \frac{\mathrm{cov}^2(X, Y)}{\mathrm{var}(X)}$
3. Linearity: $L(aY + bZ|X) = aL(Y\mid X) + bL(Y\mid X)$

<details>
<summary>Proof</summary>

1. 
$$
\begin{align*}
  \mathrm{var}[L(Y\mid X)] &= \left[\frac{\mathrm{cov}(X, Y)^2}{\mathrm{var}(X)}\right]^2 \mathrm{var}(X) \\
  &= \frac{\mathrm{cov}^2(X, Y)}{\mathrm{var}(X)}
\end{align*}
$$

2. 
$$
\begin{align*}
  \mathrm{cov}[L(Y\mid X), Y] &= \frac{\mathrm{cov}(X, Y)}{\mathrm{var}(X)}\mathrm{cov}(X, Y) \\
  &= \frac{\mathrm{cov}^2(X, Y)}{\mathrm{var}(X)}
\end{align*}
$$

3. This follows from linearity of expected value and covariance
$$
\begin{align*}
  L(aY + bZ|X) &= \mathbb{E}(aY + bZ) + \frac{\mathrm{cov}(X, aY + bZ)}{\mathrm{var}(X)}[X - \mathbb{E}(X)] \\
  &= a\left(\mathbb{Y} + \frac{\mathrm{cov}(X, Y)}{\mathrm{var}(X)}[X - \mathbb{E}(X)] \right) + b\left(\Z + \frac{\mathrm{cov}(X, Z)}{\mathrm{var}(X)}[X - \mathbb{E}(X)] \right) \\
  &= aL(Y\mid X) + bL(Y\mid X)
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Best linear predictor' boxType='proposition'>
The linear predictor $L(Y\mid X)$ is the linear function of $X$ that is closest to $Y$ in the mean square sense, i.e. for any linear function $U$ of $X$

$$
  \mathbb{E}\left( [Y - L(Y\mid X)]^2 \right) \leq \mathbb{E}[(Y - U)^2]
$$

Equality holds if and only if $U = L(Y\mid X)$ with probability $1$. The mean square error when $L(Y\mid X)$ is used as a predictor of $Y$ is

$$
  \mathbb{E}\left([Y - L(Y\mid X)]^2 \right) = \mathbb{var}(Y)\left[1 - \mathrn{cor}^2(X, Y) \right]
$$

Since the mean square error $\mathbb{E}\left([Y - L(Y\mid X)]^2 \right) \geq 0$, it follows that $\mathrm{cor}^2 (X, Y) \leq 1$. The sign of $\mathrm{cor}(X, Y)$ corresponds with sign of the slope in $L(Y\mid X)$.

<details>
<summary>Proof</summary>

For clearer notation, we abbreviate $L(Y\mid X)$ by $L$. Suppose $U$ is a linear function of $X$. Then

$$
\begin{align*}
  \mathbb{E}\left[(Y - U)^2 \right] &= \mathbb{E}\left([(Y - L)(L - U)]^2\right) \\
  &= \mathbb{E}\left[(Y - L)^2 \right] + 2\mathbb{E}[(Y - L)(L - U)] + \mathbb{E}\left[(L - U)^2\right]
\end{align*}
$$

Noting that $Y - L$ has mean $0$, we recognize the middle term as $\mathrm{cov}(Y - L, L - U)$. Since $L$ and $U$ are linear functions of $X$, then so is $L- U$, and $\mathrm{cov}(Y - L, L - U) = 0$. Hence

$$
\begin{align*}
  \mathbb{E}\left[(Y - U)^2 \right] &= \mathbb{E}\left[(Y - L)^2 \right] + \mathbb{E}\left[(L - U)^2\right] \\
  &\geq \mathbb{E}\left[(Y - L)^2\right]
\end{align*}
$$

Equality holds if and only if $\mathbb{E}\left[(L - U)^2\right] = 0$, if and only if $\mathbb{P}(L = U) = 1$.

Again, since $Y - L$ has mean $0$

$$
\begin{align*}
  \mathbb{E}\left[(Y - L)^2 \right] &= \mathrm{var}(Y - L) \\
  &= \mathrm{var}(Y) - 2\mathrm{cov}(L, Y) + \mathrm{var}(L) \\
  &= \mathrm{var}(Y) - \frac{\mathrm{cov}^2(X, Y)}{\mathrm{var}(X)} \\
  &= \mathrm{var}(Y)\left[1 - \frac{\mathrm{cov}^2 (X, Y)}{\mathrm{var}(X)\mathrm{var}(Y)} \right] \\
  &= \mathrm{var}(Y)\left[1 - \mathrm{cor}^2(X, Y) \right]
\end{align*}
$$
</details>
</MathBox>

## Conditional moments

<MathBox title='Conditional expectation' boxType='definition'>
Suppose $X\in\mathcal{L}_1(\Omega,\mathcal{F},\mathbb{P})$ is an integrable random variable, i.e. $\mathbb{E}(|X|) < \infty$, and let $\mathcal{G}\subseteq\mathcal{F}$ be a sub $\sigma$-algebra of $\mathcal{F}$. The conditional expectation of $X$ given $\mathcal{G}$ is the random variable $\mathbb{E}(X\mid \mathcal{G})$ defined by the following properties

1. $\mathbb{E}(X\mid \mathcal{G})$ is measurable with respect to $\mathcal{G}$
2. If $A\in\mathcal{G}$ then $\mathbb{E}[\mathbb{E}(X\mid \mathcal{G})\mathbf{1}_A] = \mathbb{E}(X\mathbf{1}_A)$
    1. Equivalently, if $Z\in\mathcal{L}_1(\Omega, \mathcal{G}, \mathbb{P})$ is $\mathcal{G}$-measurable and $\mathbb{E}(|ZX|) < \infty$ then $\mathbb{E}\left[Z \mathbb{E}(X\mid \mathcal{G})\right] = \mathbb{E}(ZX)$

If $X\in \mathcal{L}_2(\Omega,\mathcal{F},\mathbb{P})$ is square integrable, then the conditional expectation $\mathbb{E}(X\mid \mathcal{G})$ is defined as the orthogonal projection of $X$ onto the closed subspace $\mathcal{L}_2(\Omega,\mathcal{F},\mathbb{P})$. The orthogonal projection $\mathbb{E}(X\mid \mathcal{G})$ minimizes the squared difference $\mathbb{E}(X - Y)^2$ among all random variables $Y\in \mathcal{L}_2(\Omega,\mathcal{G},\mathbb{P})$.

<details>
<summary>Proof</summary>

The existence of $\mathbb{E}(X\mid \mathcal{G})$ can be established with the Radon-Nikodym theorem. Assuming $X\geq 0$, we define the finite $\sigma$-measure for $G\in\mathcal{G}$

$$
  \mu(G) = \int_G X\;\d\mathbb{P} := \mathbb{E}(X\cdot\mathbf{1}_G)
$$

By construction $P(G) = \mathbb{P}(X\in G) = 0 \implies \mu(X) = 0$, so that $\mu$ is absolutely continuous with respect to $P$, i.e. $\mu\ll P$. By the Radon-Nikodym theorem, there is a random variable $Y\in\mathcal{G} = \frac{\d\mu}{\d\mathbb{P}} := \mathbb{E}(X\mid \mathcal{G})$, giving

$$
  \int_G X\;\d\mathbb{P} = \int_G \mathbb{E}(X\mid \mathcal{G})\;\d\mathbb{P} \quad \forall G\in\mathcal{G}
$$

To show equivalence of the defining properties, first note that $(2a)$ implies $(2)$ since $Z = \mathbf{1}_A$ is $\mathcal{G}$-measurable if $A\in\mathcal{G}$. The converse, i.e $(2)$ implies $(2a)$ can be shown by bootstrapping. If $Z = \mathbf{1}_A$ for some $A\in\mathcal{G}$, then trivially $\mathbb{E}[Z\mathbb{E}(X\mid \mathcal{G})] = \mathbb{E}(ZX)$. Next, suppose $Z$ is a $\mathcal{G}$-measurable simple random variable of the form $Z = \sum_{i\in I} a_i \mathbf{1}_{A_i}$ where $I$ is a finite index set, $a_i \geq 0$ and $A_i \in\mathcal{G}$ for $i \in I$. Then

$$
\begin{align*}
  \mathbb{E}[Z\mathbb{E}(X\mid \mathcal{G})] &= \mathbb{E}\left[\sum_{i\in I} a_i \mathbf{1}_{A_i} \mathbb{E}(X\mid \mathcal{G}) \right] \\
  &= \sum_{i\in I} a_i \mathbb{E}[\mathbf{1}_{A_i}\mathbb{E}(X\mid \mathcal{G})] \\
  &= \sum_{i\in I} a_i \mathbb{E}\left(\mathbf{1}_{A_i} \right) \\
  &= \mathbb{E}\left(\sum_{i\in I} a_i \mathbf{1}_{A_i} X \right) = \mathbb{E}(ZX)
\end{align*}
$$

Next suppose $Z$ is nonnegative and $\mathcal{G}$-measurable. Then there exists a sequence of simple $\mathcal{G}$-measurable random variables $(Z_n)_{n\in\N}$ with $Z_n \underset{n\to\infty}{\uparrow} X$. By the previous step, $\mathbb{E}[Z_n \mathbb{E}(X\mid \mathcal{G})] = \mathbb{E}(Z_n X)$ for each $n$. Using the monotone convergence theorem in the limit $n\to\infty$ gives $\mathbb{E}[Z \mathbb{E}(X\mid \mathcal{G})] = \mathbb{E}(Z X)$. Finally, suppose $Z$ is a general $\mathcal{G}$-measurable random variable. Then $Z = Z^+ - Z^-$ where $Z^+$ and $Z^-$ are the positive and negative parts of $Z$. Since these parts are nonnegative and $\mathcal{G}$-measurable, then $\mathbb{E}[Z^{\pm}(X\mid \mathcal{G})] = \mathbb{E}(Z^{\pm}X)$. Hence

$$
\begin{align*}
  \mathbb{E}[Z\mathbb{E}(X\mid \mathcal{G})] &= \mathbb{E}\left[(Z^+ - Z^-)\mathbb{E}(X\mid \mathcal{G}) \right] \\
  &= \mathbb{E}[Z^+\mathbb{E}(X\mid \mathcal{G})] - \mathbb{E}[Z^-\mathbb{E}(X\mid \mathcal{G})] \\
  &= \mathbb{E}(Z^+) - \mathbb{E}(Z^-) = \mathbb{E}(ZX)
\end{align*}
$$
</details>
</MathBox>

For a countably generated $\mathcal{G} = \sigma(Y)$ when $Y$ is a discrete variable, such that the space $\Omega$ is partitioned into disjoint sets $\Omega = \bigcup_n G_n$, the conditional expectation of a random variable $X$ given $ \sigma(Y)$ is

$$
  \mathbb{E}[X|\sigma(Y)] \overset{a.s.}{=} \sum_n \frac{\mathbb{E}(Y\mathbf{1}_{X=x_n})}{\mathbb{P}(X=x_n)}\mathbf{1}_{X=x_n} =\sum_n \frac{\mathbb{E}(X\mathbf{1}_{G_n})}{P(G_n)}\mathbf{1}_{G_n}
$$

Suppose $X:\Omega\to S\subseteq\R$ and $Y:\Omega\to T\subseteq\R^n$ are Lebesgue-measurable random variables and that $(X,Y)$ has a joint continuous distribution with probability density function $f$. Then $Y$ has probability density function $h$ given by

$$
  h(y) = \int_S f(x,y)\d x,\quad y\in T
$$

Assuming $h(y) > 0$ for $y\in T$, a conditional probability density function of $X$ given $Y=y$ is defined by

$$
  g(x\mid y) = \frac{f(x,y)}{h(y)}
$$

If $\mathbb{E}(|X|)< \infty$, the conditional expectation of $X$ given $\sigma(Y)$ is

$$
  \mathbb{E}(X\mid Y) := \mathbb{E}[X|\sigma(Y)] = \int_S xg(X\mid \sigma(Y)),\d x
$$

<details>
<summary>Proof</summary>

We first show that the integral is measurable with respect to $\sigma(Y)$. Since $y\mapsto \int_S xg(x\mid y)\;\d x$ is measurable as a function from $T$ into $\R$, the random variable $\int_x g(x\mid Y)\;\d x$ is a measurable function of $Y$ and so is measurable with respect to $\sigma(Y)$. Next suppose $B\in\sigma(Y)$. Then $B = \Set{Y\in A}$ for some $A\in\mathcal{F}$. Then

$$
  \mathbb{E}\left[\mathbf{1}_B \int_S xg(x\mid Y)\;\d x \right] &= \mathbb{E}\left[\mathbf{1}_{Y\in A} \int_S xg(x\mid Y)\;\d x \right] \\
  &= \mathbb{E}\left[\mathbf{1}_{Y\in A} \int_S x\frac{f(x,y)}{h(y)}\;\d x \right] \\
  &= \int_A \int_S x\frac{f(x, y)}{h(y)}h(y)\;\d x\;\d y \\
  &= \int_{S\times A} xf(x,y)\d(x,y) \\
  &= \mathbb{E}(\mathbf{1}_{Y\in A}X) = \mathbb{E}(\mathbf{1}_B X)
$$
</details>

<MathBox title='Properties of conditional expectation' boxType='proposition'>
Suppose $X, Y:\Omega\to\R$ are integrable random variables

- Expectation rule: $\mathbb{E}[\mathbb{E}(X\mid \mathcal{G})] = \mathbb{E}(X)$
- Positivity: $X \geq 0 \implies \mathbb{E}(X\mid \mathcal{G}) \geq 0$
- Linearity: $\mathbb{E}(aX + bY|\mathcal{G}) = a\mathbb{E}(X\mid \mathcal{G}) + b\mathbb{E}(X\mid \mathcal{G})$ for $a,b\in\R$
- Monotonicity: $X \leq Y \implies \mathbb{E}(X\mid \mathcal{G})\leq\mathbb{E}(X\mid \mathcal{Y})$
- Independence property: If $X$ is independent of $\mathcal{G}$ then $\mathbb{E}(X\mid \mathcal{G}) = \mathbb{E}(X)$
- $\mathcal{L}_p$-contractivity: If $X\in \mathcal{L}_p$ then $\mathbb{E}(X\mid \mathcal{G}) \in\mathcal{L}_p$ and $\norm{ \mathbb{E}(X\mid \mathcal{G})\|_p \leq} X \|_p$
- Stability: if $Y$ is a $\mathcal{G}$-measurable with $\mathbb{E}(|XY|)<\infty$, then $\mathbb{E}(XY|\mathcal{G}) = Y\mathbb{E}(X\mid \mathcal{G})$
- Tower property: $\mathcal{H}\subseteq\mathcal{G} \implies \mathbb{E}\left[\mathbb{E}(X\mid \mathcal{H})|\mathcal{G}\right] = \mathbb{E}(X\mid \mathcal{H}) = \mathbb{E}\left[\mathbb{E}(X\mid \mathcal{G})|\mathcal{H}\right]$

<details>
<summary>Proof</summary>

**Law of total expectation:** This follows immediately by letting $A = \Omega$ in the definition.

**Positivity:** Let $A = \Set{\mathbb{E}(X| \mathcal{G}) < 0 }$. Note that $A\in\mathcal{G}$ and thus $\mathbb{E}(X\mathbf{1}_A) = \mathbb{E}[\mathbb{E}(X\mid \mathcal{G})\mathbf{1}_A]$. Since $X\overset{\textrm{a.s}}{\geq} 0$ we have $\mathbb{E}(X\mathbf{1}_A)\geq 0$. Conversely, if $\mathbb{P}(A) > 0$ then $\mathbb{E}[\mathbb{E}(X\mid \mathcal{G})\mathbf{1}_A]<0$ which is a contradiction. Hence, we must have $\mathbb{P}(A) = 0$.

**Monotonicity:** Note that if $X\leq Y$ then $Y - X\geq 0$. Thus, by the positivity and linearity properties 

$$
\begin{gather*}
  \mathbb{E}(Y - X|\mathcal{G}) = \mathbb{E}(Y\mid \mathcal{G}) - \mathbb{E}(Y\mid \mathcal{G}) \geq 0 \\
  \mathbb{E}(Y\mid \mathcal{G})\geq\mathbb{E}(Y\mid \mathcal{G})
\end{gather*}
$$

**Linearity:** Note that $\mathbb{E}(|aX + bY|) \leq |a|\mathbb{E}(|X|) + |b|\mathbb{E}(|Y|)$ so $\mathbb{E}(aX + bY|\mathcal{G})$ is defined. It remains to show that $a\mathbb{E}(X\mid \mathcal{G}) + b\mathbb{E}(X\mid \mathcal{G})$ satisfy the conditons for conditional expectation. Clearly, the sum is $\mathcal{G}$-measurable since both terms are. If $A\in\mathcal{G}$ then

$$
\begin{align*}
  \mathbb{E}\left(\left[a\mathbb{E}(X\mid \mathcal{G}) + b\mathbb{E}(X\mid \mathcal{G})\right]\mathbf{1}_A\right) &= a\mathbb{E}\left[\mathbb{E}(X\mid \mathcal{G})\mathbf{1}_A \right] + b\mathbb{E}\left[\mathbb{E}(X\mid \mathcal{G})\mathbf{1}_A \right] \\
  &= a\mathbb{E}(X\mathbf{1}_A) + b\mathbb{E}(Y\mathbf{1}_A) \\
  &= \mathbb{E}(aX\mathbf{1}_A) + \mathbb{E}(bY\mathbf{1}_A) \\
  &= \mathbb{E}([aX + bY]\mathbf{1}_A)
\end{align*}
$$

**Independence property:** We show that $\mathbb{E}(X)$ satisfy the conditions for conditional expectation. Clearly, $\mathbb{E}(X)$ is $\mathcal{G}$-measurable as a constant random variable. If $A\in\mathcal{G}$ then $X$ and $\mathbf{1}_A$ are independent and hence

$$
  \mathbb{E}(X\mathbf{1}_A) = \mathbb{E}(X)\mathbb{P}(A) = \mathbb{E}[\mathbb{E}(X)\mathbf{1}_A]
$$

**$\mathcal{L}_p$-contractivity:** Note that $|\mathbb{E}(X\mid \mathcal{G})|\leq \mathbb{E}(|X|\,|\mathcal{G})$. Since $t\mapsto t^p$ is increasing and convex on $[0, \infty)$ we have 

$$
  |\mathbb{E}(X\mid \mathcal{G})|^k \leq [\mathbb{E}(|X|\,|\mathcal{G})]^2 \leq \mathbb{mathbb{E}\left(|X|^k \,\mathcal{G} \right)}
$$

where the last step follows from Jensen's inequality. Taking expected values gives

$$
  \mathbb{E}\left[|\mathbb{E}(X\mid \mathcal{G})|^k \right] \leq \mathbb{E}\left(|X|^k \right)< \infty
$$

In the case $p = \infty$, it follows that $0\leq |X|\leq \| X\|_\infty\mathbf{1}_\Omega$ if $X\in\mathcal{L}_\infty$, and thus $0\leq\mathbb{E}(|X| \mid \mathcal{G})\leq\| X\|_\infty \mathbf{1}_\Omega$. Hence $\mathbb{E}(|X| \mid \mathcal{G})\in \mathcal{L}_\infty$ and $\|\mathbb{E}(|X|\;\mathcal{G})\|_\infty \leq\| X\|_\infty$.

**Stability:** We need to show that, for all $A\in\mathcal{G}$

$$
  \mathbb{E}(XY\mathbf{1}_A) = \mathbb{E}\left[Y\mathbb{E}(X\mid \mathcal{G})\mathbf{1}_A \right]
$$

This can be shown by proving 

$$
  \mathbb{E}(ZX) = \mathbb{E}[Z\mathbb{E}(X\mid \mathcal{G})]
$$

where $Z$ is $\mathcal{G}$-measurable with $\mathbb{E}(|ZX|)<\infty$. This holds for $Z = \sum_{k=1}^{n\in\N}\alpha_k \mathbf{1}_{A_k}$ by the linearity property. Assuming that both $Z$ and $X$ are nonnegative, we can find an increasing sequence $(Z_n)_{n\in\N}$ such that $0\leq Z_n\xrightarrow{n\to\infty} Z$. Then $\mathbb{E}(|Z_n X|) < \infty$ for all $n\in\N$ and the monotone convergence theorem implies that

$$
\begin{align*}
  \mathbb{E}(ZX) &= \lim_{n\to\infty} \mathbb{E}(Z_n X) \\
  &= \lim_{n\to\infty}\mathbb{E}[Z_n \mathbb{E}(X\mid \mathcal{G})] \\
  &= \mathbb{E}[Z\mathbb{E}(X\mid \mathcal{G})]
\end{align*}
$$

Assuming $X\in \mathcal{L}_+^1$, the $\mathcal{L}_p$-contractivity for $p=1$ implies that

$$
  |\mathbb{E}(X\mid \mathcal{G})| \leq\mathbb{E}(|X|\; |\mathcal{G})
$$

and thus

$$
  \left| Z_n\mathbb{E}(X\mid \mathcal{G})\right| \leq Z_n \mathbb{E}(|X|\; |\mathcal{G}) \leq Z\mathbb{E}(|X|\; |\mathcal{G})
$$

It follows that $\mathbb{E}[Z\mathbb{E}(|X| \mid \mathcal{G})] = \mathbb{E}(Z\mid X|) < \infty$. Applying the dominated convergence theorem we get

$$
\begin{align*}
  \mathbb{E}[Z\mathbb{E}(|X| \mid \mathcal{G})] &= \lim_{n\to\infty} \mathbb{E}\left[Z_n \mathbb{E}(X\mid \mathcal{G}) \right] \\
  &= \lim_{n\to\infty}\mathbb{E}(Z_n X) = \mathbb{E}(ZX)
\end{align*}
$$

The case of a general $Z$ follows by linearity. The stability property follows by taking $Z = Y\mathbf{1}_A$

**Tower propery:** Note that $\mathbb{E}(X\mid \mathcal{H})$ is $\mathcal{H}$-measurable and thus also $\mathcal{G}$-measurable. Hence by the expectation rule $\mathbb{E}\left[\mathbb{E}(X\mid \mathcal{H})|\mathcal{G}\right] = \mathbb{E}(X\mid \mathcal{H})$. 

Conversely, we show that $\mathbb{E}(X\mid \mathcal{H})$ satisfy the conditions for $\mathbb{E}\left[\mathbb{E}(X\mid \mathcal{G})|\mathcal{H}\right]$. Clearly, $\mathbb{E}(X\mid \mathbb{H})$ is $\mathcal{H}$-measurable. If $A\in\mathcal{H}$ then $A\in\mathcal{G}$ and hence

$$
  \mathbb{E}\left[\mathbb{E}(X\mid \mathcal{G})\mathbf{1}_A \right] = \mathbb{E}(X\mathbf{1}_A) = \mathbb{E}\left[\mathbb{E}(X\mid \mathcal{H})\mathbf{1}_A\right]
$$
</details>
</MathBox>

<MathBox title="Conditional Jensen's inequality" boxType='theorem'>
If $\phi: \R\to\R$ is a convex function and $\mathbb{E}\left[|\phi(X)|\right] < \infty$, then 

$$
  \mathbb{E}\left[\phi(X)|\mathcal{G}\right] \overset{\textrm{a.s.}}{\geq} \phi\left[\mathbb{E}(X\mid \mathcal{G})\right]
$$
<details>
<summary>Proof</summary>

Recall that a convex function is the supremum of countably many affine functions, i.e. for $x\in\R$

$$
  \phi(x) = \sup_{n\in\N} (a_n x + b_n),\quad a_n, b_n\in\R
$$

Thus, for all $n\in\N$ we have $\mathbb{E}[\phi(X)|\mathcal{G}] \overset{\textrm{a.s.}{\geq} a_n \mathbb{E}(X\mid \mathcal{G}) + b_n$. Using the fact that the supremum is over a countable set we get

$$
  \mathbb{E}[\phi(X)|\mathcal{G}] \geq\sup_{n\in\N}\left[a_n\mathbb{E}(X\mid \mathcal{G}) + b_n\right] = \phi[\mathbb{E}(X\mid \mathcal{G})]
$$
</details>
</MathBox>

### Conditional convergence theorems

<MathBox title='Conditional monotone convergence theorem' boxType='theorem'>
Let $(X_n)_{n\in\N}$ be an increasing sequence of random variables with $0\leq X_n \xrightarrow{n\to\infty} X$. Then

$$
  \lim_{n\to\infty} \mathbb{E|\mathcal{G}}(X_n) \overset{\textrm{a.s.}}{=} \mathbb{E}(X\mid \mathcal{G})
$$

<details>
<summary>Proof</summary>

Since $X_n$ is increasing it follows that $\mathbb{E}(X_n\mid \mathcal{G})$ is increasing for $n\in\N$. Letting $Y = \lim_{n\to\infty}\mathbb{E}(X_n\mid \mathcal{G})$, we want to show that $Y \overset{\textrm{a.s.}}{=} \mathbb{E}(X_n\mid \mathcal{G})$. Clearly, $Y$ is $\mathcal{G}$-measurable as a limit of $\mathcal{G}$-measurable random variables. Applying the monotone convergence theorem gives

$$
\begin{align*}
  \mathbb{E}(X\mathbf{1}_A) &= \lim_{n\to\infty}(X\mathbf{1}_A) \\
  &= \lim_{n\to\infty}(X\mathbf{1}_A) \\
  &= \lim_{n\to\infty}\left[\mathbb{E}(X_n\mid \mathcal{G})\mathbf{1}_A\right] \\
  &= \left[\mathbb{E}(X\mid \mathcal{G})\mathbf{1}_A\right]
\end{align*}
$$
</details>
</MathBox>

<MathBox title="Conditional Fatous's lemma" boxType='lemma'>
Let $(X_n)_{n\in\N}$ be an increasing sequence of random variables with $X_n \geq 0$ for all $n$. Then

$$
  \mathbb{E}(\liminf_{n\to\infty} X_n|\mathcal{G}) \overset{\textrm{a.s.}}{\leq} \liminf_{n\to\infty} \mathbb{E}(X\mid \mathcal{G})
$$

<details>
<summary>Proof</summary>

The sequence $\inf_{k\geq n} X_k$ is increasing in $n\in\N$ and $\lim_{n\to\infty}\inf_{k\geq n} X_k = \liminf_{n\to\infty} X_n$. By the conditional monotone convergence theorem we get

$$
  \lim_{n\to\infty}\mathbb{E}\left(\inf_{k\geq n} X_k | \mathca{G} \right) = \mathbb{E}\left(\liminf_{n\to\infty} X_n|\mathcal{G}\right)
$$

Clearly, $\mathbb{E}\left(\inf_{k\geq n} X_k | \mathcal{G} \right)\leq\inf_{k\geq n} \mathbb{E}(X_k\mid \mathcal{G})$. Passing to the limit gives the desired inequality.
</details>
</MathBox>

<MathBox title='Conditional dominated convergence theorem' boxType='theorem'>
Let $Y$ be a nonnegative random variable with $\mathbb{E}(Y)<\infty$ and suppose $(X_n)_{n\in\N}$ is a sequence of random variables with $X_n \underset{n\to\infty}{\uparrow} X$. If $|X_n| \leq Y \leq\infty$ Then

$$
  \lim_{n\to\infty} \mathbb{E}(X\mid \mathcal{G}) \overset{\textrm{a.s.}}{=} \mathbb{E}(X\mid \mathcal{G})
$$

<details>
<summary>Proof</summary>

Noting that $X_n + Y$ and $Y - X_n$ are positive random variables we may apply Fatou's lemma

$$
\begin{align*}
  \mathbb{E}(X + Y|\mathcal{G}) &= \mathbb{E}\left[ \liminf_{n\to\infty}(X_n + Y)|\mathcal{G} \right] \leq \liminf_{n\to\infty}\mathbb{E}(X_n + Y|\mathcal{G}) \\
  \mathbb{E}(X - Y|\mathcal{G}) &= \mathbb{E}\left[ \liminf_{n\to\infty}(X_n - Y)|\mathcal{G} \right] \leq \liminf_{n\to\infty}\mathbb{E}(X_n - Y|\mathcal{G})
\end{align*}
$$

Hence, we obtain $\liminf_{n\to\infty}\mathbb{E}\left(X_n\mid \mathcal{G}\right)\geq\mathbb{E}(X_n\mid \mathcal{G})$ and $\limsup_{n\to\infty}\mathbb{E}\left(X_n\mid \mathcal{G}\right)\leq\mathbb{E}(X_n\mid \mathcal{G})$
</details>
</MathBox>

### Relation to conditional probability

<MathBox title='Conditional probability' boxType='definition'>
Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space and suppose $\mathcal{G}$ is a sub $\sigma$-algebra of $\mathcal{F}$. The conditional probability of an event $A\in\mathcal{F}$ given $\mathcal{G}$ can be defined as a special case of conditional expectation

$$
  \mathbb{P}(A\mid \mathcal{G}) = \mathbb{E}(\mathbb{1}_A|\mathcal{G})
$$

The conditional probability $\mathbb{P}(A\mid \mathcal{G})$ is characterized by the following properties
1. $\mathbb{P}(A\mid \mathcal{G})$ is measurable with respect to $\mathcal{G}$
2. If $B\in\mathcal{G}$ then $\mathbb{E}[\mathbb{P}(A\mid \mathcal{G})\mathbf{1}_B] = \mathbb{P}(A\cap B)$
    1. Equivalently, if $U$ is measurable with respect to $\mathcal{G}$ and $\mathbb{E}(|U|)< \infty$ then $\mathbb{E}[U\mathbb{P}(A\mid \mathcal{G})] = \mathbb{E}(U\mathbf{1}_A)$

<details>
<summary>Proof</summary>

For the second property, note that 

$$
\begin{align*}
  \mathbb{E}[\mathbf{1}_B\mathbb{P}(A\mid \mathcal{G})] &= \mathbb{E}[\mathbf{1}_B\mathbb{E}(\mathbf{1}_A|\mathcal{G})] \\
  &= \mathbb{E}(\mathbf{1}_A\mathbf{1}_B) = \mathbb{E}(\mathbf{1}_{A\cap B}) \\
  &= \mathbb{P}(A\cap B)
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Probability of an event by conditioning' boxType='proposition'>
If $A\in\mathcal{F}$ then $\mathbb{P}(A) = \mathbb{E}[\mathbb{P}(A\mid \mathcal{G})]$.

<details>
<summary>Proof</summary>

This follows from the mean property of condtional expectation since 

$$
\begin{align*}
  \mathbb{E}[\mathbb{P}(A\mid \mathcal{G})] &= \mathbb{E}[\mathbb{E}(\mathbf{1}_A | \mathcal{G})] \\
  &= \mathbb{E}(\mathbf{1_A}) = \mathbb{P}(A)
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Conditional axioms of probability' boxType='axiom'>
The following axioms hold for conditional probability

1. $\mathbb{P}(A\mid \mathcal{G})\geq 0$ for every $A\in\mathcal{F}$
2. $\mathbb{P}(\Omega|\mathcal{G}) = 1$
3. If $\Set{A_i }_{i\in I}$ is a countable disjoint subset of $\mathcal{F}$, then

$$
  \mathbb{P}\left(\bigcup_{i\in I} A_i |\mathcal{G}\right) = \sum_{i\in I}\mathbb{P}(A_i \mid \mathcal{G})
$$
 
<details>
<summary>Proof</summary>

1. This follows from the positive definiteness of conditional expectation.
2. This is trivial since $\mathbf{1}_{\Omega} = 1$.
3. Note that $\sum_{i\in I} \mathbb{P}(A_i \mid \mathcal{G})$ is $\mathcal{G}$-measurable since each term in the sum has this property. Let $B\in\mathcal{G}$, then

$$
\begin{align*}
  \mathbb{E}\left[\sum_{i\in I} \mathbb{P}(A_i \mid \mathcal{G})\mathbf{1}_B \right] &= \sum_{i\in I}\mathbb{E}[\mathbb{P}(A_i \mid \mathcal{G})\mathbf{1}_B] \\
  &= \sum_{i\in I}\mathbb{P}(A_i \cap B) = \mathbb{P}\left(B \cap\bigcup_{i\in I} A_i \right)
\end{align*}
$$
</details>
</MathBox>

<MathBox title="Conditional Bayes' theorem" boxType='theorem'>
Suppose $A\in\mathcal{G}$ and $B\in\mathcal{F}$, then

$$
  \mathbb{P}(A\mid B) = \frac{\mathbb{E}[\mathbb{P}(A\mid \mathcal{G})\mathbf{1}_A]}{\mathbb{E}[\mathbb{E}(A\mid \mathcal{G})]}
$$

<details>
<summary>Proof</summary>

$$
\begin{align*}
  \frac{\mathbb{E}[\mathbb{P}(B\mid \mathcal{G})\mathbf{1}_A]}{\mathbb{E}[\mathbb{E}(B\mid \mathcal{G})]} &= \frac{\mathbb{E}[\mathbb{E}(\mathbf{1}_B|\mathcal{G})\mathbf{1}_A]}{\mathbb{E}[\mathbb{E}(\mathbf{1}_B)|\mathcal{G}]} \\
  &= \frac{\mathbb{E}(\mathbb{1}_A\mathbb{1}_B)}{\mathbb{P}(B)} = \frac{\mathbb{P}(A\cap B)}{\mathbb{P}(B)}
\end{align*}
$$

</details>
</MathBox>

### Best predictor

<MathBox title='' boxType='proposition'>
Suppose $X, U\in\mathcal{L}_1(\Omega,\mathcal{F},\mathbb{P})$ are random variables with $\mathbb{E}(|X|)< \infty$ and $\mathbb{E}(|XU|)< \infty$, and that $U$ is measurable with respect to $\mathcal{G}$. Then $X - \mathbb{E}(X\mid \mathcal{G})$ and $U$ are uncorrelated.

<details>
<summary>Proof</summary>

Note that $X - \mathbb{E}(X\mid \mathcal{G})$ has mean $0$ by the mean property of conditional expectation. Computing the covariance of $X - \mathbb{E}(X\mid \mathcal{G})$ and $U$

$$
\begin{align*}
  \mathrm{cov}[X - \mathbb{E}(X\mid \mathcal{G}), U] &= \mathbb{E}(U[X - \mathbb{E}(X\mid \mathcal{G})]) \\
  &= \mathbb{E}(UX) - \mathbb{E}[U\mathbb{E}(X\mid \mathcal{G})] \\
  &= \mathbb{E}(UX) - \mathbb{E}(UX) = 0
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Conditional expecation as best predictor' boxType='theorem'>
Suppose $X\in\mathcal{L}_2(\Omega,\mathcal{F},\mathbb{P})$ is a square integrable random variable with $\mathbb{E}(X^2)< \infty$. The conditional expectation $\mathbb{E}(X\mid \mathcal{G})$ is the closest random variable to $X$ in mean square sense (best predictor). That is, for any $U\in\mathcal{L}_2(\Omega,\mathcal{F},\mathbb{P})$ that is $\mathcal{G}$-measurable:

$$
  \mathbb{E}\left[X - \mathbb{E}(X\mid \mathcal{G})\right]^2 \leq\mathbb{E}\left[(X - U)^2 \right]
$$

Equality holds if and only if $\mathbb{P}[U = \mathbb{E}(X\mid \mathcal{G})] = 1$, i.e. $U \equiv\mathbb{E}(X\mid \mathcal{G})$

<details>
<summary>Proof</summary>

Note that 

$$
\begin{align*}
  \mathbb{E}\left[(X - U)^2\right] &= \mathbb{E}\left([X -\mathbb{E}(X\mid \mathcal{G}) + \mathbb{E}(X\mid \mathcal{G}) - U]^2 \right) \\
  &= \mathbb{E}\left([X - \mathbb{E}(X\mid \mathcal{G})^2] \right) + 2\mathbb{E}([X - \mathbb{E}(X\mid \mathcal{G})][\mathbb{E}(X\mid \mathcal{G}) - U]) + \mathbb{E}\left([\mathbb{E}(X\mid \mathcal{G}) - U]^2 \right)
\end{align*}
$$

Since $X - \mathbb{E}(X\mid \mathcal{G})$ has mean $0$, the middle term is 

$$
  2\mathrm{cov}[X - \mathbb{E}(X\mid \mathcal{G}), \mathbb{E}(X\mid \mathcal{G}) - U]
$$

Since $\mathbb{E}(X\mid \mathcal{G}) - U$ is $\mathcal{G}$-measurable, this covariance is $0$ by the proposition above. Hence

$$
\begin{align*}
  \mathbb{E}\left[(X - U)^2 \right] = \mathbb{E}\left([X - \mathbb{E}(X\mid \mathcal{G})]^2 \right) + \mathbb{E}\left([\mathbb{E}(X\mid \mathcal{G}) - U]^2 \right) \\
  &\geq \mathbb{E}\left([X - \mathbb{E}(X\mid \mathcal{G})]^2\right)
\end{align*}
$$

Equality holds if and only if $\mathbb{E}\left([\mathbb{E}(X\mid \mathcal{G}) - U]^2 \right) = 0$ if and only if $\mathbb{P}[U = \mathbb{E}(X\mid \mathcal{G})] = 1$.
</details>
</MathBox>

### Conditional variance

<MathBox title='Conditional covariance' boxType='definition'>
Suppose $X\in\mathcal{L}_2(\Omega,\mathcal{F},\mathbb{P})$ is a square integrable random variable with $\mathbb{E}(X^2) < \infty$, and let $\mathcal{G}\subseteq\mathcal{F}$ be a sub-$\sigma$-algebra of $\mathcal{F}$. The conditional variance of $X$ given $\mathcal{G}$ is

$$
\begin{align*}
  \mathrm{var}(X\mid \mathcal{G}) &= \mathbb{E}\left( [X - \mathbb{E}(X\mid \mathcal{G})]^2 | \mathcal{G} \right) \\
  &= \mathbb{E}\left(X^2 |\mathcal{G} \right) - \mathbb{E}(X\mid \mathcal{G})^2
\end{align*}
$$

<details>
<summary>Proof</summary>

Expanding the square in the definition and using basic properties of conditional expectation gives

$$
\begin{align*}
  \mathrm{var}(X\mid \mathcal{G}) &= \mathbb{E}\left(X^2 - 2X\mathbb{E}(X\mid \mathcal{G}) + [\mathbb{E}(X\mid \mathcal{G})]^2 |\mathcal{G} \right) \\
  &= \mathbb{E}\left(X^2 |\mathcal{G}\right) - 2\mathbb{E}[X\mathbb{E}(X\mid \mathcal{G})|\mathcal{G}] + \mathbb{E}\left([\mathbb{E}(X\mid \mathcal{G})]^2 |\mathcal{G} \right) \\
  &= \mathbb{E}\left(X^2 |\mathcal{G} \right) - 2\mathbb{E}(X\mid \mathcal{G})\mathbb{E}(X\mid \mathcal{G}) + [\mathbb{E}(X\mid \mathcal{G})]^2 \\
  &= \mathbb{E}\left(X\mid \mathcal{G} \right) - [\mathbb{E}(X\mid \mathcal{G})]^2
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Conditional variance identities' boxType='proposition'>
Suppose $X\in\mathcal{L}_2(\Omega,\mathcal{F},\mathbb{P})$ is a square integrable random variable with $\mathbb{E}(X^2) < \infty$. Then

1. $\mathrm{var}(X) = \mathbb{E}[\mathrm{var}(X\mid \mathcal{G})] + \mathrm{var}[\mathbb{E}(X\mid \mathcal{G})]$
2. $\mathbb{E}\left([X - \mathbb{E}(X\mid \mathcal{G})]^2 \right) = \mathrm{var}(X) - \mathrm{var}[\mathbb{E}(X\mid \mathcal{G})]$

<details>
<summary>Proof</summary>

1. Taking the expected value of the conditional variance formula gives
$$
  \mathbb{E}[\mathrm{var}(X\mid \mathcal{G})] = \mathbb{E}(X^2) - \mathbb{E}\left[\mathbb{E}(X\mid \mathcal{G})^2 \right]
$$

Writing out the terms

$$
\begin{align*}
  \mathbb{E}(X) &= \mathrm{var}(X) + \mathbb{E}(X)^2 \\
  \mathbb{E}\left[\mathbb{E}(X\mid \mathcal{G})^2\right] &= \mathrm{var}[\mathbb{E}(X\mid \mathcal{G})] + \mathbb{E}[\mathbb{E}(X\mid \mathcal{G})]^2 \\
  &= \mathrm{var}[\mathbb{E}(X\mid \mathcal{G})] + \mathbb{E}(X)^2
\end{align*}
$$

Substituting back gives

$$
  \mathbb{E}[\mathrm{var}(X\mid \mathcal{G})] = \mathrm{var}(X) \mathrm{var}[\mathbb{E}(X\mid \mathcal{G})]
$$

2. 
$$
\begin{align*}
  \mathbb{E}\left([X - \mathbb{E}(X\mid \mathcal{G})]^2 \right) &= \mathbb{E}[\mathrm{var}(X\mid \mathcal{G})] \\
  &= \mathrm{var}(X) - \mathrm{var}[\mathbb{E}(X\mid \mathcal{G})]
\end{align*}
$$
</details>
</MathBox>

### Conditional covariance

<MathBox title='Conditional covariance' boxType='definition'>
Suppose $X, Y\in\mathcal{L}_2(\Omega,\mathcal{F},\mathbb{P})$ are square integrable random variables with $\mathbb{E}(X^2),\mathbb{E}(Y^2) < \infty$. Let $\mathcal{G}\subseteq\mathcal{F}$ be a sub $\sigma$-algebra of $\mathcal{F}$. The conditional variance of $X$ and $Y$ given $\mathcal{G}$ is

$$
\begin{align*}
  \mathrm{var}(X, Y|\mathcal{G}) &= \mathbb{E}\left([X - \mathbb{E}(X\mid \mathcal{G})][Y - \mathbb{E}(X\mid \mathcal{G})]|\mathcal{G} \right) \\
  &= \mathbb{E}(XY|\mathcal{G}) - \mathbb{E}(X\mid \mathcal{G})\mathbb{E}(X\mid \mathcal{G})
\end{align*}
$$

The conditional covariance generalizes conditional variance as

$$
  \mathrm{cov}(X,X|\mathcal{G}) = \mathrm{var}(X\mid \mathcal{G})
$$

<details>
<summary>Proof</summary>

$$
\begin{align*}
  \mathrm{cov}(X,Y|\mathcal{G}) &= \mathbb{E}\left(XY - X\mathbb{E}(Y\mid \mathcal{G}) - Y\mathbb{E}(Y\mid \mathcal{G}) + \mathbb{E}(Y\mid \mathcal{G})\mathbb{E}(Y\mid \mathcal{G}) |\mathcal{G} \right) \\
  &= \mathbb{E}(XY|\mathcal{G}) - \mathbb{E}[X\mathbb{E}(Y\mid \mathcal{G})|\mathcal{G}] - \mathbb{E}[Y\mathbb{E}(Y\mid \mathcal{G})|\mathcal{G}] + \mathbb{E}[\mathbb{E}(Y\mid \mathcal{G})\mathbb{E}(Y\mid \mathcal{G})|\mathcal{G}] \\
  &= \mathbb{E}(XY|\mathcal{G}) - \mathbb{E}(X\mid \mathcal{G})\mathbb{E}(X\mid \mathcal{G}) - \mathbb{E}(X\mid \mathcal{G})\mathbb{E}(X\mid \mathcal{G}) + \mathbb{E}(X\mid \mathcal{G})\mathbb{E}(X\mid \mathcal{G}) \\
  &= \mathbb{E}(XY|\mathcal{G}) - \mathbb{E}(X\mid \mathcal{G})\mathbb{E}(X\mid \mathcal{G})
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Covariance by conditioning' boxType='proposition'>
Suppose $X,Y\in\mathcal{L}_2(\Omega,\mathcal{F},\mathbb{P})$ are square integrable random variables, then

$$
  \mathrm{cov}(X, Y) = \mathbb{E}[\mathrm{cov}(X,Y|\mathcal{G})] + \mathrm{cov}[\mathbb{E}(X\mid \mathcal{G}),\mathbb{E}(X\mid \mathcal{G})]
$$

<details>
<summary>Proof</summary>

Taking the expected value of the conditional covariance gives
$$
  \mathbb{E}[\mathrm{cov}(X,Y|\mathcal{G})] = \mathbb{E}(XY) - \mathbb{E}[\mathbb{E}(X\mid \mathcal{G})\mathbb{E}(X\mid \mathcal{G})]
$$

Note that $\mathbb{E}(XY) = \mathrm{cov}(X,Y) + \mathbb{E}(X)\mathbb{E}(Y)$ and similarly

$$
\begin{align*}
  \mathbb{E}[\mathbb{E}(X\mid \mathcal{G})\mathbb{E}(X\mid \mathcal{G})] &= \mathrm{cov}[\mathbb{E}(X\mid \mathcal{G}),\mathbb{E}(X\mid \mathcal{G})] + \mathbb{E}[\mathbb{E}(X\mid \mathcal{G})]\mathbb{E}[\mathbb{E}(X\mid \mathcal{G})] \\
  &= \mathrm{cov}[\mathbb{E}(X\mid \mathcal{G}),\mathbb{E}(X\mid \mathcal{G})] + \mathbb{E}(X)\mathbb{E}(Y)
\end{align*}
$$

Substituting gives

$$
  \mathbb{E}[\mathrm{cov}(X,Y|\mathcal{G})] = \mathrm{cov}(X,Y) - \mathrm{cov}[\mathbb{E}(X\mid \mathcal{G}),\mathbb{E}(X\mid \mathcal{G})]
$$

</details>
</MathBox>

## Generating functions

### Probability generating functions

In the following discussion $N:\Omega\to\N$ is random variable taking values in $\N$. 

<MathBox title='Probability generating function (PGF)' boxType='definition'>
The probability generating function $P$ of $N$ is defined by

$$
  P(t) := \mathbb{E}(t^N)
$$

for all $t\in\R$ for which the expectation exists in $\R$.
</MathBox>

<MathBox title='PGF in terms of PDF' boxType='proposition'>
Suppose $N$ has probability density function $f$ and probability generating function $P$. Then $P$ takes the form of a power series in $t\in(-r,r)$

$$
  P(t) = \sum_{n=0}^\infty f(n)t^n
$$

where $r\in[1,\infty]$ is the radius of convergence of the series.
</MathBox>

<MathBox title='PDF in terms of PGF' boxType='proposition'>
Suppose $N$ has probability density function $f$ and probability generating function $P$. Then $f$ is given by

$$
  f(k) = \frac{P^{(k)}(0)}{k!},\quad k\in\N
$$

<details>
<summary>Proof</summary>

Differentiating $k$ times gives 

$$
\begin{gather*}
  P^{(k)}(t) = \sum_{n=k}^\infty n^{(n)}f(n)t^{n-k} \\
  \implies P^{(k)}(0) = k^{(k)}f(k) = k!f(k)
\end{gather*}
$$
</details>
</MathBox>

<MathBox title='Factorial moments of PGF' boxType='proposition'>
Suppose the radius of convergence is $r > 1$. Then $P^{(k)} = \mathbb{E}\left( N^{(k)}\right)$ for $k\in\N$. In particular, $N$ has finite moments of all orders. The expectation and variance of $N$ are given by:

1. $\mathbb{E}(N) = P'(1)$
2. $\mathrm{var}(N) = P''(1) + P'(1)[1 - P'(1)]$

<details>
<summary>Proof</summary>

Recall that

$$
  P^{(k)}(t) = \sum_{n=k}^\infty n^{(n)}f(n)t^{n-k},\quad t\in(-r, r)
$$

If $r > 1$ then $P^{(k)}(1) = \sum_{n=k}^{\infty} n^{(k)}f(n) = \mathbb{E}\left(N^{(k)}\right)$. Using this rule, the expectation and variance of $N$ are calculated as

1. $\mathbb{E}(N) = \mathbb{E}\left( N^{(1)} \right) = P'(1)$
2. $\mathbb{E}(N^2) = \mathbb{E}[N(N-1)] + \mathbb{E}(N) = \mathbb{E}\left(N^{(n)}\right) + \mathbb{E}(N) = P''(1) + P'(1)$. From $(1)$ the variance becomes $\mathrm{var}(N) = P''(1) + P'(1) - [P'(1)]^2$
</details>
</MathBox>

<MathBox title='PGF of independent variables' boxType='proposition'>
Suppose $(N_i)_{i=1}^{n\in\N} :\Omega\to\N$ are independent random variables valued in $\N$ with probability generating functions $P_i$ having radii of convergence $r_i$. Then the probability generating function $P$ of $N = \sum_{i=1}^n N_i$ is given by

$$
  P(t) = \prod_{i=1}^n P_i(t)
$$

for $|t| < \bigwedge_{i=1}^n r_i$

<details>
<summary>Proof</summary>

Recall that the expected product of independent variable is the product of the expected values

$$
  P(t) = \mathbb{E}\left( t^{\sum_{i=1}^n N_i} \right) = \mathbb{E}\left( \prod_{i=1}^n t^{N_i}\right) = \prod_{i=1}^n \mathbb{E}\left(t^{N_i}\right) = \prod_{i=1}^n P_i(t),\quad |t| < \bigwedge_{i=1}^n r_i
$$
</details>
</MathBox>

### Moment generating functions

In the following discussion suppose $X:\Omega\to\R$, i.e. $X$ is real-valued random variable, with probability density function $f$.

<MathBox title='Moment generating function (MGF)' boxType='definition'>
The moment generating function of a random variable $X:\Omega\to\R$ is defined by

$$
  M(t) := \mathbb{E}\left(e^{tX}\right),\quad t\in\R
$$
</MathBox>

<MathBox title='MGF in terms of PDF' boxType='proposition'>
Suppose $X$ has moment generating function $M$ that is finite in an open interval $I$ about $0$. Then $X$ ha moments of all orders and

$$
  M(t) = \int_{-\infty}^\infty e^{tx} f(x)\;\d x
$$
</MathBox>

Recall that the two-sided Laplace transform of $f$ is given by

$$
  \mathcal{L}\Set{f}(t) = \int_{-\infty}^\infty e^{-xt}f(x)\;\d x
$$

The moment generating function of $X$ is thus a Laplace transform of the probability density function $f$ at $-t$, i.e. $M(t) = \mathcal{L}\Set{f}(-t)$.

<MathBox title='' boxType='proposition'>
Suppose $X$ has moment generating function $M$ that is finite in an open interval $I$ about $0$. Then $X$ ha moments of all orders and

$$
  M(t) = \sum_{n=0}^\infty \frac{\mathbb{E}(X^n)}{n!}t^n,\quad t\in I
$$

In particular $M^{(n)}(0) = \mathbb{E}(X^n)$.

<details>
<summary>Proof</summary>

By Fubini's theorem, the expected value operator can be interchanged with the infinite series for the exponential function

$$
  M(t) = \mathbb{E}\left(e^{tX}\right) = \mathbb{E}\left( \sum_{n=0}^\infty \frac{X^n}{n!}t^n \right) = \sum_{n=0}^\infty \frac{\mathbb{E}(X^n)}{n!}t^n,\quad t\in I
$$
</details>
</MathBox>

In combinatorial terms, the moment generating function is the exponential generating function of the sequence of moments. Thus, a random variable that does not have finite moments of all orders cannot have a finite moment generating function. Even when a random variables does have moments of all orders, the moment generating function may not exist.

<MathBox title='MGF of linear transformation' boxType='proposition'>
Suppose $X$ has moment generating function $M$ and that $a,b\in\R$. The moment generating function $N$ of $Y = a + bX$ is given by

$$
  N(t) = e^{at}M(bt),\quad t\in\R
$$

<details>
<summary>Proof</summary>

$$
  \mathbb{E}\left( e^{t(a + bX)} \right) = \mathbb{E}\left( e^{ta}e^{tbX} \right) = e^{ta}\mathbb{E}\left( e^{(tb)X} \right) = e^{at}M(bt)
$$

for $t\in\R$.
</details>
</MathBox>

<MathBox title='MGF of independent random variables' boxType='proposition'>
Suppose $(X_i)_{i=1}^{n\in\N} :\Omega\to\R$ are independent real-valued random variables with moment generating functions $M_i$. The moment generating function $M$ of $X = \sum_{i=1}^n X_i$ is given by

$$
  M(t) = \prod_{i=1}^n M_i(t),\quad t\in\R
$$

<details>
<summary>Proof</summary>

Recall that the expected product of independent variables is the product of the expected values. 

$$
  M(t) = \mathbb{E}\left( e^{t\sum_{i=1}^n X_i)} \right) = \mathbb{E}\left( \prod_{i=1}^n e^{tX_i} \right) = \prod_{i=1}^n \mathbb{E}\left( e^{tX_i}\right) = \prod_{i=1}^n M_i(t),\quad t\in\R
$$

for $t\in\R$.
</details>
</MathBox>

<MathBox title='MGF in terms of PGF' boxType='proposition'>
Suppose $X:\Omega\to\N$ with probability generating function $P$ having radius of convergence $r$. The moment generating function $M$ of $X$ is given by

$$
  M(t) = P(e^t),\quad t<\ln{r}
$$

<details>
<summary>Proof</summary>

$$
  M(t) = \mathbb{E}(e^{tX}) = \mathbb{E}\left[(e^t)^X\right] = P(e^t),\quad e^t < r
$$
</details>
</MathBox>

<MathBox title='Chernoff bounds' boxType='proposition'>
If $X$ has moment generating function $M$ then

1. $\mathbb{P}(X\geq x)\leq e^{-tx}M(t)$ for $t > 0$
2. $\mathbb{P}(X\leq x)\leq e^{-tx}M(t)$ for $t < 0$

<details>
<summary>Proof</summary>

1. From Markov's inequality $\mathbb{P}(X\geq x) = \mathbb{P}\left(e^tX \leq e^tx\right) \leq e^{-tx}\mathbb{E}\left(e^{tX}\right) = e^{-tx} M(t)$ if $t>0$.
2. Similarly, $\mathbb{P}(X\leq x) = \mathbb{P}\left(e^tX \geq e^tx\right) \leq e^{-tx} M(t)$ if $t>0$.
</details>
</MathBox>

### Characteristic functions

<MathBox title='Characteristic function' boxType='definition'>
Suppose $X$ has moment generating function $M$ that is finite in an open interval $I$ about $0$. Then $X$ ha moments of all orders and

$$
  \chi(t) := \mathbb{E}\left(e^{itX}\right) = \mathbb{E}\left[\cos(tX)\right] +i\mathbb{E}\left[\sin(tX)]\right],\quad t\in\R
$$
</MathBox>

<MathBox title='' boxType='proposition'>
If $X$ has a continuous distribution on $\R$ with probability density function $f$ and characteristic function $\chi$ then

$$
  \chi(t) = \int_{-\infty}^\infty e^{itx}f(x)\;\d x,\quad t\in\R
$$
</MathBox>

Recall that the Fourier transform of $f$ is given by

$$
  \mathcal{F}\Set{f}(t) = \int_{-\infty}^\infty e^{-itx}f(x)\;\d x
$$

The characteristic function of $X$ is thus a Fourier transform of the probability density function $f$ at $-t$, i.e. $\chi(t) = \mathcal{F}\Set{f}(-t)$.

<MathBox title='Inversion formula' boxType='proposition'>
Suppose $X$ has characteristic function $\chi$. If $a,b\in\R$ and $a<b$ then

$$
  \int_{-n}^n \frac{e^{-iat} - e^{-ibt}}{2\pi it}\chi(x)\;\d t \xrightarrow{n\to\infty} \mathbb{P}(a < X < b) + \frac{1}{2}\left[\mathbb{P}(X = b) -  \mathbb{P}(X = a)\right]
$$

If $X$ has a continuous distribution on $\R$ with probability density function $f$ and characteristic function $\chi$ then

$$
  f(x) = \frac{1}{2\pi} \int_{-\infty}^\infty e^{-itx}\chi(t)\;\d t
$$
</MathBox>

<MathBox title='Inversion formula' boxType='proposition'>
Suppose $X$ has characteristic function $\chi$. If $n\in\N$ and $\mathbb{E}(|X^n|)<\infty$ then

$$
  \chi(t) = \sum_{k=0}^n \frac{\mathbb{E}(X^k)}{k!}(it)^k + o(t^n)
$$

where $\frac{o(t^n)}{t^n} \xrightarrow{t\to\infty} 0$. In particular 

$$
  \chi^{(n)}(0) = i^n\mathbb{E}(X^n)
$$
</MathBox>

<MathBox title='Characteristic function under linear transform' boxType='proposition'>
Suppose $X$ has characteristic function $\chi$ and that $a,b\in\R$. The characteristic function $\psi$ of $Y = a + bX$ is given by $\psi(t) = e^{iat}\chi(bt)$ for $t\in\R$.

<details>
<summary>Proof</summary>

$$
  \psi(t) = \mathbb{E}\left( e^{it(a + bX)} \right) = \mathbb{E}\left(e^{ita}e^{itbX}\right) = e^{ita}\mathbb{E}\left( e^{i(tb)X} \right) = e^{iat}\chi(bt)
$$
</details>
</MathBox>

<MathBox title='Characteristic function of independent random variables' boxType='proposition'>
Suppose $(X_j)_{j=1}^{n\in\N} :\Omega\to\R$ are independent real-valued random variables with characteristic functions $\chi_j$. The moment generating function $\chi$ of $X = \sum_{j=1}^n X_j$ is given by

$$
  \chi(t) = \prod_{j=1}^n \chi_i(t),\quad t\in\R
$$

<details>
<summary>Proof</summary>

Recall that the expected product of independent variables is the product of the expected values. 

$$
  \chi(t) = \mathbb{E}\left( e^{it\sum_{j=1}^n X_j)} \right) = \mathbb{E}\left( \prod_{j=1}^n e^{itX_j} \right) = \prod_{j=1}^n \mathbb{E}\left( e^{itX_j}\right) = \prod_{j=1}^n \chi_j(t),\quad t\in\R
$$

for $t\in\R$.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Suppose $X$ has moment generating function $M$ that satisfies $M(t) < \infty$ for $t$ in an open interval $I$ about $0$. Then the characteristic function $\chi$ of $X$ satisfies 

$$
  \chi(t) = M(it),\quad t\in I
$$

</MathBox>

<MathBox title='Continuity theorem' boxType='theorem'>
Suppose $(X_n :\Omega\to\R)_{n\in\N}$ is a sequence of real-valued random variables with characteristic function $\chi_i$.

1. If the distribution of $X_n$ converges to the distribution of a random variable $X\xrightarrow{n\to\infty} X$ and $X$ has characteristic function $\chi$, then $\chi_{n}(t)\xrightarrow{n\to\infty} \chi(t)$ for all $t\in\R$
2. If $\chi_n(t)\xrightarrow{n\to\infty}\chi(t)$ for $t$ in an open interval about $0$, and if $\chi$ is continuous at $0$, then $\chi$ is the characteristic function of a random variable $X$, and the distribution of $X_n\xrightarrow{n\to\infty} X$. 
</MathBox>

## Uniformly integrable random variables

<MathBox title='' boxType='proposition'>
If $X$ is a random variable, then $\mathbb{E}(|X|) < \infty$ if and only if $\mathbb{E}\left(|X|\mathbf{1}_{|X|\geq x} \right)\xrightarrow{\to\infty} 0$.

<details>
<summary>Proof</summary>

Note $|X|\mathbf{1}_{|X|\leq x}$ is nonnegative, increasing in $x\in[0,\infty)$ and $|X|\mathbf{1}_{|X|\leq x}\xrightarrow{x\to\infty} |X|$. From the monotone convergence theorem, $\mathbb{E}\left(|X|\mathbf{1}_{|X|\leq x} \right)\xrightarrow{x\to\infty}\mathbb{E}(|X|)$. Furthermore

$$
  \mathbb{E}(|X|) = \mathbb{E}(|X|\mathbf{1}_{|X|\leq x}) + \mathbb{E}(|X|\mathbf{1}_{|X| > x})
$$

If $\mathbb{E}(|X|) < \infty$ then taking limits shows that $\mathbb{E}(|X|\mathbf{1}_{|X|> x})\xrightarrow{x\to\infty} 0$. Since $\mathbb{E}\left(|X|\mathbf{1}_{|X|\leq x}\right)\leq x$, then $\mathbb{E}\left(|X|\mathbf{1}_{|X| > x} \right) = \infty$ if $\mathbf{E}(|X|) = \infty$.
</details>
</MathBox>

<MathBox title='Uniformly integrable random variable' boxType='definition'>
Suppose $I$ is an index set, which is not necessarily countable, and that $\mathbf{X} = \Set{X_i }_{i\in I}$ is a collection of random variables. The collection $\mathbf{X}$ is *uniformly integrable* if for each $\varepsilon > 0$ there exists $x > 0$ such that for all $i\in I$

$$
  \mathbb{E}\left(|X_i|\mathbf{1}_{|X_i| > x}\right) < \varepsilon
$$

Equivalently $\mathbb{E}\left(|X_i|\mathbf{1}_{|X_i| > x}\right)\xrightarrow{x\to\infty} 0$ uniformly in $i\in I$.
</MathBox>

<MathBox title='Conditions for uniform integrability' boxType='proposition'>
Suppose $(\Omega,\mathcal{F},\mathbb{P})$ is a probability space. The collection $\mathbf{X}$ is uniformly integrable if and only if

1. $\Set{\mathbb{E}(|X_i|)}_{i\in I}$ is bounded.
2. For each $\varepsilon > 0$ there exists $\delta > 0$ such that if $A\in\mathcal{F}$ and $\mathbb{P}(A) < \delta$, then $\mathbb{E}(|X_i|\mathbf{1}_A) < \varepsilon$ for all $i\in I$.

<details>
<summary>Proof</summary>

Suppose $\mathbf{X}$ is uniformly integrable. With $\varepsilon = 1$ there exists $x > 0$ such that $\mathbb{E}\left(|X_i|\mathbf{1}_{|X_i| > x}\right) < 1$ for all $i\in I$. Thus

$$
\begin{align*}
  \mathbb{E}(|X_i|) &= \mathbb{E}(|X_i|\mathbf{1}_{|X_i|\leq x}) + \mathbb{E}(|X_i|\mathbf{1}_{|X_i| > x}) \\
  &= x + 1
\end{align*}
$$

showing that $\mathbb{E}(|X|)$ is bounded. Next, for $\varepsilon > 0$ there exist $x > 0$ such that $\mathbb{E}(|X_i|\mathbf{1}_{|X_i| > x}) < \frac{\varepsilon}{2}$ for all $i\in I$. Let $\delta = \frac{\varepsilon}{2x}$. If $A\in\mathcal{F}$ and $\mathbb{P}(A) < \delta$, then

$$
\begin{align*}
  \mathbb{E}\left(|X_i| \right) &= \mathbb{E}\left(|X_i|\mathbf{1}_{A \cap \Set{|X|\leq x}} \right) + \mathbb{E}\left(|X_i|\mathbf{1}_{A \cap \Set{|X| > x}} \right) \\
  &\leq x\mathbb{P}(A) + \mathbb{E}\left(|X_i|\mathbf{1}_{|X| > x} \right) < \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon 
\end{align*}
$$

Conversely, suppose condition $(1)$ and $(2)$ hold. By $(1)$, there exists $c > 0$ such that $\mathbb{E}(|X_i|)\leq c$ for all $i\in I$. Let $\varepsilon > 0$, then by $(2)$ there exists $\delta > 0$ such that if $A\in\mathcal{F}$ with $\mathbb{P}(A) < \delta$, then $\mathbb{E}(|X_i|\mathbf{1}_A) < \varepsilon$ for all $i\in I$. Next, by Markov's inequality

$$
  \mathbb{P}(|X_i| > x) \leq\frac{\mathbb{E}(|X_i|)}{x}\leq \frac{c}{x}
$$

Pick $x> 0$ such that $\frac{c}{x} < \infty$, then $\mathbb{P}(|X_i| > x) < \delta$ for each $i\in I$. Then for each $j\in I$, we have $\mathbb{E}(|X_i|\mathbf{1}_{|X_j| > x) < \varepsilon$ for all $i\in I$ and so in particular $\mathbb{E}(|X_i|\mathbf{1}_1 > x) < \infty$ for all $i \in I$. Hence $\mathbf{X}$ is uniformly integrable.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Suppose $\mathbf{Y} = \Set{ X_j }_{j\in J}$ is uniformly integrable, and $\mathbf{X} = \Set{ X_i }_{i\in I}$ is a collection of random variables with the property that for each $i\in I$ there exists $j\in J$ such that $|X_i |\leq |Y_j |$. Then $\mathbf{X}$ is uniformly integrable.

<details>
<summary>Proof</summary>

Clearly for each $i\in I$ there exists $j\in J$ such that $\mathbf{E}\left(|X_i|_i \mathbf{1}_{|X_i| > x} \right) \leq \mathbf{E}\left(|Y_j|\mathbf{1}_{|Y_j| > x}\right) \xrightarrow{x\to\infty} 0$ for $x\in [0, \infty)$. Hence $\mathbf{X}$ is uniformly integrable.
</details>
</MathBox>

<MathBox title='Uniform integrability is closed under addition and scalar multiplication' boxType='proposition'>
Suppose $\mathbf{X} = \Set{ X_i }_{i\in I}$ and $\mathbf{Y} = \Set{ X_i }_{i\in I}$ are uniformly integrable and that $c\in\R$. Then each of the following collections is also uniformly integrable
1. $\mathbf{X} + \mathbf{Y} = \Set{ X_i + Y_i }_{i\in I}$
2. $c\mathbf{X} = \Set{cX_i }_{i\in I}$

<details>
<summary>Proof</summary>

1. There exists $a,b\in (0,\infty)$ such that $\mathbf{E}(|X_i|)\leq a$ and $\mathbf{E}(|Y_i|)\leq b$ for all $i\in I$. Thus for all $i\in I$

$$
\begin{align*}
  \mathbb{E}\left(|X_i + Y_i| \right) &\leq \mathbb{E}\left(|X_i| + |Y_i| \right) \\
  \mathbb{E}\left(|X_i|\right) + \mathbb{E}\left(|Y_i| \right) \left a + b
\end{align*}
$$

Next, for $\varepsilon > 0$ there exists $\delta_1 > 0$ such that if $A\in\mathcal{F}$ with $\mathbb{P}(A)< \delta_1$, then $\mathbf{E}(|X_i|\mathbf{1}_A) < \frac{\varepsilon}{2}$ for all $i\in I$, and similarly, there exists $\delta_2 > 0$ such that if $A\in\mathcal{F}$ with $\mathbb{P}(A)< \delta_2$ then $\mathbb{E}(|Y_i|\mathbf{1}_A) < \frac{\varepsilon}{2}$ for all $i\in I$. Hence if $A\in\mathcal{F}$ with $\mathbb{P}(A) < \delta_1 \wedge\delta_2$ then for each $i\in I$

$$
\begin{align*}
  \mathbb{E}\left(|X_i + Y_i|\mathbf{1}_A \right) &\leq \mathbb{E}\left[(|X_i| + |Y_i|)\mathbf{1}_A \right] \\
  \mathbb{E}\left(|X_i|\mathbf{1}_A\right) + \mathbb{E}\left(|Y_i|\mathbf{1}_A \right) < \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon
\end{align*}
$$

2. There exists $a\in (0,\infty)$ such that $\mathbb{E}(|X_i|)\leq a$ for all $i\in I$. Thus for all $i\in I$

$$
  \mathbb{E}(|cX_i|) = |c|\mathbb{E}(|X_i|)\leq ca
$$

The second condition for uniform integrability is trivial for $c = 0$, so suppose $c\neq 0$. For $\varepsilon > 0$ there exists $\delta > 0$ such that if $A\in\mathcal{F}$ and $\mathbb{P}(A) < \delta$ then $\mathbb{E}(|X_i|\mathbf{1}_A) < \frac{\varepsilon}{c}$ for all $i\in I$. Hence

$$
  \mathbb{E}(|cX_i|) = |c|\mathbb{E}(|X_i|) < \varepsilon
$$
</details>
</MathBox>

<MathBox title='Uniform integrability theorem' boxType='theorem'>
Suppose $X$ is a random variable, and $\Set{X_n }_{n\in\N_+}$ is a collection of random variables. 

1. If $X_n \xrightarrow{n\to} X$ in mean, then $\Set{X_n }_{n\in\N_+}$ is uniformly integrable.
2. If $\Set{X_n }_{n\in\N_+}$ is uniformly integrable and $X_n \xrightarrow{n\to\infty} X$ in probability, then $X_n \xrightarrow{n\to\infty} X$ in mean.

<details>
<summary>Proof</summary>

1. Convergence in mean implies that $X_n \xrightarrow{n\to\infty} X$ in the vector space $\mathcal{L}_1$. That is, $\mathbb{E}(|X_n|) < \infty$ for $n\in\N_+$, and $\mathbb{E}(|X|) < \infty$, and $\mathbb{E}(|X_n - X|)\xrightarrow{n\to\infty} 0$. This implies $\mathbb{E}(|X_n|)\xrightarrow{n\to\infty}\mathbb{E}(|X|)$, so $\mathbb{E}(|X_n|)$ is bounded in $n\in\N$. For $\varepsilon > 0$ there exists $N\in\N_+$ such that if $n > N$, then $\mathbb{E}(|X_n - X|) < \frac{\varepsilon}{2}$. Since all $X_n\in\mathcal{L}_1$ for each $n$, there exists $\delta_n > 0$ such that if $A\in\mathcal{F}$ and $\mathbb{P}(A) < \delta_n$, then $\mathbb{E}(|X_n - X|\mathbf{1}_A)< \frac{\varepsilon}{2}$. Similarly, there exists $\delta_0 > 0$ such that if $A\in\mathcal{F}$ and $\mathbb{P}(A) < \delta_0$ then $\mathbb{E}(|X|\mathbf{1}_A) < \frac{\varepsilon}{2}$. Let $\delta = \min\Set{\delta_n}_{n=0}^{N} > 0$. If $A\in\mathcal{F}$ and $\mathbb{P}(A) < \delta$, then for $n\in\N_+$

$$
\begin{align*}
  \mathbb{E}(|X_n|\mathbf{1}_A) &= \mathbb{E}(|X_n - X + X|\mathbf{1}_A) \\
  &\leq \mathbb{E}(|X_n - X|\mathbf{1}_A) + \mathbb{E}(|X|\mathbf{1}_A)
\end{align*}
$$

If $n\leq N$ then $\mathbb{E}(|X_n - X|\mathbf{1}_A)\leq \frac{\varepsilon}{2}$ since $\delta\leq\delta_n$. If $n > N$ then $\mathbb{E}(|X_n - X|\mathbf{1}_A) \leq \mathbb{E}(|X_n - X|) < \frac{\varepsilon}{2}$. Since $\delta\leq\delta_0$, then $\mathbb{E}(|X|\mathbf{1}_A)< \frac{\varepsilon}{2}$. So for all $n\in\N_+$, then $\mathbb{E}(|X_n|\mathbf{1}_A) < \frac{\varepsilon}{2}$ and hence $\Set{X_n }_{n\in\N_+}$ is uniformly integrable.

2. Since $X_n\xrightarrow{n\to\infty} X$ in probability, there exists a subsequence $\left(X_{n_k} \right)_{k\in\N_+}$ of $(X_n)_{n\in\N_+}$ such that $X_{n_k}\xrightarrow{k\to\infty} X$ with probability $1$. By the uniform integrability, $\mathbb{E}(|X_n|)$ is bounded in $n\in\N_+$. Thus, by Fatou's lemma

$$
\begin{align*}
  \mathbb{E}(|X|) &= \mathbb{E}\left( \liminf_{k\to\infty} |X_{n_k}| \right) \\
  &\leq \liminf_{n\to\infty}\mathbb{E}\left(|X_{n_k}| \right) \\
  &\leq \limsup_{n\to\infty}\mathbb{E}\left(|X_{n_k}| \right) < \infty
\end{align*}
$$

Let $Y_n = X_n - X$ for $n\in\N_0$. From the result above, we know that $\Set{Y_n}_{n\in\N_+}$ is uniformly integrable, and we also know that $Y_n\xrightarrow{n\to\infty} 0$ in probability. Thus, we need to show that $Y_n\xrightarrow{n\to\infty} 0$ in mean. For $\varepsilon > 0$ there exists $\delta > 0$ such that if $A\in\mathcal{F}$ with $\mathbb{P}(A) < \delta$ then $\mathbb{E}(|Y_n|\mathbf{1}_A)< \frac{\varepsilon}{2}$ for all $n\in\N$. Since $Y_n \xrightarrow{n\to\infty}0$ in probability, there exists $N\in\N_+$ such that if $n > N$, then $\mathbb{P}\left(|Y_n| > \frac{\varepsilon}{2} \right) < \delta$. Thus, if $n > N$ then

$$
\begin{align*}
  \mathbb{E}(|Y_n|) &= \mathbb{E}\left( \mid Y_n|\mathbf{1}_{|Y_n| \leq \varepsilon/2} \right) + \mathbb{E}\left( \mid Y_n|\mathbf{1}_{|Y_n| > \varepsilon/2} \right) \\
  &< \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon
\end{align*}
$$

Hence $Y_n\xrightarrow{n\to\infty} 0$ in mean.
</details>
</MathBox>

<MathBox title='The collection of all conditional expectations is uniformly integrably' boxType='theorem'>
Suppose $X\in\mathbb{L}_1$ is a real-valued random variable on $(\Omega,\mathcal{F},\mathbb{P})$ with $\mathbb{E}(|X|) < \infty$. Then $\Set{\mathbb{E}(X| \mathcal{G}) : \mathcal{G}\textrm{ is a sub }\sigma\textrm{-algebra of }\mathcal{F}}$ is uniformly integrable.

<details>
<summary>Proof</summary>

Let $\mathcal{G}$ be a sub $\sigma$-algebra of $\mathcal{F}$. Recall that $|\mathbb{E}(X\mid \mathcal{G})| \leq\mathbb{E}(|X|\,|\mathcal{G})$ and thus

$$
  \mathbb{E}[|\mathbb{E}(X\mid \mathcal{G})] \leq \mathbb{E}[\mathbb{E}(|X|\,\mathcal{G})] = \mathbb{E}(|X|)
$$

showing that $\mathbb{E}(X\mid \mathcal{G})$ is bounded. Next, let $\varepsilon > 0$. Since $\mathbb{E}(|X|) < \infty$, there exists $\delta > 0$ such that if $A\in\mathcal{F}$ and $\mathbb{P}(A) < \delta$ then $\mathbb{E}(|X|\mathbf{1}_A) < \varepsilon$. Suppose $A\in\mathcal{G}$ with $\mathbb{P}(A) < \varepsilon$. Then $\left|\mathbb{E}(X\mid \mathcal{G}) \right|\mathbf{1}_A \leq \mathbb{E}(|X|\,\mathcal{G})\mathbf{1}_A$ so

$$
\begin{align*}
  \mathbb{E}\left[ \mathbb{E}(X\mid \mathcal{G})\mathbf{1}_A \right] &\leq \mathbb{E}\left[\mathbb{E}(|X|\,\mathcal{G})\mathbf{1}_A \right] \\
  &= \mathbb{E}\left[\mathbb{E}(|X|\mathbf{1}_A | X) \right] = \mathbb{E}(|X|\mathbf{1}_A) < \varepsilon
\end{align*}
$$
</details>
</MathBox>

# Vector spaces of random variables

Many probability concepts have elegant intepretations if we think of the collection of real-valued random variables as a vector space. In particular, variance and higher moments are related to the concept of norm and distance, while covariance is related to the inner product. 

A vector space $\mathcal{V}$ on a probability space $(\Omega, \mathcal{F},\mathbb{P})$ can be formed from all real-valued random variables $X:\Omega\to\R$. Recall, that two random variables $X, Y$ on a probability space $(\Omega,\mathcal{F},\mathbb{P})$ are equivalent if $\mathbb{P}(X=Y)=1$, in which case we write $X\equiv Y$. The vector space then consists of all equivalence classes $[X]$ under $\equiv$.

## Norm
<MathBox title='Norm' boxType='definition'>
For $k\in [1,\infty)$, the $k$-norm of $X\in\mathcal{V}$ is defined by

$$
  \norm{ X}_k := \left[\mathbb{E}\left( \mid X|^k \right) \right]^{1/k}
$$

The set of random variables with finite $k$-norm forms a subspace $\mathcal{L}_k$ of the vector space $\mathcal{V}$.
</MathBox>

<MathBox title='Properties of the norm' boxType='proposition'>
Suppose $k\in [1,\infty)$. The $k$-norm of $X\in\mathcal{V}$ has the following properties for $X, Y\in\mathcal{V}$

1. Positive definiteness: $\norm{ X}_k \geq 0$ and $\norm{ X}_k = 0$ if and only if $\mathbb{P}(X = 0) = 1$, i.e. $X\equiv 0$.
2. Scaling property: $\norm{ cX}_k = |c|\cdot\| X\|_k$ for $c\in\R$.
3. Minkowski's inequality: $\norm{ X + Y}_k \leq \norm{ X}_k + \norm{ Y}_k$
4. Lyapunov's inequality: suppose $j,k\in[1, \infty)$ with $j\leq k$, then $\norm{ X}_j \leq \norm{ X}_k$

<details>
<summary>Proof</summary>

1. First $|X|^k \geq 0$ with probability $1$, so $\mathbb{E}\left( \mid X|^k \right)\geq 0$. In addition $\mathbb{E}\left( \mid X|^k \right) = 0$ if and only if $\mathbb{P}(X = 0) = 1$.
2. 

$$
\begin{align*}
  \| cX\|_k &=  \left[\mathbb{E}\left( \mid cX|^k \right) \right]^{1/k} =  \left[\mathbb{E}\left( \mid c|^k|X|^k \right) \right]^{1/k} \\
  &=  \left[|c|^k \mathbb{E}\left( \mid X|^k \right) \right]^{1/k} = |c| \left[\mathbb{E}\left( \mid X|^k \right) \right]^{1/k} \\
  &= |c|\cdot\| X\|_k
\end{align*}
$$
3. The first quadrant $S = \Set{ (x,y)\in\R^2 | x\geq 0, y\geq 0 }$ is a convex set and $g(x, y) = \left( x^{1/k} + y^{1/k} \right)^k$ is concave on $S$. From Jensen's inequality, if $U$ and $V$ are nonnegative random variables then

$$
  \mathbb{E}\left[ \left( U^{1/k} + V^{1/k} \right)^k \right] \leq \left( \left[\mathbb{E}(U) \right]^{1/k} + \mathbb{E}(V) \right]^{1/k} \right)^k
$$

Letting $U = |X|^k$ and $V = |Y|^k$ and simplifying gives the result. To verify that $g$ is concave on $S$, we compute the Jacobian determinant of the second derivatives. Let $h(x, y) = x^{1/k} + y^{1/k}$ so that $g = h^k$.

$$
\begin{align*}
  g_{xx} &= \frac{k - 1}{k} h^{k-2} x^{1/k - 2} \left(x^{1/k} - h \right) \\
  g_{yy} &= \frac{k - 1}{k} h^{k-2} y^{1/k - 2} \left(y^{1/k} - h \right) \\
  g_{xy} &= \frac{k - 1}{k} h^{k-2} x^{1/k - 1} x^{1/k - 1} = g_{yx}
\end{align*}
$$

The Jacobian determinant is $g_{xx} g_{yy} - g_{xy}^2 = 0$ on $S$. Clearly $h(x,y) \geq x^{1/k}$ and $h(x,y) \geq y^{1/k}$ on $S$, such that $g_xx, g_yy \leq 0$ on $S$. Thus, the second derivative matrix of $g$ is negative semi-definite.
4. Note that $S = \Set{ x\in\R| x\geq 0}$ is convex and $g(x) = x^{k/j}$ is convex on $S$. From Jensens' inequality, if $U$ is a nonnegative random variable then 

$$
  \left[\mathbb{E}(U) \right]^{k/j} \leq \mathbb{E}\left(U^{k/j}\right)
$$

Letting $U = |X|^j$ and simplifying gives the result.
</details>
</MathBox>

## Metric

<MathBox title='Metric' boxType='definition'>
For $k\in [1,\infty)$, The $k$ metric between $X, Y\in\mathcal{V}$ is defined by

$$
  d_k (X,Y) := \norm{ X - Y}_k = \left[\mathbb{E}\left( \mid X - Y|^k \right) \right]^{1/k}
$$

In particular, the standard deviation is simply the 2-metric from a random variable $X$ to its mean $\mu = \mathbb{E}(X)$

$$
  \mathrm{sd}(X) = d_2 (X, \mu) = \norm{ X - \mu}_2 = \sqrt{\mathbb{E}\left[(X - \mu)^2 \right]} 
$$
</MathBox>

<MathBox title='Properties of the metric' boxType='proposition'>
Suppose $k\in [1,\infty)$, the $k$ metric has the following properties for $X, Y, Z\in\mathcal{V}$

1. Symmetry: $d_k (X, Y) = d_k (Y, X)$
2. Positive definiteness: $d_k (X, Y) \geq 0$ and $d_k (X, Y) = 0$ if and only if $\mathbb{P}(X = Y) = 1$, i.e. $X \equiv Y$
3. Triangle inequality: $d_k (X, Z) \leq d_k (X, Y) + d_k (Y, Z)$

<details>
<summary>Proof</summary>

1. This follows trivially from the definition.
2. This follows directly from the positive definiteness of the $k$ norm.
3. From Minkowski's inequality

$$
\begin{align*}
  d_k (X, Z) &= \norm{ X - Z}_k = \| (X - Y) + (Y - Z)\|_k \\
  &\leq \norm{ X - Y}_k + \norm{ Y - Z}_k \\
  &= d_k (X, Y) + d_k (Y, Z)
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Convergence' boxType='definition'>
Suppose $X_n \in\mathcal{L}_k$ for $n\in\N_+$ and that $X\in\mathcal{L}_k$ for $k\in[1, \infty)$. Then $X_n \xrightarrow{n\to\infty} X$ in $k-$th mean if $X_n \xrightarrow{n\to\infty} X$ in the vector space $\mathcal{L}$_k. That is,

$$
  d_k (X_n, X) = \norm{ X_n - X}_k \xrightarrow{n\to\infty} 0
$$

or equivalently $\mathbb{E}\left(|X_n - x|^k\right) \xrightarrow{n\to\infty} 0$.

When $k = 1$, we say that $X_n \xrightarrow{n\to\infty} X$ in *mean*, and when $k=2$ we say that $X_n \xrightarrow{n\to\infty} X$ in *mean square*.
</MathBox>

<MathBox title='Properties of convergence' boxType='proposition'>
Suppose $X_n\in\mathcal{L}_k$ for $n\in\N_+$ and that $X\in\mathcal{L}_k$, where $k\in[1,\infty)$.

1. Suppose $1\leq j\leq k$. If $X_n \xrightarrow{n\to\infty} X$ in $k$-th mean, then $X_n \xrightarrow{n\to\infty} X$ in $j$-th mean.
2. If $X_n \xrightarrow{n\to\infty} X$ in $k$-th mean then $\norm{ X_n}_k \xrightarrow{n\to\infty} \norm{ X}_k$. Equivalently, if $\mathbb{E}\left(|X - X_n|^k \right)\xrightarrow{n\to\infty} 0$, then $\mathbb{E}\left(|X_n|^k \right) \xrightarrow{n\to\infty} \mathbb{E}\left(|X|^k \right)$.

<details>
<summary>Proof</summary>

1. This follows from Lyapunov's inequality. Note that $0\leq 0 d_j (X_n, X) \xrightarrow{n\to\infty} 0$.
2. This follows from the reverse triangle inequality for normed vector space, which in this case reads

$$
  \left| \norm{ X_n}_k - \norm{ X}_k \right| \leq \| X_n - X\|_k
$$

If the right side converges to $0$ as $n\to\infty$, then so does the left side.
</details>
</MathBox>

<MathBox title='Convergence in mean implies convergence in probability' boxType='proposition'>
Suppose $X_n\in\mathcal{L}_1$ for $n\in\N_+$ and that $X\in\mathcal{L}_1$. If $X_n \xrightarrow{n\to\infty} X$ in mean, then $X_n \xrightarrow{n\to\infty} X$ in probability.

<details>
<summary>Proof</summary>

This follows from Markov's inequality. For $\varepsilon > 0$ 

$$
  0 \leq\mathbb{P}\left(|X_n - X| > \varepsilon \right) \leq \frac{\mathbb{E}(|X_n - X|)}{\varepsilon}\xrightarrow{n\to\infty} 0
$$
</details>
</MathBox>

In summary, the following implications in the various modes of convergence hold
- Convergence with probability $1$ implies convergence in probability.
- Convergence in $k$-th mean implies convergence in $j$-th mean if $j\leq k$.
- Convergence in $k$-th mean implies converge in probability.
- Convergence in probability implies convergence in distribution.

### Center and spread

<MathBox title='Root mean square error (RMSE)' boxType='definition'>
For $X\in\mathcal{L}_2$, define the root mean square error function by

$$
  d_2 (X, t) := \norm{ X - t}_2 = \sqrt{\mathbb{E}\left[(X - t)\right]^2},\quad t\in\R
$$

The 2-metric $d_2 (X, t)$ is minimized when $t = \mathbb{E}(X)$ and the minimum value is $\mathrm{sd}(X)$.

<details>
<summary>Proof</summary>

Note that $d_2 (X, t)$ share the same critical points as $d^2^2 (X, t) = \mathbb{E}\left[(X - t)^2\right]$, the mean square error function. Expanding $d_2^2(X, t)$ gives a quadratic function of $t$

$$
  \mathbb{E}\left[(X - t)^2\right] = \mathbb{E}(X^2) - 2t\mathbb{E}(X) + t^2
$$

which is a parabola opening upward. The minimum occurs at $t = \mathbb{E}(X)$, and the minimum value is $\mathrm{var}(X)$. The minimum value for $d_2(X, t)$ is thus $\mathrm{sd}(X)$.
</details>
</MathBox>

<MathBox title='Mean absolute error (MAE)' boxType='definition'>
For $X\in\mathcal{L}_1$, define the mean absolute error function by

$$
  d_1 (X, t) := \norm{ X - t}_1 = \mathbb{E}\left(|X - t|\right),\quad t\in\R
$$

The 1-metric $d_1 (X, t)$ is minimized when $t$ is any median of $X$.

<details>
<summary>Proof</summary>

Suppose first that $X\in\mathcal{L}_1$ has a discrete distribution with values in a finite set $S\subseteq\R$. Note that

$$
  \mathbb{E}\left(|X - t|\right) = \mathbb{E}(t - X, X\leq t) + \mathbb{E}(X - t, X>t)
$$

Thus $\mathbb{E}\left(|X - t|\right) = a_t t + b_t$ where $a_t = 2\mathbb{P}(X\leq t) - 1$ and $b_t = \mathbb{E}(X) - 2\mathbb{E}(X, X\leq t)$. Note that $\mathbb{E}\left(|X - t|\right)$ is a continuous, piecewise linear function of $t$, with corners at the values in $S$, which makes it a linear spline. Let $m$ be the smallest median of $X$. If $t< m$ and $t\notin S$, then the slope of the linear piece at $t$ is negative. Let $M$ be the largest median of $X$. If $t> M$ and $t\notin S$, then the slope of the linear piece at $t$ is positive. If $t\in(m, M)$, then the slope of the linear piece at $t$ is $0$. Thus, $\mathbb{E}(|X - t|)$ is minimized for every $t$ in the median interval $[m, M]$.

Suppose next that $X\in\mathcal{L}_1$ has a general distribution on $\R$. Let $s, t\in\R$ and suppose first that $s< t$. Computing the expected value over the events $X\leq s$, $s< X\leq t$ and $X\geq t$, and simplifying gives

$$
\begin{align*}
  \mathbb{E}(|X - t|) &= \mathbb{E}(|X - s|) \\
  &\quad + (t-s)\left[ 2\mathbb{P}(X \leq s) - 1 \right] \\
  &\quad + 2\mathbb{E}(t - X, s< X\leq t)
\end{align*}
$$

Suppose next that $t< s$. Using similar methods gives

$$
\begin{align*}
  \mathbb{E}(|X - t|) &= \mathbb{E}(|X - s|) \\
  &\quad + (t-s)\left[ 2\mathbb{P}(X < s) - 1 \right] \\
  &\quad + 2\mathbb{E}(X - t, t\leq X< s)
\end{align*}
$$

Note that the last terms on the right in these equations are nonnegative. If $s$ is a median of $X$, then the middle terms on the right in the equations are also nonnegative. Hence, if $s$ is a median of $X$ and $t$ is any other number, then $\mathbb{E}(|X - t|)\leq \mathbb{E}(|X - s|)$
</details>
</MathBox>

## Inner product

<MathBox title='Inner product' boxType='definition'>
The inner product of $X, Y\in\mathcal{L}_2$ is defined by

$$
  \langle X, Y \rangle := \mathbb{E}(XY)
$$

The covariance of $X$ and $Y$ is the inner product of the corresponding centered variables, while the correlation is the inner product of the corresponding standard scores:

1. $\mathrm{cov}(X, Y) = \langle X - \mathbb{E}(X), Y -\mathbb{E}(Y)\rangle$
2. $\mathrm{cor} = \left\langle \frac{X -\mathbb{E}(X)}{\mathrm{sd}(X)}, \frac{Y -\mathbb{E}(Y)}{\mathrm{sd}(Y)} \right\rangle$

Thus, $X$ and $Y$ are uncorrelated if and only if the centered variables $X - \mathbb{E}(X)$ and $Y - \mathbb{E}(Y)$ are orthogonal elements of $\mathcal{L}_2$.
</MathBox>

<MathBox title='Properties of the inner product' boxType='proposition'>
The inner product has the following properties for $X, Y, Z\in\mathcal{L}_2$ and $a, b\in\R$

1. Symmetry: $\langle X, Y \rangle = \langle Y, X\rangle$
2. Positive definiteness: $\langle X, X \rangle \geq 0$ and $\langle X , X \rangle = 0$ if and only if $\mathbb{P}(X = 0) = 1$
3. Bilinearity: 
    1. $\langle aX + bY, Z\rangle = a\langle X, Z \rangle + b\langle Y, Z \rangle$
    2. $\langle X, aY + bZ\rangle = a\langle X, Y \rangle + b\langle X, Z \rangle$ 

<details>
<summary>Proof</summary>

1. This follows trivially from the definition.
2. Note that $\mathbb{E}(X^2) \geq 0$ and $\mathbb{E}(X^2) = 0$ if and only if $\mathbb{P}(X = 0) = 1$
3. This follows from linearity of the excpected value.
</details>
</MathBox>

<MathBox title="Hölder's inequality" boxType='proposition'>
Suppose $j, k\in[1, \infty)$ and $\frac{1}{j} + \frac{1}{k} = 1$. For $X\in\mathcal{L}_j$ and $Y\in\mathcal{L}_k$

$$
  \langle |X|, |Y|\rangle \leq \norm{ X\|_j} Y \|_k
$$

When $j = k = 1$, we get the Cauchy-Schwartz inequality

$$
  \mathbb{E}(|X|\cdot|Y|) \leq \sqrt{\mathbb{E}\left(X^2\right)}\sqrt{\mathbb{E}\left(Y^2\right)}
$$

<details>
<summary>Proof</summary>

Note that $S = \Set{ (x,y)\in\R^2 | x\geq 0, y\geq 0 }$ is a convex set and $g(x, y) = x^{1/j}y^{1/k}$ is concave on $S$. From Jensen's inequality, if $U$ and $V$ are nonnegative random variable then 

$$
  \mathbb{E}\left( U^{1/j} V^{1/k} \right) \leq \left[\mathbb{E}(U)\right]^{1/j}\left[\mathbb{E}(K)\right]^{1/k}
$$

Substituting $U = |X|^j$ and $V = |Y|^k$ gives the result. To verify that $g$ is concave on $S$, we compute the Jacobian determinant of the second derivatives

$$
\begin{align*}
  g_{xx} &= \frac{1}{j}\left(\frac{1}{j} - 1\right)x^{1/j - 2}y^{1/k} \\
  g_{yy} &= \frac{1}{k}\left(\frac{1}{k} - 1\right)x^{1/j - 2}y^{1/k} \\
  g_{xy} &= \frac{1}{j}\frac{1}{k}x^{1/j - 1}y^{1/k - 1} \\
\end{align*}
$$

The Jacobian determinant is $g_{xx} g_{yy} - g_{xy}^2 = 0$ on $S$. Since $\frac{1}{j}, \frac{1}{k}< 1$, then $g_{xx}, g_{yy} < 0$ on $S$. Thus, the second derivative matrix is negative semi-definite on $S$.
</details>
</MathBox>

<MathBox title='Parallellogram rule' boxType='proposition'>
If $X, Y\in\mathcal{L}_2$ then the parallellogram rule holds

$$
  \norm{ X + Y}_2^2 + \norm{ X - Y}_2^2 = 2\norm{ X}_2^2 + 2\norm{ X}_2^2
$$

This is equivalent to the variance identity

$$
  \mathrm{var}(X + Y) + \mathrm{var}(X - Y) = 2\left[\mathrm{var}(X) + \mathrm{var}(Y)\right]
$$

<details>
<summary>Proof</summary>

This follows from bilinearity of the inner product

$$
\begin{align*}
  \norm{ X + Y}_2^2 + \norm{ X - Y}_2^2 &= \langle X + Y, X + Y \rangle + \langle X - Y, X - Y \rangle \\
  &= \left(\langle X, X \rangle + 2\langle X, Y \rangle 0 \langle Y, Y \rangle \right) + \left(\langle X, X \rangle - 2\langle X, Y \rangle 0 \langle Y, Y \rangle \right) \\
  &= 2\norm{ X}_2^2 + 2\norm{ Y}_2^2
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Pythagorean theorem' boxType='theorem'>
If $(X_i)_{i=1}^{n\in\N}$ is a sequence of orthogonal random variables in $\mathcal{L}_2$ with $\langle X_i, X_j \rangle = 0$ for $i\neq j$, then

$$
  \left\norm{ \sum_{i=1}^n X_i \right\|_2^2 = \sum_{i=1}^n} X_i \|_2^2
$$

<details>
<summary>Proof</summary>

This follows from bilinearity of the inner product

$$
  \left\| \sum_{i=1}^n X_i \right\|_2^2 = \left\langle \sum_{i=1}^n X_i , \sum_{j=1}^n X_j \right\rangle = \sum_{i=1}^n \sum_{j=1}^n \langle X_i, X_j \rangle
$$

By the orthogonality assumption this reduces to

$$
  \left\norm{ \sum_{i=1}^n X_i \right\|_2^2 = \sum_{i=1}^n \langle X_i , X_i \rangle = \sum_{i=1}^n} X_i \|_2^2
$$
</details>
</MathBox>

### Projections

<MathBox title='Projection' boxType='definition'>
Let $\mathcal{U}$ be a subspace of $\mathcal{L}_2$ and suppose $X\in\mathcal{L}_2$. The projection of $X$ onto $\mathcal{U}$ is the vector $V\in\mathcal{U}$ with the property that $X - V$ is perpendicular to $\mathcal{U}$

$$
  \langle X - V, U \rangle = 0,\quad U\in\mathcal{U}
$$
</MathBox>

<MathBox title='Properties of the projection' boxType='proposition'>
Let $\mathcal{U}$ be a subspace of $\mathcal{L}_2$ and suppose $X\in\mathcal{L}_2$. The projection of $X$ onto $\mathcal{U}$ has the following properties
1. The projection is unique.
2. If $V$ is the projection of $X$ onto $\mathcal{U}$. Then $\norm{ X - V}_2^2 \leq \norm{ X - U}_2^2$ for all $U\mathcal{U}$. Equality holds if and only if $U\equiv V$.

<details>
<summary>Proof</summary>

1. Suppose $V_1$ and $V_2$ are projections of $X$ on $mathcal{U}$, then by noting that $V_1 - V_2\in\mathcal{U}$

$$
\begin{align*}
  lVert V_1 - V_2 \|_2^2 &= \langle V_1 - V_2, V_1 - V_2 \rangle \\
  &= \langle V_1 - X + X - V_2, V_1 - V_2 \rangle \\
  &= \langle V_1 - X, V_1 - V_2\rangle + \langle X - V_2, V_1 - V_2\rangle \\
  &= 0
\end{align*}
$$

Hence $V_1 \equiv V_2$.
2. If $U\in\mathcal{U}$, then 

$$
  \norm{ X - U}_2^2 &= \norm{ X - V + V - U}_2^2 \\
  &= \norm{ X - V}_2^2 + 2\langle X - V, V - U \rangle + \norm{ V - U}_2^2 \\
  &= \norm{ X - V}_2^2 + \norm{ X - V}_2^2 \\
  &\geq \norm{ X - V}_2^2
$$

Equality holds if and only if $\norm{ V - U}_2^2 = 0 \iff V \equiv U$.
</details>
</MathBox>

<MathBox title='Best linear predictor' boxType='proposition'>
If $X\in\mathcal{L}_2$ then the set $\mathcal{W}_X = \Set{ a + bX | a,b\in\R }$ is the subspace of $\mathcal{L}_2$ generated by $X$ and $1$.

Recall that for $X, Y\in\mathcal{L}_2$, the best linear predictor of $Y$ based on $X$ is 

$$
  L(Y\mid X) = \mathbb{E}(Y) + \frac{\mathrm{var}(X, Y)}{\mathrm{var}(X)}[X - \mathbb{E}(X)]
$$

The best linear linear predictor $L(Y\mid X)$ is the projection of $Y$ onto $\mathcal{W}_X$

<details>
<summary>Proof</summary>

Note that $\mathcal{W}_X$ is the set of all linear combinations of the vectors $1$ and $X$. To show that $\mathcal{W}_X$ is a subspace of $\mathcal{L}_2$, we check the closure properties
1. If $U, V\in\mathcal{W}_X$, then $U+V\in\mathcal{W}_X$.
2. If $c \in\R$, then $cU\in\mathcal{W}_X$.

Note that $L(Y\mid X)\in\mathcal{W}_X$. To prove that $L(Y\mid X)$ is the projection of $Y$ onto $\mathcal{W}_X$ we just need to show that $Y - L(Y\mid X)$ is ortogonal to $\mathcal{W}_X$. Since $\mathcal{W}_X$ is spanned by $X$ and $1$, it suffices to show that

1. $\langle Y - L(Y\mid X), X \rangle = 0$
2. $\langle Y - L(Y\mid X), 1 \rangle = 0$

For the first inner product, note that $\mathbb{E}(X[X - \mathbb{E}(X)]) = \mathrm{var}(X)$. Thus, 

$$
\begin{align*}
  \mathbb{E}[XL(Y\mid X)] &= \mathbb{E}(X)\mathbb{E}(Y) + \mathrm{cov}(X, Y) \\
  &= \mathbb{E}(XY)
$$

which shows $(1)$. By linearity, $\mathbb{E}[L(Y\mid X)] = \mathbb{E}(Y)$ so that $(2)$ holds as well.
</details>
</MathBox>

