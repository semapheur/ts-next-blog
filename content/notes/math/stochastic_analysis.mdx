---
title: 'Stochastic Analysis'
subject: 'Mathematics'
showToc: true
references:
  - book_baldi_2017
  - book_kusuoka_2020
  - book_malliavin_1997
  - book_ogawa_2017
  - book_russo_vallois_2022
---

# Semimartingales

## Finite variation processes

<MathBox title='Finite variation process' boxType="definition">
Given a probability space $(\Omega,\mathcal{F},\mathbb{P})$ with filtration $\mathscr{F} = \Set{\mathcal{F}_t}_{t\geq 0}$, a process $A:\Omega\times\R_+ \to\R$ is called a finite variation process if it satisfies the following conditions.
1. $A$ is continuous for any $\omega\in\Omega$.
2. $A_0 = 0$ for any $\omega\in\Omega$
3. For any $\omega\in\Omega$ there is a signed measure $\mu$ such that for any $t\geq 0$
$$
  A_t = \mu([0,t])
$$

Note that the continuity and initial value assumptions implies that $\mu$ has no atoms.

<details>
<summary>Details</summary>

Note that the decomposition $\mu = \mu_+ - \mu_-$ as the difference between two positive measures is not unique. However, it is unique when constrained to

$$
  \operatorname{supp}(\mu_+) \cap\mathrm{\mu_-} = \emptyset 
$$

The uniqueness of such a composition follows the identity $\mu_+ (B) = \sup\Set{\mu(C) | C\subset B, C \text{ is a Borel set}}$. To show existence, write $\mu = \tilde{\mu}_+ - \tilde{\mu}_-$ for some positive measures $\tilde{\mu}_+$ and $\tilde{\mu}_-$. Then $\tilde{\mu}_+$ (respectively $\tilde{\mu}_-$) is absolutely continuous with respect to $\tilde{\mu} = \tilde{\mu}_+ + \tilde{\mu}_-$. By the Radon-Nikodym theorem, $\tilde{\mu}_+$ has a density $\lambda_+ (t)$ (respectively $\lambda_- (t)$) with respect to $\tilde{\mu}$. Then, the choice

$$
\begin{align*}
  \mu_+ (\d t) =& \max(\lambda_+ (t) - \lambda_- (t), 0)\tilde{\mu}(\d t) \\
  \mu_- (\d t) =& \max(\lambda_- (t) - \lambda_+ (t), 0)\tilde{\mu}(\d t)
\end{align*}
$$

gives the expected decomposition. Letting $S_+$ (respectively $S_-$) denote $\operatorname{supp}(\mu_+)$ (respectively $\operatorname{supp}(\mu_-)$) and $|\mu| = \mu_+ + \mu_-$, we get

$$
  \frac{\d\mu}{\d|\mu|} = \mathbf{1}_{S_+} - \mathbf{1}_{S_-}
$$

Moreover, for a finite variation process, $A(t) = m\mu_+ ([0,t]) - m\mu_- ([0,t])$. As $A$ is a continuous, $\mu_+$ and $\mu_-$ have no atoms because they have disjoint supports. Thus, $A$ is the difference of two continuous increasing functions beginning at $0$. This proves that for any $t > 0$

$$
  \sup_{0=t_0 < \cdots < t_n = t} \sum_{k=1}^n |A_{t_k} - A_{t_{k-1}}| < \infty
$$

where the supremum is over all $n\in\N$ and subdivisions of $[0,t]$.
</details>
</MathBox>

<MathBox title='' boxType="theorem">
Let $A$ be a finite variation process. Then for any $t > 0$

$$
  \sup_{0=t_0 < \cdots < t_n = t} \sum_{k=1}^n |A_{t_k} - A_{t_{k-1}}| = |\mu|([0,t])
$$

where the supremum is over all $n\in\N$ and subdivisions of $[0,t]$.

<details>
<summary>Proof</summary>

The inequality 

$$
  \sup_{0=t_0 < \cdots < t_n = t} \sum_{k=1}^n |A_{t_k} - A_{t_{k-1}}| \leq |\mu| ([0,t])
$$

is obvious because

$$
  |A_{t_k} - A_{t_{k-1}}| = |\mu((t_{t_{k-1}}, t_k])| \leq |\mu|((t_{k-1}, t_k])
$$

Consider any sequence of refined subdivisions of $[0,t]$ with step going to $0$, noted $0 = t_0^{(n)} < \cdots < t_{p_n}^{(n)} = t$, and the filtration $\mathcal{F}_0 \subset\cdots\subset\mathcal{F}_n \subset\cdots\subset \mathcal{B}([0,t])$ defined a subsets of the Borel algebra by

$$
  \mathcal{F}_n = \sigma((t_{k-1}^{(n)}, t_k^{(n)}])_{1\leq k\leq p_n}
$$

This is indeed a filtration because the subdivisions are refined. Take $\Omega = [0,t]$ and the probability measure

$$
  \mathbb{P}(\d s) = \frac{|\mu|(\d s)}{|\mu|([0,t])}
$$

on $\Omega$. On the probability space $(\Omega,\mathcal{B}([0,t]), \mathbb{P})$ with filtration $\mathscr{F} = \Set{\mathcal{F}_n}_{n\in\N}$, consider the random variables

$$
  X(s) = \mathbf{1}_{S_+} (s) - \mathbf{1}_{S_-} (s)
$$

and when $s\in (t_{k-1}^{(n)}, t_k^{(n)}]$

$$
\begin{align*}
  X_n =& \mathbb{E}(X|\mathcal{F}_n)(s) \\
  =& \frac{\mu((t_{k-1}^{(n)}, t_k^{(n)}])}{|\mu|((t_{k-1}^{(n)}, t_k^{(n)}])} \\
  =& \frac{A_{t_k^{(n)}} - A_{t_{k-1}^{(n)}}}{|\mu|((t_{k-1}^{(n)}, t_k^{(n)}])}
\end{align*}
$$

As $\Set{X_n}_{n\in\N}$ is a bounded martingale, it converges almost surely and in $\mathcal{L}^1$ to some $Y\in\mathcal{L}^1$ and $X_n = \mathbb{E}(Y|\mathcal{F}_n)$ As a consequence, $\mathbb{E}(X - Y | \mathcal{F}_n) = 0$ for any $n$. Since $X$ and $Y$ are in $\bigwedge_n \mathcal{F_n}$ (concerning $X$, this is a consequence of the time step going to $0$), this implies $X = Y$ almost surely. Thus, $X_n \to X$ in $\mathcal{L}^1$, so in particular $\mathbb{E}(|X_n|)\to\mathbb{E}(|X|)$, which means that

$$
  \sum_{k=1}^{p_n} |A_{t_k^{(n)}} - A_{t_{k-1}^{(n)}}| \xrightarrow{n\to\infty} |\mu|([0,t])
$$
</details>
</MathBox>

If $A$ is a finite variation process and $F:[0,t]\to\R$ is a process, measurable for any given $\omega$, such that $\int_0^t |F(s)|\cdots|\mu|(\d s)$ is finite, then we define

$$
\begin{align*}
  \int_0^t F(s)\;\d A_s =& \int_0^t F(s)\mu(\d s) \\
  \int_0^t F(s)\;|\d A_s| =& \int_0^t F(s)|\mu|(\d s)
\end{align*}
$$

<MathBox title='' boxType="proposition">
Let $A$ be a finite variation process and $F:\Omega\times[0,t]\to\R$ a left-continuous process. Then for any $\omega\in\Omega$

$$
\begin{align*}
  \int_0^t F(s)\;\d A_s =& \lim_{n\to\infty} \sum_{k=1}^{p_n} F\left(t_{k-1}^{(n)}\right) \left(A_{t_k^{(n)}} - A_{t_{k-1}^{(n)}}\right) \\
  \int_0^t F(s)\;|\d A_s| =& \lim_{n\to\infty} \sum_{k=1}^{p_n} F\left(t_{k-1}^{(n)}\right) \left|A_{t_k^{(n)}} - A_{t_{k-1}^{(n)}}\right| 
\end{align*}
$$

for sequence of subdivisions of $[0,t]$, of the form $0 = t_0^{(n)} < \cdots < t_{p_n}^{(n)} = t$, with step going to $0$. For the second identity we require the subdivisions to be refined.

<details>
<summary>Proof</summary>

Let $F_n$ be the process defined as $F\left( t_{k-1}^{(n)} \right)$ on $(t_{k-1}^{(n)}, t_k^{(n)}]$. Then, the right hand side of the first identity is $\int_0^t F_n (s) \mu(\d s)$, so the result follows by dominated convergence. For the second identity, we have

$$
\begin{align*}
  &\left| \sum_{k=1}^{p_n} F\left( t_{k-1}^{(n)} \right) \left| A_{t_k^{(n)}} - A_{t_{k-1}^{(n)}} \right| - \int_0^t F_n(s)\;\d A_s \right| \\
  \leq& \norm{ F }_{\mathcal{L}^\infty [0,1]} \left( |\mu|([0,t]) - \sum_{k=1}^{p_n} \left| A_{t_k^{(n)}} - A_{t_{k-1}^{(n)}} \right| \right)
\end{align*}
$$

From the proof of the previous theorem, this converges to $0$ along any refined sequence of subdivisions with step going to $0$, hence proving that

$$
  \int_0^t F_n(s)\;|\d A_s| \xrightarrow{n\to\infty} \int_0^t F(s)\;|\d A_s|
$$

is sufficient, and true by dominated convergence.
</details>
</MathBox>

<MathBox title='' boxType="proposition">
Given a probability space $(\Omega,\mathcal{F},\mathbb{P})$ with filtration $\mathscr{F} = \Set{\mathcal{F}_t}_{t\geq 0}$, let $A$ be a finite variation process and $F$ progressively measurable such that $\int_0^t |F_s|\cdot|\d A_s| < \infty$ for any $\omega\in\Omega$ and $t\geq 0$. Then

$$
  F\cdot A: (\omega, t)\mapsto \int_0^t F_s(\omega)\;\d A_s(\omega)
$$

is a finite variation process.

<details>
<summary>Proof</summary>

Let $\mu$ be the signed measure associated to $A$. The process $F\cdot A$ begins at $0$, is continuous and has bounded variation because

$$
  (F\cdot A)_t = \tilde{\mu}([0,t]), \tilde{\mu}(\d s) = H_s \mu(\d s)
$$

with $\tilde{\mu}$ a signed measure with no atoms (the finite mass condition holds since $\int_0^t |F_s|\cdot|\d A_s| < \infty$). Consequently, the only condition to verify carefully is the adaptedness of $F\cdot A$. It is true that $(F\cdot A)_t$ is $\mathcal{F}_t$-measurable if $F$ is ot type $\mathbf{1}_{(u,v]} (s) \mathbf{1}_{A} (\omega)$, where $u,v\leq t$ and $A\in\mathcal{F}_t$. It is then true if $F = \mathbf{1}_A$ for any $A\in\mathcal{B}([0,t])\otimes\mathcal{F}_t$ by the monotone class theorem. Finally, by taking linear combinations of such sums approximating $F$ from below, and using dominated convergence (domination by an integrable process holds since $\int_0^t |F_s|\cdot|\d A_s| < \infty$), we get the result for general $F$, because the pointwise limit of measurable functions is measurable.
</details>
</MathBox>

## Local martingales

<MathBox title='Quadratic variation of a martingale' boxType="theorem">
A process $\Set{M_t}_{t\geq 0}$ is a local martingale beginning at $0$ if it is adapted and satisfies
1. $M_0 = 0$ for all $\omega$
2. $M$ is continuous for all $\omega$
3. There exists a sequence of stopping times $T_n$ converging to $\infty$ for any $\omega$ such that $M^{T_n} := \Set{ M_{t\wedge T_n}}_{t\geq 0}$, is a uniformly integrable martingale.

For such a sequence of stopping times, $\Set{T_n}_{n\in\N}$ reduces $M$. A process $M$ is a local martingale if $M_t = M_0 + N_t$, where $M_0\in\mathcal{F}_0$ and $N$ is a local martingale beginning at $0$.
</MathBox>

<MathBox title='Quadratic variation of a martingale' boxType="theorem">
For a given probability space $(\Omega,\mathcal{F},\mathbb{P})$ with filter $\mathscr{F} = \Set{\mathcal{F}_t}_{t\geq 0}$, all the following statements hold.
1. Any continuous martingale is a local martingale.
2. There exists a sequence of stopping times $T_n$ converging to $\infty$ for any $\omega$ such that $M^{T_n} := \Set{M_{t\wedge T_n}}_{t\geq 0}$ for all $n\in\N$ is a martingale.
3. If $M$ is a local martingale and $T$ is a stopping time, then $M^T = \Set{M_{t\wedge T}}_{t\geq 0}$ is a local martingale.
4. If $M$ is a local martingale, $\Set{T_n}_{n\in\N}$ reduces $M$, and $\Set{S_n}_{n\in\N}$ are stopping times converging to $\infty$, then $\Set{S_n \wedge T_n}_{n\in\N}$ reduces $M$.
5. The set of local martingales is a vector space.
6. If $M$ is a nonnegative local martingale and $M_0 \in \mathcal{L}^1$, then $M$ is a supermartingale.
7. If $M$ is a local martingale and $|M_t| \leq X$ for all $t\geq 0$, where $X\in\mathcal{L}^1$, then $M$ is a martingale.
8. If $M$ is a local martingale beginning at $0$, then $T_n = \inf\{t\geq 0 : |M_t| = n\}$ reduces $M$.

<details>
<summary>Proof</summary>

Part **(1)** follows from the possible case $T_n = n$. Then for any constant $c \geq 0$, $\Set{M_{t\wedge c}}_{t\geq 0}$ is a uniformly integrable, as all of its values are of type $\mathbb{E}(M_c | \mathcal{G})$ for some $\sigma$-algebra $\mathcal{G}$ and $M_c \in\mathcal{L}^1$.

For part **(2)** note that if $T_n \to\infty$ and $M^{T_n}$ is a martingale, then $M^{T_n \wedge n}$ is uniformly integrable, as shown for **(1)**., and $T_n \wedge n \to\finty$.

For part **(3)** and **(4)** note that if $M^{T_n}$ is a uniformly integrable martingale, so is $M^{T_n \wedge T}$. The stability by addition mentioned in **(5)** is a dircet consequence of **(4)**, by choosing $\Set{T_n}_{n\in\N}$ reducing the first martingale and $\Set{S_n}_{n\in\N}$ reducing the second. Point **(6)** is a consequence of Fatou's lemma: if $M = M_0 + N$ and $\Set{T_n}_{n\in\N}$ reduces $N$, then

$$
\begin{align*}
  \mathbb{E}(M_t|\mathcal{F}_s) =& \mathbb{E}\left(\lim_{n\to\infty} M_{t\wedge T_n} \right) \\
  \leq& \liminf{n\to\infty} \mathbb{E}(M_{t\wedge T_n | \mathcal{F}_s}) \\
  =& \liminf_{n\to\infty} M_{s\wedge T_n} = M_s
\end{align*}
$$

Note that $M_t$ is in $\mathcal{L}^1$ precisely thanks to the above equation. The result **(7)** relies on dominated convergence applied to the indetity

$$
  M_{s\wedge T_n} = \mathbb{E}(M_{t\wedge T_n} | \mathcal{F}_s)
$$

where $\Set{T_n}_{n\in\N}$ reduces $M$. Finally, **(8)** is a direct consequence of **(2)** and **(7)**.
</details>
</MathBox>

<MathBox title='Indistinguishability of local martingales' boxType="theorem">
Let $M$ be a local martingale beginning at 0$. If $M$ is a finite variation process, then $M$ is indistinguishable from $0$.

<details>
<summary>Proof</summary>

Assume $M$ is a finite variation process, and choose

$$
  T_n = \inf\{ t\geq 0 | \int_0^t |\d M_s| \geq n \}
$$

Then $T_n \to\infty$ and $T_n$ is a stopping time. The local martingale $M^{T_n}$ is bounded by $n$, so it is a martingale by properties of local martingales. As a consequence, for any subdivision $0 = t_0 < \cdots < t_p = t$

$$
\begin{align*}
  \mathbb{E}[(M_t^{T_n})^2] =& \sum_{k=1}^n \mathbb{E}[(M_{t_k}^{T_n})^2 - (M_{t_{k-1}}^{T_n})^2] \\
  =& \sum_{k=1}^p \mathbb{E}[(M_{t_k}^{T_n} - M_{t_{k-1}}^{T_n})^2] \\
  \leq& \mathbb{E}\left( \max{\ell}\left|M_{t_\ell}^{T_n} - M_{t_{\ell - 1}}^{T_n} \right| \sum_{k=1}^ns |M_{t_k}^{T_n} - M_{t_{k-1}}^{T_}| \right) \\
  \leq& n\mathbb{E}\left( \max_{\ell} |M_{t_\ell}^{T_n} - M_{t_{\ell - 1}}^{T_n} | \right)
\end{align*}
$$

As this maximum is bounded by $n$ and $M$ has continuous trajectories, dominated convergence allows to conclude that $\mathbb{E}[(M_t^{T_n})^2] = 0$, by choosing subdivisions with time step going to $0$. By Fatou's lemma, one can take the $n\to\infty$ limit to conclude $\mathbb{E}(M_t^2) = 0$, so $M_t = 0$ almost surely. As $M$ is continuous, this is equivalent to being indistinguishable from $0$.
</details>
</MathBox>

<MathBox title='Quadratic variation of a martingale' boxType="theorem">
Let $M$ be a local martingale. Then there exists a unique (up to indistinguishability) increasing continuous variation process, noted $\langle M, M \rangle$, such that $\Set{M_t^2 - \langle M, M \rangle_t}_{t\geq 0}$ is a local martingale. Moreover, if $\Set{ 0 = t_0^{(n)} < t_1^{(n)} < \cdots}_{n\in\N_+}$ is any sequence of subdivisions of $\R_+$, with step going to $0$, then

$$
  \langle M, M \rangle_t = \lim_{n\to\infty} \sum_{k\geq 1} \left( M_{t_k^{(n)} \wedge t} - M_{t_{k-1}^{(n)}\wedge t} \right)^2
$$

uniformly in the sense of convergence in probability. The process $\langle M, M \rangle$, often noted $\langle M \rangle$, is called the *bracket* or *quadratic variation* of $M$.

<details>
<summary>Proof</summary>

Uniqueness of quadratic variation is an easy consequence of indistinguishability. We will first prove the existence of the bracket when $M$ is a true martingale, and $|M|$ is almost surely bounded by some $K > 0$. For a subdivision $\delta = \Set{0 = t_0 < t_1 < \dots}$ and a process $Y$, we note

$$
  Q_t^{(Y,\delta)} = \sum_{k\in\N_+} \left( Y_{t_k^{(n)} \wedge t} - Y_{t_{k-1}^{(n)}\wedge t} \right)^2 
$$

Furthermore,

$$
\begin{align*}
  X_t^{(\delta)} :=& M_t^2 - Q_t^{(M,\delta)} \\
  =& M_t^2 - \sum_{k\in\N_+} \left( M_{t_k^{(n)} \wedge t} - M_{t_{k-1}^{(n)}\wedge t} \right)^2 \\
  =& 2\sum_{k\in\N_+} M_{t_{k-1}^{(n)}} \left( M_{t_k^{(n)}} - M_{t_{k-1}^{(n)} \wedge t} \right)
\end{align*}
$$

Thus, $\Set{X_t^{(\delta)}}_{t\geq 0}$ is a continuous martingale. For a sequence $\Set{\delta_n}_{n\in\N}$ of subdivisions with step going to 0, we want to find a subsequence of $\Set{X^{(\delta_n)}}_{n\geq 0}$ converging uniformly on compact sets. Note that

$$
  \delta_t^{(n,m)} =& X_t^{(\delta_n)} - \delta_t^{(\delta_m)} = Q_t^{(M,\delta_m)} - Q_t^{(M,\delta_n)}
$$

which is a martingale, so

$$
  \Set{(\delta_t^{(n,m)})^2 - Q_t^{(\delta^{(n,m)},\delta_n \cup \delta_m)}}_{t\geq 0}
$$

is a martingale as well, by the same decomposition used to prove that $X^{(\delta)}$ is a martingale. As a consequence, the expectation of $(\delta_t^{(n,m)})^2$ is also a discrete analogue of the quadratic variation of a finite variation process. We therefore expect this to go to $0$, which would prove that the sequence of $\Set{X_t^{(\delta_n)}}_{n\in\N}$ is a Cauchy sequence in $\mathcal{L}^2$, hence converging. Note that, $(a-b)^2 \leq 2(a^2 + b^2)$, then

$$
  Q_t^{A-B,\delta} \leq 2\left( Q_t^{A,\delta} - Q_t^{B,\delta} \right)
$$

so in order to prove that $\mathbb{E}[(\delta_t^{(n,m)})^2]$ converges to $0$, a sufficient condition is

$$
  \mathbb{E}(Q_t^{(Q^{(M,\delta_n)}, \delta_n \cup \delta_m)}) \xrightarrow{n,m\to\infty} 0
$$

Note that $\varepsilon_n = \sup_{u,v\in[0,t]} |M_u - M_v|$ is the supremum such that $u - v$ is smaller than the time step of $\delta_n$. Then if $s_{k-1}$ and $s_k$ are successive elements of $\delta_n \cup \delta_m$, then $|Q_{s_k}^{(M,\delta_n)} - Q_{s_{k-1}}^{(M,\delta_n)}| \leq \varepsilon_n |M_{s_k} - M_{s_{k-1}}|$, hence

$$
  Q_t^{(Q^{(M,\delta)}, \delta_n \cup\delta_m)} \leq \varepsilon_n^2 \sum_{k\in\N_+} (M_{s_k \wedge t} - M_{s_{k-1} \wedge t})^2
$$

Note that for any subdivision $\delta$

$$
\begin{align*}
  (Q_t^{(M,\delta)})^2 =& \sum_{k\in\N_+} (M_{s_k \wedge t} - M_{s_{k-1}\wedge t})^4 \\
  &+ \sum_{k\in\N} (Q_{s_k\wedge t}^{(M,\delta)} - Q_{s_{k-1}\wedge t}^{M,\delta})(Q_t^{(M,\delta)} - Q_{s_k \wedge t}^{(M,\delta)})
\end{align*}
$$

As $X^(\delta)$ is a martingale, $\mathbb{E}(Q_t^{(M,\delta)} - Q_{s_{k\wedge t}}^{M,\delta} | \mathcal{F}_{s_k \wedge t} ) = \mathbb{E}(M_t^2 - M_{s_k \wedge t}^2 | \mathcal{F}_{s_k \wedge t})$, so using $|M| \leq K$, we get

$$
\begin{align*}
  \mathbb{E}[(Q_t^{(M,\delta)})^2] \leq& 4K^2 \left( \mathbb{E}(Q_t^{(M,\delta)}) + \sum_{k\in\N_+} (Q_{s_k \wedge t}^{(M,\delta)}) - Q_{s_{k-1}\wedge t}^{(M,\delta)} \right) \\
  =& 8K^2 \mathbb{E}(Q_t^{(M,\delta)}) = 8K^2 \mathbb{M_t^2} \leq 8K^4
\end{align*}
$$

As the quadratic increments is uniformly bounded in $\mathcal{L}^2$ by $8K^4$, we get by the Cauchy-Schwarz inequality

$$
  \mathbb{E}(Q_t^{(Q^{(M,\delta_n)},\delta_n \cup\delta_m)}) \leq (8K^4 \mathbb{E}(\varepsilon_n^4))^{1/2}
$$

By dominated convergence, where $\varepsilon_n \to 0$ almost surely and $\varepsilon_n \leq 2K$, this goes to $0$ in the limit $n\to\infty$. It follows that $\Delta_t^{(m,n)} \xrightarrow{n,n\to\infty} 0$ in $\mathcal{L}^2$. By Doob's inequality, this implies that

$$
  \mathbb{E}\left[ \left( \sup_{[0,t]} (X^{(\delta_n)} - X^{(\delta_m)}) \right)\right] \xrightarrow{n,m\to\infty} 0
$$

so there is a subsequence of the $X^{(\delta_n)}$ converging almost surely, uniformly, on $[0,t]$. Let $X$ denote this (continuous) limit. As the subsequence of the $X^{(\delta)}$ converge to $X$ in $\mathcal{L}^2$, their martingale property is preserved in the limit, hence $X$ is a martingale. Moreover, from the definition of $Q_t^{(M,\delta)}$ it follows that $M^2 - X^{(\delta)}$ is an increasing process. This property holds for $M^2 - X$ by uniform convergence. For $s\in[0,t]$ define

$$
  \langle M \rangle_s := M_s^2 - X_s
$$

From the previous discussion, $\langle M \rangle$ satisfies all required properties of the bracket on $[0,t]$. By uniqueness of the bracket, the value $\langle M \rangle_s$ is independent of the choice of the horizon $t\geq s$, and of the choice of the subsequence providing uniform convergence. Moreover, the above reasoning has proved that $X_s^{(\delta_n)} - X_s^{(\delta_m)}$ is Cauchy sequence in $\mathcal{L}^2$, as $\varepsilon_n$ can be chose indentical for any choice of $s\in[0,\t]$, the convergence is in $L^2$ and uninform on compact sets.

This bounded martingale case extends easily. First, note that if the result is true for local martingales beginning at $0$, it is true for local martingales. If $M_t = M_0 + N_n$ with $M_0 \in\mathcal{F}_0$ and $N$ is a local martingale beginning at $0$, as $M_0 N$ is a local martingale, so is $M_t^2 - \langle N \rangle_t = N_t^2 - \langle N \rangle_t + M_0^2 + 2M_0 N_t$. Thus, we can assume $M_0 = 0$.

We localize $M$ by $T_n = \inf\{ t \geq 0 : |M_t| = n \}$. Then $M^{T_n}$ is a local martingale, bounded by $n$, so we can apply the previous argument: there is an increasing process, noted $\langle M \rangle^{(n)}$ usch that $(M^{T_n})^2 - \langle M \rangle^{(n)}$ is a martingale. By uniqueness, $(\langle M \rangle^{(n)})^{T_m} = \langle M \rangle^{(m)}$ for $m \leq n$. From this coherence property, we can define a process $\langle M \rangle$ such that $(M^{T_n})^2 - \langle M \rangle^{T_n}$ is a martingale. As $T_n \to\infty$ almost surely, this means that $M^2 - \langle M \rangle$ is a local martingale.

The property of uniform convergence of quadratic increments to the bracket also holds for $(M^{T_n})^2 - \langle M \rangle^{T_n}$ in $\mathcal{L}^2$. Since $\mathbb{P}(T_n \leq t) \xrightarrow{n\to\infty} 0$ by dominated or monotone convergence, it follows that the uniform converge holds in probability.
</details>
</MathBox>

<MathBox title='' boxType="proposition">
Let $M$ and $N$ be two local martingales. Then

$$
  \langle M, N \rangle = \frac{1}{2} (\langle M + N, M + N \rangle - \langle M, M \rangle - \langle N, N \rangle)
$$

satisfying the following.
1. Up to indistinguishability, $\langle M, N \rangle$ is the unique finite variation process such that $MN - \langle M,N\rangle$ is a local martingale.
2. The function $(M, N) \mapsto \langle M, N \rangle$ is symmetric and bilinear.
3. If $(0 = t_0^{(n)} < t_1^{(n)} < \dots)_{n\in\N}$ is any sequence of subdivisions of $\R_+$ with step going to $0$, then
$$
  \langle M, N \rangle_t = \lim_{n\to\infty} \sum_{k\in\N_+} (M_{t_k^{(n)}\wedge t} - M_{t_{k-1}^{(n)} \wedge t}) (N_{t_k^{(n)}\wedge t} - N_{t_{k-1}^{(n)} \wedge t})
$$
in probability, uniformly for $t$ in compact sets.
4. For any stopping time $T$, then $\langle M, N \rangle_{t\wedge T} = \langle M^T, N \rangle_t = \langle M^T ,N^T \rangle_t$ for $t\geq 0$.
</MathBox>

<MathBox title='Kumita-Watanabe inequality' boxType="theorem">
Let $H$ and $K$ be progressively measurable processes and suppose $M$ and $N$ are local martingales. Then, for any $t\in\R_+ \cup\Set{\infty}$

$$
  \int_0^t |H_s K_s|\cdots|\d\langle M, N \rangle_s | \leq \left( \int_0^t H_s^2 \;\d\langle M \rangle_s \right)^{1/2} \left( \int_0^t K_s^2 \;\d\langle N \rangle_s \right)^{1/2}
$$

<details>
<summary>Proof</summary>

Note that from properties of quadratic variation of two martingales and the Cauchy-Schwarz inequality we have

$$
  |\langle M, N \rangle_t - \langle M, N \rangle_s | \leq (\langle M \rangle_t - \langle M \rangle_s)^{1/2} (\langle N \rangle_t - \langle N \rangle_s )^{1/2}
$$

almost surely. Using Cauchy-Schwarz again, for $s = t_0 < \cdots < t_n = t$, the above inequality yields

$$
\begin{align*}
  &\sum_{k=1}^n |\langle M, N \rangle_{t_k} - \langle M, N \rangle_{t_{k-1}} | \\
  \leq& \sum_{k=1}^n (\langle M \rangle_{t_k} - \langle M \rangle_{t_{k-1}})^{1/2} (\langle N \rangle_{t_k} - \langle N \rangle_{t_{k-1}})^{1/2} \\
  \leq& \left( sum_{k=1}^n (\langle M \rangle_{t_k} - \langle M \rangle_{t_{k-1}})\right)^{1/2} \left((\langle N \rangle_{t_k} - \langle N \rangle_{t_{k-1}})\right)^{1/2} \\
  =& (\langle M \rangle_t - \langle M \rangle_s )^{1/2} (\langle N \rangle_t - \langle N \rangle_s)^{1/2}
\end{align*}
$$

By the finite variation supremum theorem, we get

$$
  \int_s^t |\d\langle M, N \rangle_u | \leq \left( \int_s^t \d\langle M \rangle_u \right)^{1/2} \left( \int_s^t \d\langle N \rangle_u \right)^{1/2}
$$

For functions of type $H = \sum h_\ell \mathbf{1}_{B_\ell}$ and $K = \sum k_\ell \mathbf{1}_{B_\ell}$, with disjoint bounded Borel sets $B_i$, we have

$$
\begin{align*}
  \int |H_s K_s |\cdot |\d\langle M, N \rangle_s | =& \sum_\ell |h_\ell k_\ell | \int_{B_\ell} |\d\langle M, N \rangle_u | \\
  \leq& \sum_\ell |h_\ell k_\ell| \left( \int_{B_\ell} \d\langle M \rangle_u \right)^{1/2} \left( \int_{B_\ell} \d\langle N \rangle_u \right)^{1/2} \\
  \leq& \left( \sum_\ell h_\ell^2 \int_{B_\ell} \d\langle M \rangle_u \right)^{1/2} \left( \sum_\ell k_\ell^2 \int_{B_\ell} \d\langle N \rangle_u \right)^{1/2} \\
  =& \left( \int_0^t H_s^2 \d\langle M \rangle_s \right)^{1/2} \left( \int_0^t K_s^2 \d\langle N \rangle_s \right)^{1/2}
\end{align*}
$$

Approximation of progressively measurable processes as increasing limit of such functions completes the proof.
</details>
</MathBox>

<MathBox title='Integrability of local martingales' boxType="theorem">
Let $M$ be a local martingale with $M_0 = 0$.
1. The process $M$ is an $\mathcal{L}^2$-bounded martingale, i.e. $\sup_{t\geq 0} \mathbb{E}(|M_t|^2) < \infty$, if and only if $\mathbb{E}(\langle M \rangle_\infty) < \infty$. In such a case, $M^2 - \langle M \rangle$ is a uniformly integrable martingale.
2. The process $M$ is a square integrable martingale if and only if $\mathbb{E}(\langle M \rangle_t ) < \infty$ for $t\geq 0$. In such a case, $M^2 - \langle M \rangle$ is a martingale.

<details>
<summary>Proof</summary>

**(1):** Assume first that $M$ is an $\mathbb{L}^2$-bounded martingale. It is therefore uniformly integrable and converges almost surely to some $M_\infty$. Moreover, from Doob's inequality

$$
  \mathbb{E}\left(\sup_{t\geq 0} M_t^2 \right) \leq 4 \sup_{t\geq 0} \mathbb{E}(M_t^2) < \infty
$$

As a consequence, if we define $T_n = \inf\Set{t\geq 0 | \langle M \rangle_t \geq n}$ then

$$
  \Set{M_{t\wedge T_n}^2 - \langle M \rangle_{t\wedge T_n}}_{t\geq 0}
$$

is a local martingale bounded by $\sup_{t\geq 0} (M_t^2) + n\in \mathcal{L}^1$, so it is a true martingale. Thus

$$
  \mathbb{E}(\langle M \rangle_{t\wedge T_n}) = \mathbb{E}(M_{t\wedge T_n}^2)
$$

Dominated convergence allows to take the limit $t\to\infty$ on the right hand side, and monotone convergence on the left side,

$$
  \mathbb{E}(\langle M \rangle_{T_n}) = \mathbb{E}(M_{T_n}^2)
$$

Monotone convergence on the left and dominated convergence on the right yields

$$
  \mathbb{E}(\langle M \rangle_\infty) = \mathbb{E}(M_\infty^2)
$$

In particular, $\mathbb{E}(\langle M \rangle_\infty)$ if finite. This implies that $M^2 - \langle M \rangle$ is bounded by an integrable random variable $\mathrm{sup}_{t\geq 0} (M_t^2) + \langle M \rangle_\infty $, so it is a uniformly integrable martingale.

Conversely, suppose $\mathbb{E}(\langle M \rangle_\infty) < \infty$ and note that $\tilde{T}_n = \inf\Set{t\geq 0 : |M_t| \geq n}$. Then $M^{\tilde{T}_n}$ (bounded by $n$) and $(M^{\tilde{T}_n})^2 - \langle M \rangle^{\tilde{T}}$ (bounded by $n^2 + \langle M \rangle^{T_n} \in \mathcal{L}^1$) are uniformly integrable martingales. Thus, for any stopping time $S$

$$
  \mathbb{E}(M_{S\wedge\tilde{T}_n}^2) = \mathbb{E}(\langle M \rangle_{S\wedge\tilde{T}_n})
$$

By Fatou's lemma

$$
  \mathbb{E}(M_S^2) \leq \mathbb{E}(\langle M \rangle_S) \leq \mathbb{E}(\langle M \rangle_\infty) < \infty
$$

In particular, $M$ is $\mathcal{L}^2$-bounded. Moreover, it is a martingale and therefore in the identity

$$
  \mathbb{E}(M_{t\wedge\tilde{T}_n} | \mathcal{F}_s) = M_{s\wedge\tilde{T}_n}
$$

the limit $n\to\infty$ is allowed by dominated convergence. Indeed, the $M_{t\wedge\tilde{T}_n}$ are bounded in $\mathcal{L}^2 with (\mathbb{E}(\sup_t M_t^2)) < \infty$, so $\mathbb{E}(\sup_n M_{t\wedge\tilde{T}_n}^2) < \infty$ and hence bounded in $\mathcal{L}^1$.

**(2):** By Doob's inequality, if $M$ is square integrable, $\Set{M_{s\wedge t}}_{c\get 0}$ is $\mathcal{L}^2$-bounded, so by **(1)**, $\mathbb{E}(\langle M \rangle_t) < \infty$. Reciprocally, if $\mathbb{E}(\langle M \rangle_t) < \infty$, then **(1)** implies that $\Set{M_{s\wedge t}}_{c\get 0}$ is bounded in $\mathcal{L}^2$, in particular $\mathbb{E}(M_t^2) < \infty$. Finally, in such a case, from **(1)** the process $\Set{M_{s\wedge t}^2 - \langle M \rangle_{s\wedge t}}_{s\geq 0}$ is a uniformly integrable martingale, so $M^2 - \langle M \rangle$ is a martingale. 
</details>
</MathBox>

<MathBox title='' boxType="corollary">
Let $M$ be a local martingale, with $M_0 = 0$ almost surely. Then $M$ is indistinguishable from $0$ if and only if $\langle M \rangle$ is identically $0$.

<details>
<summary>Proof</summary>

If $M$ is indistinguishable from $0$, it is clear that the bracket vanishes (as a limit of quadratic increments). Reciprocally, if $\langle M \rangle\equiv 0$, then by integrability $M^2$ is a martingale so $\mathbb{E}(M_t^2) = 0$ for any given $t$. Thus, $M_t = 0$ almost surely. One can conclude by continuity that $M$ is indistinguishable from $0$.
</details>
</MathBox>

<MathBox title='Semimartingale process' boxType="definition">
Given a probability space $(\Omega, \mathcal{F}, \mathbb{P})$ with filtration $\mathscr{F} = \Set{\mathcal{F}_t}_{t\geq 0}$, a process $X$ is called a semimartingale if is of type

$$
  X = X_0 + M + A
$$

where $X_0 \in\mathcal{F}_0$, $M$ is a locale martingale beginning at $0$, and $A$ is a finite variation process.
</MathBox>

Note that the decomposition of a semimartingale process is unique up to indistinguishability. As a consequence, we can define without ambiguity the bracket of $X = X_0 + M + A$ and $\tilde{X} = \tilde{X}_0 + \tilde{M} + \tilde{A}$ as

$$
  \langle X, \tilde{X} \rangle = \langle M, \tilde{M} \langle
$$

In particular, the bracket of a finite variation process with any semimartingale is always $0$. Then we can show that the bracket is still given as a limit of increment (the finite variation part does not contribute). Thus, if $(0 < t_0^{(n)} < t_1^{(n)} \dots)_{n\in\N}$ is any sequence of subdivisions of $\R_+$ with step going to $0$, then in the sense of convergence in probability

$$
  \langle X, \tilde{X} \rangle_ t = \lim_{t\to\infty} \sum_{k\in\N_+} \left( X_{t_k^{(n)} \wedge t} - X_{t_{k-1}^{(n)} \wedge t} \right)\left( \tilde{X}_{t_k^{(n)} \wedge t} - \tilde{X}_{t_{k-1}^{(n)} \wedge t} \right)
$$

<MathBox title='' boxType="theorem">
If, given a probability space, the process $Y$ is continuous with independent increments, then it takes the from

$$
  Y = X + F
$$

where $X$ is a semimartingale with independent increments and $F$ is a deterministic continuous function.
</MathBox>

# Brownian motion

# Stochastic integrals

<MathBox title='' boxType="definition">
Let $H$ be a progressively measurable process and let $M$ be a local martingale. We denote $H^2$ the set of continuous $L^2$-bounded martingales, i.e. $M\in H^2$ if $\mathrm{sup}_{t\geq 0} \mathbb{E}((M_t)^2) < \infty$.
</MathBox>

<MathBox title='' boxType="proposition">
The set $H^2$ is a Hilbert space with inner product

$$
  \langle M, N \rangle_{H^2} := \mathbb{E}(\langle M, N \rangle_\infty),\, M,N\in H^2
$$

<details>
<summary>Proof</summary>

Note that, if $M\in H^2$, it converges almost surely with a finite bracket, $\mathbb{E}(\langle M \rangle_\infty) < \infty$. If $M, N\in H^2$, then by Kunita-Watanabe inequality

$$
  \mathbb{E}(|\langle M, N \rangle_\infty|) \leq \mathbb{E}(\langle M \rangle_\infty)^{1/2} \mathbb{E}(\langle N \rangle_\infty)^{1/2}
$$

This means that we can define a scalar product on $H^2$ by

$$
  (M, N)_{H^2} = \mathbb{E}(\langle M, N \rangle_\infty)
$$

and $\norm{ M }_{H^2} = (\langle M \rangle_\infty)^{1/2}$ defines a norm. We have already seen that if $\norm{ M }_{H^2} = 0$, then $M$ is indistinguishable from $0$.

It remains to show that $H^2$ is complete. Consider a Cauchy sequence $(M^{(n)})_{n\in\N}$:

$$
\begin{align*}
  &\lim_{m,n\to\infty} \mathbb{E}\left((M_\infty^{(n)} - M_\infty^{(m)})^2 \right) \\ 
  =& \lim_{m,n\to\infty} \mathbb{E}\left(\langle M^{(n)} - M^{(m)}\rangle_\infty \right) = 0
\end{align*}
$$

By the Doob inequality we have

$$
  \lim_{m,n\to\infty} \mathbb{E}\left( \sup_{t\geq 0} |M_t^{(n)} - M_t^{(m)}|^2 \right) = 0
$$

so we can find an increasing sequence $(n_k)_{k\in\N}$ such that

$$
\begin{align*}
  &\mathbb{E}\left( \sum_{k=1}^infty \sup_{t\geq 0} |M_t^{(n_k)} - M_t^{(n_{k-1})}| \right) \\
  \leq& \sum_{k=1}^infty \mathbb{E}\left( \sup_{t\geq 0} |M_t^{(n_k)} - M_t^{(n_{k-1})}|^2 \right)^{1/2} < \infty
\end{align*}
$$

As a consequence, $\sum_{k=1}^infty \sup_{t\geq 0} |M_t^{(n_k)} - M_t^{(n_{k-1})}|$ is almost surely isFinite, so $M^{(n_k)}$ converges uniformly to some continuous adapted process $M$. Here we have used the fact that the pointwise limit of measurable functions is measurable. For any given $s$ and $t$, $M_t^{(n_k)}$, respectively $M_s^{(n_k)}$, converges in $L^2$ to $M_t$, respectively $M_s$, in the martingale property

$$
  \mathbb{E}(M_t^{(n_k)}|\mathcal{F}_s) = M_s^{(n_k)}
$$

we can take the limits to conclude that $M$ is a martingale. Moreover, as the $M^{(n_k)}$ satisty the Doob inequality above, all $M_t^{(n_k)}$ are uniformly bounded in $L^2$ and $M\in H^2$. finally, $M^{(n_k)}$ converges to $M$ in $H^2$, because

$$
  \mathbb{E}\left(\langle M^{(n_k)} - M \rangle_\infty \right) = \mathbb{E}\left((M_\infty^{(n_k)} - M_\infty)^2 \right) \to \infty
$$

This implies, by the Cauchy condition, that $M^{(n)}$ converges to $M$ in $H^2$ as well.
</details>
</MathBox>

<MathBox title='' boxType="definition">
For $M\in H^2$, let $L^2 (M)$ be the space of progressively measurable processes $H$ such that

$$
  \mathbb{E}\left(\int_0^\infty H_s^2 \d\langle M \rangle_s \right) < \infty
$$

<details>
<summary>Details</summary>

It is easily seen that $L^2 (M) = L^2 (\R_+ \times\Omega, \mathcal{F}, \nu)$, where $\mathcal{F}$ is the progressive $\sigma$-algebra and $\nu(A) = \mathbb{E}(\int_0^\infty \mathbf{1}_A (s,\cdot)\;\d\langle M \rangle_s$ is a well-defined finite measure. Note that $L^2 (M)$ is a Hilbert space with the inner product

$$
  \langle H, K \rangle_{L^2 (M)} = \mathbb{E}\left( \int_0^\infty H_s K_s \;\d\langle M \rangle_s \right)
$$

in the sense that $\norm{ H }_{L^2(M)} = 0$ if and only if $\nu$-almost surely $H = 0$.
</details>
</MathBox>

<MathBox title='' boxType="definition">
The vector subspace of $L^2 (M)$ consisting of step processes is noted $\mathcal{E}$. Specifically, $H\in\mathcal{E}$ is there some $p\geq 1$ and $0 = t_0 < \cdots < t_p$ such that

$$
  H_s (\omega) = \sum_{k=0}^{p-1} H_k (\omega) \mathbb{1}_{(t_k, t_{k+1}]} (s)
$$

where $H_k \in\mathcal{F}_{t_k}$ is bounded.
</MathBox>

<MathBox title='' boxType="proposition">
For any $M\in H^2$, the subspace $\mathcal{E}$ is dense in $L^2 (M)$.

<details>
<summary>Proof</summary>

Recall that $L^2 (M) = L^2 (\R_+ \times\Omega, \mathcal{F}, \nu)$.

We need to prove that if $K\in L^2(M)$ is orthogonal to $\mathcal{E}$, then $K = 0$. If $K$ is orthogonal to $F\mathbf{1}_{(s,t]}\in\mathcal{E}$, where $F\in\mathcal{F}_s$ is bounded, then

$$
  \mathbb{E}\left( F \int_s^t K_u \;\d\langle M \rangle_u \right) = 0
$$

Let $X_t = \int_0^t K_u \;\d\langle M \rangle_u$. Then $X_t \in L^2$, as a consequence of the Cauchy-Schwarz inequality and $M\in H^2$, $K\in L^2 (M)$. It follows that $\mathbb{E}((X_t - X_s)F) = 0$ for any bounded $F\in\mathcal{F}_s$, so $X_s = \mathbb{E}(X_t | \mathcal{F}_s)$. Thus, $X$ is a martingale. Since this is also a finite variation process, it is indistinguishable from $0$. Hence, for any $t\geq 0$

$$
  \int_0^t K_u \;\d\langle M \rangle_u = 0
$$

so $K = 0$ $\nu$-almost everywhere on $\R_+ \times\Omega$.
</details>
</MathBox>

<MathBox title='' boxType="theorem">
Let $M\in H^2$. For $H\in\mathcal{E}$, written

$$
  H_s (\omega) = \sum_{k=0}^{p-1} H_k (\omega) \mathbf{1}_{(t_k, t_{k+1}]} (s)
$$

we associate $H\cdot M\in H^2$ defined by

$$
  (H\cdot M)_t = \sum_{k=1}^p H_k (M_{t_{k+1}\wedge t} - M_{t_k \wedge t})
$$

Then the following results hold.
1. The map $H\mapsto H\cdot M$ can be uniquely extended into an isometry from $L^2 (M)$ to $H^2$.
2. The process $H\cdot M$ obtained by the previous extension is characterized by 
$$
  \langle H\cdot M, N \rangle = H\cdot\langle M, N \rangle,\; N\in H^2
$$
3. If $\tau$ is a stopping time, then
$$
  (\mathbf{1}_{(0,\tau]})\cdot M = (H\cdot M)^\tau = H\cdot M^\tau
$$
4. If $G\in L^2 (M)$ and $H\in L^2 (G\cdot M)$, then $GH\in L^2(M)$ and
$$
  (GH)\cdot M = G\cdot (H\cdot M)
$$

<details>
<summary>Proof</summary>

**(1):** We first check that the map $H \mapsto H\cdot M$ is an isometry $\mathcal{E}\to H^2$. It is easy to check that $H\cdot M$ is an $L^2$-bounded martingale. Since

$$
  \langle H\cdot M \rangle_t = \sum_{k=1}^{p-1} H_k^2 (\langle M \rangle_{t_{k+1}\wedge t} - \langle M \rangle_{t_k \wedge t})
$$

we get

$$
\begin{align*}
  \norm{ H\cdot M }_{H^2}^2 =& \mathbb{E}\left( \sum_{k=1}^{p-1} H_k^2 (\langle M \rangle_{t_{k+1}} - \langle M \rangle_{t_k}) \right) \\
  =& \mathbb{E}\left( \int_0^\infty H_s^2 \;\d\langle M \rangle_s \right) = \norm{ H }_{L^2 (M)}^2
\end{align*}
$$

Since $\mathbb{E}$ is dense in the Hilbert space $L^2(M)$, this isometry can be extended in a unique way as an isometry $(L^2(M), \norm{\cdot}_{L^2 (M)} \cong (H^2, \norm{\cdot}_{H^2})$.

**(2):** For $H\in\mathcal{E}$ we get

$$
\begin{align*}
  \langle H\cdot M, N\rangle_t =& \left\langle \sum_{k=1}^p H_k (M_{t_{k+1} \wedge\cdot} -  M_{t_k \wedge\cdot}), N \right\rangle_t \\
  =& \sum_{k=1}^p H_k \langle M_{t_{k+1}\wedge\cdot} - M_{t_k \wedge\cdot}, N\rangle_t \\
  =& \sum_{k=1}^p H_k (\langle M, N \rangle_{t_{k+1 \wedge t}} - \langle M, N \rangle_{t_k \wedge t}) \\
  =& \int_0^t H_s \;\d\langle M, N \rangle_s
  =& H\cdot\langle M, N \rangle_t
\end{align*}
$$

To prove this for general $H\in L^2(M)$, consider a sequence $H^{(n)}$ in $\mathcal{E}$ converging to $H$ in $L^2 (M)$. Then, by the isometry property, $H^{(n)}\cdot M$ converges to $H\cdot M$ in $H^2$

$$
\begin{align*}
  \langle H\cdot M, N \rangle_\infty =& \lim_{n\to\infty} \langle H^{(n)} \cdot M, N \rangle_\infty \\
  =& \lim_{n\to\infty} (H^{(n)}\cdot\langle M, N \rangle)_\infty \\
  =& (H\cdot\langle M, N \rangle)_\infty
\end{align*}
$$

The first equality is in the sense of an $L^1$-limit, and a consequence of the Kunita-Watanabe inequality with $X = H^{(n)}\cdot M - H\cdot M$

$$
\begin{align*}
  \mathbb{E}(|\langle X, N \rangle_\infty|) \leq& \mathbb{E}(\langle X \rangle_\infty)^{1/2} \mathbb{E}(\langle N \rangle_\infty) \\
  =& \norm{ X }_{H^2} \mathbb{E}(\langle N \rangle_\infty)^{1/2}
\end{align*}
$$

This also proves the second equality. The third equality holds as a limit in $L^1$ and relies on Kunita-Watanabe as well

$$
\begin{align*}
  &\mathbb{E}\left| \int_0^\infty (H_s^{(n)} - H_s)\;\d\langle M, N \rangle_s \right| \\
  =& \mathbb{E}(\langle N \rangle_\infty) \norm{ H^{(n)} - H }_{L^2 (M)}
\end{align*}
$$

We have proven that $\mathbb{E}(|\langle H\cdot M, N \rangle_\infty - (H\cdot\langle M, N\rangle)_\infty |) = 0$ which yields that almost surely

$$
  \langle H\cdot M, N\rangle_\infty = (H\cdot\langle M, N\rangle)_\infty
$$

Choosing $N$ stopped at time $t$ in this identity proves the expected result.

To get the unique characterization of $H\cdot M$ by the identity in **(2)**, note that if $X$ satisfies the same property as $H\cdot M$, then for any $N\in H^2$

$$
  \langle H\cdot M - X, N \rangle_\infty = 0
$$

The choice $N = H\cdot M - X$ proves that $H\cdot M$ and $X$ are indistinguishable.

**(3):** First we prove $(\mathbf{1}_{(0,\tau]} H )\cdot M = (H\cdot M)^\tau$. For this, we just need to check that for any $N\in H^2$

$$
  \langle(\mathbf{1}_{(0,\tau]}H)\cdot M, N\rangle_\infty = \langle (H\cdot M)^\tau, N \rangle_\infty
$$

The left hand side is also $((\mathbf{1}_{(0,\tau]}H)\cdot \langle M, N \rangle)_\infty = (H\cdot\langle M, N\rangle)_\tau$, and the right hand side is $\langle H\cdot M, N \rangle_\tau$. Since $\langle H\cdot M, N\rangle = H\cdot\langle M, N \rangle$, this achieves the proof. The proof for $(H\cdot M)^\tau = H\cdot M^\tau$ proceeds in the same way.

**(4):** The first assertion follows from definitions. Because $\langle H\cdot M\rangle = H\cdot\langle M, H\cdot M\rangle = H^2 \langle M \rangle$, we have

$$
  \int_0^\infty (GH)^2 \;\d\langle M \rangle_s = \int_0^\infty H^2 \;\d\langle G\cdot M\rangle_s < \infty
$$

Since $H\in L^2 (G\cdot M)$, it follows that $GH\in L^2 (M)$. To prove the second assertion, we just need to test it with respect to any $N\in H^2$, using the associativity for integral of Stieltjes type

$$
\begin{align*}
  \langle (GH)\cdot M, N \rangle =& (GH)\cdot \langle M, N \rangle \\
  =& G\cdot (H\cdot\langle M, N \rangle) \\
  =& G\cdot\langle H\cdot M, N \rangle \\
  =& \langle G\cdot (H\cdot M), N\rangle
\end{align*}
$$

Hence, $(GH)\cdot M = G\cdot (H\cdot M)$.
</details>
</MathBox>

Using the notation $(H\cdot M)_t = \int_0^t H_s \;\d M_s$, the associativity property takes the form

$$
  \int_0^t G_s (H_s\;\d M_s) = \int_0^t (G_s H_s)\;\d M_s
$$

When iterated, the 2nd property of the previos theorem yields the joint bracket of two stochastic integrals. Because $\langle \int_0 H_s\;\d M_s, N \rangle = \int_0^t H_s\;\d\langle M, N\rangle_s$, we get for $H\in L^2 (M)$ and $G\in L^2 (N)$

$$
  \left\langle \int_0 H_s \;\d M_s, \int_0 G_s \;\d M_s \right\rangle = \int_0^t (G_s H_s)\;\d\langle M, N\rangle_s
$$

In particular, the first two moments of stochastic integrals become

$$
\begin{align*}
  \mathbb{E}\left( \int_0^t H_s\;\d M_s \right) =& 0 \\
  \mathbb{E}\left[\left( \int_0^t H_s\;\d M_s \right) \left( \int_0^t G_s \;\d N_s \right) \right] =& \mathbb{E}\left( \int_0^t (G_s H_s)\;\d\langle M, N\rangle_s \right)
\end{align*}
$$

<MathBox title='' boxType="theorem">
Let $M$ be a local martingale beginning at $0$ and define

$$
  L^2_{\mathrm{loc}} (M) := \Set{\text{progressively measurable }H | \int_0^t H_s^2 \;\d\langle M \rangle_s \overset{\text{a.s.}}{<} \infty,\; t\geq 0}
$$

For any $H\in L^2_{\mathrm{loc}} (M)$ there is a unique local martingale beginning at $0$, noted $H\cdot M$, such that for any local martingale $N$

$$
  \langle H\cdot M, N\rangle = H\cdot\langle M, N\rangle
$$

For any stopping time $\tau$

$$
  (\mathbf{1}_{(0,\tau]} H) \cdot M = (H\cdot M)^\tau = H\cdot M^\tau
$$

<details>
<summary>Proof</summary>

Note that

$$
  \tau_n = inf\Set{t \geq 0 | \int_0^t (1 + H_s^2)\;\d\langle M \rangle_s = n}
$$

Then $M^{\tau_n} \in H^2$ and $H\in L^2 (M^{\tau_n})$, so we can consider the stochastic integral $H\cdot M^{\tau_n}$ from the previous theorem. From the 3rd property, $(H\cdot M^{\tau_m})^{\tau_n} = H\cdot M^{\tau_n}$ for $m< n$. This coherence relation proves that there is a process, noted $H\cdot M$, such that $(H\cdot M)^{\tau_n} = H\cdot M^{\tau_n}$ for any $n\in\N$. Because $H\in L^2_{\mathrm{loc}}(M)$, then $\tau_n \xrightarrow{n\to\infty} \infty$. Hence, since $(H\cdot M)^{\tau_n}$ is a martingale, $H\cdot M$ is a local martingale.


To prove $\langle H\cdot M, N\rangle = H\cdot\langle M, N\rangle$, take $N$ as a local martingale and $\tilde{\tau}_n = \inf\Set{t\geq 0 : |N_t| = n}$. Then if $\sigma_n = \tilde{\tau}_n \wedge \tau_n$, we get $N^{\sigma_n} \in H^2$, allowing us to write

$$
\begin{align*}
  \langle H\cdot M, N \rangle^{\sigma_n} =& \langle (H\cdot M)^{\sigma_n}, N^{\sigma_n} \rangle \\
  =& \langle (H\cdot M^{\tau_n})^{\sigma_n}, N^{\sigma_n}\rangle \\
  =& \langle H\cdot M^{\tau_n} N^{\sigma_n} \\
  =& H\cdot\langle M^{\tau_n}, N^{\sigma_n}\rangle \\
  =& H\cdot\langle M, N\rangle^{\sigma_n} \\
  =& (H\cdot\langle M, N\rangle)^{\sigma_n}
\end{align*}
$$

We get the desired result because $\sigma_n \xrightarrow{n\to\infty} \infty$ almost surely.
</details>
</MathBox>

To extend the notion of stochastic integral to semimartingales, we need to work with locally bounded processes. A progressively measurable process $H$ is locally bounded if almost surely $\sup_{[0,t]}|H_s| < \infty$ for any $t\geq 0$. Note that is $H$ is locally bounded and $M$ is a local martingale, then $H\in L^2_{\mathrm{loc}} (M)$. Moreover, if $A$ is a finite variation process, then for any $t\geq 0$ almost surely $\int_0^t |H_s|\;|\d A_s| < \infty$. As a consequence, for a semimartingale $X = X_0 + M + A$, and a locally bounded progressively measurable process $H$, the definition $H\cdot X = H\cdot M + H\cdot A$ makes sense.

<MathBox title='Properties of stochastic integrals' boxType="proposition">
The stochastic integral of a locally bounded progressively measurable process with respect to a semimartingale satisfies the following
1. The map $(H,X)\mapsto H\cdot X$ is bilinear.
2. If $G$ and $H$ are locally bounded, $G\cdot (H\cdot X) = (GH)\cdot X$
3. If $\tau$ is a stopping time, $(\mathbf{1}_{(0,\tau]} H)\cdot X = (H\cdot X)^\tau = H\cdot X^\tau$
4. If $X$ is a locale martingale, so is $H\cdot X$
5. If $X$ is a finite variation process, so is $H\cdot X$
6. If $H$ is a step process, $H_s = \sum_{k=0}^{p-1} H_k \mathbf{1}_{(t_k, t_{k+1}]} (s)$, then
$$
  (H\cdot X)_t = \sum_{k=0}^{p-1} H_k (X_{t_{k+1} \wedge t} - X_{t_k \wedge t})
$$
7. If $H$ is assumed to be left-continuous, then in the sense of convergence in probability
$$
  \int_0^t H_s\;\d X_s = \lim_{n\to\infty} \sum_{k=0}^{p_n - 1} H_{t_k^{(n)}} \left(X_{t_{k+1}^{(n)}} - X_{t_k^{(n)}}\right)
$$
where the sequence of subdivisions $0 = t_0^{(n)} < \cdots < t_{p_n}^{(n)} = t$ has a step going to $0$.

<details>
<summary>Proof</summary>

Properties **(1)-(6)** are easy consequences of previous analogue statements concerning local martingales and finite variation processes.

**(7):** Suppose $X$ is a local martingale and let $H^{(n)}$ be the process equal to $H_{t_k^{(n)}}$ on $(t_k^{(n)}, t_{k+1}^{(n)}]$ and $0$ otherwise, and

$$
  \tau_m = \inf\Set{t\geq 0 : |H_s| + \langle M\rangle_s \geq m}
$$

Then $M^{\tau_m}$ and $H^{(n)}$ are bounded on $[0,\tau_m]$, so the definition and properties of the stochastic integral in the $H^2 / L^2(M^{\tau_m})$ case allow to use the isometry property

$$
\begin{align*}
  &\mathbb{E}\left(\left[(H^n \mathbf{1}_{[0,\tau_m]} \cdot M^{\tau_m})_t - (H\mathbf{1}_{[0,\tau_m]} \cdot M)_t \right]^2 \right) \\
  =& \mathbb{E}\left( \int_0^{t\wedge \tau_m} (H_s^n - H_s)^2 \;\d\langle M \rangle_s \right)
\end{align*}
$$

Since $H$ is left-continuous, this last term converges to $0$ as $n\to\infty$ by dominated convergence. Hence $(H^n \cdot M)_t^{\tau_m}$ converges in $L^2$ to $(H\cdot M)_t^{\tau_m}$. Because $\mathbb{P}(\tau_m > t) \xrightarrow{m\to\infty} 1$, we conclude that $(H^n \cdot M)_t$ converges in probability to $(H\cdot M)_t$.
</details>
</MathBox>

## Itô's formula

<MathBox title="Itô's formula" boxType="theorem">
Let $F:\R^d \to\R$ be second differentiable, i.e. $F\in C^2$, and let $X^1,\dots,X^d$ be (continuous) semimartingales. Then $F(X^1,\dots,X^d)$ is a semimartingale and

$$
\begin{align*}
  F(X_t^1,\dots,X_t^d) =& F(X_0^1,\dots,X_0^d) \\
  &+ \sum_{k=1}^d \int_0^t \partial_{x_k} F(X_s^1,\dots,X_s^d)\;\d X_s^k \\
  &+ \frac{1}{2}\sum_{1\leq k,\ell, d} \int_0^t \partial_{x_k x_\ell} F(X_s^1,\dots,X_s^d)\;\d\langle X^k, X^\ell \rangle_s
\end{align*}
$$

or in differential form

$$
\begin{align*}
  \d F(X_t^1,\dots,X_t^d) =& \sum_{k=1}^d \partial_{k} F(X_s^1,\dots,X_s^d)\;\d X_s^k \\
  &+ \frac{1}{2}\sum_{1\leq k,\ell, d} \partial_{k \ell} F(X_s^1,\dots,X_s^d)\;\d\langle X^k, X^\ell \rangle_s
\end{align*}
$$

<details>
<summary>Proof</summary>

We first prove this formula for $f(x,y) = xy$, which is equivalent to proving a stochastic integration by parts formula. For this purpose, by polarization, we just need to prove it for $f(x) = x^2$

$$
  X_t^2 = X_0^2 + 2 \int_0^t X_t \;\d X_t + \langle X \rangle_t
$$

From properties of stochastic integrals, we know that

$$
  2\int_0^t X_t \;\d X = \lim_{\to\infty} 2\sum_{k=0}^{p_n - 1} X_{t_k^{(n)}} (X_{t_{k+1}^{(n)}} - X_{t_k^{(n)}})
$$

where the sequence of subdivisions $0 = t_0^{(n)} < \cdots < t_{p_n}^{(n)} = t$ has a step going to $0$, and the limit is in probability. Writing this as

$$
  \lim_{n\to\infty} \left(\sum_{k=0}^{p_n - 1} (X_{t_{k+1}^{(n)}}^2 - X_{t_k^{(n)}}^2) - \sum_{k=0}^{p_n - 1} (X_{t_{k+1}^{(n)}} - X_{t_k^{(n)}})^2 \right)
$$

and using proves Itô's formula in the quadratic case, by uniqueneses of the limit in probability. In other words, we have proved that formula holds for $F$ being constant, linear of type $x_i x_j$. It follows that if the result holds for $F(x)$, a simple calculation proves that it is true for $F(x)x_i$. Hence, iterating this reasoning, the result holds for any polynomial in the variables $x_1,\dots,x_d$ of arbitrary degree.

By localizing our processes $X_1,\dots,X_d$ through their first exist time from the ball with radius $n$, and taking $n\to\infty$ at the end, we can assume the processes remain in a compact $K$. Since $F\in C^2 (K)$ on, the Stone-Weierstrass theorem gives a sequence of polynomials $(P_n)_{n\in\N}$ in the variables $x_1,\dots,x_n$ such that $P_n$, its first and second order derivatives converge uniformly to those of $F$ on $K$. We can easily show by dominated convergence that if $H$ and the $H_n$ are continuous uniformly bounded progressively measurable processes, $H_n$ converging almost surely to $H$ on $[0,t]$, then for any semimartingale $X$

$$
  \lim_{n\to\infty} \int_0^t H_s^n \;\d X_s = \int_0^t H_s \;\d X_s
$$

in the sense of convergence in probability. Thus, the result is true in the almost sure sense if $X$ is a finite variation process by the usual dominated convergence. If $X$ is $L^2$-bounded martingale, the convergence holds in $L^2$ by the isometry property. If $X$ is a local martingale, the convergence holds in probability by localization. Thus, writing the Itô formula for the polynomials $P_n$ and taking the limit $n\to\infty$ yields the result for $F$, by uniqueness of the limit in probability.

Finally, with Itô's formula proved, it is obvious that $F(X_1,\dots,X_d)$ is a semimartingale as the formula gives its explicit decomposition. Because $F$ is $C^2$, all integrated terms are locally bounded, so the stochastic and Stieltjes integrals make sense.
</details>
</MathBox>

<MathBox title='' boxType="corollary">
Let $M$ be a real local martingale and $\lambda\in\mathbb{C}$. Then the process $\left( \exp(\lambda M_t - \frac{\lambda^2}{2}\langle M \rangle_t) \right)_{t\geq 0}$ is a local martingale.

<details>
<summary>Proof</summary>

Let $F:\R^2 \to\R$ be a $C^2$. By the Itô formula for a semimartingale $M$

$$
  \d F(M_t, \langle M \rangle_t) = (\partial_x F)\;\d M_t + (\partial_y F)\;\d\langle M \rangle_t + \frac{1}{2}(\partial_{xx} F)\;\d\langle M \rangle_t 
$$

If $M$ is a local martingale and $(\partial_y + \frac{1}{2}\partial_{xx})F = 0$, then $F(M,\langle M \rangle)$ is a stochastic integral with respect to $M$, hence a local martingale. In our case, both the real and imaginary parts of $f(x,y) = \exp(\lambda x - \frac{\lambda^2}{2}y)$ satisfy the Itô formula, which yields the result.
</details>
</MathBox>

<MathBox title="Lévy's criterion" boxType="theorem">
Let $M^{(1)},\dots,M^{(d)}$ be local martingales beginning at $0$. Then the following assertions are equivalent.
1. The processes $M^{(1)},\dots,M^{(d)}$ are independent standard Brownian motions.
2. $\langle M^{(k)}, M^{(\ell)}\rangle_t = \mathbf{1}_{k=\ell}t$ for any $1\leq k,\ell \leq d$ and $t\geq 0$.

<details>
<summary>Proof</summary>

**(1)** $\implies$ **(2)**: This follows directly because $(M_t^{(k)}^2)_{t\geq 0}$ is martingale, as well as $(M_t^{(k)}, M_t^{(\ell)})_{t\geq 0}$ when $k \neq\ell$.

**(2)** $\implies$ **(1)**: Write $M = (M^{(i)})_{i=1}^d$ and consider some $u\in\R^d$. Then $u\cdot M$ is a local martingale with bracket $\langle u\cdot M \rangle_t = |u|^2 t$. From the previous corollary, it follows that $(\exp(iu\cdot M_t - \frac{1}{2}|u^2|t))_{t\geq 0}$ is a local martingale. Because it is bounded, it is a martingale:

$$
  \mathbb{E}[\exp(iu\cdot M_t - \frac{1}{2}|u^2|t)|\mathcal{F}_s] = \exp(iu\cdot M_s - \frac{1}{2}|u^2|t)
$$

It follows that for any $A\in\mathcal{F}_s$

$$
  \mathbb{E}\left( \exp(iu\cdot (M_t - M_s)) \mathbf{1}_A \right) = \exp(-\frac{1}{2}|u|^2 (t - s))\mathbb{P}(A)
$$

The choice $A = \Omega$ proves that $M_t - M_s$ is a Gaussian vector with covariance matrix $(t - s)\operatorname{id}_d$, hence with independent coordinates. Moreover, the above equation also proves that $M_t - M_s$ is independent of $\mathcal{F}_s$.
</details>
</MathBox>

<MathBox title='Dubins-Schwarz theorem' boxType="theorem">
Let $M$ be a local martingale such that $\langle M \rangle_\infty = \infty$ almost surely, and write $T_t = \inf\Set{s\geq 0 | \langle M \rangle_s > t}$. For any given $t\geq 0$, the random variable $T_t$ is an almost surely finite stopping time, and the process $B_t = M_{T_t}$ is a Brownian motion with respect to the filtration $(\mathcal{G}_t)_{t\geq 0} = (\mathcal{F}_{T_t})_{t\geq 0}$ and $M_t = B_{\langle M \rangle_t}$.

<details>
<summary>Proof</summary>

As all expected results are in the almost sure sense, we can suppose for any $\omega$, the process $M$ begins at $0$, is continuous, and $\langle M \rangle_\infty = \infty$. The process $(T_t)_{t\geq 0}$ is nondecreasing, right-continuous, and finite. Moreover $\langle M \rangle_t = \inf\Set{s\geq 0 | T_s > t}$.

Note that the processes $M$ and $\langle M \rangle$ have the same constance intervals. If $M$ is constant on $[S,T]$ so is $\langle M \rangle$ thanks to the characterization of the bracket as a limiting sum squares of increments. Conversely, if $\langle M \rangle$ is constant on $[S,T]$, since a local martingale with null bracket is indistinguishable from $0$, then the result follows by considering a proper shift of $M$ after a stopping time.

To prove that $B$ is continuous, not that this is obvious where $t$ is continuous, and if $T$ is not continuous at $t$, this follows from the above coincidence of constance intervals. By Lévy's criterion, we therefore just need to prove that $B$ and $B_t^2 - t$ are local martingales with respect to the filtration $(\mathcal{G}_t)_{t\geq 0}$.

Let $X$ denote $M$ or $(M_t^2 - t)_{t\geq 0}$. Let $S_n = \inf\Set{t\geq 0 : |X_t| \geq n}$. Then $\tilde{S} = \langle M \rangle_{S_n}$ is a $(\mathcal{G}_t)_{\t geq 0}$-stopping time. As $X^{S_n}$ is a bounded $(\mathcal{F}_t)_{t\geq 0}$ martingale, the stopping time theorem yields $X_{T_t}^{S_n} = \mathbb{E}(X_\infty^{S_n} | \mathcal{F}_{T_t})$. This means that $X_{\tau_t \wedge \tilde{S}_n}$ is a $(\mathcal{G}_t)_{t\geq 0}$-martingale, and since $\langle M \rangle_{S_n}\xrightarrow{n\to\infty}\infty$ we get the expected result.
</details>
</MathBox>

The Dubins-Schwarz theorem implies that any local martingale $M$ share many common properties with a Brownian motion.
- In the interior of nonconstant intervals, $M$ is nowhere differentiable with a Hölderian index $1/2$.
- If the bracket has a strictly positive right increasing rate at $t$, then $M$ satisfies an iterated logarithmic law.
- Up to a null set, $\Set{\omega | M \text{ converges in }\R} = \Set{\omega | \langle M \rangle_\infty < \infty}$.
- Up to a null set, $\Set{\omega | \limsup{M} = \infty, \liminf{M} = -\infty} = \Set{\omega | \langle M \rangle_\infty = \infty}$

<MathBox title='Multidimensional Dubins-Schwarz theorem' boxType="theorem">
Let $M^{(1)},\dots,M^{(d)}$ be continuous local martingales beginning at $0$ such that $\langle M^{(k)} \rangle_t \xrightarrow{t\to\infty}\infty$ for any $1 \leq k \leq d$. If $\langle M^{(k)}, M^{(\ell)} \rangle = 0$ for any $k\neq\ell$, then there exists $B^{(1)},\dots,B^{(d)}$ independent standard Brownian motions such that $M_t^{(k)} = B_{\langle M \rangle_t}^{(k)}$ for any $1 \leq k \leq d$.
</MathBox>

## Transcience, recurrence, harmonicity

<MathBox title='' boxType="theorem">
A Brownian motion $B$ of dimension $d\geq 2$ has polar points. Thus, for any $x \neq B_0$

$$
  \mathbb{P}(\exists t \geq 0 | B_t = x) = 0
$$

<details>
<summary>Proof</summary>

By a projection argument, it suffices to prove the result for $d = 2$. Additionally, by scaling and rotation-invariance of Brownian motion, we can consider that $x = (-1, 0)$, and that the bi-dimensional Brownian motion $(X,Y)$ begins at $(0,0)$. Let $M_t = e^{X_t} \cos(Y_t)$ and $N_t = e^{X_t} \sin(Y_t)$. Applying Itô's formula yields

$$
\begin{align*}
  \d M_t =& M_t \d X_t - N_t \d Y_t \\
  \d N_t =& N_t \d X_y + M_t \d Y_t
\end{align*}
$$

showing that $M$ and $N$ are local martingales. Moreover, $\langle M, N \rangle = 0$ and

$$
  \langle N \rangle_t = \langle M \rangle_t = \int_0^t e^{2X_t} \;\d t
$$

The recurrence of the Brownian motion $X$ easily implies that these brackets go to $\infty$ almost surely. By the Debins-Schwarz theorem we conclude that there are two Brownian motions $B^1$ and $B^2$ such that

$$
\begin{align*}
  M_t - 1 =& B_{\langle M \rangle_t}^1 \\
  N_t =& B_{\langle M \rangle_t}^2
\end{align*}
$$

Since $\langle M \rangle$ is continuous with almost sure range $[0,\infty)$, then by writing $B = (B^1, B^2)$ it follows that

$$
 \mathbb{P}[\exists t \geq 0 | B_t = (-1,0)] = \mathbb{P}[\exists t \geq 0 | (M_t, N_t) = (0,0)]
$$

Because $|(M_t, N_t)| = e^{X_t}$ and almost surely $X_t$ is finite for any $t\geq 0$, this last event has probability $0$.
</details>
</MathBox>

<MathBox title='' boxType="theorem">
Let $B$ be a Brownian motion in dimension $d = 2$, and $O\subset\R^2$ be open. Then

$$
  \mathbb{P}(\exist t \geq 0 | B_t \in O) = 1
$$

<details>
<summary>Proof</summary>

Assuming $B_0 = a \neq 0$, we want to prove that for any $r > 0$ almost surely $B$ is in $\mathcal{B}(r)$ for some $t$.

Define $X_t = |B_t|^2$. We will first show that the process $\ln(X_t)$ is a local martingale. For a given function $f\in C^2(\R^2)$, Itô's formula gives

$$
  \d f(B_t^1, B_t^2) = (\partial_1 f)\;\d B_t^2 + (\partial_2 f)\;\d B_t^2 + \frac{1}{2}(\nabla^2 f)\;\d t
$$

For $f(x,y) = \ln(x^2 + y^2)$ we get $\nabla^2 f = 0$, so $\ln(X_t)$ is a local martingale. 

On a sidenote, $f\notin C^2(\R^2)$ because logarithm diverges at $0$. Thus, we first need to localize $\ln(X_t)$ outside of arbitrarily small neighbourhoods of the origin and then use the polarity of $0$. Writing $S_n = \inf\Set{t\geq 0 | X_t < \frac{1}{n}}$, Itô's formula gives

$$
  \ln(X_{t\wedge S_n}) = \ln(X_0) + 2\int_0^{t\wedge S_n} \frac{B_s^1\;\d B_s^1 + B_s^2\;\d B_s^2}{X_s}
$$

By the previous theorem, $S_n \xrightarrow{n\to\infty} \infty$, so the formula above holds when replacing $t\wedge S_n$.

Let $0 < r |a| < R$ and $T_x = \inf(t\geq 0 : |B_t| = x}$. The stopping time theorem applied to $\log(X_{t\wedge T_R \wedge T_r})$ (is a bounded, hence uniformly integral martingale) yields

$$
  \mathbb{E}(\ln(X_{T_R \wedge T_r})) = \ln|a|
$$

Because the one-dimensional Brownian motion is recurrent, $T_R < \infty$ almost surely, so $\mathbb{P}(T_r < T_R) + \mathbb{P}(T_R < T_r) = 1$ and

$$
  \ln(r)\mathbb{P}(T_r < T_R) + \ln(R)\mathbb{P}(T_R < T_r) = \ln|a|
$$

This implies that

$$
  \mathbb{P}(T_r < T_R) = \frac{\ln(R) - \ln|a|}{\ln(R) - \ln(r)}
$$

so when $R\to\infty$ we get by monotone convergence $\mathbb{P}(T_r < \infty) = 1$.

</details>
</MathBox>

Note that the previous theorem can be strenghtened to prove that there are arbitrary large $t$ such that $B_t \in O$. Indeed, for any $n\geq 0$

$$
  \mathbb{P}(\exists t \geq n | B_t \in 0) = \mathbb{E}(\mathbb{E}(\mathbf{1}_{\exists t \geq n | B_t \in O} | \mathcal{F}_n))
$$

and $\mathbb{E}(\mathbf{1}_{\exists t \geq n | B_t \in O} | \mathcal{F}_n)$ is constantly 1 since, from the Markov property, this is also $\mathbb{P}(\exists t\geq 0 | \tilde{B}_t \in O_n)$ where $O_n = O + B_n$ and $\tilde{B}$ is a Brownian motion independent of $B_n$.

<MathBox title='' boxType="theorem">
Let $B$ be a Brownian motion of dimension $d = 3$. If $K\subset\R^d$ is compact, simply connected and $B_0 \notin K$, then

$$
  \mathbb{P}(\exists t\geq 0 | B_t \in K) < 1
$$

<details>
<summary>Proof</summary>

Because $K$ is simply connected and bounded, and $B_0 \notin K$, there is a path $\gamma = (x(t))_{t\in[0,1]}$ such that $x(0) = B_0$ and $x(1)$ is strictly separate from $K$ by a hyperplane $H$. Since $K$ is closed, there is some $\varepsilon > 0$ such that $\gamma_\varepsilon = \Set{x\in\R^d | \operatorname{dist}(x,\gamma) \leq\varepsilon}$ is disjoint from $K$. By the previous corollary, we get

$$
  \mathbb{P}(\forall t\in[0,1], B_t \in \gamma_\varepsilon) > 0
$$

By considering $(B_{t+1} - B_1)_{t\geq 0}$ instead of $B$ and embedding $K$ in a sphere not intersecting $H$, we just need to prove that if $B_0 > r$, then

$$
  \mathbb{P}(\exists t \geq 0 : |B_t| \leq r) < 1
$$

Note that the process $|B_t|^{2-d}$ is a local martingale. For a function $f\in C^2(\R^d)$, Itô's formula gives

$$
  \d f(B_t^1,\dots, B_t^d) = \sum_{k=1}^d (\partial_k f)\d B_t^k + \frac{1}{2}(\nabla^2 f)\;\d t
$$

For $f(x_1,\dots,x_d) = (x_1^2 +\cdots+ x_d^2)^{1-\frac{d}{2}}$, we get $\nabla^2 f = 0$, so $|B_t|^{2-d}$ is a local martingale.

Let $0 < r < |a| < R$ and $T_x = \inf\Set{t\geq 0 : |B_t| = x}$. Applying the stopping theorem to $|X_{t\wedge T_R \wedge T_r}|^{2-d}$ ( this a bounded, hence uniformly integrable martingale) yields

$$
  \mathbb{E}(|B_{T_R \wedge T_r}|^{2-d}) = |a|^{2-d}
$$

Since the one-dimensional Brownian motion is recurrent, $T_R < \infty$ almost surely, so $\mathbb{P}(T_r < T_R) + \mathbb{P}(T_R < T_r) = 1$ and

$$
  r^{2-d}\mathbb{P}(T_r < T_R) + R^{2-d}\mathbb{P}(T_R < T_r) = |a|^{2-d}
$$

This implies that

$$
   \mathbb{P}(T_r < T_R) = \frac{R^{2-d} - |a|^{2-d}}{R^{2-d} - r^{2-d}}
$$

so when $R\to\infty$ we get by monotone convergence

$$
  \mathbb{P}(T_r < \infty) = \left(\frac{r}{|a|} \right)^{d-2} < 1
$$
</details>
</MathBox>

<MathBox title='' boxType="theorem">
Let $B$ a Brownian motion in dimension $d\geq 3$. Then almost surely

$$
  \lim_{t\to\infty} |B_t| = \infty
$$

<details>
<summary>Proof</summary>

By projection a argument, it suffices to prove the result for $d = 3$. With appropriate shifting, we may assume $B_0 = (1,0,0)$. The proof of the previous theorem involved the fact that $(\frac{1}{|B_t|})_{t\geq 0}$ is a local martingale. Because it is positive, it is also a supermartingale and converges almost surely to some random variable $X$. We want to prove that $X = 0$ almost surely and because $X \geq 0$ we just need to prove $\mathbb{E}(X) = 0$. By Fatou's lemma

$$
  \mathbb{E}(X) \leq\lim_{t\to\infty} \mathbb{E}\left(\frac{1}{|B_t|}\right)
$$

This limit is $0$, because expectation is uniformly bounded and

$$
  \mathbb{E}\left(\frac{1}{|B_t|}\right) = \frac{1}{t}\mathbb{E}\left[\left(\left[\mathcal{N}_1 - \frac{1}{\sqrt{2}} \right]^2 + \mathcal{N}_2^2 + \mathcal{N}_3^2 \right)^{-1/2}\right]
$$

where $\mathcal{N_1},\mathcal{N_2},\mathcal{N}_3$ are independent standard normal variables. Hence $\frac{1}{|x|}$ is integrable around $0$ in dimension $d \geq 0$. 
</details>
</MathBox>

<MathBox title='Properties of harmonic functions' boxType="proposition">
Let $f:U\to\R$ be a $C^2$-function, where $U$ is open and connected. Then the following are equivalent.
1. The function $f$ is harmonic, i.e. $\nabla^2 f = 0$ on $U$.
2. For any $x\in\R^d$ and any $r > 0$ such that the ball $\mathcal{B}(x,r)$ is included in $U$,
$$
  f(x) = \int f(y)\;\d\sigma_r (y)
$$

where $\sigma_r$ is the uniform measure, normalized to $1$, on $\partial\mathcal{B}(x,r)$.

<details>
<summary>Proof</summary>

Let $B$ be a Brownian motion beginning at $x$ and define $\tau_r = \inf\Set{t\geq 0 : |B_t - x| = r}$. The Itô formula yields

$$
  f(B_{\tau_r}) = f(x) + \int_0^{\tau_r} \nabla f(B_s) \cdot \d B_s + \frac{1}{2}\int_0^\tau_r \nabla^2 f (B_s)\;\d s
$$

Assuming **(1)**, we get

$$
  \mathbb{E}(f(B_{\tau_r})) = f(x) + \mathbb{E}\left(\int_0^{\tau_r} \nabla f(B_s) \cdot \d B_s \right)
$$

Because the Brownian motion is invariant by rotations, the left hand side is $\int f(y)\;\d\sigma_r (y)$. Moreover, since $|\nabla f|$ is uniformly bounded on the ball $\mathcal{B}(x, r)$ with center $x$ and radius $r$, it follows that $X =\left(\int_0^{t\wedge\tau_r} \nabla f(B_s)\cdot \d B_s \right)_{t\geq 0}$ is a martingale with the expectation of its bracket uniformly bounded by

$$
  \sup_{\mathcal{B}(x,r)} |\nabla f|^2 \mathbb{E}(\tau_r) < \infty
$$

Thus, $X$ is uniformly integrable, and in particular the stopping time theorem at time $\tau_r$ implies that

$$
  \mathbb{E}\left( \int_0^{\tau_r} \nabla f(B_s)\cdot\d B_s \right) = 0
$$

proving **(2)**.

Assume the mean value condition **(2)**, and suppose $\nabla^2 f(x) \neq 0$ at some point $x\in U$. Up to considering $-f$, assume $\nabla^2 f(x) > 0$ for example. By continuity, there is $r > 0$ such that $\nabla d > 0$ on $\mathbb{B}(x, r)\subset U$. From the previous discussion, the Itô formula and the uniform integrability of the stochastic integral yield

$$
  \mathbb{E}(f(B_{\tau_r})) = f(x) + \frac{1}{2}\mathbb{E}\left(\int_0^{\tau_r} \nabla^2 f(B_s) \;\d s \right)
$$

Rotational invariance of Brownian motion combined with **(1)** yield

$$
  \mathbb{E}\left( \int_0^{\tau_r} \nabla^2 f (B_s)\;\d s \right) = 0
$$

Because $\nabla^2 f$ is strictly positive along this trajectory this is only possible if $\tau_r = 0$ almost surely, which is not the case. Hence $\nabla^2 f(x) = 0$ necessarily.
</details>
</MathBox>

Let $U\subset\R^d$ be a connected open subset with boundary $\partial U$, and let $\varphi:\partial U\to\R$ be a continuous function. The Dirichlet problem seeks to find a twice continuously differentiable function $u:U\to\R$ satisfying the partial differential equation

$$
\begin{align*}
  \nabla^2 u(x) =& 0,\; x\in U \\
  u(x) =& \varphi(x),\; x\in\partial U
\end{align*}
$$

A point $x\in\partial U$ satisfies the Poincaré cone condition (or is regular) if there is some $\delta > 0$ and a cone $C$ with vertex $x$ and strictly positive angle such that

$$
  C \cap \mathcal{B}(x,\delta) \subset U^c
$$

where $\mathcal{B}(x,\delta)$ is the ball with center $x$ and radius $\delta$, and $U^c$ is the complement of $U$ in $\R^d$.

<MathBox title='' boxType="theorem">
Consider the Dirichlet problem as described above.
1. If there is a solution $u$ to the problem, it coincides with
$$
  v(x) := \mathbb{E}(\varphi(B_\tau) | B_0 = x)
$$

where $B$ is a $d$-dimensional Brownian motion and $\tau = \inf\Set{t\geq 0 | B_t \in\partial U}$. In particular, there is at most one solution.
2. The above $v$ is harmonic and continuous at any regular point of $\partial U$. In particular, if $\partial U$ is everywhere regular, $v$ is the unique solution of the Dirichlet problem.

<details>
<summary>Proof</summary>

**(1):** By Itô's formula

$$
  u(B_\tau) = u(x) + \int_0^\tau \nabla u(B_s)\cdot\d B_s + \frac{1}{2}\int_0^\tau \nabla^2 u(B_s)\;\d s
$$

where $B$ is a Brownian motion beginning at $x$. By the same argument as in the previous proposition, the expectation of the above stochastic integral is $0$ because $\mathbb{E}(\tau) < \infty$ and $|\nabla u|$ is bounded. Thus

$$
  \mathbb{E}(u(B_\tau)) = u(x)
$$

However, the left hand side is also $\mathbb{E}(\varphi(B_\tau)|B_0 = x)$.

**(2):** We first prove that $v$ is harmonic. Take a ball $\mathcal{B}(x,\delta)\subset U$ and $\tilde{\tau} = \inf\Set{t\geq 0 | B_t \notin \mathcal{B}(x,\delta)}$. Then $\mathcal{F}_{\tilde{\tau}} \subset \mathcal{F}_\tau$ because $\tilde{\tau}\leq\tau$, so using the strong Markov property for the Brownian motion

$$
\begin{align*}
  v(x) =& \mathbb{E}_x (\varphi(B_\tau)) = \mathbb{E}_x (\mathbb{E}_x (\varphi(B_\tau)|\mathcal{F}_{\tilde{\tau}})) \\
  =& \mathbb{E}_x (\mathbb{E}_{B_{\tilde{\tau}}}(\varphi(B_\tau))) \\
  =& \mathbb{E}_x (v(B_{\tilde{\tau}})) \\
  =& \int v(y) \sigma_r (\d y)
\end{align*}
$$

where we used the invariance of $B$ by rotation around $x$ is the last equality. Here $\sigma_r$ is the uniform measure, normalized to $1$, on the sphere with radius $r$ and center $x$. It follows from properties of harmonic functions that $v$ is harmonic in $U$.

We still need to prove that $v$ is continuous at regular points. Let $x\in\partial U$ be a regular point and take $\delta > 0$, a cone $C$ with vertex $x$ and strictly positive angle such that $C \cap \mathcal{B}(x,\delta) \subset U^c$.

Take a given $\varepsilon > 0$. Because $\varphi$ is continuous on $\partial U$, we can chose the above $\delta$ such that

$$
  \sup_{y\in\partial U \cap \mathcal{B}(x,\delta)} |\varphi(y) - \varphi(x)| < \varepsilon
$$

For a Brownian motion $B$, assume there is some $0 < \theta < \delta$ such that if $|z-x| < \theta$ for $z\in U$, then

$$
  \mathbb{P}_z (\tau_C \leq \tau_\delta) > 1 - \varepsilon
$$

which is the probability for a Brownian motion starting at $z$ and where $\tau_C$ is the hitting time of $C\cap \mathcal{B}(x,\delta)$ and $\tau_\delta$ is the hitting time of $\partial\mathcal{B}(x,\delta)$. Then this would imply that for $|z-x| < \theta$ and $z\in U$

$$
\begin{align*}
  |v(z) - v(x)| =& |\mathbb{E}_z (\varphi(B_\tau) - \varphi(x))| \\
  \leq& 2\norm{\varphi}_\infty \mathbb{P}_z (\tau_\delta < \tau_C) + \varepsilon \\
  \leq& \varepsilon(2\norm{ \varphi }_\infty + 1)
\end{align*}
$$

Thus, continuity will follow if we can prove $\mathbb{P}_z (\tau_C \leq \tau_\delta) > 1 - \varepsilon$. By rotation invariance we can assume $B_1$ corresponds to the axis of the cone, with angle $\alpha$. Note that

$$
\begin{align*}
  \Set{\tau_C < \tau_\delta} \subset \bigcup_{t > 0} \left( \Set{B_1 (t) > \frac{1}{\tan\alpha}(B_2 (t)^2 +\cdots+ B_d(t)^2)^{1/2}} \right. \\
  & \left. \cap \bigcap_{k=1}^d \Set{\sup_{[0,t]} |B_k| < c} \right)
\end{align*}
$$

for some absolute constant $c > 0$ depending only on $\delta$ and $\alpha$. When $t\to 0$, these last $d$ events have a probability of converging to $1$. Concerning the first event, the probability that it occurs for some arbitrarily small $t > 0$ converges to $1$ as well; this is an easy consequence of the iterated logarithm law where $B_1 (t)$ gets as big as $\sqrt{t \ln(-\ln t)}$ almost surely, and the independence of $B_1$ from the other $(B_i)_{i=2}^d$.
</details>
</MathBox>

<MathBox title='' boxType="theorem">
Let $d\geq 1$. Bounded harmonic functions on $\R^d$ are constant.

<details>
<summary>Proof</summary>

Let $f:\R^d \to [-m, m]$ be a harmonic function. By Itô's formula and harmonicity, for any $t > 0$, we get

$$
  \mathbb{E}_x (f(B_t)) = f(x) + \mathbb{E}\left( \int_0^t \nabla f(B_s)\cdot \d B_s \right)
$$

where $B$ is a Brownian motion beginning at $x$. Chose $x$ and $y$ distinct points in $\R^d$, write $\mathcal{S}^{(n)}$ the sphere with center $\frac{x+y}{2}$ and radius $n > \norm{ \frac{x - y}{2} }$, and $H$ the median hyperplane between $x$ and $y$. Define

$$
\begin{align*}
  T_n =& \inf\Set{t \geq 0 | B_t \in\mathcal{S}_n} \\
  T_H =& \inf\Set{t \geq 0 | B_t \in H}
\end{align*}
$$

Because $|\nabla f|$ is bounded inside the sphere $\mathcal{S}^{(n)}$ at time $t \wedge T_n$ the expectation on the right hand side above is $0$, so

$$
\begin{align*}
  f(x) =& \mathbb{E}_x (f(B_{t\wedge T_n})) \\
  =& \mathbb{E}_x (f(B_{t\wedge T_n})\mathbf{1}_{T_H \leq t \wedge T_n}) + \mathbb{E}_x (f(B_{t \wedge T_n})\mathbf{1}_{T_H > t \wedge T_n})
\end{align*}
$$

The same formula holds concerning $f(y)$, and from the reflection principle

$$
  \mathbb{E}_x (f(B_{t \wedge T_n})\mathbf{1}_{T_H \leq t\wedge T_n}) = \mathbb{E}_y (f(B_{t \wedge T_n})\mathbf{1}_{T_H \leq t \wedge T_n})
$$

We thus get

$$
\begin{align*}
  |f(x) - f(y)| =& |\mathbb{E}_x (f(B_{t \wedge T_n})\mathbf{1}_{T_H > t\wedge T_n}) - \mathbb{E}_y (f(B_{t \wedge T_n})\mathbf{1}_{T_H > t \wedge T_n})| \\
  \leq& 2m \mathbb{P}(T_H > t\wedge T_n)
\end{align*}
$$

When both $t$ and $n$ go $\infty$, this last probability converges to $0$, concluding the proof.
</details>
</MathBox>

<MathBox title='' boxType="proposition">
Let $Z = X + iY$ be a complex local martingale. Then there exists a unique complex process with finite variation beginning at $0$ such that $Z^2 - \langle Z \rangle$ is a complex local martingale. The followin statements are equivalent.
1. The process $Z^2$ is a local martingale
2. The process $\langle Z \rangle$ is identically $0$
3. The bracket of the real and imaginary parts satisfy $\langle X \rangle = \langle Y \rangle$ and $\langle X, Y \rangle = 0$.
4. There is a Brownian motion such that $Z_t = B_{\langle X \rangle_t}$.

A local martingale is called *conformal* if any of the above properties is true.

<details>
<summary>Proof</summary>

The existence of the bracket is given by

$$
\begin{align*}
  \langle Z \rangle =& \langle X + iY, X + iY \rangle \\
  =& \langle X \rangle - \langle Y \rangle + 2i\langle X, Y \rangle
\end{align*}
$$

which satisfies all required properties. Uniqueness is a consequence of the bracket theorem and the equivalence between **(1)**, **(2)** and **(3)** is an immediate consequence of the above formula. Moreover, **(3)** implies **(4)** by the general Dubins-Schwarz theorem, and **(4)** obviously implies **(3)**.
</details>
</MathBox>

<MathBox title='Complex Itô formula' boxType="theorem">
Let $Z$ be a conformal local martingale. For a general function $f:\mathbb{C}\to\mathbb{C}$ of class $C^2$, Itô's formula takes the form

$$
\begin{align*}
  f(Z_t) =& f(Z_0) + \int_0^t \partial z f(Z_s)\;\d Z_s \\
  &+ \int_0^t \partial_{\overline{z}} f(Z_s)\;\d\overline{Z}_s \\
  &+ \frac{1}{2} \int_0^t \nabla^2 f(Z_s) \;\d\langle \Re(Z) \rangle_s
\end{align*}
$$

A function $f:\mathbb{C}\to\mathbb{C}$ is called holomorphic if $\partial_{\overline{z}} f = 0$ and harmonic if $\nabla^2 f = 4\partial_z \partial_{\overline{z}} f = 0$.

If $f$ is harmonic, then $f(Z)$ is a local martingale as well, and if $f$ is holomorphic, then the local martingale $f(Z)$ has the following easy decomposition

$$
  f(Z_t) = f(Z_0) + \int_0^t f'(Z_s)\;\d Z_s
$$

<details>
<summary>Details</summary>

Note that

$$
\begin{align*}
  \partial_z =& \frac{1}{2}(\partial_x - i\partial_y) \\
  \partial_{\overline{z}} =& \frac{1}{2}(\partial_x + i\partial_y)
\end{align*}
$$
</details>
</MathBox>

<MathBox title='' boxType="theorem">
Let $f$ be entire and nonconstant, and $B$ a standard complex Brownian motion. Then $f(B)$ is a time-changed Brownian motion

$$
  f(B_t) = f(B_0) + \tilde{B}_{\langle X \rangle_t}
$$

where $B$ is a standard Brownian motion, and $\langle X \rangle_t := \int_0^t |f'(B_s)|^2 \;\d s$ is strictly increasing and converges to $\infty$.

<details>
<summary>Proof</summary>

Because $f$ is an entire function, so is $f^2$ and therefore $f^2 (B)$ is a local martingale and $f(B)$ is a conformal local martingale. By the previous proposition, there is a Brownian motion $\tilde{B}$ such that

$$
  f(B_t) = f(B_0) + \tilde{B}_{\langle X \rangle_t}
$$

where $X = \Re(f(B))$ and $\langle X \rangle_t = \int_0^t |f'(B_s)|^2 \;\d s$. Since $f'$ is entire and not identically $0$, it has a countable set of zeros so $\langle X \rangle$ is strictly increasing. It converges to $\infty$ almost surely due the recurrence property of the bi-dimensional Brownian motion.
</details>
</MathBox>

<MathBox title="Spitzer's law" boxType="theorem">
Let $B$ be a planar Brownian motion beginning at $B_0 \neq 0$. Define its argument $\theta_t$ at time $t$ continuously from $\theta_0 \in [0,2\pi)$ at time $0$. Then

$$
  \lim_{t\to\infty} \frac{\theta}{\ln t} = \frac{C}{2}
$$

where $C$ is a standard Cauchy random variable, which has density $\frac{1}{\pi(1 + x^2)}$ with respect to the Lebesgue measure.

<details>
<summary>Proof</summary>

By rotation and scaling invariance, we can assume $B_0 = (1,0)$. From the previous theorem, if $X$ and $Y$ are independent standard real Brownian motions, then $\tilde{B}$ defined by 

$$
  \tilde{B}_{int_0^t e^{2X_s} \;\d s} = e^{X_t + iY_t}
$$

is a complex Brownian motion beginning at $(1,0)$. Let $a > 1$ and $S_a = \inf\Set{t \geq 0 : |B_t| = a}$. We will first prove that

$$
  \frac{\mathrm{arg}(B_{S_a})}{\ln a} \sim C
$$

The winding number $\mathrm{arg}(B_{S_a})$ is also the value $Y_{T_{\ln a}}$ where $T_x = \inf\Set{t\geq 0 | X_t = x}$. By scaling, proving that $Y_{T_1} \sim C$ is sufficient. We therefore just need to know the law of $T_1$ and compose it with that of $Y$.

The distribution of $T_1$ is well-known, e.g. by the reflection principle

$$
\begin{align*}
  \mathbb{P}(T_1 \leq t) =& \mathbb{P}(\sup_{[0,t] X \geq 1}) \\
  =& \mathbb{P}(|X_t| \geq 1) \\
  =& \mathbb{P}\left( \frac{1}{X_1^2} \leq t \right)
\end{align*}
$$

so $T_1 \sim \frac{1}{\mathcal{N}^2}$ where $\mathcal{N}$ is a standard normal variable.

The density of $Y_{T_1}$ is also that of $\sqrt{T_1} Y_1$, by conditioning on $T_1$. Hence it has the same law as $\frac{\mathcal{N}'}{\mathcal{N}}$. This is known as the Cauchy distribution.

We have shown that

$$
  \frac{\mathrm{arg}(B_{S_{\sqrt{t}}})}{\ln t} \sim \frac{C}{2}
$$

and the proof will be complete if can show that

$$
  \frac{\mathrm{arg}(B_t) - \mathrm{arg}(B_{S_{\sqrt{t}}})}{\ln t}
$$

converges to $0$ in probability. For this, consider $\tau$ the inverse of the clock, defined by

$$
  \int_0^{\tau_t} e^{2X_s} \;\d s = t
$$

For some parameters $\delta \in (0,1)$ such that $\delta\sqrt{t} \leq 1$ and $\lambda > 0$, consider the events

$$
\begin{align*}
  \mathcal{A} =& \Set{S_{\delta\sqrt{t}} \leq t \leq S_{\delta^{-1} \sqrt{t}}} \\
  \mathcal{B} = \Set{\sup_{[\tau_{S_{\delta\sqrt{t}}}, \tau_{S_{\delta^{-1}\sqrt{t}}}]}} |Y_s - Y_{\tau_{S_{\delta\sqrt{t}}}}| \leq \frac{\lambda}{2}
\end{align*}
$$

Then $\mathcal{A}\cap\mathcal{B}\subset \Set{|\mathrm{arg}(B_t) - \mathrm{arg}(B_{S_{\sqrt{t}}})| \leq \lambda}$, so we just need to prove that $\mathbb{P}(\mathcal{A}) \xrightarrow{\delta\to 0} 1$ and $\mathbb{P}(\mathcal{B}) \xrightarrow{\lambda\to\infty} 1$ for a given $\delta$.

Concerning $\mathcal{A}$, things are easy because

$$
  \Set{|B_t| > \delta\sqrt{t}} \cap \Set{\sup_{[0,t]} |B_s| < \delta^{-1}\sqrt{t}} \subset \mathcal{A}
$$

and both events have a probability of going to $1$ as $\delta\to 0$, independently of $t$ by scaling.

Concerning $\mathcal{B}$, note that

$$
  \tau_{S_a} = \inf\Set{u \geq 0 | X_u = \ln a}
$$

so from the strong Markov property

$$
  \tau_{S_{\delta^{-1}\sqrt{t}}} - \tau_{S_{\delta\sqrt{t}}} \sim T_{-2\ln\delta}
$$

where as previously $T_x = \inf\Set{t\geq 0 | X_t = x}$. Since $Y$
is independent from $X$, this means that 

$$
  \sup_{[\tau_{S_{\delta\sqrt{t}}}, \tau_{S_{\delta^{-1}\sqrt{t}}}]}} |Y_s - Y_{\tau_{S_{\delta\sqrt{t}}}}| \sim \sup_{[0,\Delta]} |Y_s|
$$

where $\Delta$ is an almost surely finite random variable independent of $Y$, and with law depending on $\delta$ but not on $t$. It follows that $\mathbb{P}(\mathcal{B}) \xrightarrow{\lambda\to\infty} 1$ since it is independent of $t$.
</details>
</MathBox>

## The Girsanov theorem

<MathBox title='' boxType="proposition">
Assume $\mathbb{Q}$ is absolutely continuous relative to $\mathbb{P}$, i.e. $\mathbb{Q} << \mathbb{P}$, on $\mathcal{F}_\infty$. For $t\in[0,\infty]$ define the process

$$
  D_t = \left.\frac{\d\mathbb{Q}}{\d\mathbb{P}}\right|_{\mathcal{F_t}}
$$

1. The process $D$ is a uniformly integrable martingale.
2. If $D$ is assumed right-continuous, then for any stopping time $T$
$$
  D_T = \left.\frac{\d\mathbb{Q}}{\d\mathbb{P}}\right|_{\mathcal{F}_T}
$$
3. If $D$ is assumed right-continuous and $\mathbb{Q}\sim\mathbb{P}$ on $\mathcal{F}$ then $D_t > 0$ for any $t\geq 0$.

<details>
<summary>Proof</summary>

**(1):** Note that for any $t > 0$ and $A\in\mathcal{F}_t$

$$
\begin{align*}
  \mathbb{E}_{\mathbb{P}} (\mathbf{1}_A D_t) =& \mathbb{E}_{\mathbb{Q}}(\mathbf{1}_A) = \mathbb{E}_{\mathbb{P}}(\mathbf{1}_A D) \\
  =& \mathbb{E}_{\mathbb{P}}(\mathbf{1}_A \mathbb{E}(D|\mathcal{F_t}))
\end{align*}  
$$

where $D = \left.\frac{\d\mathbb{Q}}{\d\mathbb{P}}\right|_{\mathcal{F}}$. By uniqueness of the Radon-Nikodym derivative, this implies $D_t = \mathbb{E}(D|\mathcal{F}_t)$ almost surely, concluding the proof as $D\in L^1$.

**(2):** By the stopping time theorem $D_T = \mathbb{E}(D|\mathcal{F}_T)$. It follows that for any $A\in\mathcal{F}_T$

$$
\begin{align*}
  \mathbb{E}_{\mathbb{P}}\left( \mathbf{1}_A \left.\frac{\d\mathbb{Q}}{\d\mathbb{Q}}\right|_{\mathcal{F}_T} \right) =& \mathbb{E}_{\mathbb{Q}}(\mathbf{1}_A) = \mathbb{E}_{\mathbb{P}}(\mathbf{1}_A D) \\
  =& \mathbb{E}_\mathbb{P} (\mathbf{1}_A D_T)
\end{align*}
$$

Hence $D_T = \left.\frac{\d\mathbb{Q}}{\d\mathbb{P}}\right|_{\mathcal{F}_T}$.

**(3):** This follows from **(2)**. For $T = \inf\Set{t\geq 0 | D_t = 0}$, by right-continuity $D_T = 0$ on $\Set{T < \infty}$, so

$$
  \mathbb{Q}(T < \infty) = \mathbb{E}_{\mathbb{P}}(\mathbf{1}_{T < \infty} D_T) = 0
$$

Because $\mathbb{Q} >> \mathbb{P}$ this implies that $\mathbb{P}(T < \infty) = 0$ as expected
</details>
</MathBox>

<MathBox title='' boxType="proposition">
Let $D$ be a strictly positive continuous local martingale. Then there exists a unique continuous local martingale $L$ such that

$$
  D_t = \mathcal{E}(L)_t := \exp(L_t - \frac{1}{2}\langle L \rangle_t)
$$

<details>
<summary>Proof</summary>

For uniqueness, note that if $L$ and $\tilde{L}$ are solutions, then

$$
  L - \tilde{L} = \frac{1}{2}(\langle L \rangle - \langle\tilde{L}\rangle)
$$

is a finite variation process and also a local martingale, so it is indistinguishable from $0$. For the existence, the choice

$$
  L_t = \ln(D_0) + \int_0^t \frac{\d D_s}{D_s}
$$

makes sense since $D$ is strictly positive and, by Itô's formula

$$
\begin{align*}
  \ln(D_t) =& \ln(D_0) + \int_0^t \frac{\d D_s}{D_s} - \frac{1}{2}\int_0^t \frac{\d\langle D \rangle_s}{D_s^2} \\
  =& L_t - \frac{1}{2}\langle L \rangle_t
\end{align*}
$$
</details>
</MathBox>

<MathBox title="Girsanov's theorem" boxType="theorem">
Let $\mathbb{Q}\sim\mathbb{P}$ on $\mathcal{F}$. Assume the process defined by

$$
  D_t = \left.\frac{\d\mathbb{Q}}{\d\mathbb{P}}\right|_{\mathcal{F}_t}
$$

is continuous, and write it as $D = \mathcal{E}(L)$, where $L$ is a local martingale. Then if $M$ is a $\mathbb{P}$-local martingale, the process

$$
  \tilde{M} := M - \langle M, L \rangle
$$

is a $\mathbb{Q}$-local martingale.

<details>
<summary>Proof</summary>

We first prove that if a process $XD$ is a $\mathbb{P}$-local martingale, then $X$ is a $\mathbb{Q}$-local martingale. For this, we show that for any stopping time $T$ and adapted continuous process $X$ such that $(XD)^T$ is a $\mathbb{P}$-martingale, then $X^T$ is a $\mathbb{Q}$-martingale.

First the integrability condition is satisfied, from the previous proposition

$$
  \mathbb{E}_\mathbb{Q}(|X_t^T|) = \mathbb{E}_\mathbb{P} (|X_t^T| |D_t^T|) < \infty
$$

because $(XD)^T$ is a $\mathbb{P}$-martingale.

For $s < t$ and $A\in\mathcal{F}_s$, we get
$$
\begin{align*}
  \mathbb{E}_{\mathbb{Q}} (\mathbf{1}_A X_t^T) =& \mathbb{E}_{\mathbb{Q}}(\mathbf{1}_{A \cap \Set{T\leq s}} X_t^T) + \mathbb{E}_{\mathbb{Q}} (\mathbf{1}_{A \cap \Set{T > s}} X_t^T) \\
  =& \mathbb{E}_{\mathbb{P}}(\mathbf{1}_{A\cap \Set{T\leq s}} X_t^T D_{t \wedge T}) + \mathbb{E}_{\mathbb{P}} (\mathbf{1}_{A\cap\Set{T > s}} X_t^T D_{t\wedge T}) \\
  =& \mathbb{E}_{\mathbb{P}} (\mathbf{1}_{A\cap\Set{T\leq s}} X_s^T D_{t\wedge T}) + \mathbb{E}_{\mathbb{P}} (\mathbf{1}_{A \cap\Set{T > s}} X_s^T D_{s\wedge T}) \\
  =& \mathbb{E}_{\mathbb{Q}} (\mathbf{1}_{A\cap\Set{T\leq s}}X_s^T) + \mathbb{E}_{\mathbb{Q}} (\mathbf{1}_{A\cap\Set{T > s}}X_s^T) \\
  =& \mathbb{E}_{\mathbb{Q}} (\mathbf{1}_A X_s^T)
\end{align*}
$$

Hence $X_s^T = \mathbb{E}_{\mathbb{Q}} (X_t^T | \mathcal{F}_s)$.

Following the previous argument, if $\tilde{M}D$ is a $\mathbb{P}$-local martingale, then it is enough to prove that $\tilde{M}$ is a $\mathbb{Q}$-local martingale. Itô's formula yields

$$
\begin{align*}
  \d((M - \langle M, L \rangle)D)_t =& (M - \langle M, L \rangle)_t \;\d D_t + (\d M_t + \d\langle M, L \rangle_t)D_t + \d\langle M, D \rangle_t \\
  =& (M - \langle M, L \rangle)_t \;\d D_t + (\d M_t + \d\langle M, L \rangle_t)D_t + D_t \;\d\langle M, L \rangle_t \\
  =& (M - \langle M, L \rangle)_t \;\d D_t + D_t \;\d M_t
\end{align*}
$$

where in the second equality we have used the fact that $\d D_t = D_t \;\d L_t$. Thus, $\tilde{M}D$ is a stochastic integral with respec to the $\mathbb{P}$-local martingales $D$ and $M$, so it is a $\mathbb{P}$-local martingale.
</details>
</MathBox>

<MathBox title='' boxType="corollary">
If the local martingale $L$ and the $\mathbb{P}$-local martingale $M$ are Brownian motions, then

$$
  \tilde{B} := M - \langle M, B \rangle
$$

is a $\mathbb{Q}$-local martingale.

<details>
<summary>Proof</summary>

If $M$ is a $\mathbb{P}$-Brownian motion, then by Girsanov's theorem $\tilde{M}$ is a $\mathbb{Q}$-local martingale and its bracket is $t$ by the quadratic increment formula. Thus, Lévy's criterion applies.
</details>
</MathBox>

The condition $\mathbb{Q}\sim\mathbb{P}$ on $\mathcal{F}$ in the hypothesis of Girsanov's theorem is quite restrictive. As an example, if $M = B$ is a Brownian motion, we could try to use the theorem till infinite time to study the properties of the Brownian motion with drift $\tilde{M} = (B_t - \nu t)_{t\geq 0}$. For this, we want a measure $\mathbb{Q}$ on $\mathcal{F}$ such that for any $t\geq 0$

$$
  \left.\frac{\d\mathbb{Q}}{\d\mathbb{P}}\right|_{\mathcal{F}_t} = \exp\left(\nu B_t - \frac{\nu^2}{2}t \right)
$$

Although it can be shown that such a measure $\mathbb{Q}$ exists, it is not absolutely continuous relative to the Wiener measure $\mathbb{P}$. To avoid this problem, Girsanov's theorem can be applied up to a finite horizon. If $\mathbb{Q}\sim\mathbb{P}$ on $\mathcal{F}_t$, then $\tilde{M}$ is a $\mathbb{Q}$-local martingale when we only consider its restriction to $[0,t]$. This is true by applying the general statement with a modified filtration such that $\mathcal{F}_u = \mathcal{F}_t$ when $u > t$.

Applying this to the case above we get that $(B_s - \nu s)_{0\leq s\leq t}$ is a $\mathbb{Q}$-Brownian motion. Hence, for any bounded continuous funcional $F$

$$
  \mathbb{E}_{\mathbb{Q}}(F(B_s - \nu s)_{0\leq s\leq t}) = \mathbb{E}_{\mathbb{P}}(F(B_s)_{0\leq s \leq t})
$$

When $F$ is a cylindric function, this is just a change of variables formula for finite-dimensional Gaussian measures. If $F$ depends only on the trajectory up to a stopping time $T\leq t$ almost surely, the above equation together with previous proposition yield

$$
  \mathbb{E}_{\mathbb{P}}\left(F(B_s - \nu s)_{0\leq s \leq T} \exp\left(\nu B_T - \frac{\nu^2}{2}T\right)\right) = \mathbb{E}_{\mathbb{P}}(F(B_s)_{0\leq s\leq T})
$$

<MathBox title='' boxType="corollary">
Let $T_a^{(\nu)} = \inf\Set{s \geq 0 | B_s + \nu s = a}$. Then for any $\lambda \geq 0$

$$
  \mathbb{E}_{\mathbb{P}} \left( \mathbf{1}_{T_a^{(\nu)}} \exp(-\lambda T_a^{(\nu)}) \right) = \exp(\nu a - |a|\sqrt{2\lambda + \nu^2})
$$

<details>
<summary>Proof</summary>

Consider the random variable $F = \mathbf{1}_{T_a^{(\nu) \leq t}} \exp(-\lambda T_a^{(\nu)})$, which is $\mathcal{F}_{t\wedge T_a}$-measurable, then

$$
  \mathbb{E}_{\mathbb{P}}\left( \mathbf{1}_{T_a^{(0)} \leq t} \exp(-\lambda T_a^{(0)}) \exp(\nu B_{t \wedge T_a^{(0)} - \frac{\nu^2}{2} T_a^{(0)}}) \right) = \mathbb{E}_{\mathbb{P}}\left( \mathbf{1}_{T_a^{(\nu)} \leq t} \exp(-\lambda T_a^{(\nu)}) \right)
$$

Dominated convergence as $t\to\infty$ yields

$$
  \mathbb{E}_{\mathbb{P}}\left( \mathbf{1}_{T_a^{(0)} < \infty} \exp\left(-\left(\lambda \frac{\nu^2}{2} T_a^{(0)} \right) \exp(\nu a) \right) \right) = \mathbb{E}_{\mathbb{P}}\left( \mathbf{1}_{T_a^{(\nu)} < \infty} \exp(-\lambda T_a^{(\nu)}) \right)
$$

The left hand side is known so

$$
  \mathbb{E}_\mathbb{P} \left(\mathbf{1}_{T_a^{(\nu)} < \infty} \exp(-\lambda T_a^{(\nu)}) \right) = \exp(\nu a - |a|\sqrt{2\lambda + \nu^2})
$$
</details>
</MathBox>

<MathBox title='' boxType="corollary">
Let $f:\R\to\R$ be continuous on $(0,1)$ with $f(0)$ and let $V_\varepsilon (f)$ be the set of continuous functions $g$ on $(0,1)$, beginning at $0$, such that $\sup_{(0,1)} |f(x) - g(x)| < \varepsilon$. Then $\mathbb{P}(V_\varepsilon) > 0$.

<details>
<summary>Proof</summary>

First, for a given $\varepsilon > 0$, there exists a continuous $f_0$, beginning at $0$, such that its $\varepsilon$-neighbourhood has a strictly positive probability (otherwise the measure would be null). We want to prove it for general $f$. By interpolation an up to choosing a smaller $\varepsilon$, we can suppose $h := f - f_0$ is $C^1$. Applying the Girsanov theorem with $M = B$ and $L_t = \int_0^t \dot{h}_s \;\d B_s$ for a Brownian motion $B$, then $M - \langle M, L \rangle$ is $B - h$ and

$$
  \mathbb{P}(|B - f|_{L^{\infty}(0,1)} < \varepsilon) = \mathbb{E}_{\mathbb{P}}\left( \mathbf{1}_{|B - f_0|_{L^\infty (0,1)} < \varepsilon} \exp\left( \int_0^1 \dot{h}_s \;\d B_s - \frac{1}{2}\int_0^1 \dot{h}_s^2 \;\d s \right) \right)
$$

The right hand side is strictly positive because $\mathbb{P}(|B - f_0|_{L^\infty (0,1)} < \varepsilon) > 0$ and $\int_0^1 \dot{h}_s \;\d B_s > -\infty$ almost surely.
</details>
</MathBox>

In most applications of Girsanov's theorem, a $\mathbb{P}$-local martingale $L$ is given, and we make the choice

$$
  \left.\frac{\d\mathbb{Q}}{\d\mathbb{P}}\right|_{\mathcal{F}_t} = \exp\left( L_t - \frac{1}{2}\langle L \rangle_t \right)
$$

hoping that the expectation of the right hand side is $1$. Note that $\mathcal{E}(L)$ is a positive local martingale, thus a supermartingale, converging almost surely and 

$$
  \mathbb{E}(\mathcal{E}(L)_\infty) \leq 1
$$

There is equality if and only if $\mathcal{E}(L)$ is a uniformly integrable martingale. Moreover, if $\mathcal{E}(L)$ is uniformly integrable, then when defining

$$
  \left.\frac{\d\mathbb{Q}}{\d\mathbb{P}}\right|_{\mathcal{F}_t} = \mathcal{E}(L)_\infty
$$

we obviously have $\mathbb{Q}\sim\mathbb{P}$ on $\mathcal{F}_t$. Thus, checking the uniform integrability of an exponential local martingale is important to apply Girsanov's theorem.

<MathBox title="Novikov's criterium" boxType="theorem">
Let $L$ be a local martingale beginning at $0$. If $\mathbb{E}\left(\exp\left(\frac{1}{2}\langle L \rangle_\infty \right)\right) < \infty$, then $\mathcal{E}(L)$ is a uniformly integrable martingale.

<details>
<summary>Proof</summary>

We first prove that the hypothesis inequality implies that $L$ is unformly integrable and that $\mathbb{E}\left(\exp\left(\frac{1}{2} L_\infty \right)\right) < \infty$, which in turn implies the result.

Obviously, $\mathbf{E}(\langle L \rangle_\infty) < \infty$, so $L$ is bounded in $\mathcal{L}^2$. Hence $L$ is a uniformly integrable martingale. Moreover, by a simple application of the Cauchy-Schwarz inequality

$$
\begin{align*}
  \mathbb{E}\left(\exp\left(\frac{1}{2}L_\infty\right) \right) =& \mathbb{E}\left( \right) \\
  \leq& \mathbb{E}(\mathcal{E}(L)_\infty)^{1/2} \mathbb{E}\left(\exp\left(\frac{1}{2}\langle L \rangle_\infty\right) \right)^{1/2} \\
  \leq& \mathbb{E}\left( \exp\left( \frac{1}{2}\langle L \rangle_\infty \right) \right)^{1/2}
\end{align*}
$$

so $\mathbb{E}\left(\exp\left(\frac{1}{2}L_\infty \right)\right) < \infty$.

Next, we want to prove the uniform integrability of $\mathcal{E}(L)$. A sufficient condition is $\mathbb{E}(\mathcal{E}(L)_\infty) = 1$. Since $L$ is uniformly integrable, $L_t = \mathbb{E}(L_\infty | \mathcal{F}_t)$, so by convexity $\exp\left( \frac{1}{2}L_t \right) < \mathbb{E}\left(\exp\left(\frac{1}{2}L_\infty \right) | \mathcal{F}_t \right)$. This proves that $\left(\exp\left( \frac{1}{2}L_t \right)\right)_{t\geq 0}$ is uniformly integrable.

Because the exponential is convex and increasing, $\left(\exp\left( \frac{1}{2}L_t \right)\right)_{t\geq 0}$ is a submartingale, uniformly integrable from the previous point. Hence, for any stopping time $T$, it follows that $\exp\left( \frac{1}{2}L_T \right) < \mathbb{E}\left( \exp\left( \frac{1}{2}L_\infty | \mathcal{F}_t \right) \right)$. Thus $\Set{\exp\left(\frac{1}{2}L_T\right) | T \text{ stopping time}}$ is uniformly integrable.

Optimally, we would like to prove that $\Set{\mathcal{E}(L)_T | T \text{ stopping time}}$ is uniformly integrable. This is slightly too much required. We can however show that $\Set{\mathcal{E}(\lambda L)_T | T \text{ stopping time}}$ is integrable for any $0 < \lambda < 1$. Take any $A \in \mathcal{F}$ (typically $A = \Set{\mathcal{E}(\lambda L)_T > x}$). Then

$$
\begin{align*}
  \mathbb{E}(\mathbf{1}_A \mathcal{E}(\lambda L)_T) =& \mathbb{E}(\mathbf{1}_A (\mathcal{E}(L)_T)^{\lambda^2} \exp(\lambda(1 - \lambda)L_T)) \\
  \leq& \mathbb{E}(\mathcal{E}(L)_T)^{\lambda^2} \mathbb{E}\left(\mathbf{1}_A \exp\left(\frac{\lambda}{1 + \lambda}L_T\right)\right)^{1-\lambda^2} \\
  \leq& \mathbb{E}\left(\mathbf{1}_A \exp\left(\frac{\lambda}{1 + \lambda}L_T \right)\right)^{1-\lambda^2} \\
  =& \mathbb{E}\left( \left(\mathbf{1}_A \exp\left( \frac{1}{2}L_T \right)^{\frac{2\lambda}{1 + \lambda}}\right) \right)^{1-\lambda^2} \\
  \leq& \mathbb{E}\left(\mathbf{1}_A \exp\left(\frac{1}{2}L_T \right)\right)^{2\lambda (1 - \lambda)}
\end{align*}
$$

where we used in the second inequality that for a positive supermartingale $M$ and any stopping time $T$, $\mathbb{E}(M_T) \leq M_0$ (this is true for bounded stopping times, and then for any of them by Fatou's lemma). The last inequality relies on the concavity of $x\mapsto x^a$ for $a < 1$. The last term above being uniformly integrable, the set $\Set{\mathcal{E}(\lambda L)_T | T \text{ stopping time}}$ is uniformly integrable for any $0 < \lambda < 1$.

This implies that $\mathcal{E}(\lambda L)$ is a uniformly integrable martingale. It follows that

$$
\begin{align*}
  1 =& \mathbb{E}(\mathcal{E}(\lambda L)_\infty) \\
  \leq& \mathbb{E}(\mathcal{E}(L)_\infty)^{\lambda^2} \mathbb{E}(\exp\left(\frac{1}{2}L_\infty \right))^{2\lambda(1 - \lambda)}
\end{align*}
$$

Taking $\lambda \to 1^-$ yields $\mathbb{E}(\mathcal{E}(L)_\infty) \geq 1$.
</details>
</MathBox>

# Stochastic differential equations

This section studies ordinary stochastic differential equations of the form

$$
  \d X_t = \sigma(t, X)\;\d B_t + b(t, X) \d t 
$$

which can be decomposed into a deterministic drift term $b(t, X) \d t$, corresponding to an ordinary differential equation, and a random perturbation term $\sigma(t, X)\;\d B_t$ driven by a Wiener process $B_t$.

<MathBox title='' boxType="definition">
A solution to the equation

$$
  \d X_t = \sigma(t, X)\;\d B_t + b(t, X) \d t 
$$

on $\R_+$ is the collection of
- a filtered probability space $(\Omega, \mathcal{F}, (\mathcal{F}_t)_{t\geq 0}, \mathbb{P})$
- a $(\mathcal{F})_t$-Brownian motion $B = (B^{(1)},\dots, B^{(m)})$ with $B^{(i)}$ a $(\mathcal{F}_t)$-martingale and $\langle B^{(i)}, B^{(j)}\rangle = \mathbf{1}_{i=j} t$
- a $(\mathcal{F}_t)$-adapted process $X = (X^{(1)},\dots,X^{(d)})$ such that

$$
  X_t = X_0 + \int_0^t \sigma(s, X)\;\d B_s + \int_0^t b(s, X)\;\d s
$$

This is abbreviated by saying the process is a solution of $\mathbb{E}(\sigma, b)$. When imposed to $X_0 = x$, X is said to be a solution of $\mathbb{E}_x (\sigma, b)$.

For the equation $\mathbb{E}(\sigma, b)$, we say that there is
- weak existence if for any $x\in\R^d$ there is a solution to $\mathbb{E}_x (\sigma, b)$
- weak existence and uniqueness if for any $x\in\R^d$ there is a solution to $\mathbb{E}_x (\sigma, b)$ and all solutions to $\mathbb{E}_x (\sigma, b)$ have the same law
- pathwise uniqueness if, given $(\Omega, \mathcal{F}, (\mathcal{F}_t)_{t\geq 0}, \mathbb{P})$ and $B$, two solutions $X$ and $X'$ such that $X_0 = X_0'$ $\mathbb{P}$-almost surely cannot be distinguished.

A solution $X$ of $\mathbb{E}_x (\sigma, b)$ is strong if $X$ is $(\mathcal{B}_t)$-adapted, where $\mathcal{B}_t = \sigma(B_s, s\leq t)$.
</MathBox>

# Ornstein-Uhlenbeck process

<MathBox title='Ornstein-Uhlenbeck process' boxType="proposition">
The Ornstein-Uhlenbeck process $X_t$ is defined by the stochastic differential equation

$$
\begin{equation}
  \d X_t = \theta (\mu - X_t) \d t + \sigma \d W_t \tag{\label{equation-1}}
\end{equation}
$$

where
- $\theta > 0$ and $\sigma > 0$ are parameters
- $\mu$ is a constant drift coefficient
- $W_t$ is a Wiener process

The solution is

$$
  X_t = X_0 e^{-\mu T} + \theta(1 - e^{-\mu T}) + \sigma \int_0^T e^{-\mu(T - t)} \d W_t
$$

<details>
<summary>Proof</summary>

First, we write Ornstein-Uhlenbeck equation $\eqref{equation-1}$ as

$$
  \d X_t + \mu X_t \d t = \mu\theta \d t + \sigma \d W_t
$$

To find the integrating factor, we consider the deterministic part

We first solve the deterministic part of $\eqref{equation-1}$

$$
  \d x + \mu x \d t = \mu\theta \d t
$$

Introducing $b = \mu\theta$, the equation is refactored as

$$
  \frac{\d x}{\d t} + \mu x = b
$$

Since $\mu$ is constant, the integrating factor is

$$
  \exp\left(\int_0^t \mu \;\d s \right) = e^{\mu t} 
$$

Multiplying by the integrating factor, we get the equation

$$
\begin{gather*}
  e^{\mu t} \frac{\d x}{\d t} + e^{\mu t} \mu x = e^{\mu t} b \\
  \frac{\d}{\d t}(e^{\mu t} x) = e^{\mu t} b \\
  \d (e^{\mu t} x) = e^{\mu} b \dt
\end{gather*}
$$

Multiplying the stochastic equation by the integrating factor, we obtain

$$
\begin{gather*}
  e^{\mu t} \d X_t + \mu e^{\mu t} X_t \d t = \mu\theta \\
  \d (e^{\mu t} X_t) = \mu\theta e^{\mu t} \d t + \sigma e^{\mu t} \;\d W_t
\end{gather*}
$$

Integrating, we get the solution

$$
\begin{gather*}
  \int_0^T \d (e^{\mu t} X_t) \int_0^T \mu\theta e^{\mu t} \;\d t + \int_0^T \sigma e^{\mu t} \;\d W_t \\
  e^{\mu T} X_T - e^0 X_0 = \mu\theta \frac{e^{\mu T} - e^0}{\mu} + \sigma \int_0^T e^{\mu} \;\d W_t \\
  X_T - X_0 e^{-\mu T} = \theta(1 - e^{-\mu T}) + \sigma e^{-\mu T} \int_0^T e^{\mu t} \;\d W_t \\
  X_T = X_0 e^{-\mu T} + \theta(1 - e^{-\mu T}) + \sigma \int_0^T e^{-\mu(T - t)} \;\d W_t
\end{gather*}
$$
</details>
</MathBox>

<MathBox title='Momenta of the Ornstein-Uhlenbeck process' boxType="proposition">
Let $X_t, X_s$ be Ornstein-Uhlenbeck processes. Then:
1. The expectation value of $X_t$ is
$$
  \mathbb{E}(X_T) = X_0 e^{-\mu T} + \theta (1 - e^{-\mu T})
$$
2. The variance of $X_t$ is
$$
  \operatorname{var}(X_t) = \frac{\sigma^2}{2\mu} (1 - e^{-2\mu T})
$$
3. The covariance of $X_t$ and $X_s$ is
$$
  \operatorname{cov}(X_t, X_s) = \frac{\sigma^2}{2\mu} \left(e^{-\mu |T - S| + 2\mu S} - e^{-\mu (S + T)} \right)
$$

<details>
<summary>Proof</summary>

**(1):** Recall that the expectation of an integral of a deterministic function with respect to a Wiener process is zero. Taking the expectation of $X_t$ gives

$$
\begin{align*}
  \mathbb{E}(X_t) =& \mathbb{E}\left(X_0 e^{-\mu T} + \theta (1 - e^{-\mu T}) + \sigma \int_0^T e^{-\mu(T - t)} \;\d W_t \right) \\
  =& \mathbb{E}(X_0 e^{-\mu T}) + \mathbb{E}[\theta(1 - e^{-\mu T})] + \underbrace{\mathbb{E}\left(\sigma \int_0^T e^{-\mu (T - t)} \;\d W_t \right)}_{=0} \\
  =& X_0 e^{-\mu T} + \theta (1 - e^{-\mu T})
\end{align*}
$$

**(2):** Calculating the variance of $X_t$ using Itô's isometry rule gives

$$
\begin{align*}
  \operatorname{var}(X_t) =& \mathbb{E}\left[(X_t - E(X_t))^2 \right] \\
  =& \mathbb{E}\left[\left(\sigma \int_0^T e^{-\mu (T - t)} \;\d W_t \right)\right] \\
  =& \sigma^2 \int_0^T e^{-2\mu (T - t)} \;\d t \\
  =& \sigma^2 e^{-2\mu T} \int_0^T e^{2\mu t} \;\d t
  =& \frac{\sigma^2}{2\mu} e^{-2\mu T} (e^{2\mu T} - e^0) \\
  =& \frac{\sigma^2}{2\mu} (1 - e^{-2\mu T})
\end{align*}
$$

**(3):** Calculating the covarianc of $X_t$ and $X_s$ gives

$$
\begin{align*}
  \operatorname{cov}(X_t, X_s) =& \mathbb{E}\left[(X_t - \mathbb{E}(X_t))(X_s - \mathbb{E}(X_s)) \right] \\
  =& \mathbb{E}\left[\left(\sigma \int_0^T e^{-\mu(T - t)} \;\d W_t \right)\left(\sigma \int_0^S e^{-\mu(S - s)} \d W_s \right) \right] \\
  =& \sigma^2 e^{-\mu(S + T)} \mathbb{E}\left(\int_0^T e^{\mu t} \; \right)
\end{align*}
$$

We assume that $s < t$ and note that the covariance over non-overlapping periods is zero. Applying Itô's isometry, we find

$$
\begin{align*}
  \operatorname{cov}(X_t, X_s) =& \sigma^2 e^{-\mu (S + T)} \mathbb{E}\left(\int_0^T e^{\mu t} \; \right) \\
  =& \sigma^2 e^{-\mu (S + T)} int_0^S e^{2\mu s} \;\d s \\
  =& \frac{\sigma^2}{2\mu} e^{-\mu(S + T)} (e^{2\mu S} - 1) \\
  =& \frac{\sigma^2}{2\mu} \left(e^{-\mu (T - S) + 2\mu S} - e^{-\mu (S + T)} \right)
\end{align*}
$$

Similarly, assuming $t < s$, we obtain

$$
  \operatorname{cov}(X_t, X_s) = \frac{\sigma^2}{2\mu} \left(e^{-\mu (S - T) + 2\mu S} - e^{-\mu (S + T)} \right)
$$

Hence,

$$
  \operatorname{cov}(X_t, X_s) = \frac{\sigma^2}{2\mu} \left(e^{-\mu |T - S| + 2\mu S} - e^{-\mu (S + T)} \right)
$$
</details>
</MathBox>

<MathBox title='Limiting Ornstein-Uhlenbeck process' boxType="proposition">
Let $X_t$ be an Ornstein-Uhlenbeck process. In the limit $T\to\infty$, we have

1. $\lim_{T\to\infty} \mathbb{E}(X_t) = \theta$
2. $\lim_{T\to\infty} \operatorname{var}(X_t) = \frac{\sigma^2}{\mu}$

The halving period $H$, which is the time required for the expected value of $X_t$ to reach the midpoint between its initial value $X_0$ and its long-term mean $\theta$, i.e.

$$
  \mathbb{E}(X_H) = X_0 + \frac{\theta - X_0}{2}
$$

is given by 

$$
  H = \frac{\ln(2)}{\mu}
$$ 

<details>
<summary>Proof</summary>

**(1):** Taking the limit $T\to\infty$ of $\mathbb{E}(X_T)$, we have

$$
\begin{align*}
  \lim_{T\to\infty} \mathbb{E}(X_T) =& \lim_{T\to\infty} \left(X_0 e^{-\mu T} + \theta (1 - e^{-\mu T}) \right) \\
  =& X_0 \lim_{T\to\infty} e^{-\mu T} + \theta \left(1 - \lim_{T\to\infty} e^{-\mu T} \right) \\
  =& \theta
\end{align*}
$$

**(2):** Taking the limit $T\to\infty$ of $\operatorname{var}(X_T)$, we have

$$
\begin{align*}
  \lim_{T\to\infty} \operatorname{var}(X_T) =& \lim_{T\to\infty} \left( \frac{\sigma^2}{2\mu} (1 - e^{-2\mu T}) \right) \\
  =& \frac{\sigma^2}{2\mu} \left(1 - \lim_{T\to\infty} e^{-2\mu T} \right) \\
  =& \frac{\sigma^2}{2\mu}
\end{align*}
$$

**Halving period:** Calculating $\mathbb{E}(X_H)$, we find

$$
\begin{align*}
  X_0 e^{-\mu H} + \theta(1 - e^{-\mu H}) =& X_0 + \frac{\theta - X_0}{2} \\
  e^{-\mu H} (X_0 - \theta) + \theta =& X_0 + \frac{\theta - X_0}{2} \\
  e^{-\mu H} (X_0 - \theta) =& \frac{-2\theta + 2X_0 + \theta - X_0}{2} \\
  e^{-\mu H} (X_0 - \theta) =& \frac{X_0 - \theta}{2} \\
  e^{-\mu H} =& \frac{1}{2}
\end{align*}
$$

Taking the logarithm and refactoring yields $H = \ln(2)/\mu$.
</details>
</MathBox>

Discretizing the Ornstein-Uhlenbeck equation with time steps $\Delta t$, we find

$$
\begin{align*}
  \mathbb{E}(X_{t + \Delta t}) =& X_t e^{-\mu\Delta t} + \theta (1 - e^{-\mu\Delta t}) \\
  \operatorname{var}(X_{t + \Delta t}) =& \frac{\sigma^2}{2\mu} (1 - e^{-2\mu\Delta t}) 
\end{align*}
$$

This gives the time series

$$
  X_{t + \Delta t} = X_t e^{-\mu\Delta t} + \theta (1 - e^{-\mu\Delta t}) + \sigma\sqrt{\frac{1 - e^{-2\mu\Delta t}}{2\mu}} \mathcal{N}(0,1)
$$