---
title: 'Stochastic Analysis'
subject: 'Mathematics'
showToc: true
---

# Stochastic processes

To review, a stochastic process on a probability space $(\Omega,\mathcal{F},\mathbb{P})$ is a collection of random variables $\mathbf{X}:=\{X_t: \Omega\to S\}_{t\in T}$, with state space $(S,\mathcal{S})$ and index space $(T,\mathcal{T})$.

A filtration $\mathscr{F} = \{\mathcal{F}_t\}_{t\in T}$ is an increasing collection of sub $\sigma$-algebra of $\mathcal{F}$, encoding the information available at time $t$. A stochastic process $\mathbf{X}$ is adapted to a filtration $\mathscr{F}$ if $X_t$ is measurable with respect to $\mathcal{F}_t$ for each $t\in T$. 

## Martingale

A martingale is a stochastic process for which the conditional expectation of the next value is equal to the present value.

<MathBox title='Martingale' boxType='definition'>
A stochastic process $\mathbf{X}$ is a *martingale* with respect to a filtration $\mathscr{F}$ if $\mathbb{E}(X_t|\mathcal{F}_s) = X_s$ for all $s, t\in T$ with $s\leq t$.

If the equality in the martingale conditions does not hold, then for  for all $s, t\in T$ with $s\leq t$, the process $\mathbf{X}$ is (with respect to $\mathscr{F}$)
1. *sub-martingale* $\mathbb{E}(X_t|\mathcal{F}_s) \geq X_s$ for all $s, t\in T$ with $s\leq t$.
2. *super-martingale* $\mathbb{E}(X_t|\mathcal{F}_s) \leq X_s$ for all $s, t\in T$ with $s\leq t$.

If $\mathfb{X} = \{X_n\}_{n\in\mathbb{N}}$ is discrete, then for all $n\in\mathbb{N}$ the process $\mathbf{X}$ is (with respect to $\mathscr{F}$)
1. martingale if and only if $\mathbb{E}(X_{n+1}|\mathcal{F}_n) = X_n$
2. sub-martingale if and only if $\mathbb{E}(X_{n+1}|\mathcal{F}_n) \geq X_n$
3. super-martingale if and only if $\mathbb{E}(X_{n+1}|\mathcal{F}_n) \leq X_n$

<details>
<summary>Details</summary>

To show the bicondationality in the discrete case, suppose that $k,n\in\mathbb{N}$ with $k< n$. Then $k\leq n - 1$ so $\mathcal{F}_k\subseteq\mathcal{F}_{n-1}$ and hence

$$
\begin{align*}
  \mathbb{E}(X_n | \mathcal{F}_k) &= \mathbb{E}[\mathbb{E}(X_n|\mathcal{F}_{n-1})] \\
  &= \mathbb{E}(X_{n-1}|\mathcal{F}_k)
\end{align*}
$$
</details>
</MathBox>

In gambling terms, martingale, sub-martingale and super-martingale processes are abstractions of fair, favourable or unfair games (from the gambler's perspective).

<MathBox title='Martingale (review)' boxType='definition'>
Let 
- $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space
- $\mathscr{F} = \left( \mathcal{F}_t \right)_{t \in T}$ be a filtration of $\mathcal{A}$ for an ordered index set $T$

A stochastic process $\left\{ X_t \right\}_{t\in T}$ is called a martingale wrt. $\mathscr{F}$ if for all $t\in T$
- $\mathrm{E}\left[\left| X_t \right|\right] < \infty$
- $X_t$ is $\mathcal{F}_t$ measurable, i.e. $X_t \in\mathcal{F}_t$
- discrete case: $\mathrm{E}\left[ X_{t+1} | \mathscr{F}_t \right] = X_t$
- continuous case: $\mathrm{E}\left[ X_{t} | \mathscr{F}_s \right] = X_s$ for $s < t$

Similarly, it is said to be a supermartingale (respectively, submartingale) if for every $n$

$$
\begin{gather*}
  \mathrm{E}\left[ X_{t+1} | \mathscr{F}_t \right] \leq(\geq) X_t \\
  \mathrm{E}\left[ X_{t} | \mathscr{F}_s \right] \leq(\geq) X_s \quad s < t
\end{gather*}
$$
</MathBox>

The most basic examples of martingales are sums of independent, mean zero random variables. Let $(Y_n)_{n\geq 0}$ be such a sequence, then the sequence of partial sums

$$
  X_n = \sum_{j=1}^n Y_j
$$

is a martingale relative to the natural filtration of $Y_n$. This can be shown using the linearity and stability properties and the independence law for conditional expectation

$$
\begin{align*}
  E\left( X_{n+1} | \mathscr{F}_n \right) &= E\left( X_n + Y_{n+1} | \mathscr{F}_n \right) = E\left( X_n | \mathscr{F}_n \right) + E\left( Y_{n+1} | \mathscr{F}_n \right) \\
  &= X_n + \mathrm{E}\left[ Y_{n+1} \right] \\
  &= X_n
\end{align*}
$$

## Examples

### Constant sequence

<MathBox title='Martingale conditions for partial sums' boxType='definition'>
Suppose that $X$ is a random variable that is measurable with respect to $\mathcal{F}_0\in\mathscr{F}$, and with $\mathbb{E}(|X|) < \infty$. Let $X_t = X$ for $t\in T$. Then the constant sequence process $\mathbf{X} = \{ X_t \}_T$ is a martingale with respect to $\mathscr{F}$.

<details>
<summary>Proof</summary>

Since $X$ is measurable with respect to $\mathcal{F}_0$, it is measurable with respect to $\mathcal{F}_t$ for all $t\in T$. Thus $\mathbb{X}$ is adapted to $\mathscr{F}$. If $s,t\in T$ with $s\leq t$, then

$$
  \mathbb{E}(X_t |\mathcal{F}_s) = \mathbb{E}(X | \mathcal{F}_s) = X = X_s
$$
</details>
</MathBox>

### Processes with stationary and independent increments (Lévy process)

<MathBox title='Independent and stationary increments' boxType='definition'>
Suppose that $\mathbf{X} = \{X_t\}_{t\in T}$ is a process adapted to $\mathscr{F} = \{\mathcal{F}_t\}_{t\in T}$. Then for all $s,t\in T$ with $s\leq t$, the process $\mathbf{X}$ is said to have
1. *Independent increments* if $X_t - X_s$ is independent of $\mathcal{F}_s$
2. *Stationary increments* if $X_t - X_s$ has the same distribution as $X_{t-s} - X_0$

Processes with independent and stationary increments are random walk processes. In continuous time, such processes are known as Lévy processes.
</MathBox>

<MathBox title='Martingale conditions for processes with independent increments' boxType='definition'>
Suppose that the continuous process $\mathbf{X} = \{X_t\}_{t\in [0,\infty]}$ has independent increments, and let $m(t) = \mathbb{E}(X_t)$ for $t\in T$. Then $\mathbf{X}$ is
1. Martingale if $m$ is constant.
2. Sub-martingale if $m$ is increasing.
3. Super-martingale if $m$ is decreasing.

<details>
<summary>Proof</summary>

Suppose that $s,t\in[0,\infty)$ with $s< t$. Then

$$
\begin{align*}
  \mathbb{E}(X_t | \mathcal{F}_s) &= \mathbb{E}\left[X_s + (X_t - X_s) |\mathcal{F}_s \right] \\
  &= \mathbb{E}(X_s |\mathcal{F}_s ) + \mathbb{E}(X_t - X_s |\mathcal{F}_s)
\end{align*}
$$

Since $X_s$ is measurable to $\mathcal{F}_s$ and $X_t - X_s$ is independent of $\mathcal{F}_s$, we obtain

$$
\begin{align*}
   \mathbb{E}(X_t | \mathcal{F}_s) &= \mathbb{E}(X_s |\mathcal{F}_s ) + \mathbb{E}(X_t - X_s |\mathcal{F}_s) \\
   &= X_s + \mathbb{E}(X_t - X_s) = X_s + m(t) - m(s)
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Second moment martingale for processes with independent increments' boxType='definition'>
Suppose that the process $\mathbf{X} = \{X_t\}_{t\in T}$ has independent increments with constant mean function, and with $\mathrm{var}(X_t) < \infty$ for $t\in T$. Let 

$$
  Y_t = X_t^2 - \mathrm{var}(X_t)\quad t\in T
$$

Then $\mathbf{Y} = \{Y_t\}$ is a martingale.

<details>
<summary>Proof</summary>

Suppose that $s,t\in T$ with $s< t$. Note that $\mathbb{E}(Y_t|\mathcal{F}_s) = \mathbb{E}(X_t^2 | \mathcal{F}_s) -\mathrm{var}(X_t)$. Rewriting $X_t^2$ as

$$
\begin{align*}
  X_t^2 &= [(X_t - X_s) + X_s]^2 \\
  &= (X_t - X_s)^2 + 2(X_t - X_s)X_s + X_s^2 
\end{align*}
$$

Since $X_t - X_s$ is independent of $\mathcal{F}_s$, $X_s$ is measurable with respect to $\mathcal{F}_s$ and $\mathbb{E}(X_t - X_s) = 0$, it follows that

$$
\begin{align*}
  \mathbb{E}(X_t^2 | \mathcal{F}_s) &= \mathbb{E}[(X_t - X_s)^2] + 2X_s \mathbb{E}(X_t - X_s) + X_s^2 \\
  &= \mathbb{E}[(X_t - X-s)^2] + X_s^2
\end{align*}
$$

Since $X_t - X_s$ also has mean $0$

$$
\begin{align*}
  \mathrm{var}(X_t) &= \mathrm{var}[(X_t - X_s) + X_s] \\
  &= \mathrm{var}(X_s) + \mathrm{var}(X_t - X_s)^2 \\
  &= \mathrm{var}(X_s) + \mathbb{E}[(X_t - X_s)^2]
\end{align*}
$$

Combining the results gives

$$
  \mathbb{E}(Y_t | \mathcal{F}_s) = X_s^2 - \mathrm{var}(X_s) = Y_s
$$
</details>
</MathBox>

<MathBox title='Martingale conditions for random walks' boxType='definition'>
Suppose that the process $\mathbf{X} = \{X_t\}_{t\in T}$ has stationary, independent increments, and let $a = \mathbb{E}(X_1 - X_0)$. Then $\mathbf{X}$ is
1. Martingale if $a=0$.
2. Sub-martingale if $a\geq 0$.
3. Super-martingale if $a\leq 0$.

<details>
<summary>Proof</summary>

Note that for a process with stationary, independent increments, the mean function reduces to $m(t) = \mathbb{E}(X_0) + at$ for $t\in T$. Follwing the same argument as for processes with independent increments, we get

$$
  \mathbb{E}(X_t | \mathcal{F}_s) = X_s + a(t - s)
$$
</details>
</MathBox>

<MathBox title='Second moment martingale for random walks' boxType='definition'>
Suppose that the process $\mathbf{X} = \{X_t\}_{t\in T}$ has independent increments with $\mathbb{E}(X_0) = \mathbb{E}(X_1)$ and $b^2 = \mathbb{E}\left(X_1^2\right) < \infty$. Let

$$
  Y_t = X_t^2 - \mathrm{var}(X_0) - b^2 t,\quad t\in T
$$

Then $\mathbf{Y} = \{Y_t\}$ is a martingale.

<details>
<summary>Proof</summary>

Since $\mathbb{E}(X_0) = \mathbb{E}(X_1)$, then $\mathbf{X}$ has constant mean function. Note that $\mathrm{var}(X_t) = \mathrm{var}(X_0) + b^2 t$. Following the same argument as for second order martingales for processes with independent increments, we obtain

$$
\begin{align*}
  \mathbb{E}(Y_s|\mathcal{F}_s) &= X_s - \mathrm{var}(X_s) \\
  &= X_s - \mathrm{var}(X_0) - b^2 t = Y_s
\end{align*}
$$
</details>
</MathBox>

#### Partial sums

In discrete time, a process with independent increments reduces to a partial sum process.

<MathBox title='Martingale conditions for partial sums' boxType='definition'>
Suppose that $\mathbf{V} = \{V_n\}_{n\in\mathbb{N}}$ is a sequence of random variables with $\mathbb{E}(|V_n|) < \infty$, and let

$$
  X_n = \sum_{k=0}^n V_n
$$

Then $\mathbf{X} = \{X_n\}$ is a partial sum process associated with $\mathbf{V}$. For $n\in\mathbb{N}$, the process $\mathbf{X}$ is called
1. sub-martingale if $\mathbb{E}(V_n)\geq 0$
2. super-martingale if $\mathbb{E}(V_n)\leq 0$
3. martingale if $\mathbb{E}(V_n) = 0$

<details>
<summary>Proof</summary>

Let $\mathcal{F}_n = \sigma\{X_i \}_{i=0}^{n\in\mathbb{N}} = \sigma\{V_i\}_{i=0}^{n\in\mathbb{N}}$. Note first that

$$
  \mathbb{E}(|X_n|) \leq \sum_{k=0}^n \mathbb{E}(|V_k|) < \infty
$$

Next

$$
\begin{align*}
  \mathbb{E}(X_{n+1}|\mathcal{F}_n) &= \mathbb{E}(X_n + V_{n+1}|\mathcal{F}_n) \\
  &= \mathbb{E}(X_n|\mathcal{F}_n) + \mathbb{E}(V_{n+1}|\mathcal{F}_n) \\
  &= X_n + \mathbb{E}(V_{n+1})
\end{align*}
$$

The last equality holds since $X_n$ is measurable with respect to $\mathcal{F}_N$ and $V_{n+1}$ is independent of $\mathcal{F}_n$.
</details>
</MathBox>

<MathBox title='Second moment martingale' boxType='definition'>
Let $\mathbf{X}$ be a partial sum process associated with $\mathbf{V}$. Suppose that $\mathbb{E}(V_k) = 0$ for $k\in\mathbb{N}_+$ and $\mathrm{var}(V_k) < \infty$ for $k\in\mathbb{N}$, and let 

$$
  Y_n = X_n^2 - \mathrm{var}(X_n)
$$

Then $\mathbf{Y} = \{ Y_n\}_{n\in\mathbb{N}}$ is a martingale with respect to $\mathbf{X}$.

<details>
<summary>Proof</summary>

Let $\mathcal{F}_n = \sigma\{X_i \}_{i=0}^{n\in\mathbb{N}}$. Since $\mathbf{V}$ is independent, note that

$$
  \mathrm{var}(X_n) = \mathrm{var}\left( \sum_{k=0}^n V_k \right) = \sum_{k=0}^n \mathrm{var}(V_k)
$$

Since $\mathbb{E}(V_k) = 0$, then $\mathrm{var}(V_k) = \mathbb{E}(V_k^2)$. In particular, $\mathbb{E}(|Y_n|) < \infty$ for $n\in\mathbb{N}$. Next

$$
\begin{align*}
  \mathbb{E}(Y_{n+1}|\mathcal{F}_n) &= \mathbb{E}[X_{n+1}^2 - \mathrm{var}(X_{n+1}|\mathcal{F}_n)] \\
  &= \mathbb{E}\left[(X_n + V_{n+1})^2 - \mathrm{var}(X_{n+1})|\mathcal{F}_n \right] \\
  &= \mathbb{E}\left[ X_n^2 + 2X_n V_{n+1} + V_{n+1}^2 - \mathrm{var}(X_{n+1})|\mathcal{F}_n \right] \\
  &= X_n^2 + 2X_n\mathbb{E}(V_{n+1}) + \mathbb{E}(V_{n+1}^2) - \mathrm{var}(X_{n+1})
\end{align*}
$$

Next

$$
\begin{align*}
  \mathbb{E}(X_{n+1}|\mathcal{F}_n) &= \mathbb{E}(X_n + V_{n+1}|\mathcal{F}_n) \\
  &= \mathbb{E}(X_n|\mathcal{F}_n) + \mathbb{E}(V_{n+1}|\mathcal{F}_n) \\
  &= X_n + \mathbb{E}(V_{n+1})
\end{align*}
$$
</details>
</MathBox>

#### Difference sequence

<MathBox title='Martingale difference sequence' boxType='definition'>
Suppose that $\mathbf{X} = \{X_n\}_{n\in\mathbb{N}}$ is a discrete process adapted to $\mathscr{F}$. Let $V_0 = X_0$ and $V_n = X_n - X_{n-1}$ for $n\in\mathbb{N}$. The process $\mathbf{V} = \{ V_n\}_{n\in\mathbb{N}}$ is the martingale difference sequence associated with $\mathbb{X}$ such that for $n\in\mathbb{N}$

$$
  X_n = \sum_{k=0}^n V_k
$$
</MathBox>

<MathBox title='Properties of martingale difference sequence' boxType='definition'>
Let $\mathbf{X} = \{X_n\}_{n\in\mathbb{N}}$ be a discrete process adapted to $\mathscr{F} = \{\mathcal{F}_n\}_{n\in\mathbb{N}}$. Suppose that $\mathbf{V} = \{V_n\}_{n\in\mathbb{N}}$ is the martingale difference sequence associate with $\mathbf{X}$. Then
1. $\mathbb{V}$ is adapted to $\mathscr{F}$.
2. $\mathbb{E}(V_n|\mathcal{F}_k) = 0$ for $k,n\in\mathbb{N}$ with $k< n$.
3. $\mathbb{E}(V_n) = 0$ for $n\in\mathbb{N}_+$
4. If $\mathrm{var}(X_n) < \infty$ for $n\in\mathbb{N}$, then $\mathbf{V}$ is an uncorrelated sequence and

$$
  \mathrm{var}(X_n) = \sum_{k=0}^n \mathrm{var}(V_k) = \mathrm{var}(X_0) + \sum_{k=1}^n \mathbb{E}\left(V_k^2\right)
$$

<details>
<summary>Proof</summary>

1. Obviously, $V_0 = X_0$ is measurable with respect to $\mathcal{F}_0$. For $n\in\mathbb{N}_+$, then $X_n$ and $X_{n-1}$ and thus $V_n$ are measurable with respect to $\mathcal{F}_n$. Hence $\mathbf{V}$ is adapted to $\mathscr{F}$.
2. Let $k\in\mathbb{N}$. By the martingale and adapted properties

$$
\begin{align*}
  \mathbb{E}\left( V_{k+1} | \mathcal{F}_k \right) &= \mathbb{E}(X_{k+1}|\mathcal{F}_k) - \mathbb{E}(X_k | \mathcal{F}_k) \\
  &= X_k - X_k = 0
\end{align*}
$$

By the tower property of conditional expectation

$$
  \mathbb{E}\left( V_{k+1} | \mathcal{F}_k \right) = \mathbb{E}[\mathbb{E}(V_{k+2}|\mathcal{F}_{k+1})|\mathcal{F}_k] = 0
$$

The result follows from induction.
3. Since $\mathbf{X}$ is a martingale, it has constant mean. Hence $\mathbb{E}(V_n) = \mathbb{E}(X_n) - \mathbb{E}(X_{n-1}) = 0$ for $n\in\mathbb{N}_+$.
4. Let $k,n\in\mathbb{N}$ with $k< n$. To show that $V_k$ and $V_n$ are uncorrelated, we just have to show that $\mathbb{E}(V_k V_n) = 0$ since $\mathbb{E}(V_n) = 0$. By $(3)$

$$
\begin{align*}
  \mathbb{E}(V_k V_n) &= \mathbb{E}[\mathbb{E}(V_k V_n |\mathcal{F}_k)] \\
  &= \mathbb{E}[V_k\mathbb{E}(V_n|\mathcal{F}_k)] = 0
\end{align*}
$$

To prove the formula for $\mathrm{var}(X_n)$, note that the variance of a sum of uncorrelated variables is the sum of the variances. Since $V_k$ has mean $0$, then $\mathrm{var}(V_k) = \mathbb{E}(V_k^2)$ for $k\in\mathbb{N}_+$. Hence it follows that

$$
  \mathrm{var}(X_n) = \sum_{k=0}^n \mathrm{var}(V_k) = \mathrm{var}(X_0) + \sum_{k=1}^n \mathbb{E}\left(V_k^2\right)
$$
</details>
</MathBox>


## Martingale difference sequence

A martingale $\{X_n\}_{n\geq 0}$ relative to a filtration $\mathscr{F}_n$ may be decomposed as a partial sum

$$
  X_n = X_0 + \sum_{j=1}^n \xi_j
$$

where $\left\{\xi_n \right\}$ is the martingale difference sequence defined as

$$
  \xi_n = X_n - X_{n-1}
$$

which satisfies for all $n \geq 0$

$$
  E\left( \xi_{n+1} | \mathscr{F}_n \right) = E\left( X_{n+1} - X_n | \mathscr{F}_n \right) = E\left( X_{n+1} | \mathscr{F}_n \right) - E\left( X_n | \mathscr{F}_n \right) = 0
$$

For all $n \geq 0$ we have

$$
\begin{align*}
  \mathrm{E}\left[X_n \right] &= \mathrm{E}\left[ E(X_n | \mathscr{F}_n)  \right] = \mathrm{E}\left[ E\left(X_0 + \sum_{j=1}^n + \xi_j \middle| \mathscr{F}_n \right)  \right] \\
  &= \mathrm{E}\left[ E\left(X_0 | \mathscr{F}_n \right) + \sum_{j=1}^n E\left( \xi_j | \mathscr{F}_n \right) \right] \\
  &= \mathrm{E}\left[X_0 \right]
\end{align*}
$$

If $\mathrm{E}\left[X_n^2 \right] < \infty$ for some $n \geq 1$, then for $j \leq n$ the random variables $\xi_j$ are square-integrable and uncorrelated giving

$$
  \mathrm{E}\left[X_n^2 \right] = \mathrm{E}\left[X_0^2 \right] + \sum_{j=1}^n \mathrm{E}\left[\xi_j^2 \right]
$$

### Martingale transform

Let $\{X_n\}_{n\geq 0}$ be a martingale relative to a filtration $\mathscr{F}_n$. The martingale difference sequence associated with the martingale $X_n$ is defined as

$$
  \xi_n = X_n - X_{n-1}
$$

A predictable sequence $\{Z_n\}_{n\geq 1}$ relative to the filtration $\mathscr{F}_n$ is a sequence of random variables such that for every $n$ the random variable is measurable relative to $\mathscr{F}_{n-1}$. The martingale transform $\left\{ (Z \cdot X)_n \right\}_{n\geq 0}$ is defined by

$$
  E\left( X_n + Y_{n+1} | \mathscr{F}_n \right) = X_0 + \sum_{j=1}^n Z_j \xi_j
$$

Assume that the predictable sequence $\{Z_n\}_{n\geq 1}$ consists of bounded random variables. Then the martingale transform $\left\{ (Z \cdot X)_n \right\}_{n\geq 0}$ is itself a martingale

$$
\begin{align*}
  E\left( (Z\cdot X)_{n+1} | \mathscr{F}_n \right) &= (Z\cdot X)_n + E\left( Z_{n+1} \xi_{n+1} | \mathscr{F}_n \right) \\
  &= (Z\cdot X)_n + Z_{n+1}E\left( \xi_{n+1} | \mathscr{F}_n \right) \\
  &= (Z\cdot X)_n
\end{align*}
$$

Let $(Z_j)_{j\geq 0}$ be a predictable sequence of bounded nonnegative random variables. Then the submartingale transform $\{ Z \cdot X \}_{n\geq 0}$ (respectively, supermartingale) is defined by

$$
  (Z \cdot X)_n = Z_0 X_0 + \sum_{k=1}^n Z_k \xi_k
$$

which is also a submartingale. If $0 \leq Z_n \leq 1$ for each $n \geq 0$, then

$$
  E(Z \cdot X)_n \leq \mathrm{E}[X_n]
$$

To show that $(Z\cdot X)_n$ is a submartingale, if suffices to verify that the differences $Z_j \xi_j$ constitute a submartingale difference sequence

$$
\begin{gather*}
  E\left( Z_j \xi_j | \mathscr{F}_{j-1} \right) = Z_j E\left( \xi_j | \mathscr{F}_{j-1} \right) \\
  \implies E\left( Z_j \xi_j \right) \leq \mathrm{E}\left[ \xi_j \right]
\end{gather*}
$$

### Doob's optional sampling theorem

Doob's theorem states that stopping a martingale at random time $\tau$ does not alter the martingale property, provided the decision about when to stop is solely based on information available up to $\tau$. 

<MathBox title="Doob's optional sampling theorem" boxType='theorem'>
Let $\{X_n\}_{n\geq \mathbb{Z}^+}$ be a martingale relative to a filtration $\mathscr{F}_n$ and let $\tau$ be a stopping time. Then the stopped sequence $\{ X_{\tau \land n} \}_{n\geq 0}$ is a martingale. Consequently, for any $n \in \mathbb{N}$

$$
  \mathrm{E}\left[ X_{\tau \land n} \right] = \mathrm{E}\left[ X_0 \right]
$$
</MathBox>

<details>
<summary>Proof</summary> 

The sequence $\{ X_{\tau \land n} \}_{n\geq 0}$ may be represented as a transform of the sequence $\{X_n\}_{n\geq \mathbb{Z}^+}$

$$
\begin{gather*}
  X_{\tau \land n} = (Z\cdot X)_n \\
  Z_n = \begin{cases} 1, &\quad \tau \geq n \\ 0, &\quad \tau < n \end{cases}
\end{gather*}
$$

The transform is easily verified

$$
\begin{align*}
  (Z\cdot X)_n &= X_0 + \sum_{j=1}^n Z_j \left( X_j - X_{j - 1} \right) \\
  &= X_0 + \sum_{j=1}^{\tau \land n} \left( X_j - X_{j - 1} \right) \\
  &= X_{\tau \land n}
\end{align*}
$$
</details>

<MathBox title='Wald identities' boxType='proposition'>
Let $S_n = \sum_{i=1}^n \xi_i$, then the following identities hold: 
- First identity: if $\mathrm{E}\left[ |\xi_i | \right] < \infty$ and $\mathrm{E}\left[ \tau \right] < \infty$ then $ \mathrm{E}[|S_\tau|] < \infty$ and  
$$
  \mathrm{E}[S_\tau] = \mathrm{E}[\tau] \cdot \mathrm{E}[\xi_k]
$$
- Second identity: if $\mathrm{E}\left[ \xi_i \right] = 0$ and $\sigma^2 = \mathrm{E}\left[ \xi_i^2 \right] < \infty$ then 
$$
  \mathrm{E}[S_\tau^2] = \sigma^2 \mathrm{E}[\tau]
$$
- Third identity: assume that $\mathrm{E}\left[ e^{\theta \xi_1} \right] = e^{-\psi(\theta)} < \infty$, then for every bounded stopping time
$$
  \mathrm{E}\left[ \theta S_\tau - \tau \psi(\theta) \right] = 1
$$
</MathBox>

## Brownian motion