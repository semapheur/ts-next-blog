---
title: 'Stochastic Analysis'
subject: 'Mathematics'
showToc: true
---

# Semimartingales

## Finite variation processes

<MathBox title='Finite variation process' boxType='definition'>
Given a probability space $(\Omega,\mathcal{F},\mathbb{P})$ with filtration $\mathscr{F} = \Set{\mathcal{F}_t}_{t\geq 0}$, a process $A:\Omega\times\R_+ \to\R$ is called a finite variation process if it satisfies the following conditions.
1. $A$ is continuous for any $\omega\in\Omega$.
2. $A_0 = 0$ for any $\omega\in\Omega$
3. For any $\omega\in\Omega$ there is a signed measure $\mu$ such that for any $t\geq 0$
$$
  A_t = \mu([0,t])
$$

Note that the continuity and initial value assumptions implies that $\mu$ has no atoms.

<details>
<summary>Details</summary>

Note that the decomposition $\mu = \mu_+ - \mu_-$ as the difference between two positive measures is not unique. However, it is unique when constrained to

$$
  \mathrm{supp}(\mu_+) \cap\mathrm{\mu_-} = \emptyset 
$$

The uniqueness of such a composition follows the identity $\mu_+ (B) = \sup\Set{\mu(C) | C\subset B, C \text{ is a Borel set}}$. To show existence, write $\mu = \tilde{\mu}_+ - \tilde{\mu}_-$ for some positive measures $\tilde{\mu}_+$ and $\tilde{\mu}_-$. Then $\tilde{\mu}_+$ (respectively $\tilde{\mu}_-$) is absolutely continuous with respect to $\tilde{\mu} = \tilde{\mu}_+ + \tilde{\mu}_-$. By the Radon-Nikodym theorem, $\tilde{\mu}_+$ has a density $\lambda_+ (t)$ (respectively $\lambda_- (t)$) with respect to $\tilde{\mu}$. Then, the choice

$$
\begin{align*}
  \mu_+ (\mathrm{d}t) =& \max(\lambda_+ (t) - \lambda_- (t), 0)\tilde{\mu}(\mathrm{d}t) \\
  \mu_- (\mathrm{d}t) =& \max(\lambda_- (t) - \lambda_+ (t), 0)\tilde{\mu}(\mathrm{d}t)
\end{align*}
$$

gives the expected decomposition. Letting $S_+$ (respectively $S_-$) denote $\mathrm{supp}(\mu_+)$ (respectively $\mathrm{supp}(\mu_-)$) and $|\mu| = \mu_+ + \mu_-$, we get

$$
  \frac{\mathrm{d}\mu}{\mathrm{d}|\mu|} = \mathbf{1}_{S_+} - \mathbf{1}_{S_-}
$$

Moreover, for a finite variation process, $A(t) = m\mu_+ ([0,t]) - m\mu_- ([0,t])$. As $A$ is a continuous, $\mu_+$ and $\mu_-$ have no atoms because they have disjoint supports. Thus, $A$ is the difference of two continuous increasing functions beginning at $0$. This proves that for any $t > 0$

$$
  \sup_{0=t_0 < \cdots < t_n = t} \sum_{k=1}^n |A_{t_k} - A_{t_{k-1}}| < \infty
$$

where the supremum is over all $n\in\N$ and subdivisions of $[0,t]$.
</details>
</MathBox>

<MathBox title='' boxType='theorem'>
Let $A$ be a finite variation process. Then for any $t > 0$

$$
  \sup_{0=t_0 < \cdots < t_n = t} \sum_{k=1}^n |A_{t_k} - A_{t_{k-1}}| = |\mu|([0,t])
$$

where the supremum is over all $n\in\N$ and subdivisions of $[0,t]$.

<details>
<summary>Proof</summary>

The inequality 

$$
  \sup_{0=t_0 < \cdots < t_n = t} \sum_{k=1}^n |A_{t_k} - A_{t_{k-1}}| \leq |\mu| ([0,t])
$$

is obvious because

$$
  |A_{t_k} - A_{t_{k-1}}| = |\mu((t_{t_{k-1}}, t_k])| \leq |\mu|((t_{k-1}, t_k])
$$

Consider any sequence of refined subdivisions of $[0,t]$ with step going to $0$, noted $0 = t_0^{(n)} < \cdots < t_{p_n}^{(n)} = t$, and the filtration $\mathcal{F}_0 \subset\cdots\subset\mathcal{F}_n \subset\cdots\subset \mathcal{B}([0,t])$ defined a subsets of the Borel algebra by

$$
  \mathcal{F}_n = \sigma((t_{k-1}^{(n)}, t_k^{(n)}])_{1\leq k\leq p_n}
$$

This is indeed a filtration because the subdivisions are refined. Take $\Omega = [0,t]$ and the probability measure

$$
  \mathbb{P}(\mathrm{d}s) = \frac{|\mu|(\mathrm{d}s)}{|\mu|([0,t])}
$$

on $\Omega$. On the probability space $(\Omega,\mathcal{B}([0,t]), \mathbb{P})$ with filtration $\mathscr{F} = \Set{\mathcal{F}_n}_{n\in\N}$, consider the random variables

$$
  X(s) = \mathbf{1}_{S_+} (s) - \mathbf{1}_{S_-} (s)
$$

and when $s\in (t_{k-1}^{(n)}, t_k^{(n)}]$

$$
\begin{align*}
  X_n =& \mathbb{E}(X|\mathcal{F}_n)(s) \\
  =& \frac{\mu((t_{k-1}^{(n)}, t_k^{(n)}])}{|\mu|((t_{k-1}^{(n)}, t_k^{(n)}])} \\
  =& \frac{A_{t_k^{(n)}} - A_{t_{k-1}^{(n)}}}{|\mu|((t_{k-1}^{(n)}, t_k^{(n)}])}
\end{align*}
$$

As $\Set{X_n}_{n\in\N}$ is a bounded martingale, it converges almost surely and in $\mathcal{L}^1$ to some $Y\in\mathcal{L}^1$ and $X_n = \mathbb{E}(Y|\mathcal{F}_n)$ As a consequence, $\mathbb{E}(X - Y | \mathcal{F}_n) = 0$ for any $n$. Since $X$ and $Y$ are in $\bigwedge_n \mathcal{F_n}$ (concerning $X$, this is a consequence of the time step going to $0$), this implies $X = Y$ almost surely. Thus, $X_n \to X$ in $\mathcal{L}^1$, so in particular $\mathbb{E}(|X_n|)\to\mathbb{E}(|X|)$, which means that

$$
  \sum_{k=1}^{p_n} |A_{t_k^{(n)}} - A_{t_{k-1}^{(n)}}| \xrightarrow{n\to\infty} |\mu|([0,t])
$$
</details>
</MathBox>

If $A$ is a finite variation process and $F:[0,t]\to\R$ is a process, measurable for any given $\omega$, such that $\int_0^t |F(s)|\cdots|\mu|(\mathrm{d}s)$ is finite, then we define

$$
\begin{align*}
  \int_0^t F(s)\;\mathrm{d}A_s =& \int_0^t F(s)\mu(\mathrm{d}s) \\
  \int_0^t F(s)\;|\mathrm{d}A_s| =& \int_0^t F(s)|\mu|(\mathrm{d}s)
\end{align*}
$$

<MathBox title='' boxType='proposition'>
Let $A$ be a finite variation process and $F:\Omega\times[0,t]\to\R$ a left-continuous process. Then for any $\omega\in\Omega$

$$
\begin{align*}
  \int_0^t F(s)\;\mathrm{d}A_s =& \lim_{n\to\infty} \sum_{k=1}^{p_n} F\left(t_{k-1}^{(n)}\right) \left(A_{t_k^{(n)}} - A_{t_{k-1}^{(n)}}\right) \\
  \int_0^t F(s)\;|\mathrm{d}A_s| =& \lim_{n\to\infty} \sum_{k=1}^{p_n} F\left(t_{k-1}^{(n)}\right) \left|A_{t_k^{(n)}} - A_{t_{k-1}^{(n)}}\right| 
\end{align*}
$$

for sequence of subdivisions of $[0,t]$, of the form $0 = t_0^{(n)} < \cdots < t_{p_n}^{(n)} = t$, with step going to $0$. For the second identity we require the subdivisions to be refined.

<details>
<summary>Proof</summary>

Let $F_n$ be the process defined as $F\left( t_{k-1}^{(n)} \right)$ on $(t_{k-1}^{(n)}, t_k^{(n)}]$. Then, the right hand side of the first identity is $\int_0^t F_n (s) \mu(\mathrm{d}s)$, so the result follows by dominated convergence. For the second identity, we have

$$
\begin{align*}
  &\left| \sum_{k=1}^{p_n} F\left( t_{k-1}^{(n)} \right) \left| A_{t_k^{(n)}} - A_{t_{k-1}^{(n)}} \right| - \int_0^t F_n(s)\;\mathrm{d}A_s \right| \\
  \leq& \lVert F \rVert_{\mathcal{L}^\infty [0,1]} \left( |\mu|([0,t]) - \sum_{k=1}^{p_n} \left| A_{t_k^{(n)}} - A_{t_{k-1}^{(n)}} \right| \right)
\end{align*}
$$

From the proof of the previous theorem, this converges to $0$ along any refined sequence of subdivisions with step going to $0$, hence proving that

$$
  \int_0^t F_n(s)\;|\mathrm{d}A_s| \xrightarrow{n\to\infty} \int_0^t F(s)\;|\mathrm{d}A_s|
$$

is sufficient, and true by dominated convergence.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Given a probability space $(\Omega,\mathcal{F},\mathbb{P})$ with filtration $\mathscr{F} = \Set{\mathcal{F}_t}_{t\geq 0}$, let $A$ be a finite variation process and $F$ progressively measurable such that $\int_0^t |F_s|\cdot|\mathrm{d}A_s| < \infty$ for any $\omega\in\Omega$ and $t\geq 0$. Then

$$
  F\cdot A: (\omega, t)\mapsto \int_0^t F_s(\omega)\;\mathrm{d}A_s(\omega)
$$

is a finite variation process.

<details>
<summary>Proof</summary>

Let $\mu$ be the signed measure associated to $A$. The process $F\cdot A$ begins at $0$, is continuous and has bounded variation because

$$
  (F\cdot A)_t = \tilde{\mu}([0,t]), \tilde{\mu}(\mathrm{d}s) = H_s \mu(\mathrm{d}s)
$$

with $\tilde{\mu}$ a signed measure with no atoms (the finite mass condition holds since $\int_0^t |F_s|\cdot|\mathrm{d}A_s| < \infty$). Consequently, the only condition to verify carefully is the adaptedness of $F\cdot A$. It is true that $(F\cdot A)_t$ is $\mathcal{F}_t$-measurable if $F$ is ot type $\mathbf{1}_{(u,v]} (s) \mathbf{1}_{A} (\omega)$, where $u,v\leq t$ and $A\in\mathcal{F}_t$. It is then true if $F = \mathbf{1}_A$ for any $A\in\mathcal{B}([0,t])\otimes\mathcal{F}_t$ by the monotone class theorem. Finally, by taking linear combinations of such sums approximating $F$ from below, and using dominated convergence (domination by an integrable process holds since $\int_0^t |F_s|\cdot|\mathrm{d}A_s| < \infty$), we get the result for general $F$, because the pointwise limit of measurable functions is measurable.
</details>
</MathBox>

## Local martingales

<MathBox title='Quadratic variation of a martingale' boxType='theorem'>
A process $\Set{M_t}_{t\geq 0}$ is a local martingale beginning at $0$ if it is adapted and satisfies
1. $M_0 = 0$ for all $\omega$
2. $M$ is continuous for all $\omega$
3. There exists a sequence of stopping times $T_n$ converging to $\infty$ for any $\omega$ such that $M^{T_n} := \Set{ M_{t\wedge T_n}}_{t\geq 0}$, is a uniformly integrable martingale.

For such a sequence of stopping times, $\Set{T_n}_{n\in\N}$ reduces $M$. A process $M$ is a local martingale if $M_t = M_0 + N_t$, where $M_0\in\mathcal{F}_0$ and $N$ is a local martingale beginning at $0$.
</MathBox>

<MathBox title='Quadratic variation of a martingale' boxType='theorem'>
For a given probability space $(\Omega,\mathcal{F},\mathbb{P})$ with filter $\mathscr{F} = \Set{\mathcal{F}_t}_{t\geq 0}$, all the following statements hold.
1. Any continuous martingale is a local martingale.
2. There exists a sequence of stopping times $T_n$ converging to $\infty$ for any $\omega$ such that $M^{T_n} := \Set{M_{t\wedge T_n}}_{t\geq 0}$ for all $n\in\N$ is a martingale.
3. If $M$ is a local martingale and $T$ is a stopping time, then $M^T = \Set{M_{t\wedge T}}_{t\geq 0}$ is a local martingale.
4. If $M$ is a local martingale, $\Set{T_n}_{n\in\N}$ reduces $M$, and $\Set{S_n}_{n\in\N}$ are stopping times converging to $\infty$, then $\Set{S_n \wedge T_n}_{n\in\N}$ reduces $M$.
5. The set of local martingales is a vector space.
6. If $M$ is a nonnegative local martingale and $M_0 \in \mathcal{L}^1$, then $M$ is a supermartingale.
7. If $M$ is a local martingale and $|M_t| \leq X$ for all $t\geq 0$, where $X\in\mathcal{L}^1$, then $M$ is a martingale.
8. If $M$ is a local martingale beginning at $0$, then $T_n = \inf\{t\geq 0 : |M_t| = n\}$ reduces $M$.

<details>
<summary>Proof</summary>

Part **(1)** follows from the possible case $T_n = n$. Then for any constant $c \geq 0$, $\Set{M_{t\wedge c}}_{t\geq 0}$ is a uniformly integrable, as all of its values are of type $\mathbb{E}(M_c | \mathcal{G})$ for some $\sigma$-algebra $\mathcal{G}$ and $M_c \in\mathcal{L}^1$.

For part **(2)** note that if $T_n \to\infty$ and $M^{T_n}$ is a martingale, then $M^{T_n \wedge n}$ is uniformly integrable, as shown for **(1)**., and $T_n \wedge n \to\finty$.

For part **(3)** and **(4)** note that if $M^{T_n}$ is a uniformly integrable martingale, so is $M^{T_n \wedge T}$. The stability by addition mentioned in **(5)** is a dircet consequence of **(4)**, by choosing $\Set{T_n}_{n\in\N}$ reducing the first martingale and $\Set{S_n}_{n\in\N}$ reducing the second. Point **(6)** is a consequence of Fatou's lemma: if $M = M_0 + N$ and $\Set{T_n}_{n\in\N}$ reduces $N$, then

$$
\begin{align*}
  \mathbb{E}(M_t|\mathcal{F}_s) =& \mathbb{E}\left(\lim_{n\to\infty} M_{t\wedge T_n} \right) \\
  \leq& \liminf{n\to\infty} \mathbb{E}(M_{t\wedge T_n | \mathcal{F}_s}) \\
  =& \liminf_{n\to\infty} M_{s\wedge T_n} = M_s
\end{align*}
$$

Note that $M_t$ is in $\mathcal{L}^1$ precisely thanks to the above equation. The result **(7)** relies on dominated convergence applied to the indetity

$$
  M_{s\wedge T_n} = \mathbb{E}(M_{t\wedge T_n} | \mathcal{F}_s)
$$

where $\Set{T_n}_{n\in\N}$ reduces $M$. Finally, **(8)** is a direct consequence of **(2)** and **(7)**.
</details>
</MathBox>

<MathBox title='Indistinguishability of local martingales' boxType='theorem'>
Let $M$ be a local martingale beginning at 0$. If $M$ is a finite variation process, then $M$ is indistinguishable from $0$.

<details>
<summary>Proof</summary>

Assume that $M$ is a finite variation process, and choose

$$
  T_n = \inf\{ t\geq 0 | \int_0^t |\mathrm{d}M_s| \geq n \}
$$

Then $T_n \to\infty$ and $T_n$ is a stopping time. The local martingale $M^{T_n}$ is bounded by $n$, so it is a martingale by properties of local martingales. As a consequence, for any subdivision $0 = t_0 < \cdots < t_p = t$

$$
\begin{align*}
  \mathbb{E}[(M_t^{T_n})^2] =& \sum_{k=1}^n \mathbb{E}[(M_{t_k}^{T_n})^2 - (M_{t_{k-1}}^{T_n})^2] \\
  =& \sum_{k=1}^p \mathbb{E}[(M_{t_k}^{T_n} - M_{t_{k-1}}^{T_n})^2] \\
  \leq& \mathbb{E}\left( \max{\ell}\left|M_{t_\ell}^{T_n} - M_{t_{\ell - 1}}^{T_n} \right| \sum_{k=1}^ns |M_{t_k}^{T_n} - M_{t_{k-1}}^{T_}| \right) \\
  \leq& n\mathbb{E}\left( \max_{\ell} |M_{t_\ell}^{T_n} - M_{t_{\ell - 1}}^{T_n} | \right)
\end{align*}
$$

As this maximum is bounded by $n$ and $M$ has continuous trajectories, dominated convergence allows to conclude that $\mathbb{E}[(M_t^{T_n})^2] = 0$, by choosing subdivisions with time step going to $0$. By Fatou's lemma, one can take the $n\to\infty$ limit to conclude $\mathbb{E}(M_t^2) = 0$, so $M_t = 0$ almost surely. As $M$ is continuous, this is equivalent to being indistinguishable from $0$.
</details>
</MathBox>

<MathBox title='Quadratic variation of a martingale' boxType='theorem'>
Let $M$ be a local martingale. Then there exists a unique (up to indistinguishability) increasing continuous variation process, noted $\langle M, M \rangle$, such that $\Set{M_t^2 - \langle M, M \rangle_t}_{t\geq 0}$ is a local martingale. Moreover, if $\Set{ 0 = t_0^{(n)} < t_1^{(n)} < \cdots}_{n\in\N_+}$ is any sequence of subdivisions of $\R_+$, with step going to $0$, then

$$
  \langle M, M \rangle_t = \lim_{n\to\infty} \sum_{k\geq 1} \left( M_{t_k^{(n)} \wedge t} - M_{t_{k-1}^{(n)}\wedge t} \right)^2
$$

uniformly in the sense of convergence in probability. The process $\langle M, M \rangle$, often noted $\langle M \rangle$, is called the *bracket* or *quadratic variation* of $M$.

<details>
<summary>Proof</summary>

Uniqueness of quadratic variation is an easy consequence of indistinguishability. We will first prove the existence of the bracket when $M$ is a true martingale, and $|M|$ is almost surely bounded by some $K > 0$. For a subdivision $\delta = \Set{0 = t_0 < t_1 < \dots}$ and a process $Y$, we note

$$
  Q_t^{(Y,\delta)} = \sum_{k\in\N_+} \left( Y_{t_k^{(n)} \wedge t} - Y_{t_{k-1}^{(n)}\wedge t} \right)^2 
$$

Furthermore,

$$
\begin{align*}
  X_t^{(\delta)} :=& M_t^2 - Q_t^{(M,\delta)} \\
  =& M_t^2 - \sum_{k\in\N_+} \left( M_{t_k^{(n)} \wedge t} - M_{t_{k-1}^{(n)}\wedge t} \right)^2 \\
  =& 2\sum_{k\in\N_+} M_{t_{k-1}^{(n)}} \left( M_{t_k^{(n)}} - M_{t_{k-1}^{(n)} \wedge t} \right)
\end{align*}
$$

Thus, $\Set{X_t^{(\delta)}}_{t\geq 0}$ is a continuous martingale. For a sequence $\Set{\delta_n}_{n\in\N}$ of subdivisions with step going to 0, we want to find a subsequence of $\Set{X^{(\delta_n)}}_{n\geq 0}$ converging uniformly on compact sets. Note that

$$
  \delta_t^{(n,m)} =& X_t^{(\delta_n)} - \delta_t^{(\delta_m)} = Q_t^{(M,\delta_m)} - Q_t^{(M,\delta_n)}
$$

which is a martingale, so

$$
  \Set{(\delta_t^{(n,m)})^2 - Q_t^{(\delta^{(n,m)},\delta_n \cup \delta_m)}}_{t\geq 0}
$$

is a martingale as well, by the same decomposition used to prove that $X^{(\delta)}$ is a martingale. As a consequence, the expectation of $(\delta_t^{(n,m)})^2$ is also a discrete analogue of the quadratic variation of a finite variation process. We therefore expect this to go to $0$, which would prove that the sequence of \Set{X_t^{(\delta_n)}}_{n\in\N} is a Cauchy sequence in $\mathcal{L}^2$, hence converging. Note that, $(a-b)^2 \leq 2(a^2 + b^2)$, then

$$
  Q_t^{A-B,\delta} \leq 2\left( Q_t^{A,\delta} - Q_t^{B,\delta} \right)
$$

so in order to prove that $\mathbb{E}[(\delta_t^{(n,m)})^2]$ converges to $0$, a sufficient condition is

$$
  \mathbb{E}(Q_t^{(Q^{(M,\delta_n)}, \delta_n \cup \delta_m)}) \xrightarrow{n,m\to\infty} 0
$$

Note that $\varepsilon_n = \sup_{u,v\in[0,t]} |M_u - M_v|$ is the supremum such that $u - v$ is smaller than the time step of $\delta_n$. Then if $s_{k-1}$ and $s_k$ are successive elements of $\delta_n \cup \delta_m$, then $|Q_{s_k}^{(M,\delta_n)} - Q_{s_{k-1}}^{(M,\delta_n)}| \leq \varepsilon_n |M_{s_k} - M_{s_{k-1}}|$, hence

$$
  Q_t^{(Q^{(M,\delta)}, \delta_n \cup\delta_m)} \leq \varepsilon_n^2 \sum_{k\in\N_+} (M_{s_k \wedge t} - M_{s_{k-1} \wedge t})^2
$$

Note that for any subdivision $\delta$

$$
\begin{align*}
  (Q_t^{(M,\delta)})^2 =& \sum_{k\in\N_+} (M_{s_k \wedge t} - M_{s_{k-1}\wedge t})^4 \\
  &+ \sum_{k\in\N} (Q_{s_k\wedge t}^{(M,\delta)} - Q_{s_{k-1}\wedge t}^{M,\delta})(Q_t^{(M,\delta)} - Q_{s_k \wedge t}^{(M,\delta)})
\end{align*}
$$

As $X^(\delta)$ is a martingale, $\mathbb{E}(Q_t^{(M,\delta)} - Q_{s_{k\wedge t}}^{M,\delta} | \mathcal{F}_{s_k \wedge t} ) = \mathbb{E}(M_t^2 - M_{s_k \wedge t}^2 | \mathcal{F}_{s_k \wedge t})$, so using $|M| \leq K$, we get

$$
\begin{align*}
  \mathbb{E}[(Q_t^{(M,\delta)})^2] \leq& 4K^2 \left( \mathbb{E}(Q_t^{(M,\delta)}) + \sum_{k\in\N_+} (Q_{s_k \wedge t}^{(M,\delta)}) - Q_{s_{k-1}\wedge t}^{(M,\delta)} \right) \\
  =& 8K^2 \mathbb{E}(Q_t^{(M,\delta)}) = 8K^2 \mathbb{M_t^2} \leq 8K^4
\end{align*}
$$

As the quadratic increments is uniformly bounded in $\mathcal{L}^2$ by $8K^4$, we get by the Cauchy-Schwarz inequality

$$
  \mathbb{E}(Q_t^{(Q^{(M,\delta_n)},\delta_n \cup\delta_m)}) \leq (8K^4 \mathbb{E}(\varepsilon_n^4))^{1/2}
$$

By dominated convergence, where $\varepsilon_n \to 0$ almost surely and $\varepsilon_n \leq 2K$, this goes to $0$ in the limit $n\to\infty$. It follows that $\Delta_t^{(m,n)} \xrightarrow{n,n\to\infty} 0$ in $\mathcal{L}^2$. By Doob's inequality, this implies that

$$
  \mathbb{E}\left[ \left( \sup_{[0,t]} (X^{(\delta_n)} - X^{(\delta_m)}) \right)\right] \xrightarrow{n,m\to\infty} 0
$$

so there is a subsequence of the $X^{(\delta_n)}$ converging almost surely, uniformly, on $[0,t]$. Let $X$ denote this (continuous) limit. As the subsequence of the $X^{(\delta)}$ converge to $X$ in $\mathcal{L}^2$, their martingale property is preserved in the limit, hence $X$ is a martingale. Moreover, from the definition of $Q_t^{(M,\delta)}$ it follows that $M^2 - X^{(\delta)}$ is an increasing process. This property holds for $M^2 - X$ by uniform convergence. For $s\in[0,t]$ define

$$
  \langle M \rangle_s := M_s^2 - X_s
$$

From the previous discussion, $\langle M \rangle$ satisfies all required properties of the bracket on $[0,t]$. By uniqueness of the bracket, the value $\langle M \rangle_s$ is independent of the choice of the horizon $t\geq s$, and of the choice of the subsequence providing uniform convergence. Moreover, the above reasoning has proved that $X_s^{(\delta_n)} - X_s^{(\delta_m)}$ is Cauchy sequence in $\mathcal{L}^2$, as $\varepsilon_n$ can be chose indentical for any choice of $s\in[0,\t]$, the convergence is in $L^2$ and uninform on compact sets.

This bounded martingale case extends easily. First, note that if the result is true for local martingales beginning at $0$, it is true for local martingales. If $M_t = M_0 + N_n$ with $M_0 \in\mathcal{F}_0$ and $N$ is a local martingale beginning at $0$, as $M_0 N$ is a local martingale, so is $M_t^2 - \langle N \rangle_t = N_t^2 - \langle N \rangle_t + M_0^2 + 2M_0 N_t$. Thus, we can assume that $M_0 = 0$.

We localize $M$ by $T_n = \inf\{ t \geq 0 : |M_t| = n \}$. Then $M^{T_n}$ is a local martingale, bounded by $n$, so we can apply the previous argument: there is an increasing process, noted $\langle M \rangle^{(n)}$ usch that $(M^{T_n})^2 - \langle M \rangle^{(n)}$ is a martingale. By uniqueness, (\langle M \rangle^{(n)})^{T_m} = \langle M \rangle^{(m)} for $m \leq n$. From this coherence property, we can define a process $\langle M \rangle$ such that $(M^{T_n})^2 - \langle M \rangle^{T_n}$ is a martingale. As $T_n \to\infty$ almost surely, this means that $M^2 - \langle M \rangle$ is a local martingale.

The property of uniform convergence of quadratic increments to the bracket also holds for $(M^{T_n})^2 - \langle M \rangle^{T_n}$ in $\mathcal{L}^2$. Since $\mathbb{P}(T_n \leq t) \xrightarrow{n\to\infty} 0$ by dominated or monotone convergence, it follows that the uniform converge holds in probability.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $M$ and $N$ be two local martingales. Then

$$
  \langle M, N \rangle = \frac{1}{2} (\langle M + N, M + N \rangle - \langle M, M \rangle - \langle N, N \rangle)
$$

satisfying the following.
1. Up to indistinguishability, $\langle M, N \rangle$ is the unique finite variation process such that $MN - \langle M,N\rangle$ is a local martingale.
2. The function $(M, N) \mapsto \langle M, N \rangle$ is symmetric and bilinear.
3. If $(0 = t_0^{(n)} < t_1^{(n)} < \dots)_{n\in\N}$ is any sequence of subdivisions of $\R_+$ with step going to $0$, then
$$
  \langle M, N \rangle_t = \lim_{n\to\infty} \sum_{k\in\N_+} (M_{t_k^{(n)}\wedge t} - M_{t_{k-1}^{(n)} \wedge t}) (N_{t_k^{(n)}\wedge t} - N_{t_{k-1}^{(n)} \wedge t})
$$
in probability, uniformly for $t$ in compact sets.
4. For any stopping time $T$, then $\langle M, N \rangle_{t\wedge T} = \langle M^T, N \rangle_t = \langle M^T ,N^T \rangle_t$ for $t\geq 0$.
</MathBox>

<MathBox title='Kumita-Watanabe inequality' boxType='theorem'>
Let $H$ and $K$ be progressively measurable processes and suppose that $M$ and $N$ are local martingales. Then, for any $t\in\R_+ \cup\Set{\infty}$

$$
  \int_0^t |H_s K_s|\cdots|\mathrm{d}\langle M, N \rangle_s | \leq \left( \int_0^t H_s^2 \;\mathrm{d}\langle M \rangle_s \right)^{1/2} \left( \int_0^t K_s^2 \;\mathrm{d}\langle N \rangle_s \right)^{1/2}
$$

<details>
<summary>Proof</summary>

Note that from properties of quadratic variation of two martingales and the Cauchy-Schwarz inequality we have

$$
  |\langle M, N \rangle_t - \langle M, N \rangle_s | \leq (\langle M \rangle_t - \langle M \rangle_s)^{1/2} (\langle N \rangle_t - \langle N \rangle_s )^{1/2}
$$

almost surely. Using Cauchy-Schwarz again, for $s = t_0 < \cdots < t_n = t$, the above inequality yields

$$
\begin{align*}
  &\sum_{k=1}^n |\langle M, N \rangle_{t_k} - \langle M, N \rangle_{t_{k-1}} | \\
  \leq& \sum_{k=1}^n (\langle M \rangle_{t_k} - \langle M \rangle_{t_{k-1}})^{1/2} (\langle N \rangle_{t_k} - \langle N \rangle_{t_{k-1}})^{1/2} \\
  \leq& \left( sum_{k=1}^n (\langle M \rangle_{t_k} - \langle M \rangle_{t_{k-1}})\right)^{1/2} \left((\langle N \rangle_{t_k} - \langle N \rangle_{t_{k-1}})\right)^{1/2} \\
  =& (\langle M \rangle_t - \langle M \rangle_s )^{1/2} (\langle N \rangle_t - \langle N \rangle_s)^{1/2}
\end{align*}
$$

By the finite variation supremum theorem, we get

$$
  \int_s^t |\mathrm{d}\langle M, N \rangle_u | \leq \left( \int_s^t \mathrm{d}\langle M \rangle_u \right)^{1/2} \left( \int_s^t \mathrm{d}\langle N \rangle_u \right)^{1/2}
$$

For functions of type $H = \sum h_\ell \mathbf{1}_{B_\ell}$ and $K = \sum k_\ell \mathbf{1}_{B_\ell}$, with disjoint bounded Borel sets $B_i$, we have

$$
\begin{align*}
  \int |H_s K_s |\cdot |\mathrm{d}\langle M, N \rangle_s | =& \sum_\ell |h_\ell k_\ell | \int_{B_\ell} |\mathrm{d}\langle M, N \rangle_u | \\
  \leq& \sum_\ell |h_\ell k_\ell| \left( \int_{B_\ell} \mathrm{d}\langle M \rangle_u \right)^{1/2} \left( \int_{B_\ell} \mathrm{d}\langle N \rangle_u \right)^{1/2} \\
  \leq& \left( \sum_\ell h_\ell^2 \int_{B_\ell} \mathrm{d}\langle M \rangle_u \right)^{1/2} \left( \sum_\ell k_\ell^2 \int_{B_\ell} \mathrm{d}\langle N \rangle_u \right)^{1/2} \\
  =& \left( \int_0^t H_s^2 \mathrm{d}\langle M \rangle_s \right)^{1/2} \left( \int_0^t K_s^2 \mathrm{d}\langle N \rangle_s \right)^{1/2}
\end{align*}
$$

Approximation of progressively measurable processes as increasing limit of such functions completes the proof.
</details>
</MathBox>

<MathBox title='Integrability of local martingales' boxType='theorem'>
Let $M$ be a local martingale with $M_0 = 0$.
1. The process $M$ is an $\mathcal{L}^2$-bounded martingale, i.e. $\sup_{t\geq 0} \mathbb{E}(|M_t|^2) < \infty$, if and only if $\mathbb{E}(\langle M \rangle_\infty) < \infty$. In such a case, $M^2 - \langle M \rangle$ is a uniformly integrable martingale.
2. The process $M$ is a square integrable martingale if and only if $\mathbb{E}(\langle M \rangle_t ) < \infty$ for $t\geq 0$. In such a case, $M^2 - \langle M \rangle$ is a martingale.

<details>
<summary>Proof</summary>

**(1):** Assume first that $M$ is an $\mathbb{L}^2$-bounded martingale. It is therefore uniformly integrable and converges almost surely to some $M_\infty$. Moreover, from Doob's inequality

$$
  \mathbb{E}\left(\sup_{t\geq 0} M_t^2 \right) \leq 4 \sup_{t\geq 0} \mathbb{E}(M_t^2) < \infty
$$

As a consequence, if we define $T_n = \inf\Set{t\geq 0 | \langle M \rangle_t \geq n}$ then

$$
  \Set{M_{t\wedge T_n}^2 - \langle M \rangle_{t\wedge T_n}}_{t\geq 0}
$$

is a local martingale bounded by $\sup_{t\geq 0} (M_t^2) + n\in \mathcal{L}^1$, so it is a true martingale. Thus

$$
  \mathbb{E}(\langle M \rangle_{t\wedge T_n}) = \mathbb{E}(M_{t\wedge T_n}^2)
$$

Dominated convergence allows to take the limit $t\to\infty$ on the right hand side, and monotone convergence on the left side,

$$
  \mathbb{E}(\langle M \rangle_{T_n}) = \mathbb{E}(M_{T_n}^2)
$$

Monotone convergence on the left and dominated convergence on the right yields

$$
  \mathbb{E}(\langle M \rangle_\infty) = \mathbb{E}(M_\infty^2)
$$

In particular, $\mathbb{E}(\langle M \rangle_\infty)$ if finite. This implies that $M^2 - \langle M \rangle$ is bounded by an integrable random variable $\mathrm{sup}_{t\geq 0} (M_t^2) + \langle M \rangle_\infty $, so it is a uniformly integrable martingale.

Conversely, suppose that $\mathbb{E}(\langle M \rangle_\infty) < \infty$ and note that $\tilde{T}_n = \inf\Set{t\geq 0 : |M_t| \geq n}$. Then $M^{\tilde{T}_n}$ (bounded by $n$) and $(M^{\tilde{T}_n})^2 - \langle M \rangle^{\tilde{T}}$ (bounded by $n^2 + \langle M \rangle^{T_n} \in \mathcal{L}^1$) are uniformly integrable martingales. Thus, for any stopping time $S$

$$
  \mathbb{E}(M_{S\wedge\tilde{T}_n}^2) = \mathbb{E}(\langle M \rangle_{S\wedge\tilde{T}_n})
$$

By Fatou's lemma

$$
  \mathbb{E}(M_S^2) \leq \mathbb{E}(\langle M \rangle_S) \leq \mathbb{E}(\langle M \rangle_\infty) < \infty
$$

In particular, $M$ is $\mathcal{L}^2$-bounded. Moreover, it is a martingale and therefore in the identity

$$
  \mathbb{E}(M_{t\wedge\tilde{T}_n} | \mathcal{F}_s) = M_{s\wedge\tilde{T}_n}
$$

the limit $n\to\infty$ is allowed by dominated convergence. Indeed, the $M_{t\wedge\tilde{T}_n}$ are bounded in $\mathcal{L}^2 with (\mathbb{E}(\sup_t M_t^2)) < \infty$, so $\mathbb{E}(\sup_n M_{t\wedge\tilde{T}_n}^2) < \infty$ and hence bounded in $\mathcal{L}^1$.

**(2):** By Doob's inequality, if $M$ is square integrable, $\Set{M_{s\wedge t}}_{c\get 0}$ is $\mathcal{L}^2$-bounded, so by **(1)**, $\mathbb{E}(\langle M \rangle_t) < \infty$. Reciprocally, if $\mathbb{E}(\langle M \rangle_t) < \infty$, then **(1)** implies that $\Set{M_{s\wedge t}}_{c\get 0}$ is bounded in $\mathcal{L}^2$, in particular $\mathbb{E}(M_t^2) < \infty$. Finally, in such a case, from **(1)** the process $\Set{M_{s\wedge t}^2 - \langle M \rangle_{s\wedge t}}_{s\geq 0}$ is a uniformly integrable martingale, so $M^2 - \langle M \rangle$ is a martingale. 
</details>
</MathBox>

<MathBox title='' boxType='corollary'>
Let $M$ be a local martingale, with $M_0 = 0$ almost surely. Then $M$ is indistinguishable from $0$ if and only if $\langle M \rangle$ is identically $0$.

<details>
<summary>Proof</summary>

If $M$ is indistinguishable from $0$, it is clear that the bracket vanishes (as a limit of quadratic increments). Reciprocally, if $\langle M \rangle\equiv 0$, then by integrability $M^2$ is a martingale so $\mathbb{E}(M_t^2) = 0$ for any given $t$. Thus, $M_t = 0$ almost surely. One can conclude by continuity that $M$ is indistinguishable from $0$.
</details>
</MathBox>

<MathBox title='Semimartingale process' boxType='definition'>
Given a probability space $(\Omega, \mathcal{F}, \mathbb{P})$ with filtration $\mathscr{F} = \Set{\mathcal{F}_t}_{t\geq 0}$, a process $X$ is called a semimartingale if is of type

$$
  X = X_0 + M + A
$$

where $X_0 \in\mathcal{F}_0$, $M$ is a locale martingale beginning at $0$, and $A$ is a finite variation process.
</MathBox>

Note that the decomposition of a semimartingale process is unique up to indistinguishability. As a consequence, we can define without ambiguity the bracket of $X = X_0 + M + A$ and $\tilde{X} = \tilde{X}_0 + \tilde{M} + \tilde{A}$ as

$$
  \langle X, \tilde{X} \rangle = \langle M, \tilde{M} \langle
$$

In particular, the bracket of a finite variation process with any semimartingale is always $0$. Then we can show that the bracket is still given as a limit of increment (the finite variation part does not contribute). Thus, if $(0 < t_0^{(n)} < t_1^{(n)} \dots)_{n\in\N}$ is any sequence of subdivisions of $\R_+$ with step going to $0$, then in the sense of convergence in probability

$$
  \langle X, \tilde{X} \rangle_ t = \lim_{t\to\infty} \sum_{k\in\N_+} \left( X_{t_k^{(n)} \wedge t} - X_{t_{k-1}^{(n)} \wedge t} \right)\left( \tilde{X}_{t_k^{(n)} \wedge t} - \tilde{X}_{t_{k-1}^{(n)} \wedge t} \right)
$$

<MathBox title='' boxType='theorem'>
If, given a probability space, the process $Y$ is continuous with independent increments, then it takes the from

$$
  Y = X + F
$$

where $X$ is a semimartingale with independent increments and $F$ is a deterministic continuous function.
</MathBox>

# Brownian motion

# Stochastic integrals

<MathBox title='' boxType='definition'>
Let $H$ be a progressively measurable process and let $M$ be a local martingale. We denote $H^2$ the set of continuous $L^2$-bounded martingales, i.e. $M\in H^2$ if $\mathrm{sup}_{t\geq 0} \mathbb{E}((M_t)^2) < \infty$.
</MathBox>

<MathBox title='' boxType='proposition'>
The set $H^2$ is a Hilbert space with inner product

$$
  \langle M, N \rangle_{H^2} := \mathbb{E}(\langle M, N \rangle_\infty),\, M,N\in H^2
$$

<details>
<summary>Proof</summary>

Note that, if $M\in H^2$, it converges almost surely with a finite bracket, $\mathbb{E}(\langle M \rangle_\infty) < \infty$. If $M, N\in H^2$, then by Kunita-Watanabe inequality

$$
  \mathbb{E}(|\langle M, N \rangle_\infty|) \leq \mathbb{E}(\langle M \rangle_\infty)^{1/2} \mathbb{E}(\langle N \rangle_\infty)^{1/2}
$$

This means that we can define a scalar product on $H^2$ by

$$
  (M, N)_{H^2} = \mathbb{E}(\langle M, N \rangle_\infty)
$$

and $\lVert M \rVert_{H^2} = (\langle M \rangle_\infty)^{1/2}$ defines a norm. We have already seen that if $\lVert M \rVert_{H^2} = 0$, then $M$ is indistinguishable from $0$.

It remains to show that $H^2$ is complete. Consider a Cauchy sequence $(M^{(n)})_{n\in\N}$:

$$
\begin{align*}
  &\lim_{m,n\to\infty} \mathbb{E}\left((M_\infty^{(n)} - M_\infty^{(m)})^2 \right) \\ 
  =& \lim_{m,n\to\infty} \mathbb{E}\left(\langle M^{(n)} - M^{(m)}\rangle_\infty \right) = 0
\end{align*}
$$

By the Doob inequality we have

$$
  \lim_{m,n\to\infty} \mathbb{E}\left( \sup_{t\geq 0} |M_t^{(n)} - M_t^{(m)}|^2 \right) = 0
$$

so we can find an increasing sequence $(n_k)_{k\in\N}$ such that

$$
\begin{align*}
  &\mathbb{E}\left( \sum_{k=1}^infty \sup_{t\geq 0} |M_t^{(n_k)} - M_t^{(n_{k-1})}| \right) \\
  \leq& \sum_{k=1}^infty \mathbb{E}\left( \sup_{t\geq 0} |M_t^{(n_k)} - M_t^{(n_{k-1})}|^2 \right)^{1/2} < \infty
\end{align*}
$$

As a consequence, $\sum_{k=1}^infty \sup_{t\geq 0} |M_t^{(n_k)} - M_t^{(n_{k-1})}|$ is almost surely isFinite, so $M^{(n_k)}$ converges uniformly to some continuous adapted process $M$. Here we have used the fact that the pointwise limit of measurable functions is measurable. For any given $s$ and $t$, $M_t^{(n_k)}$, respectively $M_s^{(n_k)}$, converges in $L^2$ to $M_t$, respectively $M_s$, in the martingale property

$$
  \mathbb{E}(M_t^{(n_k)}|\mathcal{F}_s) = M_s^{(n_k)}
$$

we can take the limits to conclude that $M$ is a martingale. Moreover, as the $M^{(n_k)}$ satisty the Doob inequality above, all $M_t^{(n_k)}$ are uniformly bounded in $L^2$ and $M\in H^2$. finally, $M^{(n_k)}$ converges to $M$ in $H^2$, because

$$
  \mathbb{E}\left(\langle M^{(n_k)} - M \rangle_\infty \right) = \mathbb{E}\left((M_\infty^{(n_k)} - M_\infty)^2 \right) \to \infty
$$

This implies, by the Cauchy condition, that $M^{(n)}$ converges to $M$ in $H^2$ as well.
</details>
</MathBox>

<MathBox title='' boxType='definition'>
For $M\in H^2$, let $L^2 (M)$ be the space of progressively measurable processes $H$ such that

$$
  \mathbb{E}\left(\int_0^\infty H_s^2 \mathrm{d}\langle M \rangle_s \right) < \infty
$$

<details>
<summary>Details</summary>

It is easily seen that $L^2 (M) = L^2 (\R_+ \times\Omega, \mathcal{F}, \nu)$, where $\mathcal{F}$ is the progressive $\sigma$-algebra and $\nu(A) = \mathbb{E}(\int_0^\infty \mathbf{1}_A (s,\cdot)\;\mathrm{d}\langle M \rangle_s$ is a well-defined finite measure. Note that $L^2 (M)$ is a Hilbert space with the inner product

$$
  \langle H, K \rangle_{L^2 (M)} = \mathbb{E}\left( \int_0^\infty H_s K_s \;\mathrm{d}\langle M \rangle_s \right)
$$

in the sense that $\lVert H \rVert_{L^2(M)} = 0$ if and only if $\nu$-almost surely $H = 0$.
</details>
</MathBox>

<MathBox title='' boxType='definition'>
The vector subspace of $L^2 (M)$ consisting of step processes is noted $\mathcal{E}$. Specifically, $H\in\mathcal{E}$ is there some $p\geq 1$ and $0 = t_0 < \cdots < t_p$ such that

$$
  H_s (\omega) = \sum_{k=0}^{p-1} H_k (\omega) \mathbb{1}_{(t_k, t_{k+1}]} (s)
$$

where $H_k \in\mathcal{F}_{t_k}$ is bounded.
</MathBox>

<MathBox title='' boxType='proposition'>
For any $M\in H^2$, the subspace $\mathcal{E}$ is dense in $L^2 (M)$.

<details>
<summary>Proof</summary>

Recall that $L^2 (M) = L^2 (\R_+ \times\Omega, \mathcal{F}, \nu)$.

We need to prove that if $K\in L^2(M)$ is orthogonal to $\mathcal{E}$, then $K = 0$. If $K$ is orthogonal to $F\mathbf{1}_{(s,t]}\in\mathcal{E}$, where $F\in\mathcal{F}_s$ is bounded, then

$$
  \mathbb{E}\left( F \int_s^t K_u \;\mathrm{d}\langle M \rangle_u \right) = 0
$$

Let $X_t = \int_0^t K_u \;\mathrm{d}\langle M \rangle_u$. Then $X_t \in L^2$, as a consequence of the Cauchy-Schwarz inequality and $M\in H^2$, $K\in L^2 (M)$. It follows that $\mathbb{E}((X_t - X_s)F) = 0$ for any bounded $F\in\mathcal{F}_s$, so $X_s = \mathbb{E}(X_t | \mathcal{F}_s)$. Thus, $X$ is a martingale. Since this is also a finite variation process, it is indistinguishable from $0$. Hence, for any $t\geq 0$

$$
  \int_0^t K_u \;\mathrm{d}\langle M \rangle_u = 0
$$

so $K = 0$ $\nu$-almost everywhere on $\R_+ \times\Omega$.
</details>
</MathBox>

<MathBox title='' boxType='theorem'>
Let $M\in H^2$. For $H\in\mathcal{E}$, written

$$
  H_s (\omega) = \sum_{k=0}^{p-1} H_k (\omega) \mathbf{1}_{(t_k, t_{k+1}]} (s)
$$

we associate $H\cdot M\in H^2$ defined by

$$
  (H\cdot M)_t = \sum_{k=1}^p H_k (M_{t_{k+1}\wedge t} - M_{t_k \wedge t})
$$

Then the following results hold.
1. The map $H\mapsto H\cdot M$ can be uniquely extended into an isometry from $L^2 (M)$ to $H^2$.
2. The process $H\cdot M$ obtained by the previous extension is characterized by 
$$
  \langle H\cdot M, N \rangle = H\cdot\langle M, N \rangle,\; N\in H^2
$$
3. If $\tau$ is a stopping time, then
$$
  (\mathbf{1}_{(0,\tau]})\cdot M = (H\cdot M)^\tau = H\cdot M^\tau
$$
4. If $G\in L^2 (M)$ and $H\in L^2 (G\cdot M)$, then $GH\in L^2(M)$ and
$$
  (GH)\cdot M = G\cdot (H\cdot M)
$$

<details>
<summary>Proof</summary>

**(1):** We first check that the map $H \mapsto H\cdot M$ is an isometry $\mathcal{E}\to H^2$. It is easy to check that $H\cdot M$ is an $L^2$-bounded martingale. Since

$$
  \langle H\cdot M \rangle_t = \sum_{k=1}^{p-1} H_k^2 (\langle M \rangle_{t_{k+1}\wedge t} - \langle M \rangle_{t_k \wedge t})
$$

we get

$$
\begin{align*}
  \lVert H\cdot M \rVert_{H^2}^2 =& \mathbb{E}\left( \sum_{k=1}^{p-1} H_k^2 (\langle M \rangle_{t_{k+1}} - \langle M \rangle_{t_k}) \right) \\
  =& \mathbb{E}\left( \int_0^\infty H_s^2 \;\mathrm{d}\langle M \rangle_s \right) = \lVert H \rVert_{L^2 (M)}^2
\end{align*}
$$

Since $\mathbb{E}$ is dense in the Hilbert space $L^2(M)$, this isometry can be extended in a unique way as an isometry $(L^2(M), \lVert\cdot\rVert_{L^2 (M)} \cong (H^2, \lVert\cdot\rVert_{H^2})$.

**(2):** For $H\in\mathcal{E}$ we get

$$
\begin{align*}
  \langle H\cdot M, N\rangle_t =& \left\langle \sum_{k=1}^p H_k (M_{t_{k+1} \wedge\cdot} -  M_{t_k \wedge\cdot}), N \right\rangle_t \\
  =& \sum_{k=1}^p H_k \langle M_{t_{k+1}\wedge\cdot} - M_{t_k \wedge\cdot}, N\rangle_t \\
  =& \sum_{k=1}^p H_k (\langle M, N \rangle_{t_{k+1 \wedge t}} - \langle M, N \rangle_{t_k \wedge t}) \\
  =& \int_0^t H_s \;\mathrm{d}\langle M, N \rangle_s
  =& H\cdot\langle M, N \rangle_t
\end{align*}
$$

To prove this for general $H\in L^2(M)$, consider a sequence $H^{(n)}$ in $\mathcal{E}$ converging to $H$ in $L^2 (M)$. Then, by the isometry property, $H^{(n)}\cdot M$ converges to $H\cdot M$ in $H^2$

$$
\begin{align*}
  \langle H\cdot M, N \rangle_\infty =& \lim_{n\to\infty} \langle H^{(n)} \cdot M, N \rangle_\infty \\
  =& \lim_{n\to\infty} (H^{(n)}\cdot\langle M, N \rangle)_\infty \\
  =& (H\cdot\langle M, N \rangle)_\infty
\end{align*}
$$

The first equality is in the sense of an $L^1$-limit, and a consequence of the Kunita-Watanabe inequality with $X = H^{(n)}\cdot M - H\cdot M$

$$
\begin{align*}
  \mathbb{E}(|\langle X, N \rangle_\infty|) \leq& \mathbb{E}(\langle X \rangle_\infty)^{1/2} \mathbb{E}(\langle N \rangle_\infty) \\
  =& \lVert X \rVert_{H^2} \mathbb{E}(\langle N \rangle_\infty)^{1/2}
\end{align*}
$$

This also proves the second equality. The third equality holds as a limit in $L^1$ and relies on Kunita-Watanabe as well

$$
\begin{align*}
  &\mathbb{E}\left| \int_0^\infty (H_s^{(n)} - H_s)\;\mathrm{d}\langle M, N \rangle_s \right| \\
  =& \mathbb{E}(\langle N \rangle_\infty) \lVert H^{(n)} - H \rVert_{L^2 (M)}
\end{align*}
$$

We have proven that $\mathbb{E}(|\langle H\cdot M, N \rangle_\infty - (H\cdot\langle M, N\rangle)_\infty |) = 0$ which yields that almost surely

$$
  \langle H\cdot M, N\rangle_\infty = (H\cdot\langle M, N\rangle)_\infty
$$

Choosing $N$ stopped at time $t$ in this identity proves the expected result.

To get the unique characterization of $H\cdot M$ by the identity in **(2)**, note that if $X$ satisfies the same property as $H\cdot M$, then for any $N\in H^2$

$$
  \langle H\cdot M - X, N \rangle_\infty = 0
$$

The choice $N = H\cdot M - X$ proves that $H\cdot M$ and $X$ are indistinguishable.

**(3):** First we prove $(\mathbf{1}_{(0,\tau]} H )\cdot M = (H\cdot M)^\tau$. For this, we just need to check that for any $N\in H^2$

$$
  \langle(\mathbf{1}_{(0,\tau]}H)\cdot M, N\rangle_\infty = \langle (H\cdot M)^\tau, N \rangle_\infty
$$

The left hand side is also $((\mathbf{1}_{(0,\tau]}H)\cdot \langle M, N \rangle)_\infty = (H\cdot\langle M, N\rangle)_\tau$, and the right hand side is $\langle H\cdot M, N \rangle_\tau$. Since $\langle H\cdot M, N\rangle = H\cdot\langle M, N \rangle$, this achieves the proof. The proof for $(H\cdot M)^\tau = H\cdot M^\tau$ proceeds in the same way.

**(4):** The first assertion follows from definitions. Because $\langle H\cdot M\rangle = H\cdot\langle M, H\cdot M\rangle = H^2 \langle M \rangle$, we have

$$
  \int_0^\infty (GH)^2 \;\mathrm{d}\langle M \rangle_s = \int_0^\infty H^2 \;\mathrm{d}\langle G\cdot M\rangle_s < \infty
$$

Since $H\in L^2 (G\cdot M)$, it follows that $GH\in L^2 (M)$. To prove the second assertion, we just need to test it with respect to any $N\in H^2$, using the associativity for integral of Stieltjes type

$$
\begin{align*}
  \langle (GH)\cdot M, N \rangle =& (GH)\cdot \langle M, N \rangle \\
  =& G\cdot (H\cdot\langle M, N \rangle) \\
  =& G\cdot\langle H\cdot M, N \rangle \\
  =& \langle G\cdot (H\cdot M), N\rangle
\end{align*}
$$

Hence, $(GH)\cdot M = G\cdot (H\cdot M)$.
</details>
</MathBox>

Using the notation $(H\cdot M)_t = \int_0^t H_s \;\mathrm{d}M_s$, the associativity property takes the form

$$
  \int_0^t G_s (H_s\;\mathrm{d}M_s) = \int_0^t (G_s H_s)\;\mathrm{d}M_s
$$

When iterated, the 2nd property of the previos theorem yields the joint bracket of two stochastic integrals. Because $\langle \int_0 H_s\;\mathrm{d}M_s, N \rangle = \int_0^t H_s\;\mathrm{d}\langle M, N\rangle_s$, we get for $H\in L^2 (M)$ and $G\in L^2 (N)$

$$
  \left\langle \int_0 H_s \;\mathrm{d}M_s, \int_0 G_s \;\mathrm{d}M_s \right\rangle = \int_0^t (G_s H_s)\;\mathrm{d}\langle M, N\rangle_s
$$

In particular, we the first two moments of stochastic integrals become

$$
\begin{align*}
  \mathbb{E}\left( \int_0^t H_s\;\mathrm{d}M_s \right) =& 0 \\
  \mathbb{E}\left[\left( \int_0^t H_s\;\mathrm{d}M_s \right) \left( \int_0^t G_s \;\mathrm{d}N_s \right) \right] = \mathbb{E}\left( \int_0^t (G_s H_s)\;\mathrm{d}\langle M, N\rangle_s \right)
\end{align*}
$$

<MathBox title='' boxType='theorem'>
Let $M$ be a local martingale beginning at $0$ and define

$$
  L^2_{\mathrm{loc}} (M) := \Set{\text{progressively measurable }H | \int_0^t H_s^2 \;\mathrm{d}\langle M \rangle_s \stackrel{\text{a.s.}}{<} \infty,\; t\geq 0}
$$

For any $H\in L^2_{\mathrm{loc}} (M)$ there is a unique local martingale beginning at $0$, noted $H\cdot M$, such that for any local martingale $N$

$$
  \langle H\cdot M, N\rangle = H\cdot\langle M, N\rangle
$$

For any stopping time $\tau$

$$
  (\mathbf{1}_{(0,\tau]} H) \cdot M = (H\cdot M)^\tau = H\cdot M^\tau
$$

<details>
<summary>Proof</summary>

Note that

$$
  \tau_n = ìnf\Set{t \geq 0 | \int_0^t (1 + H_s^2)\;\mathrm{d}\langle M \rangle_s = n}
$$

Then $M^{\tau_n} \in H^2$ and $H\in L^2 (M^{\tau_n})$, so we can consider the stochastic integral $H\cdot M^{\tau_n}$ from the previous theorem. From the 3rd property, $(H\cdot M^{\tau_m})^{\tau_n} = H\cdot M^{\tau_n}$ for $m< n$. This coherence relation proves that there is a process, noted $H\cdot M$, such that $(H\cdot M)^{\tau_n} = H\cdot M^{\tau_n}$ for any $n\in\N$. Because $H\in L^2_{\mathrm{loc}}(M)$, then $\tau_n \xrightarrow{n\to\infty} \infty$. Hence, since $(H\cdot M)^{\tau_n}$ is a martingale, $H\cdot M$ is a local martingale.


To prove $\langle H\cdot M, N\rangle = H\cdot\langle M, N\rangle$, take $N$ as a local martingale and $\tilde{\tau}_n = \inf\Set{t\geq 0 : |N_t| = n}$. Then if $\sigma_n = \tilde{\tau}_n \wedge \tau_n$, we get $N^{\sigma_n} \in H^2$, allowing us to write

$$
\begin{align*}
  \langle H\cdot M, N \rangle^{\sigma_n} =& \langle (H\cdot M)^{\sigma_n}, N^{\sigma_n} \rangle \\
  =& \langle (H\cdot M^{\tau_n})^{\sigma_n}, N^{\sigma_n}\rangle \\
  =& \langle H\cdot M^{\tau_n} N^{\sigma_n} \\
  =& H\cdot\langle M^{\tau_n}, N^{\sigma_n}\rangle \\
  =& H\cdot\langle M, N\rangle^{\sigma_n} \\
  =& (H\cdot\langle M, N\rangle)^{\sigma_n}
\end{align*}
$$

We get the desired result because $\sigma_n \xrightarrow{n\to\infty} \infty$ almost surely.
</details>
</MathBox>

To extend the notion of stochastic integral to semimartingales, we need to work with locally bounded processes. A progressively measurable process $H$ is locally bounded if almost surely $\sup_{[0,t]}|H_s| < \infty$ for any $t\geq 0$. Note that is $H$ is locally bounded and $M$ is a local martingale, then $H\in L^2_{\mathrm{loc}} (M)$. Moreover, if $A$ is a finite variation process, then for any $t\geq 0$ almost surely $\int_0^t |H_s|\;|\mathrm{d}A_s| < \infty$. As a consequence, for a semimartingale $X = X_0 + M + A$, and a locally bounded progressively measurable process $H$, the definition $H\cdot X = H\cdot M + H\cdot A$ makes sense.

<MathBox title='Properties of stochastic integrals' boxType='proposition'>
The stochastic integral of a locally bounded progressively measurable process with respect to a semimartingale satisfies the following
1. The map $(H,X)\mapsto H\cdot X$ is bilinear.
2. If $G$ and $H$ are locally bounded, $G\cdot (H\cdot X) = (GH)\cdot X$
3. If $\tau$ is a stopping time, $(\mathbf{1}_{(0,\tau]} H)\cdot X = (H\cdot X)^\tau = H\cdot X^\tau$
4. If $X$ is a locale martingale, so is $H\cdot X$
5. If $X$ is a finite variation process, so is $H\cdot X$
6. If $H$ is a step process, $H_s = \sum_{k=0}^{p-1} H_k \mathbf{1}_{(t_k, t_{k+1}]} (s)$, then
$$
  (H\cdot X)_t = \sum_{k=0}^{p-1} H_k (X_{t_{k+1} \wedge t} - X_{t_k \wedge t})
$$
7. If $H$ is assumed to be left-continuous, then in the sense of convergence in probability
$$
  \int_0^t H_s\;\mathrm{d}X_s = \lim_{n\to\infty} \sum_{k=0}^{p_n - 1} H_{t_k^{(n)}} (X_{t_{k+1}^{(n)}} - X_{t_k^{(n)}})
$$
where the sequence of subdivisions $0 = t_0^{(n)} < \cdots < t_{p_n}^{(n)} = t$ has a step going to $0$.

<details>
<summary>Proof</summary>

Properties **(1)-(6)** are easy consequences of previous analogue statements concerning local martingales and finite variation processes.

**(7):** Suppose that $X$ is a local martingale and let $H^{(n)}$ be the process equal to $H_{t_k^{(n)}}$ on $(t_k^{(n)}, t_{k+1}^{(n)}]$ and $0$ otherwise, and

$$
  \tau_m = \inf\Set{t\geq 0 : |H_s| + \langle M\rangle_s \geq m}
$$

Then $M^{\tau_m}$ and $H^{(n)}$ are bounded on $[0,\tau_m]$, so the definition and properties of the stochastic integral in the $H^2 / L^2(M^{\tau_m})$ case allow to use the isometry property

$$
\begin{align*}
  &\mathbb{E}\left(\left[(H^n \mathbf{1}_{[0,\tau_m]} \cdot M^{\tau_m})_t - (H\mathbf{1}_{[0,\tau_m]} \cdot M)_t \right]^2 \right) \\
  =& \mathbb{E}\left( \int_0^{t\wedge \tau_m} (H_s^n - H_s)^2 \;\mathrm{d}\langle M \rangle_s \right)
\end{align*}
$$

Since $H$ is left-continuous, this last term converges to $0$ as $n\to\infty$ by dominated convergence. Hence $(H^n \cdot M)_t^{\tau_m}$ converges in $L^2$ to $(H\cdot M)_t^{\tau_m}$. Because $\mathbb{P}(\tau_m > t) \xrightarrow{m\to\infty} 1$, we conclude that $(H^n \cdot M)_t$ converges in probability to $(H\cdot M)_t$.
</details>
</MathBox>

## Itô's formula

<MathBox title="Itô's formula" boxType='theorem'>
Let $F:\R^d \to\R$ be second differentiable, i.e. $F\in C^2$, and let $X^1,\dots,X^d$ be (continuous) semimartingales. Then $F(X^1,\dots,X^d)$ is a semimartingale and

$$
\begin{align*}
  F(X_t^1,\dots,X_t^d) =& F(X_0^1,\dots,X_0^d) \\
  &+ \sum_{k=1}^d \int_0^t \partial_{x_k} F(X_s^1,\dots,X_s^d)\;\mathrm{d}X_s^k \\
  &+ \frac{1}{2}\sum_{1\leq k,\ell, d} \int_0^t \partial_{x_k x_\ell} F(X_s^1,\dots,X_s^d)\;\mathrm{d}\langle X^k, X^\ell \rangle_s
\end{align*}
$$

or in differential form

$$
\begin{align*}
  \mathrm{d}F(X_t^1,\dots,X_t^d) =& \sum_{k=1}^d \partial_{k} F(X_s^1,\dots,X_s^d)\;\mathrm{d}X_s^k \\
  &+ \frac{1}{2}\sum_{1\leq k,\ell, d} \partial_{k \ell} F(X_s^1,\dots,X_s^d)\;\mathrm{d}\langle X^k, X^\ell \rangle_s
\end{align*}
$$

<details>
<summary>Proof</summary>

We first prove this formula for $f(x,y) = xy$, which is equivalent to proving a stochastic integration by parts formula. For this purpose, by polarization, we just need to prove it for $f(x) = x^2$

$$
  X_t^2 = X_0^2 + 2 \int_0^t X_t \;\mathrm{d}X_t + \langle X \rangle_t
$$

From properties of stochastic integrals, we know that

$$
  2\int_0^t X_t \;\mathrm{d}X = \lim_{\to\infty} 2\sum_{k=0}^{p_n - 1} X_{t_k^{(n)}} (X_{t_{k+1}^{(n)}} - X_{t_k^{(n)}})
$$

where the sequence of subdivisions $0 = t_0^{(n)} < \cdots < t_{p_n}^{(n)} = t$ has a step going to $0$, and the limit is in probability. Writing this as

$$
  \lim_{n\to\infty} \left(\sum_{k=0}^{p_n - 1} (X_{t_{k+1}^{(n)}}^2 - X_{t_k^{(n)}}^2) - \sum_{k=0}^{p_n - 1} (X_{t_{k+1}^{(n)}} - X_{t_k^{(n)}})^2 \right)
$$

and using proves Itô's formula in the quadratic case, by uniqueneses of the limit in probability. In other words, we have proved that formula holds for $F$ being constant, linear of type $x_i x_j$. It follows that if the result holds for $F(x)$, a simple calculation proves that it is true for $F(x)x_i$. Hence, iterating this reasoning, the result holds for any polynomial in the variables $x_1,\dots,x_d$ of arbitrary degree.

By localizing our processes $X_1,\dots,X_d$ through their first exist time from the ball with radius $n$, and taking $n\to\infty$ at the end, we can assume that the processes remain in a compact $K$. Since $F\in C^2 (K)$ on, the Stone-Weierstrass theorem gives a sequence of polynomials $(P_n)_{n\in\N}$ in the variables $x_1,\dots,x_n$ such that $P_n$, its first and second order derivatives converge uniformly to those of $F$ on $K$. We can easily show by dominated convergence that if $H$ and the $H_n$ are continuous uniformly bounded progressively measurable processes, $H_n$ converging almost surely to $H$ on $[0,t]$, then for any semimartingale $X$

$$
  \lim_{n\to\infty} \int_0^t H_s^n \;\mathrm{d}X_s = \int_0^t H_s \;\mathrm{d}X_s
$$

in the sense of convergence in probability. Thus, the result is true in the almost sure sense if $X$ is a finite variation process by the usual dominated convergence. If $X$ is $L^2$-bounded martingale, the convergence holds in $L^2$ by the isometry property. If $X$ is a local martingale, the convergence holds in probability by localization. Thus, writing the Itô formula for the polynomials $P_n$ and taking the limit $n\to\infty$ yields the result for $F$, by uniqueness of the limit in probability.

Finally, with Itô's formula proved, it is obvious that $F(X_1,\dots,X_d)$ is a semimartingale as the formula gives its explicit decomposition. Because $F$ is $C^2$, all integrated terms are locally bounded, so the stochastic and Stieltjes integrals make sense.
</details>
</MathBox>

<MathBox title='' boxType='corollary'>
Let $M$ be a real local martingale and $\lambda\in\mathbb{C}$. Then the process $\left( \exp(\lambda M_t - \frac{\lambda^2}{2}\langle M \rangle_t) \right)_{t\geq 0}$ is a local martingale.

<details>
<summary>Proof</summary>

Let $F:\R^2 \to\R$ be a $C^2$. By the Itô formula for a semimartingale $M$

$$
  \mathrm{d}F(M_t, \langle M \rangle_t) = (\partial_x F)\;\mathrm{d}M_t + (\partial_y F)\;\mathrm{d}\langle M \rangle_t + \frac{1}{2}(\partial_{xx} F)\;\mathrm{d}\langle M \rangle_t 
$$

If $M$ is a local martingale and $(\partial_y + \frac{1}{2}\partial_{xx})F = 0$, then $F(M,\langle M \rangle)$ is a stochastic integral with respect to $M$, hence a local martingale. In our case, both the real and imaginary parts of $f(x,y) = \exp(\lambda x - \frac{\lambda^2}{2}y)$ satisfy the Itô formula, which yields the result.
</details>
</MathBox>

<MathBox title="Lévy's criterion" boxType='theorem'>
Let $M^{(1)},\dots,M^{(d)}$ be local martingales beginning at $0$. Then the following assertions are equivalent.
1. The processes $M^{(1)},\dots,M^{(d)}$ are independent standard Brownian motions.
2. $\langle M^{(k)}, M^{(\ell)}\rangle_t = \mathbf{1}_{k=\ell}t$ for any $1\leq k,\ell \leq d$ and $t\geq 0$.

<details>
<summary>Proof</summary>

**(1)** $\implies$ **(2)**: This follows directly because $(M_t^{(k)}^2)_{t\geq 0}$ is martingale, as well as $(M_t^{(k)}, M_t^{(\ell)})_{t\geq 0}$ when $k \neq\ell$.

**(2)** $\implies$ **(1)**: Write $M = (M^{(i)})_{i=1}^d$ and consider some $u\in\R^d$. Then $u\cdot M$ is a local martingale with bracket $\langle u\cdot M \rangle_t = |u|^2 t$. From the previous corollary, it follows that $(\exp(iu\cdot M_t - \frac{1}{2}|u^2|t))_{t\geq 0}$ is a local martingale. Because it is bounded, it is a martingale:

$$
  \mathbb{E}[\exp(iu\cdot M_t - \frac{1}{2}|u^2|t)|\mathcal{F}_s] = \exp(iu\cdot M_s - \frac{1}{2}|u^2|t)
$$

It follows that for any $A\in\mathcal{F}_s$

$$
  \mathbb{E}\left( \exp(iu\cdot (M_t - M_s)) \mathbf{1}_A \right) = \exp(-\frac{1}{2}|u|^2 (t - s))\mathbb{P}(A)
$$

The choice $A = \Omega$ proves that $M_t - M_s$ is a Gaussian vector with covariance matrix $(t - s)\mathrm{id}_d$, hence with independent coordinates. Moreover, the above equation also proves that $M_t - M_s$ is independent of $\mathcal{F}_s$.
</details>
</MathBox>

<MathBox title='Dubins-Schwarz theorem' boxType='theorem'>
Let $M$ be a local martingale such that $\langle M \rangle_\infty = \infty$ almost surely, and write $T_t = \inf\Set{s\geq 0 | \langle M \rangle_s > t}$. For any given $t\geq 0$, the random variable $T_t$ is an almost surely finite stopping time, and the process $B_t = M_{T_t}$ is a Brownian motion with respect to the filtration $(\mathcal{G}_t)_{t\geq 0} = (\mathcal{F}_{T_t})_{t\geq 0}$ and $M_t = B_{\langle M \rangle_t}$.

<details>
<summary>Proof</summary>

As all expected results are in the almost sure sense, we can suppose that for any $\omega$, the process $M$ begins at $0$, is continuous, and $\langle M \rangle_\infty = \infty$. The process $(T_t)_{t\geq 0}$ is nondecreasing, right-continuous, and finite. Moreover $\langle M \rangle_t = \inf\Set{s\geq 0 | T_s > t}$.

Note that the processes $M$ and $\langle M \rangle$ have the same constance intervals. If $M$ is constant on $[S,T]$ so is $\langle M \rangle$ thanks to the characterization of the bracket as a limiting sum squares of increments. Conversely, if $\langle M \rangle$ is constant on $[S,T]$, since a local martingale with null bracket is indistinguishable from $0$, then the result follows by considering a proper shift of $M$ after a stopping time.

To prove that $B$ is continuous, not that this is obvious where $t$ is continuous, and if $T$ is not continuous at $t$, this follows from the above coincidence of constance intervals. By Lévy's criterion, we therefore just need to prove that $B$ and $B_t^2 - t$ are local martingales with respect to the filtration $(\mathcal{G}_t)_{t\geq 0}$.

Let $X$ denote $M$ or $(M_t^2 - t)_{t\geq 0}$. Let $S_n = \inf\Set{t\geq 0 : |X_t| \geq n}$. Then $\tilde{S} = \langle M \rangle_{S_n}$ is a $(\mathcal{G}_t)_{\t geq 0}$-stopping time. As $X^{S_n}$ is a bounded $(\mathcal{F}_t)_{t\geq 0}$ martingale, the stopping time theorem yields $X_{T_t}^{S_n} = \mathbb{E}(X_\infty^{S_n} | \mathcal{F}_{T_t}). This means that $X_{\tau_t \wedge \tilde{S}_n}$ is a $(\mathcal{G}_t)_{t\geq 0}$-martingale, and since $\langle M \rangle_{S_n}\xrightarrow{n\to\infty}\infty$ we get the expected result.
</details>
</MathBox>

The Dubins-Schwarz theorem implies that any local martingale $M$ share many common properties with a Brownian motion.
- In the interior of nonconstant intervals, $M$ is nowhere differentiable with a Hölderian index $1/2$.
- If the bracket has a strictly positive right increasing rate at $t$, then $M$ satisfies an iterated logarithmic law.
- Up to a null set, $\Set{\omega | M \text{ converges in }\R} = \Set{\omega | \langle M \rangle_\infty < \infty}$.
- Up to a null set, $\Set{\omega | \limsup{M} = \infty, \liminf{M} = -\infty} = \Set{\omega | \langle M \rangle_\infty = \infty}$

<MathBox title='Multidimensional Dubins-Schwarz theorem' boxType='theorem'>
Let $M^{(1)},\dots,M^{(d)}$ be continuous local martingales beginning at $0$ such taht $\langle M^{(k)} \rangle_t \xrightarrow{t\to\infty}\infty$ for any $1 \leq k \leq d$. If $\langle M^{(k)}, M^{(\ell)} \rangle = 0$ for any $k\neq\ell$, then there exists $B^{(1)},\dots,B^{(d)}$ independent standard Brownian motions such that $M_t^{(k)} = B_{\langle M \rangle_t}^{(k)}$ for any $1 \leq k \leq d$.
</MathBox>

## Transcience, recurrence, harmonicity

<MathBox title='' boxType='theorem'>
A Brownian motion $B$ of dimension $d\geq 2$ has polar points. Thus, for any $x \neq B_0$

$$
  \mathbb{P}(\exists t \geq 0 | B_t = x) = 0
$$

<details>
<summary>Proof</summary>

By projection on a subspace of dimension $2$, it suffices to prove the result for $d = 0$. Additionally, by scaling and rotation-invariance of Brownian motion, we can consider that $x = (-1, 0)$, and that the bi-dimensional Brownian motion $(X,Y)$ begins at $(0,0)$. Let $M_t = e^{X_t} \cos(Y_t)$ and $N_t = e^{X_t} \sin(Y_t)$. Applying Itô's formula yields

$$
\begin{align*}
  \mathrm{d}M_t =& M_t \mathrm{d}X_t - N_t \mathrm{d}Y_t \\
  \mathrm{d}N_t =& N_t \mathrm{d}X_y + M_t \mathrm{d}Y_t
\end{align*}
$$

showing that $M$ and $N$ are local martingales. Moreover, $\langle M, N \rangle = 0$ and

$$
  \langle N \rangle_t = \langle M \rangle_t = \int_0^t e^{2X_t} \;\mathrm{d}t
$$

The recurrence of the Brownian motion $X$ easily implies that these brackets go to $\infty$ almost surely. By the Debins-Schwarz theorem we conclude that there are two Brownian motions $B^1$ and $B^2$ such that

$$
\begin{align*}
  M_t - 1 =& B_{\langle M \rangle_t}^1 \\
  N_t =& B_{\langle M \rangle_t}^2
\end{align*}
$$

Since $\langle M \rangle$ is continuous with almost sure range $[0,\infty)$, then by writing $B = (B^1, B^2)$ it follows that

$$
 \mathbb{P}[\exists t \geq 0 | B_t = (-1,0)] = \mathbb{P}[\exists t \geq 0 | (M_t, N_t) = (0,0)]
$$

Because $|(M_t, N_t)| = e^{X_t}$ and almost surely $X_t$ is finite for any $t\geq 0$, this last event has probability $0$.
</details>
</MathBox>

<MathBox title='' boxType='theorem'>
Let $B$ be a Brownian motion in dimension $d = 2$, and $O\subset\R^2$ be open. Then

$$
  \mathbb{P}(\exist t \geq 0 | B_t \in O) = 1
$$

<details>
<summary>Proof</summary>

Assuming $B_0 = a \neq 0$, we want to prove that for any $r > 0$ almost surely $B$ is in $\mathcal{B}(r)$ for some $t$.

Define $X_t = |B_t|^2$. We will first show that the process $\ln(X_t)$ is a local martingale. Note that logarithm diverges at $0$, albeit as a polar point. Writing $S_n = \inf\Set{t\geq 0 | X_t < \frac{1}{n}}$, Itô's formula gives

$$
  \ln(X_{t\wedge S_n}) = \ln(X_0) + 2\int_0^{t\wedge S_n} \frac{B_s^1\;\mathrm{d}B_s^1 + B_s^2\;\mathrm{d}B_s^2}{X_s}
$$

By the previous theorem, $S_n \xrightarrow{n\to\infty} \infty$, so the formula above holds when replacing $t\wedge S_n$. For a given function $f\in C^2(\R^2)$, Itô's formula gives

$$
  \mathrm{d}f(B_t^1, B_t^2) = (\partial_1 f)\;\mathrm{d}B_t^2 + (\partial_2 f)\;\mathrm{d}B_t^2 + \frac{1}{2}(\Delta)\;\mathrm{d}t
$$

For $f(x,y) = \ln(x^2 + y^2)$ we get $\Delta f = 0$, so $\ln(X_t)$ is a local martingale. 

Let $0 < r |a| < R$ and $T_x = \inf(t\geq 0 : |B_t| = x}$. The stopping time theorem applied to $\log(X_{t\wedge T_R \wedge T_r})$ (is a bounded, hence uniformly integral martingale) yields

$$
  \mathbb{E}(\ln(X_{T_R \wedge T_r})) = \ln|a|
$$

Because the one-dimensional Brownian motion is recurrent, $T_R < \infty$ almost surely, so $\mathbb{P}(T_r < T_R) + \mathbb{P}(T_R < T_r) = 1$ and

$$
  \ln(r)\mathbb{P}(T_r < T_R) + \ln(R)\mathbb{P}(T_R < T_r) = \ln|a|
$$

This implies that

$$
  \mathbb{P}(T_r < T_R) = \frac{\ln(R) - \ln|a|}{\ln(R) - \ln(r)}
$$

so when $R\to\infty$ we get by monotone convergence $\mathbb{P}(T_r < \infty) = 1$.

</details>
</MathBox>

Note that the previous theorem can be strenghtened to prove that there are arbitrary large $t$ such that $B_t \in O$. Indeed, for any $n\geq 0$

$$
  \mathbb{P}(\exists t \geq n | B_t \in 0) = \mathbb{E}(\mathbb{E}(\mathbf{1}_{\exists t \geq n | B_t \in O} | \mathcal{F}_n))
$$

and $\mathbb{E}(\mathbf{1}_{\exists t \geq n | B_t \in O} | \mathcal{F}_n)$ is constantly 1 since, from the Markov property, this is also $\mathbb{P}(\exists t\geq 0 | \tilde{B}_t \in O_n)$ where $O_n = O + B_n$ and $\tilde{B}$ is a Brownian motion independent of $B_n$.

<MathBox title='' boxType='theorem'>
Let $B$ be a Brownian motion of dimension $d = 3$. If $K\subset\R^d$ is compact, simply connected and $B_0 \neq K$, then

$$
  \mathbb{P}(\exists t\geq 0 | B_t \in K) < 1
$$

<details>
<summary>Proof</summary>

</details>
</MathBox>