---
title: 'Stochastic Analysis'
subject: 'Mathematics'
showToc: true
---

# Semimartingales

## Finite variation processes

<MathBox title='Finite variation process' boxType='definition'>
Given a probability space $(\Omega,\mathcal{F},\mathbb{P})$ with filtration $\mathscr{F} = \Set{\mathcal{F}_t}_{t\geq 0}$, a process $A:\Omega\times\R_+ \to\R$ is called a finite variation process if it satisfies the following conditions.
1. $A$ is continuous for any $\omega\in\Omega$.
2. $A_0 = 0$ for any $\omega\in\Omega$
3. For any $\omega\in\Omega$ there is a signed measure $\mu$ such that for any $t\geq 0$
$$
  A_t = \mu([0,t])
$$

Note that the continuity and initial value assumptions implies that $\mu$ has no atoms.

<details>
<summary>Details</summary>

Note that the decomposition $\mu = \mu_+ - \mu_-$ as the difference between two positive measures is not unique. However, it is unique when constrained to

$$
  \mathrm{supp}(\mu_+) \cap\mathrm{\mu_-} = \emptyset 
$$

The uniqueness of such a composition follows the identity $\mu_+ (B) = \sup\Set{\mu(C) | C\subset B, C \text{ is a Borel set}}$. To show existence, write $\mu = \tilde{\mu}_+ - \tilde{\mu}_-$ for some positive measures $\tilde{\mu}_+$ and $\tilde{\mu}_-$. Then $\tilde{\mu}_+$ (respectively $\tilde{\mu}_-$) is absolutely continuous with respect to $\tilde{\mu} = \tilde{\mu}_+ + \tilde{\mu}_-$. By the Radon-Nikodym theorem, $\tilde{\mu}_+$ has a density $\lambda_+ (t)$ (respectively $\lambda_- (t)$) with respect to $\tilde{\mu}$. Then, the choice

$$
\begin{align*}
  \mu_+ (\mathrm{d}t) =& \max(\lambda_+ (t) - \lambda_- (t), 0)\tilde{\mu}(\mathrm{d}t) \\
  \mu_- (\mathrm{d}t) =& \max(\lambda_- (t) - \lambda_+ (t), 0)\tilde{\mu}(\mathrm{d}t)
\end{align*}
$$

gives the expected decomposition. Letting $S_+$ (respectively $S_-$) denote $\mathrm{supp}(\mu_+)$ (respectively $\mathrm{supp}(\mu_-)$) and $|\mu| = \mu_+ + \mu_-$, we get

$$
  \frac{\mathrm{d}\mu}{\mathrm{d}|\mu|} = \mathbf{1}_{S_+} - \mathbf{1}_{S_-}
$$

Moreover, for a finite variation process, $A(t) = m\mu_+ ([0,t]) - m\mu_- ([0,t])$. As $A$ is a continuous, $\mu_+$ and $\mu_-$ have no atoms because they have disjoint supports. Thus, $A$ is the difference of two continuous increasing functions beginning at $0$. This proves that for any $t > 0$

$$
  \sup_{0=t_0 < \cdots < t_n = t} \sum_{k=1}^n |A_{t_k} - A_{t_{k-1}}| < \infty
$$

where the supremum is over all $n\in\N$ and subdivisions of $[0,t]$.
</details>
</MathBox>

<MathBox title='' boxType='theorem'>
Let $A$ be a finite variation process. Then for any $t > 0$

$$
  \sup_{0=t_0 < \cdots < t_n = t} \sum_{k=1}^n |A_{t_k} - A_{t_{k-1}}| = |\mu|([0,t])
$$

where the supremum is over all $n\in\N$ and subdivisions of $[0,t]$.

<details>
<summary>Proof</summary>

The inequality 

$$
  \sup_{0=t_0 < \cdots < t_n = t} \sum_{k=1}^n |A_{t_k} - A_{t_{k-1}}| \leq |\mu| ([0,t])
$$

is obvious because

$$
  |A_{t_k} - A_{t_{k-1}}| = |\mu((t_{t_{k-1}}, t_k])| \leq |\mu|((t_{k-1}, t_k])
$$

Consider any sequence of refined subdivisions of $[0,t]$ with step going to $0$, noted $0 = t_0^{(n)} < \cdots < t_{p_n}^{(n)} = t$, and the filtration $\mathcal{F}_0 \subset\cdots\subset\mathcal{F}_n \subset\cdots\subset \mathcal{B}([0,t])$ defined a subsets of the Borel algebra by

$$
  \mathcal{F}_n = \sigma((t_{k-1}^{(n)}, t_k^{(n)}])_{1\leq k\leq p_n}
$$

This is indeed a filtration because the subdivisions are refined. Take $\Omega = [0,t]$ and the probability measure

$$
  \mathbb{P}(\mathrm{d}s) = \frac{|\mu|(\mathrm{d}s)}{|\mu|([0,t])}
$$

on $\Omega$. On the probability space $(\Omega,\mathcal{B}([0,t]), \mathbb{P})$ with filtration $\mathscr{F} = \Set{\mathcal{F}_n}_{n\in\N}$, consider the random variables

$$
  X(s) = \mathbf{1}_{S_+} (s) - \mathbf{1}_{S_-} (s)
$$

and when $s\in (t_{k-1}^{(n)}, t_k^{(n)}]$

$$
\begin{align*}
  X_n =& \mathbb{E}(X|\mathcal{F}_n)(s) \\
  =& \frac{\mu((t_{k-1}^{(n)}, t_k^{(n)}])}{|\mu|((t_{k-1}^{(n)}, t_k^{(n)}])} \\
  =& \frac{A_{t_k^{(n)}} - A_{t_{k-1}^{(n)}}}{|\mu|((t_{k-1}^{(n)}, t_k^{(n)}])}
\end{align*}
$$

As $\Set{X_n}_{n\in\N}$ is a bounded martingale, it converges almost surely and in $\mathcal{L}^1$ to some $Y\in\mathcal{L}^1$ and $X_n = \mathbb{E}(Y|\mathcal{F}_n)$ As a consequence, $\mathbb{E}(X - Y | \mathcal{F}_n) = 0$ for any $n$. Since $X$ and $Y$ are in $\bigwedge_n \mathcal{F_n}$ (concerning $X$, this is a consequence of the time step going to $0$), this implies $X = Y$ almost surely. Thus, $X_n \to X$ in $\mathcal{L}^1$, so in particular $\mathbb{E}(|X_n|)\to\mathbb{E}(|X|)$, which means that

$$
  \sum_{k=1}^{p_n} |A_{t_k^{(n)}} - A_{t_{k-1}^{(n)}}| \xrightarrow{n\to\infty} |\mu|([0,t])
$$
</details>
</MathBox>

If $A$ is a finite variation process and $F:[0,t]\to\R$ is a process, measurable for any given $\omega$, such that $\int_0^t |F(s)|\cdots|\mu|(\mathrm{d}s)$ is finite, then we define

$$
\begin{align*}
  \int_0^t F(s)\;\mathrm{d}A_s =& \int_0^t F(s)\mu(\mathrm{d}s) \\
  \int_0^t F(s)\;|\mathrm{d}A_s| =& \int_0^t F(s)|\mu|(\mathrm{d}s)
\end{align*}
$$

<MathBox title='' boxType='proposition'>
Let $A$ be a finite variation process and $F:\Omega\times[0,t]\to\R$ a left-continuous process. Then for any $\omega\in\Omega$

$$
\begin{align*}
  \int_0^t F(s)\;\mathrm{d}A_s =& \lim_{n\to\infty} \sum_{k=1}^{p_n} F\left(t_{k-1}^{(n)}\right) \left(A_{t_k^{(n)}} - A_{t_{k-1}^{(n)}}\right) \\
  \int_0^t F(s)\;|\mathrm{d}A_s| =& \lim_{n\to\infty} \sum_{k=1}^{p_n} F\left(t_{k-1}^{(n)}\right) \left|A_{t_k^{(n)}} - A_{t_{k-1}^{(n)}}\right| 
\end{align*}
$$

for sequence of subdivisions of $[0,t]$, of the form $0 = t_0^{(n)} < \cdots < t_{p_n}^{(n)} = t$, with step going to $0$. For the second identity we require the subdivisions to be refined.

<details>
<summary>Proof</summary>

Let $F_n$ be the process defined as $F\left( t_{k-1}^{(n)} \right)$ on $(t_{k-1}^{(n)}, t_k^{(n)}]$. Then, the right hand side of the first identity is $\int_0^t F_n (s) \mu(\mathrm{d}s)$, so the result follows by dominated convergence. For the second identity, we have

$$
\begin{align*}
  &\left| \sum_{k=1}^{p_n} F\left( t_{k-1}^{(n)} \right) \left| A_{t_k^{(n)}} - A_{t_{k-1}^{(n)}} \right| - \int_0^t F_n(s)\;\mathrm{d}A_s \right| \\
  \leq& \lVert F \rVert_{\mathcal{L}^\infty [0,1]} \left( |\mu|([0,t]) - \sum_{k=1}^{p_n} \left| A_{t_k^{(n)}} - A_{t_{k-1}^{(n)}} \right| \right)
\end{align*}
$$

From the proof of the previous theorem, this converges to $0$ along any refined sequence of subdivisions with step going to $0$, hence proving that

$$
  \int_0^t F_n(s)\;|\mathrm{d}A_s| \xrightarrow{n\to\infty} \int_0^t F(s)\;|\mathrm{d}A_s|
$$

is sufficient, and true by dominated convergence.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Given a probability space $(\Omega,\mathcal{F},\mathbb{P})$ with filtration $\mathscr{F} = \Set{\mathcal{F}_t}_{t\geq 0}$, let $A$ be a finite variation process and $F$ progressively measurable such that $\int_0^t |F_s|\cdot|\mathrm{d}A_s| < \infty$ for any $\omega\in\Omega$ and $t\geq 0$. Then

$$
  F\cdot A: (\omega, t)\mapsto \int_0^t F_s(\omega)\;\mathrm{d}A_s(\omega)
$$

is a finite variation process.

<details>
<summary>Proof</summary>

Let $\mu$ be the signed measure associated to $A$. The process $F\cdot A$ begins at $0$, is continuous and has bounded variation because

$$
  (F\cdot A)_t = \tilde{\mu}([0,t]), \tilde{\mu}(\mathrm{d}s) = H_s \mu(\mathrm{d}s)
$$

with $\tilde{\mu}$ a signed measure with no atoms (the finite mass condition holds since $\int_0^t |F_s|\cdot|\mathrm{d}A_s| < \infty$). Consequently, the only condition to verify carefully is the adaptedness of $F\cdot A$. It is true that $(F\cdot A)_t$ is $\mathcal{F}_t$-measurable if $F$ is ot type $\mathbf{1}_{(u,v]} (s) \mathbf{1}_{A} (\omega)$, where $u,v\leq t$ and $A\in\mathcal{F}_t$. It is then true if $F = \mathbf{1}_A$ for any $A\in\mathcal{B}([0,t])\otimes\mathcal{F}_t$ by the monotone class theorem. Finally, by taking linear combinations of such sums approximating $F$ from below, and using dominated convergence (domination by an integrable process holds since $\int_0^t |F_s|\cdot|\mathrm{d}A_s| < \infty$), we get the result for general $F$, because the pointwise limit of measurable functions is measurable.
</details>
</MathBox>

## Local martingales

<MathBox title='Quadratic variation of a martingale' boxType='theorem'>
A process $\Set{M_t}_{t\geq 0}$ is a local martingale beginning at $0$ if it is adapted and satisfies
1. $M_0 = 0$ for all $\omega$
2. $M$ is continuous for all $\omega$
3. There exists a sequence of stopping times $T_n$ converging to $\infty$ for any $\omega$ such that $M^{T_n} := \Set{ M_{t\wedge T_n}}_{t\geq 0}$, is a uniformly integrable martingale.

For such a sequence of stopping times, $\Set{T_n}_{n\in\N}$ reduces $M$. A process $M$ is a local martingale if $M_t = M_0 + N_t$, where $M_0\in\mathcal{F}_0$ and $N$ is a local martingale beginning at $0$.
</MathBox>

<MathBox title='Quadratic variation of a martingale' boxType='theorem'>
For a given probability space $(\Omega,\mathcal{F},\mathbb{P})$ with filter $\mathscr{F} = \Set{\mathcal{F}_t}_{t\geq 0}$, all the following statements hold.
1. Any continuous martingale is a local martingale.
2. There exists a sequence of stopping times $T_n$ converging to $\infty$ for any $\omega$ such that $M^{T_n} := \Set{M_{t\wedge T_n}}_{t\geq 0}$ for all $n\in\N$ is a martingale.
3. If $M$ is a local martingale and $T$ is a stopping time, then $M^T = \Set{M_{t\wedge T}}_{t\geq 0}$ is a local martingale.
4. If $M$ is a local martingale, $\Set{T_n}_{n\in\N}$ reduces $M$, and $\Set{S_n}_{n\in\N}$ are stopping times converging to $\infty$, then $\Set{S_n \wedge T_n}_{n\in\N}$ reduces $M$.
5. The set of local martingales is a vector space.
6. If $M$ is a nonnegative local martingale and $M_0 \in \mathcal{L}^1$, then $M$ is a supermartingale.
7. If $M$ is a local martingale and $|M_t| \leq X$ for all $t\geq 0$, where $X\in\mathcal{L}^1$, then $M$ is a martingale.
8. If $M$ is a local martingale beginning at $0$, then $T_n = \inf\{t\geq 0 : |M_t| = n\}$ reduces $M$.

<details>
<summary>Proof</summary>

Part **(1)** follows from the possible case $T_n = n$. Then for any constant $c \geq 0$, $\Set{M_{t\wedge c}}_{t\geq 0}$ is a uniformly integrable, as all of its values are of type $\mathbb{E}(M_c | \mathcal{G})$ for some $\sigma$-algebra $\mathcal{G}$ and $M_c \in\mathcal{L}^1$.

For part **(2)** note that if $T_n \to\infty$ and $M^{T_n}$ is a martingale, then $M^{T_n \wedge n}$ is uniformly integrable, as shown for **(1)**., and $T_n \wedge n \to\finty$.

For part **(3)** and **(4)** note that if $M^{T_n}$ is a uniformly integrable martingale, so is $M^{T_n \wedge T}$. The stability by addition mentioned in **(5)** is a dircet consequence of **(4)**, by choosing $\Set{T_n}_{n\in\N}$ reducing the first martingale and $\Set{S_n}_{n\in\N}$ reducing the second. Point **(6)** is a consequence of Fatou's lemma: if $M = M_0 + N$ and $\Set{T_n}_{n\in\N}$ reduces $N$, then

$$
\begin{align*}
  \mathbb{E}(M_t|\mathcal{F}_s) =& \mathbb{E}\left(\lim_{n\to\infty} M_{t\wedge T_n} \right) \\
  \leq& \liminf{n\to\infty} \mathbb{E}(M_{t\wedge T_n | \mathcal{F}_s}) \\
  =& \liminf_{n\to\infty} M_{s\wedge T_n} = M_s
\end{align*}
$$

Note that $M_t$ is in $\mathcal{L}^1$ precisely thanks to the above equation. The result **(7)** relies on dominated convergence applied to the indetity

$$
  M_{s\wedge T_n} = \mathbb{E}(M_{t\wedge T_n} | \mathcal{F}_s)
$$

where $\Set{T_n}_{n\in\N}$ reduces $M$. Finally, **(8)** is a direct consequence of **(2)** and **(7)**.
</details>
</MathBox>

<MathBox title='Indistinguishability of local martingales' boxType='theorem'>
Let $M$ be a local martingale beginning at 0$. If $M$ is a finite variation process, then $M$ is indistinguishable from $0$.

<details>
<summary>Proof</summary>

Assume that $M$ is a finite variation process, and choose

$$
  T_n = \inf\{ t\geq 0 | \int_0^t |\mathrm{d}M_s| \geq n \}
$$

Then $T_n \to\infty$ and $T_n$ is a stopping time. The local martingale $M^{T_n}$ is bounded by $n$, so it is a martingale by properties of local martingales. As a consequence, for any subdivision $0 = t_0 < \cdots < t_p = t$

$$
\begin{align*}
  \mathbb{E}[(M_t^{T_n})^2] =& \sum_{k=1}^n \mathbb{E}[(M_{t_k}^{T_n})^2 - (M_{t_{k-1}}^{T_n})^2] \\
  =& \sum_{k=1}^p \mathbb{E}[(M_{t_k}^{T_n} - M_{t_{k-1}}^{T_n})^2] \\
  \leq& \mathbb{E}\left( \max{\ell}\left|M_{t_\ell}^{T_n} - M_{t_{\ell - 1}}^{T_n} \right| \sum_{k=1}^ns |M_{t_k}^{T_n} - M_{t_{k-1}}^{T_}| \right) \\
  \leq& n\mathbb{E}\left( \max_{\ell} |M_{t_\ell}^{T_n} - M_{t_{\ell - 1}}^{T_n} | \right)
\end{align*}
$$

As this maximum is bounded by $n$ and $M$ has continuous trajectories, dominated convergence allows to conclude that $\mathbb{E}[(M_t^{T_n})^2] = 0$, by choosing subdivisions with time step going to $0$. By Fatou's lemma, one can take the $n\to\infty$ limit to conclude $\mathbb{E}(M_t^2) = 0$, so $M_t = 0$ almost surely. As $M$ is continuous, this is equivalent to being indistinguishable from $0$.
</details>
</MathBox>

<MathBox title='Quadratic variation of a martingale' boxType='theorem'>
Let $M$ be a local martingale. Then there exists a unique (up to indistinguishability) increasing continuous variation process, noted $\langle M, M \rangle$, such that $\Set{M_t^2 - \langle M, M \rangle_t}_{t\geq 0}$ is a local martingale. Moreover, if $\Set{ 0 = t_0^{(n)} < t_1^{(n)} < \cdots}_{n\in\N_+}$ is any sequence of subdivisions of $\R_+$, with step going to $0$, then

$$
  \langle M, M \rangle_t = \lim_{n\to\infty} \sum_{k\geq 1} \left( M_{t_k^{(n)} \wedge t} - M_{t_{k-1}^{(n)}\wedge t} \right)^2
$$

uniformly in the sense of convergence in probability. The process $\langle M, M \rangle$, often noted $\langle M \rangle$, is called the *bracket* or *quadratic variation* of $M$.

<details>
<summary>Proof</summary>

Uniqueness of quadratic variation is an easy consequence of indistinguishability. We will first prove the existence of the bracket when $M$ is a true martingale, and $|M|$ is almost surely bounded by some $K > 0$. For a subdivision $\delta = \Set{0 = t_0 < t_1 < \dots}$ and a process $Y$, we note

$$
  Q_t^{(Y,\delta)} = \sum_{k\in\N_+} \left( Y_{t_k^{(n)} \wedge t} - Y_{t_{k-1}^{(n)}\wedge t} \right)^2 
$$

Furthermore,

$$
\begin{align*}
  X_t^{(\delta)} :=& M_t^2 - Q_t^{(M,\delta)} \\
  =& M_t^2 - \sum_{k\in\N_+} \left( M_{t_k^{(n)} \wedge t} - M_{t_{k-1}^{(n)}\wedge t} \right)^2 \\
  =& 2\sum_{k\in\N_+} M_{t_{k-1}^{(n)}} \left( M_{t_k^{(n)}} - M_{t_{k-1}^{(n)} \wedge t} \right)
\end{align*}
$$

Thus, $\Set{X_t^{(\delta)}}_{t\geq 0}$ is a continuous martingale. For a sequence $\Set{\delta_n}_{n\in\N}$ of subdivisions with step going to 0, we want to find a subsequence of $\Set{X^{(\delta_n)}}_{n\geq 0}$ converging uniformly on compact sets. Note that

$$
  \delta_t^{(n,m)} =& X_t^{(\delta_n)} - \delta_t^{(\delta_m)} = Q_t^{(M,\delta_m)} - Q_t^{(M,\delta_n)}
$$

which is a martingale, so

$$
  \Set{(\delta_t^{(n,m)})^2 - Q_t^{(\delta^{(n,m)},\delta_n \cup \delta_m)}}_{t\geq 0}
$$

is a martingale as well, by the same decomposition used to prove that $X^{(\delta)}$ is a martingale. As a consequence, the expectation of $(\delta_t^{(n,m)})^2$ is also a discrete analogue of the quadratic variation of a finite variation process. We therefore expect this to go to $0$, which would prove that the sequence of \Set{X_t^{(\delta_n)}}_{n\in\N} is a Cauchy sequence in $\mathcal{L}^2$, hence converging. Note that, $(a-b)^2 \leq 2(a^2 + b^2)$, then

$$
  Q_t^{A-B,\delta} \leq 2\left( Q_t^{A,\delta} - Q_t^{B,\delta} \right)
$$

so in order to prove that $\mathbb{E}[(\delta_t^{(n,m)})^2]$ converges to $0$, a sufficient condition is

$$
  \mathbb{E}(Q_t^{(Q^{(M,\delta_n)}, \delta_n \cup \delta_m)}) \xrightarrow{n,m\to\infty} 0
$$

Note that $\varepsilon_n = \sup_{u,v\in[0,t]} |M_u - M_v|$ is the supremum such that $u - v$ is smaller than the time step of $\delta_n$. Then if $s_{k-1}$ and $s_k$ are successive elements of $\delta_n \cup \delta_m$, then $|Q_{s_k}^{(M,\delta_n)} - Q_{s_{k-1}}^{(M,\delta_n)}| \leq \varepsilon_n |M_{s_k} - M_{s_{k-1}}|$, hence

$$
  Q_t^{(Q^{(M,\delta)}, \delta_n \cup\delta_m)} \leq \varepsilon_n^2 \sum_{k\in\N_+} (M_{s_k \wedge t} - M_{s_{k-1} \wedge t})^2
$$

Note that for any subdivision $\delta$

$$
\begin{align*}
  (Q_t^{(M,\delta)})^2 =& \sum_{k\in\N_+} (M_{s_k \wedge t} - M_{s_{k-1}\wedge t})^4 \\
  &+ \sum_{k\in\N} (Q_{s_k\wedge t}^{(M,\delta)} - Q_{s_{k-1}\wedge t}^{M,\delta})(Q_t^{(M,\delta)} - Q_{s_k \wedge t}^{(M,\delta)})
\end{align*}
$$

As $X^(\delta)$ is a martingale, $\mathbb{E}(Q_t^{(M,\delta)} - Q_{s_{k\wedge t}}^{M,\delta} | \mathcal{F}_{s_k \wedge t} ) = \mathbb{E}(M_t^2 - M_{s_k \wedge t}^2 | \mathcal{F}_{s_k \wedge t})$, so using $|M| \leq K$, we get

$$
\begin{align*}
  \mathbb{E}[(Q_t^{(M,\delta)})^2] \leq& 4K^2 \left( \mathbb{E}(Q_t^{(M,\delta)}) + \sum_{k\in\N_+} (Q_{s_k \wedge t}^{(M,\delta)}) - Q_{s_{k-1}\wedge t}^{(M,\delta)} \right) \\
  =& 8K^2 \mathbb{E}(Q_t^{(M,\delta)}) = 8K^2 \mathbb{M_t^2} \leq 8K^4
\end{align*}
$$

As the quadratic increments is uniformly bounded in $\mathcal{L}^2$ by $8K^4$, we get by the Cauchy-Schwarz inequality

$$
  \mathbb{E}(Q_t^{(Q^{(M,\delta_n)},\delta_n \cup\delta_m)}) \leq (8K^4 \mathbb{E}(\varepsilon_n^4))^{1/2}
$$

By dominated convergence, where $\varepsilon_n \to 0$ almost surely and $\varepsilon_n \leq 2K$, this goes to $0$ in the limit $n\to\infty$. It follows that $\Delta_t^{(m,n)} \xrightarrow{n,n\to\infty} 0$ in $\mathcal{L}^2$. By Doob's inequality, this implies that

$$
  \mathbb{E}\left[ \left( \sup_{[0,t]} (X^{(\delta_n)} - X^{(\delta_m)}) \right)\right] \xrightarrow{n,m\to\infty} 0
$$

so there is a subsequence of the $X^{(\delta_n)}$ converging almost surely, uniformly, on $[0,t]$. Let $X$ denote this (continuous) limit. As the subsequence of the $X^{(\delta)}$ converge to $X$ in $\mathcal{L}^2$, their martingale property is preserved in the limit, hence $X$ is a martingale. Moreover, from the definition of $Q_t^{(M,\delta)}$ it follows that $M^2 - X^{(\delta)}$ is an increasing process. This property holds for $M^2 - X$ by uniform convergence. For $s\in[0,t]$ define

$$
  \langle M \rangle_s := M_s^2 - X_s
$$

From the previous discussion, $\langle M \rangle$ satisfies all required properties of the bracket on $[0,t]$. By uniqueness of the bracket, the value $\langle M \rangle_s$ is independent of the choice of the horizon $t\geq s$, and of the choice of the subsequence providing uniform convergence. Moreover, the above reasoning has proved that $X_s^{(\delta_n)} - X_s^{(\delta_m)}$ is Cauchy sequence in $\mathcal{L}^2$, as $\varepsilon_n$ can be chose indentical for any choice of $s\in[0,\t]$, the convergence is in $L^2$ and uninform on compact sets.

This bounded martingale case extends easily. First, note that if the result is true for local martingales beginning at $0$, it is true for local martingales. If $M_t = M_0 + N_n$ with $M_0 \in\mathcal{F}_0$ and $N$ is a local martingale beginning at $0$, as $M_0 N$ is a local martingale, so is $M_t^2 - \langle N \rangle_t = N_t^2 - \langle N \rangle_t + M_0^2 + 2M_0 N_t$. Thus, we can assume that $M_0 = 0$.

We localize $M$ by $T_n = \inf\{ t \geq 0 : |M_t| = n \}$. Then $M^{T_n}$ is a local martingale, bounded by $n$, so we can apply the previous argument: there is an increasing process, noted $\langle M \rangle^{(n)}$ usch that $(M^{T_n})^2 - \langle M \rangle^{(n)}$ is a martingale. By uniqueness, (\langle M \rangle^{(n)})^{T_m} = \langle M \rangle^{(m)} for $m \leq n$. From this coherence property, we can define a process $\langle M \rangle$ such that $(M^{T_n})^2 - \langle M \rangle^{T_n}$ is a martingale. As $T_n \to\infty$ almost surely, this means that $M^2 - \langle M \rangle$ is a local martingale.

The property of uniform convergence of quadratic increments to the bracket also holds for $(M^{T_n})^2 - \langle M \rangle^{T_n}$ in $\mathcal{L}^2$. Since $\mathbb{P}(T_n \leq t) \xrightarrow{n\to\infty} 0$ by dominated or monotone convergence, it follows that the uniform converge holds in probability.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $M$ and $N$ be two local martingales. Then

$$
  \langle M, N \rangle = \frac{1}{2} (\langle M + N, M + N \rangle - \langle M, M \rangle - \langle N, N \rangle)
$$

satisfying the following.
1. Up to indistinguishability, $\langle M, N \rangle$ is the unique finite variation process such that $MN - \langle M,N\rangle$ is a local martingale.
2. The function $(M, N) \mapsto \langle M, N \rangle$ is symmetric and bilinear.
3. If $(0 = t_0^{(n)} < t_1^{(n)} < \dots)_{n\in\N}$ is any sequence of subdivisions of $\R_+$ with step going to $0$, then
$$
  \langle M, N \rangle_t = \lim_{n\to\infty} \sum_{k\in\N_+} (M_{t_k^{(n)}\wedge t} - M_{t_{k-1}^{(n)} \wedge t}) (N_{t_k^{(n)}\wedge t} - N_{t_{k-1}^{(n)} \wedge t})
$$
in probability, uniformly for $t$ in compact sets.
4. For any stopping time $T$, then $\langle M, N \rangle_{t\wedge T} = \langle M^T, N \rangle_t = \langle M^T ,N^T \rangle_t$ for $t\geq 0$.
</MathBox>

<MathBox title='Kumita-Watanabe inequality' boxType='theorem'>
Let $H$ and $K$ be progressively measurable processes and suppose that $M$ and $N$ are local martingales. Then, for any $t\in\R_+ \cup\Set{\infty}$

$$
  \int_0^t |H_s K_s|\cdots|\mathrm{d}\langle M, N \rangle_s | \leq \left( \int_0^t H_s^2 \;\mathrm{d}\langle M \rangle_s \right)^{1/2} \left( \int_0^t K_s^2 \;\mathrm{d}\langle N \rangle_s \right)^{1/2}
$$

<details>
<summary>Proof</summary>

Note that from properties of quadratic variation of two martingales and the Cauchy-Schwarz inequality we have

$$
  |\langle M, N \rangle_t - \langle M, N \rangle_s | \leq (\langle M \rangle_t - \langle M \rangle_s)^{1/2} (\langle N \rangle_t - \langle N \rangle_s )^{1/2}
$$

almost surely. Using Cauchy-Schwarz again, for $s = t_0 < \cdots < t_n = t$, the above inequality yields

$$
\begin{align*}
  &\sum_{k=1}^n |\langle M, N \rangle_{t_k} - \langle M, N \rangle_{t_{k-1}} | \\
  \leq& \sum_{k=1}^n (\langle M \rangle_{t_k} - \langle M \rangle_{t_{k-1}})^{1/2} (\langle N \rangle_{t_k} - \langle N \rangle_{t_{k-1}})^{1/2} \\
  \leq& \left( sum_{k=1}^n (\langle M \rangle_{t_k} - \langle M \rangle_{t_{k-1}})\right)^{1/2} \left((\langle N \rangle_{t_k} - \langle N \rangle_{t_{k-1}})\right)^{1/2} \\
  =& (\langle M \rangle_t - \langle M \rangle_s )^{1/2} (\langle N \rangle_t - \langle N \rangle_s)^{1/2}
\end{align*}
$$

By the finite variation supremum theorem, we get

$$
  \int_s^t |\mathrm{d}\langle M, N \rangle_u | \leq \left( \int_s^t \mathrm{d}\langle M \rangle_u \right)^{1/2} \left( \int_s^t \mathrm{d}\langle N \rangle_u \right)^{1/2}
$$

For functions of type $H = \sum h_\ell \mathbf{1}_{B_\ell}$ and $K = \sum k_\ell \mathbf{1}_{B_\ell}$, with disjoint bounded Borel sets $B_i$, we have

$$
\begin{align*}
  \int |H_s K_s |\cdot |\mathrm{d}\langle M, N \rangle_s | =& \sum_\ell |h_\ell k_\ell | \int_{B_\ell} |\mathrm{d}\langle M, N \rangle_u | \\
  \leq& \sum_\ell |h_\ell k_\ell| \left( \int_{B_\ell} \mathrm{d}\langle M \rangle_u \right)^{1/2} \left( \int_{B_\ell} \mathrm{d}\langle N \rangle_u \right)^{1/2} \\
  \leq& \left( \sum_\ell h_\ell^2 \int_{B_\ell} \mathrm{d}\langle M \rangle_u \right)^{1/2} \left( \sum_\ell k_\ell^2 \int_{B_\ell} \mathrm{d}\langle N \rangle_u \right)^{1/2} \\
  =& \left( \int_0^t H_s^2 \mathrm{d}\langle M \rangle_s \right)^{1/2} \left( \int_0^t K_s^2 \mathrm{d}\langle N \rangle_s \right)^{1/2}
\end{align*}
$$

Approximation of progressively measurable processes as increasing limit of such functions completes the proof.
</details>
</MathBox>

<MathBox title='Integrability of local martingales' boxType='theorem'>
Let $M$ be a local martingale with $M_0 = 0$.
1. The process $M$ is an $\mathcal{L}^2$-bounded martingale, i.e. $\sup_{t\geq 0} \mathbb{E}(|M_t|^2) < \infty$, if and only if $\mathbb{E}(\langle M \rangle_\infty) < \infty$. In such a case, $M^2 - \langle M \rangle$ is a uniformly integrable martingale.
2. The process $M$ is a square integrable martingale if and only if $\mathbb{E}(\langle M \rangle_t ) < \infty$ for $t\geq 0$. In such a case, $M^2 - \langle M \rangle$ is a martingale.

<details>
<summary>Proof</summary>

**(1):** Assume first that $M$ is an $\mathbb{L}^2$-bounded martingale. It is therefore uniformly integrable and converges almost surely to some $M_\infty$. Moreover, from Doob's inequality

$$
  \mathbb{E}\left(\sup_{t\geq 0} M_t^2 \right) \leq 4 \sup_{t\geq 0} \mathbb{E}(M_t^2) < \infty
$$

As a consequence, if we define $T_n = \inf\Set{t\geq 0 | \langle M \rangle_t \geq n}$ then

$$
  \Set{M_{t\wedge T_n}^2 - \langle M \rangle_{t\wedge T_n}}_{t\geq 0}
$$

is a local martingale bounded by $\sup_{t\geq 0} (M_t^2) + n\in \mathcal{L}^1$, so it is a true martingale. Thus

$$
  \mathbb{E}(\langle M \rangle_{t\wedge T_n}) = \mathbb{E}(M_{t\wedge T_n}^2)
$$

Dominated convergence allows to take the limit $t\to\infty$ on the right hand side, and monotone convergence on the left side,

$$
  \mathbb{E}(\langle M \rangle_{T_n}) = \mathbb{E}(M_{T_n}^2)
$$

Monotone convergence on the left and dominated convergence on the right yields

$$
  \mathbb{E}(\langle M \rangle_\infty) = \mathbb{E}(M_\infty^2)
$$

In particular, $\mathbb{E}(\langle M \rangle_\infty)$ if finite. This implies that $M^2 - \langle M \rangle$ is bounded by an integrable random variable $\mathrm{sup}_{t\geq 0} (M_t^2) + \langle M \rangle_\infty $, so it is a uniformly integrable martingale.

Conversely, suppose that $\mathbb{E}(\langle M \rangle_\infty) < \infty$ and note that $\tilde{T}_n = \inf\Set{t\geq 0 : |M_t| \geq n}$. Then $M^{\tilde{T}_n}$ (bounded by $n$) and $(M^{\tilde{T}_n})^2 - \langle M \rangle^{\tilde{T}}$ (bounded by $n^2 + \langle M \rangle^{T_n} \in \mathcal{L}^1$) are uniformly integrable martingales. Thus, for any stopping time $S$

$$
  \mathbb{E}(M_{S\wedge\tilde{T}_n}^2) = \mathbb{E}(\langle M \rangle_{S\wedge\tilde{T}_n})
$$

By Fatou's lemma

$$
  \mathbb{E}(M_S^2) \leq \mathbb{E}(\langle M \rangle_S) \leq \mathbb{E}(\langle M \rangle_\infty) < \infty
$$

In particular, $M$ is $\mathcal{L}^2$-bounded. Moreover, it is a martingale and therefore in the identity

$$
  \mathbb{E}(M_{t\wedge\tilde{T}_n} | \mathcal{F}_s) = M_{s\wedge\tilde{T}_n}
$$

the limit $n\to\infty$ is allowed by dominated convergence. Indeed, the $M_{t\wedge\tilde{T}_n}$ are bounded in $\mathcal{L}^2 with (\mathbb{E}(\sup_t M_t^2)) < \infty$, so $\mathbb{E}(\sup_n M_{t\wedge\tilde{T}_n}^2) < \infty$ and hence bounded in $\mathcal{L}^1$.

**(2):** By Doob's inequality, if $M$ is square integrable, $\Set{M_{s\wedge t}}_{c\get 0}$ is $\mathcal{L}^2$-bounded, so by **(1)**, $\mathbb{E}(\langle M \rangle_t) < \infty$. Reciprocally, if $\mathbb{E}(\langle M \rangle_t) < \infty$, then **(1)** implies that $\Set{M_{s\wedge t}}_{c\get 0}$ is bounded in $\mathcal{L}^2$, in particular $\mathbb{E}(M_t^2) < \infty$. Finally, in such a case, from **(1)** the process $\Set{M_{s\wedge t}^2 - \langle M \rangle_{s\wedge t}}_{s\geq 0}$ is a uniformly integrable martingale, so $M^2 - \langle M \rangle$ is a martingale. 
</details>
</MathBox>

<MathBox title='' boxType='corollary'>
Let $M$ be a local martingale, with $M_0 = 0$ almost surely. Then $M$ is indistinguishable from $0$ if and only if $\langle M \rangle$ is identically $0$.

<details>
<summary>Proof</summary>

If $M$ is indistinguishable from $0$, it is clear that the bracket vanishes (as a limit of quadratic increments). Reciprocally, if $\langle M \rangle\equiv 0$, then by integrability $M^2$ is a martingale so $\mathbb{E}(M_t^2) = 0$ for any given $t$. Thus, $M_t = 0$ almost surely. One can conclude by continuity that $M$ is indistinguishable from $0$.
</details>
</MathBox>

<MathBox title='Semimartingale process' boxType='definition'>
Given a probability space $(\Omega, \mathcal{F}, \mathbb{P})$ with filtration $\mathscr{F} = \Set{\mathcal{F}_t}_{t\geq 0}$, a process $X$ is called a semimartingale if is of type

$$
  X = X_0 + M + A
$$

where $X_0 \in\mathcal{F}_0$, $M$ is a locale martingale beginning at $0$, and $A$ is a finite variation process.
</MathBox>

Note that the decomposition of a semimartingale process is unique up to indistinguishability. As a consequence, we can define without ambiguity the bracket of $X = X_0 + M + A$ and $\tilde{X} = \tilde{X}_0 + \tilde{M} + \tilde{A}$ as

$$
  \langle X, \tilde{X} \rangle = \langle M, \tilde{M} \langle
$$

In particular, the bracket of a finite variation process with any semimartingale is always $0$. Then we can show that the bracket is still given as a limit of increment (the finite variation part does not contribute). Thus, if $(0 < t_0^{(n)} < t_1^{(n)} \dots)_{n\in\N}$ is any sequence of subdivisions of $\R_+$ with step going to $0$, then in the sense of convergence in probability

$$
  \langle X, \tilde{X} \rangle_ t = \lim_{t\to\infty} \sum_{k\in\N_+} \left( X_{t_k^{(n)} \wedge t} - X_{t_{k-1}^{(n)} \wedge t} \right)\left( \tilde{X}_{t_k^{(n)} \wedge t} - \tilde{X}_{t_{k-1}^{(n)} \wedge t} \right)
$$

<MathBox title='' boxType='theorem'>
If, given a probability space, the process $Y$ is continuous with independent increments, then it takes the from

$$
  Y = X + F
$$

where $X$ is a semimartingale with independent increments and $F$ is a deterministic continuous function.
</MathBox>

# Brownian motion

# Stochastic integrals

<MathBox title='' boxType='definition'>
Let $H$ be a progressively measurable process and let $M$ be a local martingale. We denote $H^2$ the set of continuous $L^2$-bounded martingales, i.e. $M\in H^2$ if $\mathrm{sup}_{t\geq 0} \mathbb{E}((M_t)^2) < \infty$.
</MathBox>

<MathBox title='' boxType='proposition'>
The set $H^2$ is a Hilbert space with inner product

$$
  \langle M, N \rangle_{H^2} := \mathbb{E}(\langle M, N \rangle_\infty),\, M,N\in H^2
$$

<details>
<summary>Proof</summary>

Note that, if $M\in H^2$, it converges almost surely with a finite bracket, $\mathbb{E}(\langle M \rangle_\infty) < \infty$. If $M, N\in H^2$, then by Kunita-Watanabe inequality

$$
  \mathbb{E}(|\langle M, N \rangle_\infty|) \leq \mathbb{E}(\langle M \rangle_\infty)^{1/2} \mathbb{E}(\langle N \rangle_\infty)^{1/2}
$$

This means that we can define a scalar product on $H^2$ by

$$
  (M, N)_{H^2} = \mathbb{E}(\langle M, N \rangle_\infty)
$$

and $\lVert M \rVert_{H^2} = (\langle M \rangle_\infty)^{1/2}$ defines a norm. We have already seen that if $\lVert M \rVert_{H^2} = 0$, then $M$ is indistinguishable from $0$.

It remains to show that $H^2$ is complete. Consider a Cauchy sequence $(M^{(n)})_{n\in\N}$:

$$
\begin{align*}
  &\lim_{m,n\to\infty} \mathbb{E}\left((M_\infty^{(n)} - M_\infty^{(m)})^2 \right) \\ 
  =& \lim_{m,n\to\infty} \mathbb{E}\left(\langle M^{(n)} - M^{(m)}\rangle_\infty \right) = 0
\end{align*}
$$

By the Doob inequality we have

$$
  \lim_{m,n\to\infty} \mathbb{E}\left( \sup_{t\geq 0} |M_t^{(n)} - M_t^{(m)}|^2 \right) = 0
$$

so we can find an increasing sequence $(n_k)_{k\in\N}$ such that

$$
\begin{align*}
  &\mathbb{E}\left( \sum_{k=1}^infty \sup_{t\geq 0} |M_t^{(n_k)} - M_t^{(n_{k-1})}| \right) \\
  \leq& \sum_{k=1}^infty \mathbb{E}\left( \sup_{t\geq 0} |M_t^{(n_k)} - M_t^{(n_{k-1})}|^2 \right)^{1/2} < \infty
\end{align*}
$$

As a consequence, $\sum_{k=1}^infty \sup_{t\geq 0} |M_t^{(n_k)} - M_t^{(n_{k-1})}|$ is almost surely isFinite, so $M^{(n_k)}$ converges uniformly to some continuous adapted process $M$. Here we have used the fact that the pointwise limit of measurable functions is measurable. For any given $s$ and $t$, $M_t^{(n_k)}$, respectively $M_s^{(n_k)}$, converges in $L^2$ to $M_t$, respectively $M_s$, in the martingale property

$$
  \mathbb{E}(M_t^{(n_k)}|\mathcal{F}_s) = M_s^{(n_k)}
$$

we can take the limits to conclude that $M$ is a martingale. Moreover, as the $M^{(n_k)}$ satisty the Doob inequality above, all $M_t^{(n_k)}$ are uniformly bounded in $L^2$ and $M\in H^2$. finally, $M^{(n_k)}$ converges to $M$ in $H^2$, because

$$
  \mathbb{E}\left(\langle M^{(n_k)} - M \rangle_\infty \right) = \mathbb{E}\left((M_\infty^{(n_k)} - M_\infty)^2 \right) \to \infty
$$

This implies, by the Cauchy condition, that $M^{(n)}$ converges to $M$ in $H^2$ as well.
</details>
</MathBox>

<MathBox title='' boxType='definition'>
For $M\in H^2$, let $L^2 (M)$ be the space of progressively measurable processes $H$ such that

$$
  \mathbb{E}\left(\int_0^\infty H_s^2 \mathrm{d}\langle M \rangle_s \right) < \infty
$$

<details>
<summary>Details</summary>

It is easily seen that $L^2 (M) = L^2 (\R_+ \times\Omega, \mathcal{F}, \nu)$, where $\mathcal{F}$ is the progressive $\sigma$-algebra and $\nu(A) = \mathbb{E}(\int_0^\infty \mathbf{1}_A (s,\dot)\;\mathrm{d}\langle M \rangle_s$ is a well-defined finite measure. Note that $L^2 (M)$ is a Hilbert space with the inner product

$$
  \langle H, K \rangle_{L^2 (M)} = \mathbb{E}\left( \int_0^\infty H_s K_s \;\mathrm{d}\langle M \rangle_s \right)
$$

in the sense that $\lVert H \rVert_{L^2(M)} = 0$ if and only if $\nu$-almost surely $H = 0$.
</details>
</MathBox>

<MathBox title='' boxType='definition'>
The vector subspace of $L^2 (M)$ consisting of step processes is noted $\mathcal{E}$. Specifically, $H\in\mathcal{E}$ is there some $p\geq 1$ and $0 = t_0 < \cdots < t_p$ such that

$$
  H_s (\omega) = \sum_{k=0}^{p-1} H_k (\omega) \mathbb{1}_{(t_k, t_{k+1}]} (s)
$$

where $H_k \in\mathcal{F}_{t_k}$ is bounded.
</MathBox>