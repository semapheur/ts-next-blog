---
title: 'Functional Analysis'
subject: 'Mathematics'
showToc: true
---

# Metric space

<MathBox title='Metric space' boxType='definition'>
A metric space $(X, d)$ consists of a set $X$ and a metric $d: X \times X \to [0, \infty)$ with the following properties for $x,y,z \in X$:

1. $d(x,y) \geq 0$ and $d(x, y) = 0 \iff x = y$ (positive definite)
2. $d(x, y) = d(y, x)$ (symmetric)
3. $d(x, z) \leq d(x, z) + d(z, y)$ (triangle inequality)
</MathBox>

For $X = \R^n$ the line segment between to points is given by the Euclidean metric $d(x, y) = \sqrt{\sum_{i=1}^n (x_i - y_i)^2}$

## Topology of a metric space

<MathBox title='Open ball' boxType='definition'>
Let $(X, d)$ be a metric space. The open ball about a point $x\in X$ with radius $\epsilon\in(0, \infty)$ is the set

$$
  B_\epsilon (x) := \Set{ y \in X | d(x, y) < \epsilon }
$$
</MathBox>

<MathBox title='Open and closed sets' boxType='definition'>
Let $(X, d)$ be a metric space. A subset $A \subseteq X$ is open if for each $x \in A$ there is an open ball with $B_\epsilon (x) \subseteq A$. Conversely, a subset $B\subseteq X$ is closed if its complement $B^c := X \setminus A$ is open.
<details>
<summary>Proof</summary>

To show that open balls in fact are open, consider $x\in B_{r}(x)\subseteq X$. Let $y\in B_\epsilon(x)$, and let $a = d(x, y) < r$. We need to find $s > 0$ such that $B_s(y)\subseteq B_r (x)$. If $z\in B_{r-a}(y), then $d(y, z) < r - a$, so by the triangle inequality

$$
  d(x, z) < a + (r - a) = r
$$

Thus $z\in B_r (x)$ and $B_{r - a}(y)\subseteq B_r(x)$. Hence $B_r (x)$ is open.
</details>
</MathBox>

<MathBox title='Boundary' boxType='definition'>
Let $(X, d)$ be a metric space. A point $x \in X$ is called a boundary point for $A \subseteq X$ if for all $\epsilon > 0$: 

$$
  B_\epsilon (x) \cap A \neq \emptyset \quad \mathrm{and} \quad  B_\epsilon (x) \cap A^c \neq \emptyset, \quad A^c = X \setminus A
$$

The set of boundary points in $A$ is denoted $\partial A$.
</MathBox>

<MathBox title='Closure' boxType='definition'>
Let $(X, d)$ be a metric space. The closure of an open set $A\subseteq X$ is the union of the set with its boundary, $\bar{A} := A \cup \partial A$, and is always closed.
</MathBox>

<MathBox title='Induced topology' boxType='proposition'>
Let $(X, d)$ be a metric space. The collection $\mathcal{T}_d$ of open subsets of $X$ is a topology.

<details>
<summary>Proof</summary>

We show that $\mathcal{T}_d$ satisfy the axioms of topology.
1. For any $x\in X$, there is $\epsilon > 0$ such that $B_\epsilon (x)\subseteq X$, hence $X$ is open. Also, $\emptyset$ is open by definition. That is, $X, \emptyset\in\mathcal{T}_d$
2. Let $\Set{U_i \subseteq X}_{i\in I}$ be a collection of open sets in $X$ for an arbitrary index set $I$. For any $x\in U = \bigcup_{i\in I} U_i$, clearly $x\in U_j$ for some $j\in I$. Since $U_j$ is open, there is $\epsilon > 0$ such that $B_\epsilon(x)\subseteq U_j\subseteq U$. Hence, $U$ is also open and $\mathcal{T}_d$ is closed under arbitrary unions.
3. Let $\Set{U_i \subseteq X}_{i\in I}$ be a finite collection of open sets in $X$ for a finite index set $I$. Trivially, if $U=\bigcap_{i\in I} U_i = \emptyset$, then $U$ is open. For any $x\in U$, clearly $x\in U_i$ for every $i\in I$. For each $i\in I$ there is $\epsilon_i > 0$ such that $B_{\epsilon_i}(x)\subseteq U_i$. Take $\epsilon = \min\Set{ \epsilon_i }_{i\in I} > 0$. Since $I$ is finite, $B_\epsilon(x) \subseteq B_{\epsilon_i}(x)\subseteq U_i$ for each $i\in I$. Hence $B_\epsilon(x)\subseteq U$, so $U$ is open and $\mathcal{T}_d$ is closed under finite intersection.  
</details>
</MathBox>

<MathBox title='Metric spaces are Hausdorff' boxType='proposition'>
A metric space $(X, d)$ is a Hausdorff space.

<details>
<summary>Proof</summary>

Consider distinct point $x, y\in X$ with $r = d(x, y) > 0$. Note that $B_{\frac{r}{2}}(x)$ and $B_{\frac{r}{2}}(y)$ are open and contain $x$ and $y$, respectively. Suppose $z\in B_{\frac{r}{2}}(x)\cap B_{\frac{r}{2}}(y)$. Applying the triangle inequality

$$
  d(x, y) \leq d(x, z) + d(z, y) < \frac{r}{2} + \frac{r}{2} = r
$$

creates a contradiction. Hence $B_{\frac{r}{2}}(x) \cap B_{\frac{r}{2}}(y) = \emptyset$ (disjoint). Since $x$ and $y$ are arbitrarily chosen, we conclude that $(X, d)$ is a Hausdorff space.
</details>
</MathBox>

## Convergence

<MathBox title='Convergent sequence' boxType='definition'>
Let $(X, d)$ be a metric space. A sequence $\left( x_n \in X \right)_{n \in \N}$ of points in $X$ is convergent if there is $\tilde{x}\in X$ with 

$$
  \forall \epsilon > 0 \quad \exists N \in \N \quad \forall n \geq N : d\left(x_n, \tilde{x} \right) < \epsilon
$$

A limit is denoted $x_n \xrightarrow{n\to\infty} \tilde{x}$ or $\lim_{n\to\infty} x_n = \tilde{x}$. Equivalently this can be stated as

$$
  x_n \xrightarrow{n\to\infty} \tilde{x} \iff d(x_n, \tilde{x})\xrightarrow{n\to\infty} 0
$$
</MathBox>

<MathBox title='Uniqueness of limits' boxType='proposition'>
Let $(X, d)$ be a metric space, and let $\left( x_n \in X \right)_{n \in \N}$ be a sequence of points in $X$. If $x_n \xrightarrow{n\to\infty} x$ and $x_n \xrightarrow{n\to\infty} y$ for $x,y\in X$, then $x = y$. 

<details>
<summary>Proof</summary>

Let $\epsilon > 0$, then

$$
\begin{gather*}
  \exists M \in \N \; \forall n \geq M: \; d(x_n, x) < \frac{\epsilon}{2} \\
  \exists N \in \N \; \forall n \geq N: \; d(x_n, y) < \frac{\epsilon}{2}
\end{gather*}
$$

Let $n \geq \max(N, \tilde{N})$, and apply the triangle inequality

$$
  d(x, y) \leq d(x, x_n) + d(x_n, y) < \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon
$$

Hence $d(x, y) \xrightarrow{n\to\infty} 0$ implying $x = y$
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $(X, d)$ be a metric space. A subset $A \subseteq X$ is closed if and only if for every convergent sequence $\left(a_n \in A \right)_{n \in \N}$ of points in $A$, the limit is in $A$, i.e. $\lim_{n\to\infty} a_n \in A$.

<details>
<summary>Proof</summary>

Suppose $A$ is closed and that $\left(a_n \in A \right)_{n \in \N}$ is a sequence of points in $A$ such that $a_n \xrightarrow{n\to\infty} x \in X$. Suppose $x\in A^c$. Since $A^c$ is open, $x^n \in A^c$ for sufficiently large $n$, giving a contradiction. Hence $x\in A$. Conversely, suppose $A$ is not closed. Then $A^c$ is not open and there exists $x\in A^c$ with the property that every neighbourhood of $x$ has points in $A$. Note that for each $n\in\N$ there exist $x_n \in B_{\frac{1}{n}}(x)$ with $x_n \in A$. However, clearly $x_n \xrightarrow{n\to\infty} x$, giving a contraction.
</details>
</MathBox>

## Completeness (Cauchy sequence)

<MathBox title='Bounded subset' boxType='definition'>
Let $(X, d)$ be a metric space. A subset $E\subseteq X$ is bounded if for every $x\in X$ there is $M > 0$ such that $d(p,x) < M$ for all $p\in E$.
</MathBox>

<MathBox title='Cauchy sequence' boxType='definition'>
Let $(X, d)$ be a metric space. A sequence $\left( x_n \in X \right)_{n \in \N}$ is called a Cauchy sequence if 

$$
  \forall \epsilon > 0 \; \exists N \in \N: \forall n, m \geq N: \; d(x_n, x_m) < \epsilon
$$

A metric space is complete if all Cauchy sequences converge.
</MathBox>

<MathBox title='Convergent sequences are Cauchy' boxType='proposition'>
Let $(X, d)$ be a metric space. If a sequence $\left( x_n \in X \right)_{n \in \N}$ converges, then the sequence is Cauchy.
<details>
<summary>Proof</summary>

Assume the sequence $\left( x_n \in X \right)_{n\in\N}$ converges to $x\in X$. Let $\epsilon > 0$, then there exists $N\in\N$ such that $d(x_n, x) < \frac{\epsilon}{2}$ for every $n \geq N$. If $m, n > N$, then by the triangle inequality

$$
  d(x_m, x_n) \leq d(x_m, x) + d(x_n, x) < \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon
$$

Hence $\left( x_n \right)_{n\in\N}$ is Cauchy.
</details>
</MathBox>

<MathBox title='Cauchy sequences are bounded' boxType='proposition' tag='proposition-6'>
A Cauchy sequence in a metric space is bounded.

<details>
<summary>Proof</summary>

Let $(x_k)_{k\in\N_+}$ be a Cauchy sequence in a metric space $(X, d)$. For $\epsilon = 1$ there is $N\in\N$ such that $d(x_n, x_m) < 1$ for all $n, m \geq N$. For a fixed $x\in X$ and any $m \geq N we have

$$
  d(x_n, x) \leq d(x_n, x_m) + d(x_m, x) \leq 1 + d(x_m, x),\; n\geq N
$$

Set

$$
  M = \max\Set{d(x_1,x),\dots, d(x_{N-1},x), 1 + d(x_m,x)}
$$

Then $d(x_k, x) \leq M$ for all $k\in\N$.
</details>
</MathBox>

<MathBox title='Complete sets are closed' boxType='proposition'>
Let $(X, d)$ be a metric space. A complete set $A\subseteq X$ is also closed.

<details>
<summary>Proof</summary>

Assume the sequence $\left( a_n \in A \right)_{n\in\N}$ converges to $x \in X$. Then $\left( a_n \in A \right)_{n \in \N}$ is a Cauchy sequence, and by completeness $x\in A$. Hence $A$ is closed.
</details>
</MathBox>

<MathBox title='Cauchy sequences with a convergent subsequence are convergent' boxType='proposition' tag='proposition-8'>
Let $(X, d)$ be a metric space, and let $\left( x_n \in X \right)_{n\in\N}$ be a Cauchy sequence. If there is a subsequence $\left( x_{n_k} \right)_{k\in\N}$ with $x_{n_k}\xrightarrow{k\to\infty} x \in X$, then $x_n \xrightarrow{n\to\infty} x$.

<details>
<summary>Proof</summary>

Assume the subsequence $\left( x_{n_k} \in A \right)_{k\in\N}$ converges to $x \in X$. Let $\epsilon > 0$, then there exists $j\in\N$ such that $d\left(x_{n_k}, x) < \frac{\epsilon}{2}$. Since $\left( x_n \right)_{n\in\N}$ is Cauchy, there exists $N\in\N$ such that for $d(x_m, x_n) < \frac{\epsilon}{2}$ for $m, n \geq N$. Choose $k\in\N$ such that $k \geq j$ and $n_k \geq N$. By the triangle inequality

$$
  d(x_m, x) \leq d\left(x_m, x_{n_k} \right) + d\left(x_{n_k}, x \right) \leq \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon
$$

Hence $x_n \xrightarrow{n\to\infty} x$.
</details>
</MathBox>

## Continuity

<MathBox title='Continuity' boxType='definition'>
Given two metric spaces $(X, d_X)$ and $(Y, d_Y)$, a function $f: X \to Y$ is continuous if $f^{-1}[B] \subseteq X$ is open for all open sets $B \subseteq Y$. A function $f$ is called sequentially continuous if for all $\tilde{x} \in X$ and $\left( x_n \right)_{n \in \N} \subseteq X$ with $x_n \xrightarrow{n\to\infty} \tilde{x}$ holds $f(x_n) \xrightarrow{n\to\infty} f(\tilde{x})$. For metric spaces, the two continuity concepts are equivalent. 
</MathBox>

### Uniform continuity

<MathBox title='Point-wise continuity' boxType='definition'>
Given two metric spaces $(X, d_X)$ and $(Y, d_Y)$, a function $f: X \to Y$ is continuous at $x\in X$ if

$$
  \forall \epsilon > 0 \; \exists \delta > 0 : y\in X \land d_X(x, y) < \delta \implies d_Y(f(x), f(y)) < \epsilon
$$

<details>
<summary>Proof</summary>

The definition of point-wise continuity is equivalent with the topoligical definition of continuity. Suppose $f$ is continuous at $x$. For $\epsilon > 0$, the open ball $B_{d_Y}(f(x), \epsilon)$ is a neighbourhood of $f(x)$ and hence $U = f^{-1}\left[ B_{d_Y}(f(x), \epsilon) \right]$ is a neighbourhood of $x$. Thus there exists $\delta > 0$ such that $B_{d_Y}(x, \delta) \subseteq U$. Conversely, suppose $V$ is a neighbourhood of $f(x)$. Then there exists $\epsilon > 0$ such that $B_{d_Y}(f(x), \epsilon)\subseteq V$. By assumption, there exists $\delta > 0$ such that if $y\in B_{d_X}(x, \delta)$, then $f(y) \in B_{d_Y}(f(x), \epsilon)\subseteq V$. Hence $f^{-1}[V]$ is a neighbourhood of $x$.
</details>
</MathBox>

<MathBox title='Uniform continuity' boxType='definition'>
Given two metric spaces $(X, d_X)$ and $(X, d_Y)$, a map $f: X \to Y$ is uniformly continuous if for all $\epsilon > 0$ there exists a $\delta > 0$ such that for every $x, y \in X$ with $d_X(x, y) < \delta$, we have that $d_Y (f(x), f(y)) < \epsilon$. If $f$ is uniformly continuous, then $f$ is continuous (on $X$).
</MathBox>

### Equicontinuity

<MathBox title='Equicontinuity' boxType='definition'>
Let $F \subseteq C(X, Y)$ be a family of continuous maps from $X$ to $Y$. Then $F$ is uniformly equicontinuous if for all $\epsilon > 0$, there exists a $\delta > 0$ such that $d_Y (f(x), f(y)) < \epsilon$ for all $f \in F$ and all $x, y \in X$ such that $d_X (x, y) < \delta$. Equivalently 

$$
  \sup_{f \in F} d_Y (f(x), f(y)) \xrightarrow{d_X (x, y)\to 0} 0
$$
</MathBox>

### Hölder continuity

<MathBox title='Hölder continuity' boxType='definition'>
Given two metric spaces $\left( X, d_X \right)$ and $\left( Y, d_Y \right)$, a function $f: X\to Y$ is called Hölder continuous with exponent $\alpha\in(0,\infty)$ if there is $C\in(0, \infty)$ such that, for all $x, \tilde{x}\in X$

$$
  d_Y\left( f(x), f\left(\tilde{x}\right) \right) \leq Cd_X\left( x, \tilde{x} \right)^\alpha
$$

For any $\alpha > 0$, the Hölder condition implies that $f$ is uniformly continuous. If $\alpha = 1$, then function satisfies the Lipschitz condition.
</MathBox>

<MathBox title='Contraction' boxType='definition'>
Given two metric spaces $\left( X, d_X \right)$ and $\left( Y, d_Y \right)$, a function $f: X\to Y$ is a contraction if there is $C\in(0, 1)$ such that, for all $x, \tilde{x}\in X$

$$
  d_Y\left( f(x), f\left(\tilde{x}\right) \right) \leq Cd_X\left( x, \tilde{x} \right)
$$

Contractions have Hölder exponent $\alpha = 1$, and are thus uniformly continuous.
</MathBox>

<MathBox title='Banach fixed-point theorem' boxType='theorem'>
Let $\left( X, d \right)$ be a complete metric space. Then any contraction $f:X\to X$ has a unique fixed point. That is, there exists $x*\in X$ with $f(x*) = x*$. Alternatively, let $x_0\in X$ and recursively define $x_n = f(x_{n-1})$ for $n\in\N$. Then $x_n \xrightarrow{n\to\infty} x*$.  
</MathBox>

### Lipschitz continuity

<MathBox title='Lipschitz continuity' boxType='definition'>
Given two metric spaces $\left( X, d_X \right)$ and $\left( Y, d_Y \right)$, a function $f: X\to Y$ is called Lipschitz continuous if there exists $C\in (0, \infty)$ such that, for all $x, \tilde{x}\in X$

$$
  d_Y\left( f(x), f\left(\tilde{x}\right) \right) \leq Cd_X\left( x, \tilde{x} \right)
$$

The Lipschitz condition ensures uniform continuity, as it corresponds with a Hölder exponent $\alpha > 0$.
</MathBox>

## Compactness and boundedness

<MathBox title='Bounded set' boxType='definition'>
Let $(X, d)$ be a metric space. A subset $A \subseteq X$ is bounded if there exists $r > 0$ such that $d(x, y) \leq r$ for all $x, y \in X$. The diameter of $A$ is defined as

$$
  \mathrm{diam}(A) := \inf\Set{ r > 0 | d(x, y) < r \; \forall x,y \in A }
$$

So $A$ is bounded if and only if $\mathrm{diam}(A) < \infty$.
</MathBox>

<MathBox title='Totally bounded set' boxType='definition'>
Let $(X, d)$ be a metric space. A subset $A \subseteq X$ is totally bounded if for every $r > 0$ there is a finite cover of $A$ with open balls of radius $r$.
</MathBox>

<MathBox title='' boxType='proposition'>
Let $(X, d)$ be a metric space. If $A \subseteq X$ is totally bounded, then $A$ is bounded.
<details>
<summary>Proof</summary>

If $A$ is totally bounded, there exists a finite cover of $A$ with open unit balls (radius $1$). Let $C$ denote the center of the balls, and let $c = \max\Set{ d(u, v) | u, v\in C }$ be the maximum distance between two centers. Since $C$ is finite, $c<\infty$. Now let $x, y\in A$. Since the balls cover $A$, there exists $u, v\in C$ with $x\in B_1(u)$ and $y\in B_1(v)$. By the triangle inequality

$$
  d(x, y) \leq d(x, u) + d(u, v) + d(v, y) \leq 2 + c
$$

This shows that $\mathrm{diam}(A) < \infty$, hence $A$ is bounded.
</details>
</MathBox>

<MathBox title='Sequential compactness' boxType='definition'>
Let $(X, d)$ be a metric space. A subset $A \subseteq X$ is (sequentially) compact if for each sequence $\left( x_n \right)_{n \in \N} \subseteq A$ there is a convergent subsequence $\left( x_{n_k} \right)_{k \in \N}$ with $\tilde{x} := \lim_{k\to\infty} x_{n_k} \in A$.
</MathBox>

<MathBox title='Compactness' boxType='definition'>
Let $(X, d)$ be a metric space and $J\subseteq\N$ an index set. A collection $(G_\alpha)_{\alpha\in J}$ of open sets is an open cover of a set $E\subset X$ if

$$
  E \subset \bigcup_{\alpha\in J} G_\alpha
$$

A set $E\subset X$ is compact if every open cover of $E$ has finite subcover, i.e. there is a finite subset $J' \subset J$ of such that

$$
  E \subset \bigcup_{\alpha\in J'} G_\alpha
$$


A subset $A \subseteq X$ is (sequentially) compact if for each sequence $\left( x_n \right)_{n \in \N} \subseteq A$ there is a convergent subsequence $\left( x_{n_k} \right)_{k \in \N}$ with $\tilde{x} := \lim_{k\to\infty} x_{n_k} \in A$.
</MathBox>

<MathBox title='Lebesgue number' boxType='definition'>
Let $(X,d)$ be a metric space. An $\epsilon_0 \geq 0$ is a Lebesgue number of an open cover $(G_\alpha)_{\alpha\in J}$ if for all $x\in X$ and for all $0\leq\epsilon < \epsilon_0$, the ball $B(x,\epsilon)$ is contained in some $G_\alpha$.
</MathBox>

<MathBox title='Characteristics of compactness' boxType='proposition' tag='proposition-10'>
Let $(X,d)$ be a metric space. The following are equivalent:
1. $X$ is compact.
2. Every collection $\mathcal{C}$ of closed subsets of $X$ with finite intersection property has a nonempty intersection.
3. $X$ is sequentially compact.
4. $X$ is totally bounded and every open cover has a positive Lebesgue number.

<details>
<summary>Proof</summary>

**(1)$\implies$(2):** Suppose $X$ is compact. Let $\mathcal{C} = (F_\alpha)_{\alpha\in J}$ be a collection of closed sets with the finite intersection property. If $\bigcap_{\alpha\in J} F_\alpha = \emptyset$, then $(F_\alpha^c)_{\alpha\in J}$ is an open cover of $X$. By compactness of $X$, there is a finite subcover $(F_{\alpha_k}^c)_{k=1}^n$ of $X$ which implies that $(F_{\alpha_k})_{k=1}^n$ has empty intersection. This contrdiction implies that $\bigcap_{\alpha\in J} F_\alpha \neq\emptyset$.

**(2)$\implies$(3):** Let $(x_k)_{k\in\N_+}$ be a sequence in $X$. For each $n\in\N$, let $B_n = \Set{x_n, x_{n+1},\dots}$. Then $(\overline{B}_n)_{n\in\N_+}$ is a collection of closed sets with the finite intersection property. By hypothesis, the intersection $\bicap_{n\in\N_+} \overline{B}_n$ is nonempty.

Let $x\in\bigcap_{n\in\N_+} \overline{B}_n$. For each $i\in\N$, choose $x_{k_i} \in B(x, 1/i)$ with $k_i < k_{i+1}$. Then $(x_{k_i})_{i\in\N_+}$ is a subsequence that converges to $x$.
</details>
</MathBox>

<MathBox title='' boxType='proposition' tag='proposition-11'>
A closed subset of a compact set is compact.

<details>
<summary>Proof</summary>

Let $F$ be a closed subset of a compact set $K$. For each open covering $\mathcal{G} = (G_\alpha)_{\alpha\in J}$ of $F$, the collection $\mathcal{G}\cup F^c$ is an open covering of $K$. Since $K$ is compact there is a finite subcover $\Set{F^c, G_{\alpha_1},\dots,G_{\alpha_n}}$ of $K$. The finite collection $\Set{G_{\alpha_i}}_{i=1}^n$ is a finite subcover of $F$, making $F$ compact.
</details>
</MathBox>

<MathBox title='' boxType='proposition' tag='proposition-12'>
A compact subspace of a metric space is bounded and closed.

<details>
<summary>Proof</summary> 

Let $A \subseteq X$ be a compact subspace of a metric space $X$, and let $\left(x_n \right)_{n \in \N} \subseteq A$ be convergent with limit $\tilde{x} \in X$. Because $A$ is compact, there is a convergent subsequence $\left( x_{n_k} \right)_{k \in \N}$ with limit $\tilde{\tilde{x}} \in A$. Furthermore, since limits are unique, then $\tilde{x} = \tilde{\tilde{x}} \in A$. This shows that $A$ is closed.

Proving that compactness implies boundedness can be done by contraposition. Assuming that $A$ is not bounded, then for a given $a \in A$, there are $x_n \in A$ with $d(a, x_n) > n$. For any subsequence $\left( x_{n_k} \right)_{k \in \N}$ and any point $b \in A$ we have

$$
\begin{gather*}
  n_k < d(a, x_{n_k}) \leq d(a, b) + d(b, x_{n_k}) \\
  \implies n_k - d(a, b) \leq d(b,  x_{n_k}) \\
  \implies \lim_{k \to \infty} d(b,  x_{n_k}) \neq 0 \quad \forall b \in A
\end{gather*}
$$

Hence, $A$ is not compact. 
</details>
</MathBox>

<MathBox title='Heine-Borel theorem' boxType='theorem'>
For $\R^n$ with the standard Euclidean metric, if a subset $K$ is bounded and closed, then $K$ is compact.

<details>
<summary>Proof</summary>

We show first that every $n$-cell $[a,b]$ is compact. Suppose $\mathcal{G} = (G_\alpha)_{\alpha\in J}$ is an open cover of $I_1 = [a,b]$ that has no finite subcover. Let $c = (a+b)/2$ be the midpint of $a$ and $b$, i.e. $c_k = (a_k + b_k)/2$ for $k = 1,\dots,n$. The intervals $[a_k, c_k]$ and $[c_k, b_k]$ subdivide $I_1$ into $2^n$ $n$-cells. At least one of these $2^n$ $n$-cells is not covered by a finite subcollection of $\mathcal{G}$. Let $I_2$ be one of these cells.

We subdivide $I_2$ and repeat the argument to obtain a sequence of nested $n$-cells $(I_k)_{k\in\N_+}$, i.e. $I_{k+1}\subset I_k$ for all $k$, where each $I_k$ is not covered by a finite subcollection of $\mathcal{G}$. Furthermore, for $x,y\in I_k$ we have $\lVert x - y \rVert_2 \leq 2^{-n} \lVert b - a\rVert_2$.

We form a sequence $(x_k)_{k\in\N_+}$ by chooseing $x_k \in I_k$ for all $k$. This sequence is Cauchy because for $\epsilon > 0$ the choice of $N\in\N$ satisfying $2^{-N} \lVert b - a \rVert < \epsilon$ gives $\lVert x_n - x_m \rVert_2 \leq 2^{-N} \lVert b - a \rVert < \epsilon$ whenever $n, m \geq N$.Since $\R^n$ is complete, the Cauchy sequence $(x_k)_{k\in\N_+}$ converges to $x\in\R^n$.

Since $\mathcal{G}$ is an open cover of $[a,b]$, there is some $\alpha\in J$ such that $x\in G_\alpha$. Since $G_\alpha$ is open and $x\in G_\alpha$, there is $r > 0$ such that $B(x,r)\subset G_\alpha$. For this $r > 0$, there is $N\in\N$ such that $2^{-N} \lVert b - a\rVert_2 < r$. Since $\lVert x - y \rVert_2 \leq 2^{-N} \lVert b - a \rVert_2 < r$ for all $x, y \in I_N$ we obtain $I_N \subset B(x,r) \subset G_\alpha$.

Since $I_n \subset I_N$, we obtain $I_n \subset G_\alpha$ for all $n \geq N$. This gives a finite subcover of $I_n$ for all $n\geq N$, which is a contradiction. Thus, $[a,b]$ is compact.

Next we show that any closed and bounded subset $K \subset\R^n$ is compact. The boundedness of $K$ implies that there is an $n$-cell $[a,b]$ such that $K\subset[a,b]$. Since $K$ is closed, it is a closed subset of the compact $[a,b]$. By Proposition $\ref{proposition-11}$, the set $K$ is compact.
</details>
</MathBox>

<MathBox title='Generalized Heine-Borel theorem' boxType='theorem'>
A metric space is compact if and only if it is complete and totally bounded.

<details>
<summary>Proof</summary>

Suppose $(X,d)$ is a compact metric space. By Proposition $\ref{proposition-10}$, the metric space $X$ is totally bounded. To show that $X$ is complete, let $(x_k)_{k\in\N_+}$ be a Cauchy sequence in $X$. By Proposition $\ref{proposition-10}$, the sequence $(x_k)_{k\in\N_+}$ has a convergent subsequence. By Proposition $\ref{proposition-8}$, any Cauchy sequence with a convergent subsequence is convergent. Thus, the Cauchy sequence $(x_k)_{k\in\N_+}$ converges in $X$, making $X$ complete.

Conversely, suppose $X$ is complete and totally bounded. To get compactness of $X$, we show that $X$ is sequentially compact and use Proposition $\ref{proposition-10}$. Suppose $(x_k)_{k\in\N_+}$ is a sequence in $X$. Let $E = \Set{x_k}_{k\in\N_+}$. If $E$ is finite, then $(x_k)_{k\in\N_+}$ has a convergent subsequence.

Next, suppose $E$ is infinite. Since $X$ is totally bounded, it can be covered with finitely many open balls of radius $1$. One of these balls, call it $B_1$, contains infinitely many elements of $E$. Now cover $X$ with open balls of radius $1/2$. One of these balls, call it $B_2$, intersect $B_1$ and contains infinitely many elements of $E$. 

Repeating this process gives a sequence of nested open balls $((B_i))_{i\in\N_+}$ where the radius of $B_i$ is $1/i$. For each $i\in\N$, choose $x_{k_i} \in B_i$, where $k_i < k_{i+1}$ for all $i$. Thus, $(x_{k_i})_{i\in\N_+}$ is a subsequence that is Cauchy because $d(x_{k_j}, x_{k_l}) < 1/i$ for all $j, k \geq i$. Since $X$ is complete, the Cauchy sequence converges, and so $(x_k)_{k\in\N_+}$ has a convergent subsequences. Hence, $X$ is sequentially compact.
</details>
</MathBox>

## Continuity

<MathBox title='Continuous images of compact subsets are compact' boxType='proposition'>
Let $X$ and $Y$ be metric spaces. If $f:X\to Y$ is continuous and $K$ is a compact subset of $X$, then $f(K)$ is a compact subset of $Y$.

<details>
<summary>Proof</summary>

Let $(G_\alpha)_{\alpha\in J}$ be an open covering of $f(K)$. By the continuity of $f$, the collection $(f^{-1} (G_\alpha))_{\alpha\in J}$ is an open covering of $K$. Since $K$ is compact, there exists a finite subcovering $(f^{-1} (G_\alpha))_{\alpha\in J'}$ for a finite $J' \subset J$. Thus, $(G_\alpha)_{\alpha\in J'}$ is a finite subcover of $f(K)$ implying that $f(K)$ is compact.
</details>
</MathBox>

<MathBox title='Extreme value theorem' boxType='theorem'>
Let $X$ be a metric space. If $f:X\to\R$ is continuous and $K\subset X$ is nonempty and compact, then $f(K)$ contains its infimum and supremum.

<details>
<summary>Proof</summary>

The image $f(K)$ is compact in $\R$, so that by Proposition $\ref{proposition-12}$, the set $f(K)$ is closed and bounded. Because $f(K)$ is bounded, the infimum and supremum of $f(K)$ are both finite. Let $M$ be the supremum of $f(K)$. Then there exists a "maximizing" sequence $(x_k)_{k\in\N_+} \subset f(K)$ such that $f(x_k) \xrightarrow{k\to\infty} M$. Since $K$ is compact, the sequence $(x_k)_{k\in\N_+}$ has a convergent subsequence $(x_k)_{k\in\N_+}$ with limit $x\in K$. Since $f(x_k) \xrightarrow{k\to\infty} M$, the continuity of $f$ implies that $M = f(x)\in f(K)$. A similar argument applies to the infimum $m$ of $f(K)$.
</details>
</MathBox>

<MathBox title='' boxType='proposition' tag='proposition-14'>
Let $X$ and $Y$ be metric spaces. If $K$ is a compact subset of $X$ and $f:K\to Y$ is continuous, then $f$ is uniformly continuous on $K$.

<details>
<summary>Proof</summary>

Let $\epsilon > 0$. Since $f:K\to Y$ is continuous, there is for each $x\in K$ a $\delta_x > 0$ such that $\rho(f(x), f(y)) < \epsilon/2$ when $d(x,y) < \delta_x$. The collection $(B(x, \delta_x /2))_{x\in K}$ is an open cover of $K$.

By compactness of $K$ there is a finite subcollecion $(B(x_k, \delta_{x_k} /2))_{k=1}^n$ that is a subcover of $K$. For any $y\in K$ there is $k\in\Set{1,\dots,n}$ such that $d(y, x_k) < \delta_{x_k}/2$.

Set $\delta = \min\Set{\delta_{x_1},\dots,\delta_{x_k}}$. For $y,z\in K$ with $d(y,z) < \delta/2$ we have

$$
  d(x_k, z) \leq d(x_k, y) + d(y, z) < \frac{\delta_{x_k}}{2} + \frac{\delta}{2} \leq \frac{\delta_{x_k}}{2} + \frac{\delta_{x_k}}{2} = \delta_{x_k}
$$

This implies that $\rho(f(x_k), f(z)) < \epsilon/2$, so that

$$
\begin{align*}
  \rho(f(y),f(z)) \leq& \rho(f(y),f(x_k)) + \rho(f(x_k),f(z)) \\
  \leq& \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon
\end{align*}
$$

Thus, $f$ is uniformly continuous on $K$.
</details>
</MathBox>

## Isometry

<MathBox title='Isometry' boxType='definition'>
Let $(X, d_X)$ and $(Y, d_Y)$ be two metric spaces. A function $f: X \to Y$ is called an isometry if for any $a, b \in X$

$$
  d_Y \left( f(a), f(b) \right) = d_X \left(a, b \right)
$$

That is, $f$ preserves distance. The metric spaces $(X, d_X)$ and $(Y, d_Y)$ are isometric if there exists a bijective isometry $f:X\to Y$.
</MathBox>

<MathBox title='' boxType='proposition'>
Let $(X, d_X)$ and $(Y, d_Y)$ be two metric spaces, and consider a function $f:X\to Y$. If function $f$ is isometric, then $f$ is injective and Lipschitz continuous.
<details>
<summary>Proof</summary> 

By definition, an isometry $f$ is Hölder continuous with $\alpha = 1$ and $C = 1$, and hence $f$ is also Lipschitz continuous. To show injectivity, note that for any $x, y\in X$ with $x \neq y$, then $d_Y(f(x), f(y)) = d(x, y) > 0$. Hence $f(x)\neq f(y)$, showing that $f$ is injective (one-to-one). 
</details>
</MathBox>

<MathBox title='Isometry is an equivalence relation' boxType='proposition'>
Isometry is an equivalence relation on metric spaces that satisifies, for metric spaces $(X, d_X)$, $(Y, d_Y)$ and $(Z, d_Z)$
1. reflexivity: $X$ is isometric to $X$
2. symmetry: if $X$ is isometric to $Y$, then $Y$ is isometric to $X$
3. transitivity: if $X$ is isometric to $Y$ and $Y$ is isometric to $Z$, then $X$ is isometric to $Z$
<details>
<summary>Proof</summary> 

1. The identity function $\operatorname{id}:X\to X$ is an isometry from $X$ to $X$.
2. if $X$ is isometric to $Y$, there is a bijective isometry $f:X\to Y$ with an isometric inverse $f^{-1}: Y\to X$. Hence $Y$ is isometric to $X$.
3. if $X$ is isometric to $Y$ and $Y$ is isometric to $Z$, there are two bijective isometries $f: X\to Y$ and $g:Y\to Z$. Then the composition $g\circ f: X\to Z$ is an isometry from $X$ to $Z$.
</details>
</MathBox>

# Normed space

<MathBox title='Normed space' boxType='definition'>
Let $\mathbb{F} \in \Set{\R, \mathbb{C} }$ be a field of numbers and let $X$ be a $\mathbb{F}$-vector space. A map $\|\cdot\| : X \mapsto [0, \infty)$ is called a norm if it satisfies

1. $\lVert x\rVert = 0 \iff x = 0$ (positive definite)
2. $\lVert \lambda x\rVert = |\lambda| \lVert x\rVert, \quad \lambda \in \mathbb{F}$ (absolutely homogenous)
3. $\lVert x + y\rVert \leq \lVert x\rVert + \lVert y\rVert$ (triangle inequality)

The pair $(X, \lVert\cdot\|)$ is called a normed space. The norm $\| \cdot\rVert$ induces a metric by

$$
  d_{\lVert \cdot\rVert}(x, y) := \lVert x - y\rVert
$$

which makes the normed space $(X, \|\cdot\|)$ into a metric space $(X, d_{\|\cdot\|})$.
</MathBox>

The norm $\lVert \cdot\rVert$ is a continuous map. Considering the sequence $\left( x_n \right)_{n\in\N} \subseteq X$ with limit $\tilde{x} \in X$, then by the triangle inequality

$$
\begin{gather*}
  \lVert x_n\rVert = \lVert \left(x_n - \tilde{x}\right) + \tilde{x}\rVert \leq \lVert x_n - \tilde{x}\rVert + \lVert \tilde{x}\rVert = d \left(x_n, \tilde{x} \right) + \lVert \tilde{x}\rVert \\
  \implies \lim_{n\to\infty} \lVert x_n\rVert \leq \lVert \tilde{x}\rVert
\end{gather*}
$$

Conversely,

$$
\begin{gather*}
  \lVert \tilde{x}\rVert = \lVert \left(\tilde{x} - x_n \right) + x_n\rVert \leq \lVert \tilde{x} - x_n\rVert + \lVert x_n\rVert = d(\tilde{x}, x_n) +  \lVert x_n\rVert \\
  \implies \lVert \tilde{x}\rVert \leq \lim_{n\to\infty}  \lVert x_n\rVert
\end{gather*}
$$

<MathBox title='Banach space' boxType='definition'>
A normed space $(X,\lVert\cdot\|)$ is called a Banach space if $\left(X, d_{\| \cdot\rVert} \right)$ is a complete metric space. A Banach space is a complete normed space.
</MathBox>

<MathBox title='Closures of Banach subspaces are subspaces' boxType='definition'>
Let $X$ be a Banach space with a subspace $M\subseteq X$. The closure $\overline{M}$ is also a subspace of $X$.

<details>
<summary>Proof</summary>

For $x, y\in\overlin{M}$ there exists sequences $\Set{x_n}_{n\in\N}$ and $\Set{y_n}_{n\in\N}$ in $M$ such that $x_n \xrightarrow{n\to\infty} x$ and $y_n \xrightarrow{n\to\infty} y$. Then

$$
  \lVert (x_n + y_n) - (x + y) \rVert \leq \lVert x_n - x \rVert + \lVert y_n - y \rVert \xrightarrow{n\to\infty} 0
$$

so that $x + y \in\overline{M}$, and for $\alpha\in\mathbb{F}$

$$
  \lVert \alpha x_n - \alpha x \rVert = |\alpha|\cdot\lVert x_n - x\rVert \xrightarrow{n\to\infty} 0
$$

so that $\alpha x\in\overline{M}$. Hence, $\overline{M}$ is a subspace.
</details>
</MathBox>

## Asymptotic behaviour

<MathBox title='Bachmann-Landau notation' boxType='definition'>
Let $X, Y$ be normed spaces, and let $f:X\to Y$ and $\alpha: X\to [0,\infty)$ be functions. We write

$$
  f(x) = O(\alpha(x)),\; x\to a \in X
$$

if there exists some constants $C > 0$ and $r > 0$ such that for $a\in X$

$$
  \lVert f(x) \rVert_Y \leq C\alpha (x),\; \forall x\in B(a,r) \subset X
$$

We write

$$
  f(x) = o(\alpha(x)),\; x\to a \in X
$$

if for every $\epsilon > 0$ there is $\delta > 0$ such that

$$
  \frac{\lVert f(x) \rVert_Y}{\alpha (x)} \leq \epsilon,\; \forall x\in B(a,\delta) \subset X
$$

or equivalently

$$
  \lim_{x\to a} \frac{\lVert f(x) \rVert_X}{\alpha (x)} = 0
$$

Similarly, we write

$$
  f(x) = O(\alpha(x)),\; x\to\infty
$$

if there exists constants $C > 0$ and $K > 0$ such that $\lVert f(x) \rVert_Y < C \alpha(x)$ for all $x\in X$ with $\lVert x \rVert_X > K$.

We write 

$$
  f(x) = o(\alpha(x)),\; x\to\infty
$$

if for every $\epsilon > 0$ there is $K > 0$ such that

$$
  \frac{\lVert f(x) \rVert_Y}{\alpha (x)} \leq \epsilon
$$

for all $x \in X$ with $\lVert x \rVert$, or equivalently

$$
  \lim_{\lVert x \rVert_X \to\infty} \frac{\lVert f(x) \rVert_Y}{\alpha(x)} = 0
$$
</MathBox>

## Dual space

<MathBox title='Dual space' boxType='definition'>
The dual space $X'$ of a normed space $X$ is the space of all continuous (bounded) linear operators (functionals) $\ell : X \to \mathbb{F}$. 
</MathBox>

<MathBox title='Dual spaces are Banach spaces' boxType='proposition'>
Let $X'$ be the dual space of a normed space $X$. The dual space itself is a normed space $\left(X', \lVert \cdot\rVert_{X \to \mathbb{F}}\right)$, which is also a Banach space.

<details>
<summary>Proof</summary> 

Let $\left( \ell_k \right)_{k\in\N} \subseteq X'$ be a Cauchy sequence, i.e.

$$
  \forall \epsilon > 0 \quad \exists N \in \N \quad \forall n, m \geq N : \lVert \ell_n - \ell_m\rVert_{X\to\mathbb{F}} < \epsilon
$$

Since the operator norm $\lVert \cdot\rVert_{X\to\mathbb{F}}$ is defined by a supremum, we know that for all $x \in X$ with $x\neq 0$

$$
\begin{gather*}
  \frac{1}{\lVert x\rVert_X} \left| \ell_n - \ell_m \right| \leq \lVert \ell_n - \ell_m\rVert_{X\to\mathbb{F}} < \epsilon \\
  \left| \ell_n - \ell_m \right| < \epsilon \lVert x\rVert_X
\end{gather*}
$$

Hence, $\left( \ell_k (x) \right)_{k\in\N} \subseteq \mathbb{F}$ is a Cauchy sequence for all $x \in X$, with the limit

$$
  \ell(x) := \lim_{k\to\infty} \ell_k (x)
$$

We then have to show that $\ell: X \to \mathbb{F}$ is linear and bounded and that $\lVert \ell_k - \ell\rVert_{X \to \mathbb{F}} \xrightarrow{k\to\infty} 0$. Boundedness follows from the Cauchy property of $\left( \ell_k \right)_{k\in\N}$:

$$
\begin{gather*}
  \lVert \ell_n\rVert_{X\to\mathbb{F}} \overset{\triangle}{\leq} \underbrace{\lVert \ell_n - \ell_N\rVert_{X\to\mathbb{F}}}_{< \epsilon} + \underbrace{\lVert \ell_N\rVert_{X\to\mathbb{F}}}_{:= C} \leq \epsilon + C \quad \forall n\geq N \\
  \implies \left| \ell(x) \right| = \left| \lim_{k\to\infty} \ell_k (x) \right| = \lim_{k\to\infty} \left| \ell_k (x) \right| \leq \lim_{k\to\infty} \underbrace{\lVert \ell_{k}\rVert_{X\to\mathbb{F}}}_{\leq \tilde{C}} \lVert x\rVert_X \\
  \implies \lVert \ell\rVert_{X\to\mathbb{F}} = \frac{1}{\lVert x\rVert_X} \left| \ell(x) \right| \leq \tilde{C} < \infty
\end{gather*}
$$

To show the convergence of the operator norm $\lVert \cdot\rVert_{X\to\mathbb{F}}$, we have the for $\epsilon > 0$ there is a $N \in \N$ such that for all $n, m \geq N$

$$
\begin{gather*}
  \frac{1}{\lVert x\rVert_X} \left| \ell_n (x) - \ell_m (x) \right| < \epsilon \\
  \implies \sup_{\overset{x\in X}{x\neq 0}} \frac{1}{\lVert x\rVert_X}  \left| \ell_n (x) - \lim_{m\to\infty} \ell_m (x) \right| = \sup_{\overset{x\in X}{x\neq 0}} \frac{1}{\lVert x\rVert_X}  \left| \ell_n (x) - \ell (x) \right| \leq \epsilon \\
  \implies \lVert \ell_n - \ell\rVert_{X\to\mathbb{F}} \leq \epsilon
\end{gather*}
$$
</details>
</MathBox>

<MathBox title='Hahn-Banach theorem' boxType='theorem'>
Let $\left(X, \lVert \cdot\rVert_X \right)$ be a normed space, $U \subset X$ a subspace and $\ell \in U^*$ a continuous linear functional on $U$. Then there exists a linear functional $\tilde{\ell} \in X^*$ that extends $\ell$ with $\tilde{\ell}(u) = \ell(u)$ for all $u \in U$ and $\lVert \tilde{\ell}\rVert_{X^*} = \lVert \ell\rVert_{U^*}$.
</MathBox>

## Arzelà-Ascoli theorem

<MathBox title='Arzelà-Ascoli theorem' boxType='theorem'>
Consider the Banach space $\left(C([a, b]), \lVert \cdot\rVert_\infty \right)$ of continuous functions on an interval $I = [a, b] \in \R$ with the supremum norm $\lVert f\rVert_\infty := \sup \Set{ |f(t)| | t\in[a, b] }$. A subset $F \in C([a, b])$ is compact if and only if $F$ is closed, bounded and uniformly equicontinuous.
</MathBox>

## Lebesgue space ($L^p$)

<MathBox title='$L^p$ space' boxType='definition'>
Let $(X,\mathcal{A},\mu)$ be a measure space and suppose $f: X\to\R$ is measurable. For $p\in(0,\infty)$ we define the $p$-norm

$$
  \lVert f\rVert_p := \left( \int_X |f|^p \;\d\mu \right)^{1/p}
$$

and for $p=\infty$ we define the essential supremum norm

$$
  \lVert f\rVert_\infty := \inf\Set{b\in[0,\infty] : |f|\overset{\textrm{a.e.}}{\leq} b }
$$

The function $f$ is called $p$-integrable if $\| f\|_p < \infty$. The set of all $p$-integrable functions is denoted

$$
  L^p (X,\mathcal{A},\mu) := \Set{ f:X\to\R: \| f\|_p < \infty } \subseteq \mathcal{M}(X,\R)
$$

and is called a Lebesgue space. Here $\mathcal{M}(X,\R)$ denotes the collection of all measurable functions from $X$ into $\R$.
</MathBox>

<MathBox title='$L^p$ space are normed vector spaces' boxType='proposition'>
For $p\in[1,\infty]$, let $\mathcal{L}^p = \Set{[f] : f\in L^p }$ and define $\lVert[f]\|_p =\rVert f \|_p$ for $f\in\mathcal{M}(X,\R)$ where $\mathcal{M}(X,\R)$ is the collection of measurable functions $f:X\to\R$. Then $\mathcal{L}^p \subseteq\mathcal{M}\setminus\equiv$ and $\|\cdot\|$ is a norm on $\mathcal{L}^p$ that satisfies for $f,g\in L^p$ and $c\in\mathcal{R}$
1. $\| f\|_p \geq 0$ and $\| f\|_p = 0$ if and only if $f\equiv 0$ (positive definite)
2. $\| cf\|_p = |c|\cdot\| f\|_p$ (absolutely homogenous)
3. $\lVert f+g\|_p \leq\rVert f\lVert_p +\rVert p\|_p$ (triangle inequality)

For every $p\in[0,\infty]$, the Lebesgue space $L^p$ is a Banach space.
<details>
<summary>Proof</summary> 

That $\mathcal{L}^p \subseteq\mathcal{M}\setminus\equiv$ follows immediately from that $L^p\subseteq\mathcal{M}$. It remains to show that $\|\cdot\|_p$ satisfies the norm properties.
1. The predicate $\lVert f\|_p \geq 0$ follows from the definitions. For the second part, we first consider the case $p\in(0,\infty)$. Trivially, $\| 0\|_p = 0$. Conversely, if $\| f\|_p = 0$ then $\int_X\rVert f\|^p\;\d\mu = 0$ and hence $|f|^p \overset{\textrm{a.e.}}{=} 0$ and $f \overset{\textrm{a.e.}}{=} 0$. For the case $p = \infty$, it follows trivially that $\| 0\|_\infty = 0$. Conversely, suppose $\| f\|_\infty = 0$. Then for each $n\in\N$ there exists $b_n\in[0,\infty)$ with $b_n \xrightarrow{n\to\infty}0$ and $|f\overset{\textrm{a.e.}\leq b_n$, resulting in $f\overset{\textrm{a.e.}}{=}0$.
2. For $p\in(0,\infty)$ we have

$$
\begin{gather*}
  \| cf\|^p \int_X |cf|^p\;\d\mu = |c|^p \int_X |f|^p\;\d\mu = |c|^p\cdot\| f\|^p \\
  \iff \| cf\| = |c|\cdot\| f\|_p
\end{gather*}
$$

For $p=\infty$, the result is trivially true if $c=0$. For $c\neq 0$, note that $b\in[0,\infty]$ is an essential bound of $|f|$  if and only if f $|c|b$ is an essential bound of $|cf|$.
3. This follows from the Minkowski inequality.
</details>
</MathBox>

### Hölder's inequality

<MathBox title='Conjugate indices' boxType='definition'>
Two indices $p,q\in(1,\infty)$ are called conjugate if $\frac{1}{p} + \frac{1}{q} = $. Note that

$$
  q = \frac{p}{p-1} = \frac{1}{1 - 1/p}
$$

so that $q\uparrow\infty$ as $p\downarrow 1$. Hence $1$ and $\infty$ are conjugate indices.
</MathBox>

<MathBox title="Young's inequality" boxType='theorem'>
If $x, y\in(0, \infty)$ and $p, q\in(1,\infty)$ are conjugate indices, then 

$$
  xy \leq \frac{1}{p}x^p + \frac{1}{q}y^q
$$

Equality occurs if and only if $x^p = y^p$

<details>
<summary>Proof</summary>

Fix $y\geq 0$ and define $f:(0,\infty)\to\R$ by

$$
\begin{align*}
  f(x) =& \frac{x^p}{p} + \frac{y^q}{q} - xy \quad x\geq 0 \\
  f'(x) =& x^{p-1} - y \\
  f''(x) =& (p-1)x^{p-2}
\end{align*}
$$

Solving for $f'(x) = 0$ shows that $f$ has a minimum at $x_0 = y^{1/(p-1)} = y^{q/p}$ as $f''(x_0) > 0$. Noticing that $q = \frac{p}{p-1}$, we get

$$
\begin{align*}
  f(x_0) =& \frac{y^{p/(p-1)}}{p} + \frac{y^q}{q} - y^{1/(p-1)}y \\
  =& y^q \left(\frac{1}{p} + \frac{1}{q}) - y^q \\
  =& y^q - y^q = 0
\end{align*}
$$

Since $x_0$ is a minima it follows that $xy\leq \frac{x^p}{p} + \frac{y^q}{p}$. 
</details>
</MathBox>

<MathBox title="Hölder's inequality" boxType='theorem'>
Suppose $f,g:X\to\R$ are measurable on $(X,\mathcal{A},\mu)$ and that $p$ and $q$ are conjugate indices. Then

$$
  \lVert fg\|_1 = \int_X |fg|\;\d\mu\leq\| f\rVert_p \cdot \lVert g\rVert_q
$$

If $(X,\mathcal{P}(X),\#)$ is a discrete measure, Hölder's inequality reduces to

$$
  \lVert xy\rVert_1 = \left(\sum_{i\in X} |x_i + y_i| \right) \leq \lVert x\rVert_p \cdot \lVert y\rVert_q
$$

<details>
<summary>Proof</summary>

The result follows trivially if $\| f\|_p = \infty$ or $\| g\|_\infty = \infty$, so assume $f\in L^p$ and $g\in L^q$. For the case $p = 1$ and $q = \infty$, note that $|g|\overset{\mu\textrm{-a.e.}{\leq}}\| g\|_\infty$, hence

$$
  \int_X |fg|\;\d\mu = \int_X |f|\cdot|g|\;\d\mu \leq\lVert g\|_\infty \int_X |f|\;\d\mu =\rVert f\lVert_1 \cdot\rVert g\|_\infty
$$

Consider the case where $p,q\in(1,\infty)$. By positive definiteness, the result holds if $\| f\|_p = 0$ or $\| g\|_q = 0$, so assume $\| f\|_p\geq 0$ and $\| g\|_q\geq 0$. By Young's inequality

$$
  |fg|\leq\frac{1}{p}|f|^p + \frac{1}{q}|g|^q
$$

Suppose first that $\lVert f\|_p =\rVert g\|_q = 1$. From the increasing monotonicity and linearity properties of the integral

$$
  \int_X |fg|\;\d\mu\leq \frac{1}{p}\int_X |f|^p \;\d\mu + \frac{1}{q}\int_X |g|^q \;\d\mu = \frac{1}{p} + \frac{1}{q} = 1
$$

For the general case where $\lVert f\|_p > 0$ and $\| g\|_q > 0$, let $f_1 = \frac{f}{\| f\|_p}$ and $g_1 = \frac{g}{\| g\|_q}$. Then $\| f_1\|_p =\rVert g_1\|_q = 1$ and $\| f_1g_1\|_q\leq 1$. By the scaling property of the norm

$$
  \| f_1g_1\|_1 = \frac{\| fg\|_1}{\| f\|_q\cdot\| g\|_q}\leq 1
$$
</details>
</MathBox>

### Minkowski's inequality

Minkowski's inequality produces the triangle inequality for the $p$-norm and establishes that $L^p$-spaces are normed vector spaces.

<MathBox title="Minkowski's inequality" boxType='theorem'>
Suppose $f,g:X\to\R$ are measurable on $(X,\mathcal{A},\mu)$ and that $p\in[1,\infty]$. Then

$$
  \lVert f + g\|_p = \left(\int_X |f + g|^p \;\d\mu\right)^{1/p} \leq\rVert f \lVert_p +\rVert g \|_p
$$

If $(X,\mathcal{P}(X),\#)$ is a discrete measure, Minkowski's inequality reduces to

$$
  \lVert x + y\rVert_p = \left(\sum_{i\in X} |x_i + y_i|^p\right)^{1/p} \leq \lVert x\rVert_p + \lVert y\rVert_p \quad
$$

<details>
<summary>Proof</summary>

The result is trivial if $\| f\|_p = \infty$ or $\| g\|_\infty = \infty$, so assume $f,g\in L^p$. When $p = 1$, the result is the triangle inequality

$$
\begin{align*}
  \lVert f + g\rVert_p =& \int_X |f + g|\;\d\mu \leq\int_X (|f| + |g|)\;\d\mu \\
  =& \int_X |f|\;\d\mu + \int_X |g|\;\d\mu = \lVert f\|_1 +\rVert g\|_1
\end{align*}
$$

For the case $p = \infty$, note that if $a,b\in[0,\infty]$ are an essential bounds for $f$ and $g$, respectively, then $a+b$ is an essential bound for $f + g$. Hence $\lVert f + g\Vert_\infty \leq\| f\|_\infty +\rVert g\|_\infty$.

Consider the case where $p\in(1,\infty)$ and let $q$ be the conjugate index to $p$. Then

$$
\begin{align*}
  |f+g|^p =& |f+g|^{p-1}\cdot|f+g| \leq |f+g|^{p-1}(|f|+|g|) \\
  =& |f+g|^{p-1}\cdot|f| + |f+g|^{p-1}\cdot|g|
\end{align*}
$$

Integrating over $X$ gives

$$
  \| f + g\|_p^p \leq \int_X |f+g|^{p-1}|f|\;\d\mu + \int_X |f+g|^{p-1}|g|\;\d\mu
$$

By Hölder's inequality

$$
\begin{align*}
  \int_X |f+g|^{p-1}|f|\;\d\mu &\leq \left\lVert |f+g|^{p-1} \right\|_q \cdot\rVert f\|_p \\
  \int_X |f+g|^{p-1}|g|\;\d\mu &\leq \left\lVert |f+g|^{p-1} \right\|_q \cdot\rVert g\|_p
\end{align*}
$$

Combining the inequalities gives

$$
  \lVert f + g\|_p^p \leq \left\| |f+g|^{p-1} \right\|_q (\| f\|_p +\rVert g\|_p)
$$

Since $\frac{1}{q} = \frac{p-1}{p}$ we get

$$
\begin{align*}
  \left\| |f+g|^{p-1} \right\|_q =& \left(\int_X |f+g|^{p-1}q \;\d\mu \right)^{1/q} \\
  =& \left(\int_X |f+g|^p \;\d\mu \right)^{(p-1)/p} = \lVert f+g\rVert_p^{p-1}
\end{align*}
$$

Hence we get

$$
\begin{gather*}
  \lVert f+g\rVert_p^p = \lVert f+g\rVert_p^{p-1}\left(\lVert f\|_p +\rVert g\|_p \right) \\
  \iff \lVert f + g\|_p \leq\rVert f \lVert_p +\rVert g \|_p
\end{gather*}
$$

**Discrete case:**
For $p = 1$, the ordinary triangle inequality applies

$$
  \lVert x + y\rVert_1 = \sum_{j=1}^\infty \left| x_j + y_j \right| \leq \sum_{j=1}^\infty \left| x_j \right| + \left| y_j \right| = \lVert x\rVert_1 + \lVert y\rVert_1
$$

For $p \in (1, \infty)$, we note $q = \frac{p}{p-1}$

$$
\begin{align*}
  \lVert x + y\rVert_p^p =& \sum_{j=1}^\infty \left| x_j + y_j \right|^p = \lim_{n\to\infty} \sum_{j=1}^n \left| x_j + y_j \right|^p \\
  \leq& \lim_{n\to\infty} \sum_{j=1}^n \left(\left| x_j \right| + \left| y_j \right| \right)^p
\end{align*}
$$

The summand can be written as

$$
\begin{align*}
  \left(\left| x_j \right| + \left| y_j \right| \right)^p =& \left(\left| x_j \right| + \left| y_j \right| \right) \left(\left| x_j \right| + \left| y_j \right| \right)^{p-1} \\
  =& \left| x_j \right| \left(\left| x_j \right| + \left| y_j \right| \right)^{p - 1} = \left| y_j \right| \left(\left| x_j \right| + \left| y_j \right| \right)^{p-1} \\
  \equiv& a_j b_j + c_j b_j
\end{align*}
$$

Applying Hölder's inequality $\lVert a b\rVert_1 + \lVert c b\rVert_1 \leq \lVert a\rVert_p \cdot \lVert b\rVert_{q} + \lVert c\rVert_p \cdot \lVert b\rVert_{q}$ with

$$
  \lVert b\rVert_{q} = \left( \sum_{j=1}^n \left| \left( \left| x_j \right| + \left| y_j \right| \right)^{p-1} \right|^{q} \right)^{\frac{1}{q}} = \left( \sum_{j=1}^n\left( \left| x_j \right| + \left| y_j \right| \right)^p \right)^{\frac{1}{q}}
$$

We get

$$
\begin{align*}
  \sum_{j=1}^n \left(\left| x_j \right| + \left| y_j \right| \right)^p \leq& \lVert a\rVert_p \cdot \lVert b\rVert_{q} + \lVert c\rVert_p \cdot \lVert b\rVert_{q} \\
  =& \left( \lVert a\rVert_p + \lVert c\rVert_p \right) \left( \sum_{j=1}^n \left( \left| x_j \right| + \left| y_j \right| \right)^p \right)^{\frac{1}{q}} \\
  \implies \left( \sum_{j=1}^n \left( \left| x_j \right| + \left| y_j \right| \right)^p \right)^{1 - \frac{1}{q}} =& \left( \sum_{j=1}^n \left( \left| x_j \right| + \left| y_j \right| \right)^p \right)^{\frac{1}{p}} \\
  \leq& \lVert a\rVert_p + \lVert c\rVert_p = \left( \sum_{j=1}^n \left| x_j \right|^p \right)^{\frac{1}{p}} + \left( \sum_{j=1}^n \left| y_j \right|^p \right)^{\frac{1}{p}}
\end{align*}
$$

In the limit $n \to \infty$ we retrieve the Minkowski inequality.
</details>
</MathBox>

### Sequence space ($\ell^p$)

Discrete measure spaces $(X,\mathcal{P}(X),\#)$ gives a special case for $L^p$ called sequence spaces. For a measurable function $x:X\to\R$ we introduce the notation $x_i = x(i)$.

<MathBox title='Sequence space' boxType='definition'>
Let $(X,\mathcal{P}(X),\#)$ be a discrete measure space and let $x:X\to\R$ be measurable. For $p \in [1, \infty)$, we define the $p$-norm

$$
  \lVert x\rVert_p := \left( \sum_{i\in X} |x_i|^p \right)^(1/p)
$$

and for $p=\infty$ we define $\| x\|_\infty = \sup\Set{x_i | i\in X}$. The Lebesgue space takes the form

$$
  L^p(X,\mathcal{P}(X),\#) = \Set{ (x_i)_{i\in X} \in\R^n : \lVert p\rVert_p < \infty}
$$

When $X = \N$ the $L^p$ space is usually denoted $\ell^p$.
</MathBox>

<MathBox title='Sequence spaces are Banach spaces' boxType='definition'>
The normed space $\left( \ell^p, \lVert \cdot\rVert_p \right)$ is a Banach space.

<details>
<summary>Proof</summary> 

To show the completeness of $\left( \ell^p, \lVert \cdot\rVert_p \right)$ we consider a Cauchy sequence $\left( x^{(k)} \right)_{k \in \N}$ in $\ell^p$. 

$$
\begin{align*}
  x^{(1)} =& \left( x_1^{(1)}, x_2^{(1)}, \dots \right) \\
  x^{(2)} =& \left( x_1^{(2)}, x_2^{(2)}, \dots \right) \\
  \vdots
\end{align*}
$$

The elementwise difference between two points is

$$
  \left\lvert x_m^{(k)} - x_m^{(l)} \right\rvert^p \leq \sum_{n=1}^\infty \left\lvert x_n^{(k)} - x_n^{(l)} \right\rvert^p = \lVert x^{k} - x^{l}\rVert_p^p
$$

Because $\left( x^{(k)} \right)_{k \in \N}$ is a Cauchy sequence, it follows that $\forall \epsilon > 0 \; \exists N \in \N:\; \left\| x^{(k)} - x^{(l)} \right\|^p$. Hence, $\left( x_m^{(k)} \right)_{k \in \N}$ is a Cauchy sequence in $\mathbb{F}$ converging to a limit $\tilde{x}_m \in \mathbb{F}$. This suggests that $\left( x^{(k)} \right)_{k \in \N}$ converges to a limit $\tilde{x} := \left( \tilde{x}_i \right)_{i \in \N}$, which we will now show.

Let $\epsilon > 0$ and choose $K \in \N$ such that $\forall k, l \geq K:\; \left\| x^{(k)} - x^{(l)} \right\|_p < \epsilon'$

$$
\begin{align*}
  \left\| x^{(k)} - \tilde{x} \right\|_p^p =& \sum_{n=1}^\infty \left\lvert x^{(k)} - \tilde{x} \right\rvert^p = \lim_{N \to \infty} \sum_{n=1}^N \left\lvert x^{(k)} - \tilde{x} \right\rvert^p \\
  =& \lim_{N \to \infty} \lim_{l\to\infty} \sum_{n=1}^N \left\lvert x^{(k)} - x^{(l)} \right\rvert^p \leq (\epsilon')^p
\end{align*}
$$

Thus $\forall k \geq K: \left\| x^{(k)} - \tilde{x} \right\|_p \leq \epsilon' < \epsilon$, since $\epsilon'$ can be choosen arbitrarily small. Furthermore, it follows that $\tilde{x} = \left(\tilde{x} - x^{(k)} \right) + x^{(k)} \in \ell^p$ since both $\tilde{x} - x^{(k)}$ and $x^{(k)}$ are in $\ell^p$ and $\ell^p$ is a vector space and thus closed under addition.
</details>
</MathBox>

<MathBox title='Isomorphism of $\ell^p$-dual space' boxType='proposition'>
The dual space of $\ell^p (\N)$ has a natural isomorphism with $\ell^q (\N)$ where $q$ is the Hölder conjugate satisfying $\frac{1}{p} + \frac{1}{q} = 1$. This isomorphism associates $x \in \ell^q$ with the functional $T: \ell^q(\N) \to \ell^p (\N)^*$ given by

$$
\begin{gather*}
  (Tx)(y) := \sum_{j=1}^\infty x_j y_j \quad \forall y \in \ell^p (\N) \\
  x \mapsto \langle \bar{x}, \cdot \rangle_{\ell^2 (\N)}
\end{gather*}
$$

<details>
<summary>Proof</summary>

Since the inner product by definition is linear in its second argument, it follows that $T$ is linear. By Hölder's inequality we have

$$
  \lvert (Tx)(y) \rvert \leq \lim_{n\to\infty} \sum_{j=1}^n \left| y_j x_j \right| \leq \lVert y\rVert_p \cdot \lVert x\rVert_q < \infty
$$

which shows that $Tx$ is bounded for all $x \in \ell^q(\N)$, hence it is well-defined. The operator norm for $T$ is

$$
\begin{gather*}
  \lVert Tx\rVert_{\ell^p (\N) \to \mathbb{F}} = \sup\Set{ \left| (Tx)(y) \right| | \lVert y\rVert_p = 1 } \leq \sup\Set{\lVert y\rVert_p \cdot \lVert x\rVert_q | \lVert y\rVert_p = 1} \leq \lVert x\rVert_q \\
  \implies \lVert T\rVert = \frac{\lVert Tx\rVert_{\ell^p (\N)}}{\lVert x\rVert_q} \leq 1
\end{gather*}
$$

Let $y' \in \ell^p (\N)^*$ and define $x_j := y'(e_j)$ and $x := \left( x_j \right)_{j \in \N}$ with $e_j = \left(\delta_{jk}\right)_{k\in\N}$ (unit sequence). To prove that $T$ is surjective, we need to show that $x \in \ell^q (\N)$ and $Tx = y'$.

$$
\begin{align*}
  \sum_{j=1}^n \left| x_j \right|^{q} =& \sum_{j=1}^n x_j t_j \quad\quad t_j = \begin{cases} \frac{\left| x_j \right|^q}{x_j}, & x_j \neq 0 \\ 0, & x = 0 \end{cases} \\
  =& \sum_{j=1}^n t_j y'(e_j) = y'\sum_{j=1}^n t_j e_j \\
  \leq& \lVert y'\rVert_{\ell^p (\N) \to \mathbb{F}} \lVert \sum_{j=1}^n t_j e_j\rVert_p = \lVert y'\rVert_{\ell^p (\N) \to \mathbb{F}} \left( \sum_{j=1}^n \left| t_j \right|^p \right)^{1/p}
\end{align*}
$$

since

$$
  \left| t_j \right|^p = \left(\frac{\left| x_j \right|^q}{\left| x_j \right|}\right)^p = \left| x_j \right|^{(q - 1)p} = \left| x_j \right|^q
$$

we get

$$
\begin{gather*}
  \sum_{j=1}^n \left| x_j \right|^{q} \leq \lVert y'\rVert_{\ell^p (\N) \to \mathbb{F}} \left( \sum_{j=1}^n \left| x_j \right|^q \right)^{1/p} \\
  \implies \left( \sum_{j=1}^n \left| x_j \right|^{q} \right)^{1 - 1/p} = \left( \sum_{j=1}^n \left| x_j \right|^{q} \right)^{1/q} \leq \lVert y'\rVert_{\ell^p (\N) \to \mathbb{F}} \\
  \xRightarrow{n\to\infty} \lVert x\rVert_q \leq \lVert y'\rVert_{\ell^p (\N) \to \mathbb{F}} \implies x \in \ell^q (\N)
\end{gather*}
$$

For $y \in \ell^p (\N)$ we have

$$
\begin{gather*}
\begin{aligned}
  (Tx - y')(y) =& (Tx - y')\left(\lim_{n\to\infty} \sum_{j=1}^n y_j e_j \right) \\
  =& \lim_{n\to\infty} (Tx - y') \left(\sum_{j=1}^n y_j e_j \right) \\
  =& \lim_{n\to\infty} \sum_{j=1}^n y_j (Tx - y')(e_j) = 0
\end{aligned} \\
  \implies Tx = y'
\end{gather*}
$$

To prove that $T$ is injective and thus bijective, it suffices to show that $T$ is isometric. We already know that 

$$
  \lVert Tx\rVert_{\ell^p (\N) \to \mathbb{F})} \leq \lVert x\rVert_q \leq \lVert y\rVert_{\ell^p (\N) \to \mathbb{F})}
$$

At the same time $\lVert Tx\rVert_{\ell^p (\N) \to \mathbb{F})} = \lVert y\rVert_{\ell^p (\N) \to \mathbb{F})}$, so that $\lVert Tx\rVert_{\ell^p (\N) \to \mathbb{F})} = \lVert x\rVert_q$ which shows that $T$ is ismotric.
</details>
</MathBox>

# Inner product space

<MathBox title='Inner product space' boxType='definition'>
Let $\mathbb{F} \in \Set{\R, \mathbb{C} }$ be a field of numbers and let $X$ be a $\mathbb{F}$-vector space. A map $\langle \cdot, \cdot \rangle : X \times X \to \mathbb{F}$ is called an inner product if it satisfies

1. Positive definite $\langle x, x \rangle \geq 0 \quad \forall x \in X$ and $\langle x, x \rangle = 0 \iff x = 0$
2. Conjugate symmetry
    - $\langle x, y \rangle = \langle y, x\rangle$ for $\mathbb{F} = \R$
    - $\langle x, y \rangle = \overline{\langle y, x \rangle}$ for $\mathbb{F} = \mathbb{C}$
3. Linearity in the 2nd argument:
    - $\langle x, y_1 + y_2\rangle = \langle x, y_1 \rangle + \langle x, y_2 \rangle$
    - $\langle x, \lambda y \rangle = \lambda \langle x, y \rangle, \quad \lambda \in \mathbb{F}$
4. Conjugate linearity in the 1st argument
    - $\langle \lambda x, y \rangle = \bar{\lambda} \langle x, y \rangle, \quad \lambda \in \mathbb{F}$
    
If $\langle \cdot, \cdot \rangle$ is an inner product, then $\lVert x\rVert_{\langle \cdot, \cdot \rangle} = \sqrt{\langle x, x \rangle}$ defines a norm. 
</MathBox>

The inner product $\langle \cdot, \cdot \rangle$ is a continuous map. Considering the sequence $\left( x_n \right)_{n\in\N}$ with limit $\tilde{x} \in X$ and a fixed $x_0 \in X$, then by the Cauchy-Schwartz inequality

$$
\begin{align*}
  \lvert \langle x_0, x_n \rangle - \langle x_0, \tilde{x} \rangle \rvert =& \lvert \langle x_0, x_n - \tilde{x} \rangle\rvert \\
  \leq& \lVert x_0\rVert \cdot \lVert x_n - \tilde{x}\rVert \xrightarrow{n\to\infty} 0 \\
  \implies f(x_n) \xrightarrow{n\to\infty} f(\tilde{x})
\end{align*}
$$

## Hilbert space

<MathBox title='Hilbert space' boxType='definition'>
If $\left(X, \lVert \cdot\rVert_{\langle \cdot, \cdot \rangle} \right)$ defines a Banach space, then $\left(X, \langle \cdot, \cdot \rangle \right)$ is called a Hilbert space 
</MathBox>

## $\ell^2$-space

The set of square summable sequences $\ell^2 (\N, \mathbb{F})$ endowed with the inner product  

$$
  \langle x, y \rangle = \sum_{i=1}^\infty \bar{x}_i y_i
$$

is a Hilbert space because $\ell^p$ is a Banach space. The map $\langle \cdot, \cdot \rangle: \ell^2 \times \ell^2 \to \mathbb{F}$ is indeed an inner product because it satisfies

1. Positivity: 
$$
\begin{gather*}
  \langle x, x \rangle = \sum_{i=1}^\infty \bar{x}_i x_i = \sum_{i=1}^\infty \lvert x_i \rvert^2 \geq 0 \\
  \langle x, x \rangle = 0 \implies \lvert x_i \rvert^2 = 0\; \forall i \in \N \implies x = 0
\end{gather*}
$$
2. Conjugate symmetry:
$$
  \overline{\langle y, x \rangle} = \sum_{i=1}^\infty \overline{\bar{y}_i x_i} = \sum_{i=1}^\infty y_i \bar{x}_i = \langle x, y \rangle
$$
3. Linearity in the 2nd argument: 
$$
\begin{align*}
  \langle x, \alpha y + \beta z \rangle =& \sum_{i=1}^\infty \bar{x}_i \left(\alpha y_i + \beta z_i \right) \\
  =& \alpha \sum_{i=1}^\infty \bar{x}_i y_i + \beta \sum_{i=1}^\infty \bar{x}_i z_i \\
  =& \alpha \langle x, y \rangle + \beta \langle x, z \rangle
\end{align*}
$$

## Cauchy-Schwarz inequality

<MathBox title='Cauchy-Scharz inequality' boxType='theorem'>
Let $\left(X, \langle \cdot, \cdot \rangle \right)$ be an inner product space with norm $\lVert x\rVert = \sqrt{\langle x, x \rangle}$. Then for all $x, y \in X$

$$
  \lvert \langle x, y \rangle \rvert \leq \lVert x\rVert \cdot \lVert y\rVert
$$

where the equality holds if and only if $x$ and $y$ are linearly dependent, ie. $y = \lambda x$. The triangle inequality follows from the Cauchy-Schwarz inequality

$$
\begin{align*}
  \lVert x + y\rVert^2 =& \langle x + y, x + y \rangle = \lVert x\rVert^2 + 2\lvert \langle x, y \rangle \rvert + \lVert y\rVert^2 \\
  \leq& \lVert x\rVert^2 + 2\lVert x\rVert \cdot \lVert y\rVert + \lVert y\rVert^2 = \left( \lVert x\rVert + \lVert y\rVert \right)^2
\end{align*}
$$

Taking the square root we obtain the triangle inequality $\lVert x + y\rVert \leq \lVert x\rVert + \lVert y\rVert$.

<details>
<summary>Proof</summary> 

Considering the non-trivial case $x \neq 0$, we define the unit vector $\hat{x} := \frac{x}{\lVert x\rVert}$ and take the orthogonal projection of $y$ onto $\hat{x}$ (parallel component) given by $y_\parallel = \langle \hat{x}, y \rangle \hat{x}$. The Cauchy-Schwarz inequality can be obtained from taking the norm of the orthogonal component $y_\perp = y - y_\parallel$

$$
\begin{align*}
  0 \leq \lVert y_\perp\rVert^2 =& \lVert y - y_\parallel\rVert^2 = \langle y - \langle \hat{x}, y \rangle \hat{x}, y - \langle \hat{x}, y \rangle \hat{x} \rangle \\
  =& \langle y - \langle \hat{x}, y  \rangle \hat{x}, y \rangle - \langle y - \langle \hat{x}, y  \rangle \hat{x}, \langle \hat{x}, y  \rangle \hat{x} \rangle \\
  =& \langle y, y \rangle - \langle \langle \hat{x}, y  \rangle \hat{x}, y \rangle - \langle y,  \langle \hat{x}, y  \rangle \hat{x} \rangle + \langle \langle \hat{x}, y  \rangle \hat{x}, \langle \hat{x}, y  \rangle \hat{x}  \rangle \\
  =& \lVert y\rVert^2 - \overline{\langle \hat{x}, y \rangle}\langle \hat{x}, y \rangle - \langle \hat{x}, y \rangle \overline{\langle \hat{x}, y \rangle}  + \lvert \langle \hat{x}, y  \rangle \rvert^2 \underbrace{\lVert \hat{x}\rVert^2}_{=1} \\
  =& \lVert y\rVert^2 - \lvert \langle \hat{x}, y  \rangle \rvert^2
\end{align*}
$$

rearranging the inequality we get

$$
\begin{gather*}
  \lVert y\rVert^2 \geq \lvert \langle \hat{x}, y  \rangle \rvert^2 = \left\lvert \left\langle \frac{x}{\lVert x\rVert}, y  \right\rangle \right\rvert^2 = \frac{1}{\lVert x\rVert^2} \lvert \langle x, y \rangle \rvert^2 \\
  \implies \lvert \langle x, y \rangle \rvert \leq \lVert x\rVert \cdot \lVert y\rVert
\end{gather*}
$$
</details>
</MathBox>

## Orthogonality

<MathBox title='Orthogonality' boxType='definition'>
Let $\left(X, \langle \cdot, \cdot \rangle \right)$ be an inner product space. Two vectors $x, y \in X$ are orthogonal if $\langle x, y \rangle = 0$, denoted $x \perp y$. Two sets $U, V \in X$ are orthogonal, denoted $U \perp V$, if $u \perp v$ for all $u \in U$ and $v \in V$. The orthogonal component of a set $U \subseteq X$ is

$$
  U^\perp = \Set{ x \in X | \langle x, u \rangle = 0 \; \forall u\in U }
$$
</MathBox>

If $x \perp y$, then the triangle inequality reduces to the Pytagorean theorem

$$
\begin{align*}
  \lVert x + y\rVert^2 =& \langle x + y, x + y \rangle \\
  =& \lVert x\rVert^2 + 2\lvert \langle x, y \rangle \rvert + \lVert y\rVert^2 \\
  =& \lVert x\rVert^2 + \lVert y\rVert^2
\end{align*}
$$

<MathBox title='Orthogonal components' boxType='proposition'>
Let $(X, \langle\cdot,\cdot\rangle)$ be an inner product space and let $U^\perp$ be the orthogonal component of $U\subseteq X$. Then $U^\perp$ is closed. 

<details>
<summary>Proof</summary>

This is proved by considering a sequence $\left( x_n \right)_{n\in\N} \subseteq U^\perp$ showing that the limit $\tilde{x} \in X$ is also in $U^\perp$. It follows that for all $u \in U$

$$
  \langle x_n, u \rangle = 0 \implies \lim_{n\to\infty} \langle x_n, u \rangle = 0 \implies \langle \tilde{x}, u \rangle = 0 \implies \tilde{x}\in U^\perp
$$

Hence, $U^\perp$ is closed. Here we applied the continuity property of the inner product resulting in $\langle x_n, 0 \rangle \xrightarrow{n\to\infty} \langle \tilde{x}, u \rangle$.

If $U \subseteq V$ then $U^\perp \supseteq V^\perp$. For $x \in V^\perp$ we have $\langle x, v \rangle = 0$ for all $v \in V$. Because $U \subseteq V$, we have $\langle x, u \rangle = 0$ for all $u \in U$. This implies that $x \in U^\perp$, hence $U^\perp \supseteq V^\perp$. 
</details>
</MathBox>

## Riesz representation theorem

<MathBox title='Riesz representation theorem' boxType='theorem'>
For each continuous linear functional $\ell: X \to \mathbb{F}$ there is exactly one $x_\ell \in X$ such that $\ell(x) = \langle x_\ell, x \rangle$ for all $x \in X$ and $\lVert \ell\rVert = \lVert x_\ell\rVert_X$.

<details>
<summary>Proof</summary>

To prove the existence of $x_\ell$ we consider the kernel of $\ell$ defined as $\ker(\ell) := \Set{ x \in X | \ell(x) = 0 }$. In the trivial case $\ker (\ell) = X$ it follows that $x_\ell = 0$, so we assume $\ker(\ell) \neq X$. Evidently $\ell$ is continuous because $\ker(\ell) = \ell^{-1}(\Set{ 0 })$. Thus $\ker(\ell)$ is a closed subset of $X$. Consider $\hat{x} \in \ker(\ell)^\perp$ with $\lVert \hat{x}\rVert_X = 1$ and set $x_\ell := \overline{\ell(\hat{x})}\hat{x}$, then

$$
\begin{align*}
  \ell(x) =& \ell\left(x - \frac{\ell(x)}{\ell(\hat{x})}\hat{x} + \frac{\ell(x)}{\ell(\hat{x})}\hat{x} \right) = \ell\left(x - \frac{\ell(x)}{\ell(\hat{x})}\hat{x} \right) + \ell\left( \frac{\ell(x)}{\ell(\hat{x})}\hat{x} \right) \\
  =& \ell(x) - \frac{\ell(x)}{\ell(\hat{x})} \ell(\hat{x}) + \ell(\lambda \hat{x}) \\
  =& \lambda \ell(\hat{x}) \langle \hat{x}, \hat{x} \rangle = \lambda \langle \overline{\ell(\hat{x})}\hat{x}, \hat{x} \rangle = \langle x_\ell, \lambda\hat{x} - x + x\rangle \\
  =& \langle x_\ell, \lambda \hat{x} - x \rangle + \langle x_\ell, x \rangle \\
  =& \langle x_\ell, x \rangle
\end{align*}
$$

To prove the uniqueness of $x_\ell$, we assume $x_\ell, \tilde{x} \in X$ fulfil 

$$
\begin{align*}
  \ell(x) = \langle x_\ell, x \rangle = \langle \tilde{x}_\ell, x \rangle \\
  \implies \langle x_\ell - \tilde{x}_\ell, x \rangle = 0, \; \forall x \in X \\
  \implies \langle x_\ell - \tilde{x}_\ell, x_\ell - \tilde{x}_\ell \rangle = 0
\end{align*}
$$

Because the inner product is positive definite we conclude that $x_\ell = \tilde{x}_\ell$.

To prove that the operator norm is $\lVert \ell\rVert = \lVert x_\ell\rVert$, we have

$$
  \lVert \ell \rVert = \sup\Set{ \lvert \ell(x) \rvert : \lVert x \rVert_X = 1 } = \sup\Set{ \underbrace{\lvert \langle x_\ell, x\rangle \rvert}_{\leq \lVert x_\ell \rVert_X \cdot \lVert x \rVert_X } : \lVert x \rVert_X \leq 1 } \leq \lVert x_\ell \rVert
$$

Similarly

$$
  \lVert \ell\rVert \geq \left\lvert \ell \left( \frac{x_\ell}{\lVert x_\ell\rVert} \right) \right\rvert = \left\lvert \left\langle x_\ell, \frac{x_\ell}{\lVert x_\ell\rVert} \right\rangle \right\rvert = \lVert x_\ell\rVert
$$

Hence, we conclude that $\lVert \ell\rVert = \lVert x_\ell\rVert$.
</details>
</MathBox>

# Normed algebra

<MathBox title='Normed algebra' boxType='definition'>
A *normed algebra* is a normed space $A$ equipped with a multiplication $\times: A\times A$ as $(x,y)\mapsto xy$, which satisfies for $x,y,z\in A$ and $\alpha\in\mathbb{F}$
1. **Asssociates:** $(xy)z = x(yz)$
2. **Distributivity:** 
    - $(\alpha x + y)z = \alpha xz + yz$
    - $z(\alpha x + y) = \alpha zx + zy$
3. **Sub-multiplicativity of norm**: $\lVert xy\rVert\leq\lVert x\rVert\cdot\lVert y \rVert$

A normed space is *unital* if it has a multiplicative identity, i.e. if there exists an element $1\in A$ such that $1x = x1 = x$ for all $x\in A$.
</MathBox>

<MathBox title='Banach algebra' boxType='definition'>
A *Banach algebra* is a normed algebra which is complete as a normed space. 

<details>
<summary>Details</summary>

Suppse $A_0$ is a normed algebra, and $A$ denotes its completion as a Banach space. Then $A$ acquires a natural Banach algebra structure as follows: if $x,y\inA$, pick sequences $(x_n)$ and $(y_n)$ in $A_0$ such that $x_n \xrightarrow{n\to\infty} x$ and $y_n \xrightarrow{n\to\infty} y$. Then, pick a constant $K$ such that $\lVert x_n \rVert, \lVert y_n \rVert \leq n$, note that

$$
\begin{align*}
  \lVert x_n y_n - x_m y_m \rVert =& \lVert x_n (y_n - y_m) + (x_n - x_m)y_m \rVert \\
  \leq& \lVert x_n (y_n - y_m) \rVert + \lVert (x_n - x_m)y_m \rVert \\
  \leq& \lVert x_n \rVert \cdot \lVert y_n - y_m \rVert + \lVert x_n - x_m \rVert \cdot \lVert y_m \rVert \\
  \leq& (\lVert y_n - y_m \rVert + \lVert x_n - x_m \rVert)
\end{align*}
$$

and conclude that $(x_n y_n)$ is a Cauchy sequence and hence convergent, define $xy = \lim_{n\to\infty} x_n y_n$. To see that the product we have proposed is unambigously defined defined, we must verify that the above limit is independent of the choice of the approximating sequences, and depends only on $x$ and $y$. To see this, suppose $x_{i,n} \xrightarrow{n\to\infty} x$ and $y_{n,1} \xrightarrow{n\to\infty} y$ for $i=1,2$. Define 

$$
  x_{2n-1} =& x_{1,n} \\
  x_{2n} =& x_{2,n} \\
  y_{2n-1} =& y_{1,n} \\
  y_{2n} =& y_{2,n}
$$

and the apply the preceding reasoning to conclude that $(x_n, y_n)$ is a convergent sequence with some limit $z$; then also $(x_{1,n} y_{1,n} = x_{2n-1} y_{2n-1})$ converges to $z$, since a subsequence of a convergent sequence converges to the same limit. Similarly, $(x_{2,n} y_{2,n} = x_{2n} y_{2n})$ converges to $z$.

Thus, we have unambiguously defined the product of any two elements of $A$.
</details>
</MathBox>

<MathBox title='Group of units' boxType='definition'>
Suppose $A$ is a normed algebra. An element $x\in A$ is *invertible* if there is a unique element $x^{-1} \in A$ such that $xx^{-1} = x^{-1}x = 1$. The collection of all invertible elements of $A$ is denoted $G(A)$ and is called the *group of units* in $A$
</MathBox>

<MathBox title='Group of units properties' boxType='proposition' tag='proposition-21'>
Let $A$ be a normed algebra with group of units $G(A)$
1. If $x\in G(A)$ and if $L_x$ is a map given by left-multiplication of $x$, i.e. $L_x(y) = xy$ for $y\in A$, then $L_x \in \mathcal{L}(A)$ is a linear invertible operator on $A$ with $(L_x)^{-1} = L_{x^{-1}}$.
2. If $x\in G(A)$ and $y\in A$, then $xy \in G(A) \implies yx \in G(A) \implies y\in G(A)$
3. If $\Set{x_i}_{i=1}^n \subset A$ is a set of pairwise commuting elements, i.e. $x_i x_j = x_j x_i$ for all $i,j =1,\dots,n$, then the product $\prod_{i=1}^n x_i$ is invertible if and only if each $x_i$ is invertible.
4. If $x\in A$ and $\lVert 1 - x\rVert < 1$, then $x\in G(A)$ and
$$
\begin{equation}
  x^{-1} = \sum_{n=0}^\infty (1 - x)^n \tag{\label{equation-1}}
\end{equation}
$$
In particular, if $\lambda\in\mathbb{C}$ and $|\lambda| > \lVert x \rVert$, then $(x - \lambda)$ is invertible and
$$
\begin{equation}
  (x - \lambda)^{-1} = -\sum_{n=0}^\infty \frac{x^n}{\lambda^{n+1}}  \tag{\label{equation-2}}
\end{equation}
$$
5. $G(A)$ is an open set in $A$ and the mapping $x\mapsto x^{-1}$ is a homeomorphism of $G(A)$ onto itself.

<details>
<summary>Proof</summary>

**(1):** This follows trivially.

**(2):** If $y$ is invertible, so is $yx$, respectively $yx$. Conversely, if $xy$ is invertible, respectively $yx$, so is $y = x^{-1}(xy)$, respectively $y = (yx)x^{-1}$.

**(3):** One implication follows from the fact that $G(A)$ is closed under multiplication. Conversely, suppose $x = \prod_{i=1}^n x_i$ is invertible. It is clear that $x_i$ commutes with $x$, for each $i$. It follows easily that each $x_i$ also commutes with $x^{-1}$, and that the inverse of $x_i$ is given by the product of $x^{-1}$ with the product of all $x_j$ with $j\neq 1$.

**(4):** The series $\sum_{n=0}^\infty (1 - x)^n$ is summable since the appropriate geometric series converges under the hypothesis. Let $\Set{s_n = \sum_{i=0}^n (1 - x)^i}$ be the sequence of partial sums, and let $s$ denote the limit. Clearly, each $s_n$ commutes with $(1 - x)$, and also $(1 - x)s_n = s_n (1 - x) = s_{n+1} - 1$. Thus, in the limit $n\to\infty$ we have $(1 - x)s = s(1 - x) = s - 1$, i.e. $xs = sx = 1$.

As for $\eqref{equation-2}$, we may apply $\eqref{equation-1}$ to $(1 - x/\lambda)$ in place of $x$, and justify the steps

$$
\begin{align*}
  (x - \lambda)^{-1} =& -\lambda^{-1} \left(1 - \frac{x}{\lambda} \right)^{-1} \\
  =& -\lambda^{-1} \lambda_{n=0}^\infty \frac{x^n}{\lambda^n} \\
  =& -\sum_{n=0}^\infty \frac{x^n}{\lambda^{n+1}}
\end{align*}
$$

**(5):** The assertions in **(4)** imply that $1\in\opertorname{int}(G(A))$ and is a point of continuity of the inversion map. By **(1)** and **(2)**, we may map this local picture at $1$ via the linear homeomorphism $L_x$ (which maps $G(A)$ onto itself) to a corresponing local picture at $x$, and deduce that any element of $G(A)$ is an interior point and is a point of continuity of the inversion map.
</details>
</MathBox>

<MathBox title='Spectrum' boxType='definition'>
Let $A$ be a unital Banach algebra, and let $x\in A$. The *spectrum* of $x$ is the set, denoted by $\sigma(x)$, defined by

$$
  \sigma(x) = \Set{\lambda\in\mathbb{F} | (x - \lambda)\notin G(A)}
$$

and the *spectral radius* of $x$ is defined by

$$
\begin{equation}
  r(x) = \sup\Set{\lvert\lambda\rvert | \lambda\in\sigma(x)} \leq\lVert x \rVert \tag{\label{equation-3}}
\end{equation}
$$

The *resolvent set* of $x$ is the complementary set $\rho(x)$ defined by

$$
  \rho(x) = \mathbb{F} - \sigma(x) = \Set{\lambda\in\mathbb{F}| (x - \lambda) \in G(A)}
$$

and the map $R_x : \rho(x) \to G(A)$ by $\lambda\mapsto (x - \lambda)^{-1}$ is called the *resolvent function* of $x$.
</MathBox>

<MathBox title='Resolvent properties' boxType='proposition' tag='proposition-22'>
Let $x\in A$, then
1. $\lim_{|\lambda|\to\infty} \lVert R_x (\lambda) \rVert = 0$
2. **Resolvent equation:**
$$
  R_x (\lambda) - R_x (\mu) = (\lambda - \mu) R_x (\lambda) R_x (\mu),\; \lambda, \mu \in\rho(x)
$$
3. The resolvent function is "weakly analytic", meaning that if $\phi\in A^*$, then the map $\phi\circ R_x:\rho(x)\to\mathbb{F}$ is an analytic function such that
$$
  \lim_{|\lambda|\to\infty} \phi\circ R_x (\lambda) = 0
$$

<details>
<summary>Proof</summary>

**(1):** By $\eqref{equation-2}$, there is a constant $C > 0$ such that
$$
  \lVert R_x (\lambda) \rVert \leq \frac{C}{|\lambda|},\; |\lambda| > \lVert x \rVert
$$

**(2):** If $\lambda,\mu\in\rho(x)$, then
$$
\begin{align*}
  (\lambda-\mu)R_x (\lambda) R_x (\mu) =& (\lambda - \mu)(x - \lambda)^{-1} (x - \mu)^{-1} \\
  =& R_x (\lambda) ((x - \mu) - (x - \lambda))R_x (\mu) \\
  =& R_x (\lambda) - R_x (\mu)
\end{align*}
$$

**(3):** If $\phi\in A^*$ is any continuous linear functional on $A$, it follows from Proposition $\ref{proposition-21}.5$ and **(2)** that if $\mu\in\rho(x)$, and if $\lambda$ is sufficiently close to $\mu$, then $\mu\in\rho(x)$, and

$$
\begin{align*}
  \lim_{\lambda\to\mu} \left(\frac{\phi\circ R_x (\lambda) - \phi\circ R_x (\mu)}{\lambda - \mu} \right) =& \lim_{\lambda\to\mu} (\phi(R_x (\lambda) R_y(\mu))) \\
  =& \phi(R_x(\mu)^2)
\end{align*}
$$

This shows that $\phi\circ R_x$ is differentiable, and thereby analytic, at the point $\mu$. By **(1)** and the boundedness of the linear functional $\phi$, it follows that

$$
  \lim_{\lambda\to\mu} \phi\circ R_x (\lambda)
$$
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
If $A$ is any Banach algebra and $x\in A$, then $\sigma(x)$ is a non-empty compact subset of $\mathbb{F}$.

<details>
<summary>Proof</summary>

Assume on the contrary, so that $\rho(x) = \mathbb{F}$. Let $\phi\in A^*$ be arbitrary. By Proposition $\ref{proposition-22}.3$, this means that $\phi\circ R_x$ is an entire function, differentiable on $\mathbb{F}$, which vanishes at infinity. It then follows from Liouville's theorem that $\phi\circ R_x (\lambda) = 0$ for all $\lambda\in\mathbb{F}$. Since $\phi$ is arbitrary, and since $A^*$ separates points of $A$, we may conclude that $R_x (\lambda) = 0$ for all $\lambda\in\mathbb{F}$. However, since $R_x (\lambda) \in G(A)$, this is a contradiction, completing the proof.
</details>
</MathBox>

<MathBox title='' boxType='proposition' tag='proposition-24'>
If $p(z) = \sum_{n=0}^N a_n z^n$ is polynomial with $a_n \in \mathbb{F}$, and if we define $p(x) = a_0\cdot 1 + \sum_{n=1}^N a_n x^n$ for each $x\in A$, then
1. $p(z) \mapsto p(x)$ is a homomorphism from the algebra $\mathbb{F}[z]$ of polynomials onto the subalgebra of $A$ which is generated by $\Set{1,x}$.
2. **Spectral mapping theorem:** If $p$ is any polynomial as above, then
$$
  \sigma(p(x)) = p(\sigma(x)) = \Set{p(\lambda) | \lambda\in\sigma(x)}
$$

<details>
<summary>Proof</summary>

**(1):** This is obvious.

**(2):** Temporarily fix $\lambda\in\mathbb{F}$. If $p(z) = \sum_{n=0}^N a_n z^n$, assume without loss of generality that $a_N \neq 0$, and by the fundamental theorem of algebra there exist $\lambda_1,\dots,\lambda_N$ such that

$$
  p(z) - \lambda = a_N \prod_{n=1}^N (z - \lambda_i)
$$

From **(1)** we have that

$$
  p(x) - \lambda = a_N \prod_{n=1}^N (x - \lambda_i)
$$

By Proposition $\ref{proposition-21}.3$, it follows that

$$
\begin{align*}
  \lambda\notin\sigma(p(x)) \iff& \lambda_i \notin\sigma(x) \forall 1\leq i \leq n \\
  \iff& \lambda\notin p(\sigma(x))
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Spectral radius formula' boxType='theorem'>
If $A$ is a Banach algebra and if $x\in A$, then

$$
  r(x) = \lim_{n\to\infty} \lVert x^n \rVert^{1/n}
$$

<details>
<summary>Proof</summary>

Fix an arbitrary $\phi\in A^*$, and let $F = \phi\circ R_x$. By definition, this is analytic in the exterior of the disc $\Set{\lambda\in\mathbb{F}: |\lambda| \leq r(x)}$. On the other hand, we may conclude from $\eqref{}$ that $F$ has the Laurentn series expansion

$$
  F(\lambda) = -\sum_{n=0}^\infty \frac{\phi(x^n)}{\lambda^{n+1}}
$$

which is valid in the exterior of the disc $\Set{\lambda\in\mathbb{F}: |\lambda|\leq\lVert x\rVert}$. Since $F$ vanishes at infinity, the function $F$ is analytic at the point at infinity, and consequently the Laurent expansion above is valid in the larger region $|\lambda| > r(x)$.

If we temporarily fix a $\lambda$ with $|\lambda| > r(x)$, then we find, in particular, that

$$
  \lim_{n\to\infty} \frac{\varphi(x^n)}{\lambda^n} = 0
$$

Since $\varphi$ was arbitrary, it follows from the uniform boundedness principle that there is a constant $K > 0$ such that

$$
\begin{gather*}
  \lVert x^n \rVert \leq K|\lambda|^n,\; \forall n\in\N \\
  \lVert x^n \rVert^{1/n} \leq K^{1/n} |\lambda|
\end{gather}
$$

By allowing $|\lambda|$ to decrease to $r(x)$, we may conclude that

$$
  \limsup_{n\in\N} \lVert x^n \rVert^{1/n} \leq r(x)
$$

On the other hand, from Proposition $\ref{proposition-24}.2$ and equation $\eqref{equation-3}$ that $r(x) = r(x^n)^{1/n} \leq \lVert x^n \rVert^{1/n}$. Hence, we have 

$$
  r(x) \leq \liminf_{n\in\N} \lVert x^n \rVert^{1/n}
$$
</details>
</MathBox>


# Linear operators

<MathBox title='Linear operator' boxType='definition'>
Let $(X, \lVert \cdot\rVert_X)$ and $(Y, \lVert \cdot\rVert_Y)$ be two normed spaces. An operator $T: X \to Y$ is linear if it is closed under addition and scalar multiplication

$$
\begin{gather*}
  T(x + \tilde{x}) = Tx + T\tilde{x} \\
  T(\lambda x) = \lambda T x
\end{gather*}
$$

for all $x, \tilde{x} \in X$ and $\lambda \in \mathbb{F}$. The operator norm of $T$ is defined as

$$
  \lVert T\rVert = \lVert T\rVert_{X\to Y} := \sup\Set{ \frac{\lVert T_X\rVert_Y}{\lVert X\rVert_X} | x \in X\setminus\Set{0} }
$$
</MathBox>

## Bounded operators

<MathBox title='Bounded operator' boxType='definition'>
Let $T$ be a linear operator. If there exists some $M > 0$ such that $\lVert T x\rVert_Y \leq M\lVert x\rVert_X  < \infty$ for all $x \in X$, then $T$ is a bounded operator with operator norm $M$. The set of all linear and bounded operators $T: X \to Y$ is denoted $\mathcal{B}(X, Y)$.
</MathBox>

<MathBox title='The space of bounded linear operators is Banach' boxType='proposition'>
Let $X$ and $Y$ be normed spaces, where $X$ is also a Banach space. The space $\mathcal{B}(X,Y)$ of bounded linear operators from $X$ to $Y$ is a Banach space when equipped with the induced norm $\lVert\cdot\rVert_{X\to Y}$.

<details>
<summary>Proof</summary>

Since $\mathcal{B}(X,Y)$ is a normed space, it remains to show that is complete. Let $(A_k)_{k\in\N_+}$ be a Cauchy sequence in $\mathcal{B}(X,Y)$. We show that the Cauchy sequence converges in three steps.

**(1):** We construct a candidate function $A:X\to Y$ for the limit of the Cauchy sequence. For $\epsilon > 0$ there is $N\in\N$ such that

$$
  \sup\Set{\frac{\lVert A_m (x) - A_n (x) \rVert_Y}{\lVert x \rVert_X} | x\in X\setminus\Set{0}} = \lVert A_m - A_n \rVert_{X\to Y} < \epsilon
$$

whenever $m, n \geq N$. Thus, for each nonzero $x\in X$ we have

$$
  \lVert A_m (x) - A_n (x) \rVert_Y < \epsilon\lVert x \rVert_X
$$

This says that for each fixed nonzero $x\in X$, the sequence $(A_n (x))_{n\in\N_+}^\infty$ is Cauchy in $Y$. Since $Y$ is a Banach space, the sequence $(A_n (x))_{n\in\N_+}$ converges to an element $A(x)\in Y$. We define $A(0) = 0$, giving a function $A:X\to Y$.

**(2):** We show that $A\in\mathcal{B}(X,Y)$. The function $A$ is linear because for $\alpha, \beta\in\mathbb{F}$

$$
\begin{align*}
  A(\alpha x_1 + \beta x_1) =& \lim_{k\to\infty} A_n (\alpha x_1 + \beta x_2) \\
  =& \lim_{k\to\infty} (\alpha A_k (x_1) + \beta A_k (x_2)) \\
  =& \alpha A(x_1) + \beta A(x_2)
\end{align*}
$$

We show that $A$ is bounded. For $\epsilon > 0$ there is $N\in\N$ such that $\lVert A_m - A_n \rVert_{X\to Y} < \epsilon$ whenever $m,n \geq N$. This implies that for all $x\in X$

$$
  \lVert A_m (x) - A_n (x) \rVert_Y \leq \lVert A_n - A_m \rVert_{X\to Y} \cdot\lVert x \rVert_X < \epsilon\lVert x \rVert_X
$$

Taking the limit $m\to\infty$ and using the continuity of the norm we get

$$
  \lVert A(x) - A_n (x) \rVert_Y \leq \epsilon\lVert x \rVert_X
$$

By the triangle inequality, we have for any $n\geq N$ that

$$
\begin{align*}
  \lVert A(x) \rVert_Y \leq& \lVert A(x) - A_n (x)\rVert_Y + \lVert A_n (x) \rVert \\
  \leq& \lVert x \rVert_X + \lVert A_n \rVert_{X\to Y} \cdot \lVert x \rVert_X \\
  =& (\epsilon + \lVert A_n \rVert_{X\to Y})\lVert X \rVert_X
\end{align*}
$$

Since $(A_n)_{n\in\N_+}$ is Cauchy, it is bounded by Proposition $\ref{proposition-6}$. Thus, there is $M > 0$ such that $\lVert A_n \rVert_{X\to Y} \leq M$ for all $n\in\N$. This implies that

$$
  \lVert A(x) \rVert_Y \leq (\epsilon +  M)\lVert X \rVert_X
$$

and also that

$$
  \lVert A \rVert_{X\to Y} = \sup\Set{\frac{\lVert A(x) \rVert_Y}{\lVert x \rVert_X} | x\in X\setminus\Set{0}} \leq (\epsilon + M)
$$

Hence $A$ is bounded.

**(3):** We show that $\lim_{k\to\infty} \lVert A - A_k \rVert_{X\to Y} = 0$. We established in **(2)** that for $\epsilon > 0$ there is $N\in\N$ such that $\lVert A(x) - A_n (x) \rVert_Y < \epsilon\lVert x \rVert_X$ whenever $n \leq N$.

This implies that for each nonzero $x$ and for all $n\geq N$ that

$$
  \frac{\lVert A(x) - A_n (x)\rVert_Y}{\lVert x \rVert_X} < \epsilon
$$

Taking the supremum of the left-hand side gives

$$
  \lVert A - A_n \rVert_{X\to Y} \leq\epsilon
$$

Hence, $(A_n)_{n\in\N_+}$ converges to $A$ in the norm $\lVert\cdot\rVert_{X\to Y}$.
</details>
</MathBox>

<MathBox title='' boxType='proposition' tag='proposition-18'>
A linear normed operator is bounded if and only if it is continuous.

<details>
<summary>Proof</summary> 

Suppose $T$ is bounded. Then for a sequence $\left(x_n \right)_{n\in\N} \subseteq X$ with limit $\tilde{x} \in X$ we have

$$
  \lVert Tx_n - T\tilde{x}\rVert_Y = \lVert T(x_n - \tilde{x})\rVert_Y \leq M \lVert x_n - \tilde{x}\rVert_X \xrightarrow{n\to\infty} 0
$$

which shows that $T$ is continuous. Conversly, suppose $T$ is continuous. Then there exists $\delta > 0$ such that $\lVert Tx\rVert_Y < 1$ for all $x \in X$ with $\lVert x\rVert_X < \delta$, thus

$$
  \lVert Tx\rVert_Y = \left\lVert \frac{\| x\rVert}{\delta} T\left( \delta \frac{x}{\lVert x\rVert} \right) \right\lVert = \frac{\| x\rVert}{\delta} \left\lVert T\left( \delta \frac{x}{\| x\rVert} \right) \right\lVert \leq \frac{\| x\rVert}{\delta} < \infty
$$
</details>
</MathBox>

<MathBox title='Norm-preserving linear extension of bounded operators' boxType='theorem'>
Let $X$ and $Z$ be normed spaces, $S\subseteq Z$ a subspace of $Z$ with norm $\lVert\cdot\rVert_Z$. If $T\in\mathcal{B}(S,Z)$ is a bounded linear operator, a norm-preserving extension of $T$ is a bounded linear operator $\overline{T}\in\mathcal{Z,X}$ such that $\overline{T}(s) = T(s)$ for all $s\in S$ and $\lVert\overline{T}\rVert_{Z\to Y} = \lVert T \rVert_{S\to Z}$.
</MathBox>

<MathBox title='Continuous linear extension theorem' boxType='theorem' tag='theorem-9'>
Let $Z$ be a normed space, $S\subseteq Z$ a dense subset of $Z$ and $X$ a Banach space. If $T\in\mathcal{B}(S,X)$, then $T$ has a unique norm-preserving extension $\overline{T}\in\mathbb{B}(Z,X)$.

<details>
<summary>Proof</summary>

Since $S$ is dense in $Z$, there is for each $z\in Z$ a Cauchy sequence $(s_k)_{k\in\N_+}$ in $S$ such that $\lim_{k\to\infty} \lVert z - s_k \rVert_Z = 0$. Since $T\in\mathcal{B}(S,X)$, the operator $T$ is uniformly continuous on $S$ (Proposition $\ref{proposition-18}$). Thus, the sequence $(T(s_k))_{k\in\N_+}$ is Cauchy in $X$. This sequence converges because $X$ is a Banach space. For each $z\in Z$ we define $\overline{T}:Z\to X$ by $\overline{T}(z) = \lim_{k\to\infty} T(s_k)$. Note that for $s\in S$ we get $\overline{T}(s) = T(s)$ by the continuity of $T$.

Because we have used a sequence to define $\overline{T}$ we cannot assume that $\overline{T}$ is well-defined. For $\epsilon > 0$, there is $K_1 \in\N$ such that $\lVert\overline{T}(z) - T(s_k)\rVert_X < \epsilon/2$ whenever $k \geq K_1$. Suppose $(s'_k)_{k\in\N_+}$ is another sequence in $S$ converging to $z$. By the uniform continuity of $T$ on $S$, for each $\epsilon < 0$ there is $\delta > 0$ such that $\lVert T(a) - T(b) \rVert_X < \epsilon/2$ when $\lVert a - b \rVert_S < \delta$. For this $\delta > 0$, there exists $K_2, K_3 \in\N$ such that $\lVert s_k - Z \rVert_Z < \delta$ when $k \geq K_2$ and $\lVert s'_k - z \rVert_Z < \delta$ when $k \geq K_3$.

Choose $K = \max\Set{K_1,K_2,K_3}$, then $\lVert s_k - z \rVert_Z < \delta$ and $\lVert s'_k - z \rVert < \delta$ whenever $k \geq K$. Thus, we obtain

$$
\begin{align*}
  \lVert T(s'_k) - \overline{T}(z)\rVert_X \leq& \lVert T(s'_k) - T(s_k) \rVert + \lVert T(s_k) - \overline{T}(z) \rVert_X \\
  < \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon 
\end{align*}
$$

whenever $k \geq K$. This implies that $\lim_{k\to\infty} T(s'_k) \to\overline{T}(z)$, so that $\overline{T}(z)$ is well-defined.

Next, we show that $\overline{T}$ is linear. For $(s_k)_{k\in\N_+}$ in $S$ converging to $z\in Z$ and $(\tilde{s}_k)_{k\in\N_+}$ in $S$ converging to $\tilde{z}\in Z$ and $\alpha, \beta \in\mathbb{F}$ we have

$$
  \overline{T}(\alpha z + \beta\tilde{z}) - \alpha\overline{T}(z) - \beta\overline{T}(\hat{z}) =& \lim_{k\to\infty} (T(\alpha s_k + \beta\tilde{s}_k) - \alpha T(s_k) - \beta T(\hat{s}_k)) = 0
$$

Next, we show that $\lVert\overline{T}\rVert_{Z\to X} = \lVert T \rVert_{S\to X}$. For $(s_k)_{k\in\N_+}$ in $S$ converging to $z\in Z$, we have

$$
\begin{align*}
  \lVert\overline{T}(z)\rVert_Z =& \left\lVert \lim_{k\to\infty} T(s_k) \rVert \\
  =& \lim_{k\to\infty} \lVert T(s_k) \rVert \\
  \leq& \lim_{k\to\infty} \lVert T \rVert_{S\to X} \lVert s_k \rVert_S \\
  =& \lVert T \rVert_{S\to X} \lVert z \rVert_Z 
\end{align*}
$$

This implies that

$$
  \lVert\overline{T}\rVert_{Z\to X} = \sup\Set{\frac{\lVert\overline{T}(z)\rVert_X}{\lVert z \rVert_Z} | z\in Z\setminus\Set{0}} \leq \lVert T \rVert_{S\to X}
$$

On the other hand, since $\overline{T}(s) = T(s)$ for all $s\in S$, we have

$$
\begin{align*}
  \lVert\overline{T}\rVert_{Z\to X} =& \sup\Set{\frac{\lVert\overline{T}(z)\rVert_X}{\lVert z \rVert_Z} | z\in Z\setminus\Set{0}} \\
  \geq& \frac{\lVert\overline{T}(s)\rVert_X}{\lVert z \rVert_S} = \lVert T \rVert_{S\to X}
\end{align*}
$$

Thus, $\lVert\overline{T}\rVert_{Z\to X} = \lVert T \rVert_{S\to X} < \infty$, which implies that $\overline{T}$ is bounded.

Finally, we show that $\overline{T}$ is unique. Suppose there is another norm-preserving linear extension $\tilde{T}$ of $T$ from $S$ to $Z$. If $\tilde{T}(z) \neq T(z)$ for some $z\in Z$, then for a sequence $(s_k)_{k\in\N_+}$ converging to $z$, we have

$$
\begin{align*}
  0 \neq& \tilde{T}(z) - T(z) \\
  =& \tilde{T}\left(\lim_{k\to\infty} s_k \right) - T\left(\lim_{k\to\infty} s_k \right) \\
  =& \lim_{k\to\infty} (\tilde{T}(s_k) - T(s_k)) \\
  =& \lim_{k\to\infty} (0) = 0
\end{align*}
$$

because $\tilde{T}(s) = T(s)$ for all $s\in S$.
</details>
</MathBox>

## Isomorphism

<MathBox title='Isomorphism' boxType='definition'>
A map linear and bijective map $f: X \to Y$ is an isometric isomorphism if $\lVert f(x)\rVert_Y = \lVert x\rVert_X$ for any $x \in X$.
</MathBox>

## Uniform boundedness principle

<MathBox title='Banach-Steinhaus theorem' boxType='theorem'>
Let $X$ and $Y$ be normed spaces, where $X$ is also a Banach space. For every subset $\mathcal{M} \subseteq \mathcal{B}(X, Y)$, then $\mathcal{M}$ is bounded pointwise on $X$ if and only if $\mathcal{M}$ is uniformly bounded. Pointwise boundedness implies that for all $x\in X$ there is a $C_x \geq 0$ such that $\lVert Tx\rVert_Y \leq C_x$, while uniformly boundedness implies that for all $T \in \mathcal{M}$ there is a $C \geq 0$ such that $\lVert T\rVert_{X \to Y} \leq C$.
</MathBox>

## Open mapping theorem

<MathBox title='Banach-Schauder theorem' boxType='theorem'>
Let $X$ and $Y$ be Banach spaces, and $T \in \mathcal{B}(X, Y)$ a bounded linear operator $T: X \to Y$. The operator $T$ is surjective if and only if $T$ is an open map.
</MathBox>

## Bounded inverse theorem

<MathBox title='Bounded inverse theorem' boxType='theorem'>
Let $X$ and $Y$ be Banach spaces, and $T \in \mathcal{B}(X, Y)$ a bounded linear operator $T: X \to Y$. If $T$ is bijective then $T^{-1} \in \mathcal{B}(X, Y)$, i.e. continuous.
</MathBox>

## Neumann series

<MathBox title='Neumann series' boxType='definition'>
A Neumann series is a geometric series for an operator $T$

$$
  \sum_{k=0}^\infty T^k
$$

where $T^k = T^{k-1} \circ T$. If $T$ is a bounded linear operator on a normed space $X$, and the Neumann series converges in the operator norm then $I - T$ is invertible with the inverse as the series

$$
  (I - T)^{-1} = \sum_{k=0}^\infty
$$

In analogy to a geometric series $\sum_{k=0}^n = x^k = 1 - x^{n+1}$, we have that

$$
\begin{align*}
  \lim_{n\to\infty} (I - T)\sum_{k=0}^\infty T^k =& \lim_{n\to\infty} \left(\sum_{k=0}^\infty T^k - \sum_{k=1}^\infty T^{k + 1} \right) \\
  =& \lim_{n\to\infty} I - T^{n + 1} = I
\end{align*}
$$

Convergence is guaranteed when $X$ is a Banach space with $\lVert T\rVert < 1$.
</MathBox>

# Spectral theory

<MathBox title='Spectrum and resolvent set' boxType='definition'>
Let $X$ be a complex Banach space and $T: X \to X$ be a bounded linear operator. The spectrum of $T$ is defined by

$$
  \sigma(T) := \Set{ \lambda \in \mathbb{C} | (T - \lambda I) \textrm{ not bijective}  }
$$

The resolvent set of $T$ is defined by

$$
  \rho(T) := \Set{ \lambda \in \mathbb{C} | (T - \lambda I) \textrm{ bijective and } (T - \lambda I)^{-1} \textrm{ bounded}  }
$$

The bounded inverse theorem implies that $\sigma(T) = \mathbb{C}\setminus \rho(T)$. The spectrum can be split into disjoint sets $\sigma(T) = \sigma_p(T) \cup \sigma_c(T) \cup \sigma_r(T)$ where

- Point spectrum (eigenvalues): $\sigma_p(T) := \Set{ \lambda \in \mathbb{C} | (T - \lambda I) \textrm{ not injective}  }$
- Continuous spectrum: $\sigma_c(T) := \Set{ \lambda \in \mathbb{C} | (T - \lambda I) \textrm{ injective and not surjective with } \overline{\operatorname{ran}(T - \lambda I)} = X }$
- Residual spectrum: $\sigma_c(T) := \Set{ \lambda \in \mathbb{C} | (T - \lambda I) \textrm{ injective and not surjective with } \overline{\operatorname{ran}(T - \lambda I)} \neq X }$
</MathBox>

<MathBox title='Properties of spectra and resolvent sets' boxType='proposition'>
The spectrum $\sigma(T)$ and the resolvent $\rho(T)$ has the following properties
1. The resolvent $\rho(T)$ is open, while the spectrum $\sigma(T)$ is closed
2. The map $\rho(T): \to \mathcal{B}(X)$ given by $\lambda \mapsto (T - \lambda)^{-1}$ is analytical, i.e. it can be locally expressed as a Taylor series
3. The inverse of the distance of any $\lambda \in \rho(T)$ to the spectrum, written $d(\lambda, \sigma(T))$, satisfies

$$
  \lVert (T - \lambda)^{-1}\rVert \geq \frac{1}{d(\lambda, \sigma(T))}
$$

<details>
<summary>Proof</summary>

The first property can be shown by choosing $\lambda_0 \in \rho(T)$ and setting $c := \lVert (T - \lambda_0)^{-1}\rVert$ and $\epsilon := \frac{1}{c}$. Consider any $\lambda \in \mathbb{C}$ with $\left| \lambda - \lambda_0 \right| < \epsilon$, then

$$
  T - \lambda = (T - \lambda_0) - (\lambda - \lambda_0) = (T - \lambda_0) \left( I - (\lambda - \lambda_0)(T - \lambda_0)^{-1} \right) = (T - \lambda_0) (I - S)
$$

Furthermore, $\lVert S\rVert = \lVert \lambda - \lambda_0\rVert \cdot \lVert (T - \lambda_0)^{-1}\rVert < \epsilon c = 1$. This implies that $I - S$ is a convergent Neumann series, and thus invertible. Since, $T - \lambda$ is product a two invertible operators, it is also invertible. This shows that $\lambda \in \rho(T)$, meaning that $\rho(T)$ is open. Thus, $\sigma(T)$ is closed.

The second property follows from the results above, by writing out the inverse of $T - \lambda$

$$
\begin{align*}
  (T - \lambda)^{-1} =& (I - S)^{-1} (T - \lambda)^{-1} = \sum_{k=0}^\infty S^k (T - \lambda_0)^{-1} \\
  =& \sum_{k=0}^\infty (\lambda - \lambda_0 )^k (T - \lambda_0)^{-k} (T - \lambda_0)^{-1} \\
  =& \sum_{k=0}^\infty (T - \lambda_0)^{-(k + 1)} (\lambda - \lambda_0)^k
\end{align*}
$$

which takes the form of a Taylor series.

The third property also follows from the results above as

$$
  |\lambda - \lambda_0 | \geq \epsilon \implies \frac{1}{| \lambda - \lambda_0 |} \leq C = \lVert (T - \lambda)^{-1}\rVert
$$

Hence, we get

$$
  \frac{1}{d(\lambda, \sigma(T))} = \frac{1}{\inf_{\lambda \in \sigma(T)} | \lambda - \lambda_0 |} = \sup_{\lambda \in \sigma(T)} \frac{1}{| \lambda - \lambda_0 |} \leq \lVert (T - \lambda)^{-1}\rVert
$$
</details>
</MathBox>

## Spectral radius

<MathBox title='Properties of spectra and spectral radi' boxType='proposition'>
Let $X$ be a complex Banach space and $T: X \to X$ be a bounded linear operator. The spectrum $\sigma(T)$ has the following properties

1. $\sigma(T) \subseteq \mathbb{C}$ is compact (closed and bounded).
2. $\sigma(T) \neq \emptyset$ for $X \neq \Set{ 0 }$
3. $r(T) := \sup_{\lambda\in\sigma(T)}|\lambda| = \lim_{k\to\infty} \lVert T^k\rVert^{1/k} = \inf_{k\in\N} \lVert T^k\rVert^{1/k} \leq \lVert T\rVert < \infty$

where $r(T)$ denotes the spectral radius. 

<details>
<summary>Proof</summary>

The boundedness implied by the first and last property can be proved by choosing $\lambda \in \mathbb{C}$ with $|\lambda| > \lVert T\rVert$ and consider $\frac{T}{\lambda}$ as a Neumann series

$$
\begin{gather*}
  \left( I - \frac{T}{\lambda} \right)^{-1} = \sum_{k=0}^\infty \left( \frac{T}{\lambda} \right)^k \\
  \implies (T - \lambda)^{-1} = -\frac{1}{\lambda} \left(I - \frac{T}{\lambda} \right)^{-1} = -\frac{1}{\lambda} \sum_{k=0}^\infty \left( \frac{T}{\lambda} \right)^k \\
  \sup_{\lambda \in \sigma(T)} |\lambda | \leq \lVert T\rVert
\end{gather*}
$$

which shows that $\sigma(T)$ is bounded. 

The second property can be proved by contraposition, assuming $\sigma(T) = \emptyset \implies \rho(T) = \mathbb{C}$. Recall that the map $\rho(T) \to \mathcal{B}(X)$ given by $\lambda \mapsto (T - \lambda)^{-1}$ is analytic. Thus, we can take any linear functional $\ell \in \mathcal{B}(X)^*$ and apply it on the map above resulting in the map $f_\ell: \mathbb{C} \to \mathbb{C}$ given by $\lambda \mapsto \ell\left[ (T - \lambda)^{-1} \right]$. Then for any $\lambda \geq \lVert T\rVert$ we have

$$
  \left| f_\ell(\lambda) \right| \leq \lVert \ell\rVert \cdot \lVert (T - \lambda)^{-1}\rVert \leq \lVert \ell\rVert \frac{1}{|\lambda|}\sum_{k=0}^\infty \left\lVert \frac{T}{\lambda} \right\|^k \leq \frac{\| \ell\rVert}{\lVert T\rVert}
$$

which shows that $f_\ell$ is a bounded entire function. Applying Liouville's theorem implies that $f_\ell$ is constant. We can then proceed to calculate this constant for $\lambda_0 = 0$

$$
  f_\ell (0) = \ell(T^{-1})
$$

This should hold for all $\lambda \in \mathbb{C}$

$$
  f_\ell (\lambda) = \ell\left[ (T - \lambda)^{-1} \right] = \ell\left( \sum_{k=0}^\infty (T - \lambda_0)^{-(k + 1) (\lambda - \lambda_0)^k} \right) = \sum_{k=0}^\infty \ell \left( T^{-(k+1)} \right)\lambda^k
$$

Consequently $\ell\left(T^{-2}\right) = 0$ for all $\ell \in \mathcal{B}(X)^*$. By the Hahn-Banach theorem, we must have $T^{-2} = 0$, but this implies that $X = \Set{ 0 }$ since $T$ is bijective. Hence $\sigma(T) \neq \emptyset$ for  $X \neq \Set{0}$.
</details>
</MathBox>

## Adjoint operators

<MathBox title='Adjoint operator' boxType='definition'>
Let $X$ be a Hilbert space and $T: X \to X$ a bounded linear operator. The adjoint operator of $T$, denoted $T^\dagger: X \to X$, satisfies

$$
  \langle y, Tx \rangle = \langle T^* y, x \rangle \quad \forall x, y \in X
$$

A bounded linear operator $T$ is called
1. self-adjoint if $T^* = T$
2. skew-adjoint if $T^* = -T$
3. normal if $T^* T = T T^*$

If $T$ is normal then the spectral radius of $T$ is equal to the operator norm, i.e. $r(T) = \lVert T\rVert$.
</MathBox>

## Compact operators

<MathBox title='Compact operator' boxType='proposition'>
Let $\left(X ,\lVert \cdot\rVert_X\right)$ and $\left(X ,\lVert \cdot\rVert_X\right)$ be normed spaces. A bounded linear operator $T: X \to Y$ is *compact* if $\overline{T\left[\mathcal{B}_1(0)\right]}$ is compact.
</MathBox>

<MathBox title='' boxType='proposition'>
Suppose $\left(X ,\lVert \cdot\rVert_X\right)$ is a Banach space, and $T: X \to X$ a compact operator, then
1. $\sigma(T)$ is a countable set
2. $\dim(X) = \infty \implies 0 \in \sigma(T)$
3. $\sigma(T)\setminus\Set{0}$ could be empty or finite. Otherwise $\sigma(T)\setminus\Set{0} = \Set{ \lambda_n }_{n\in\N}$ with no accumulation points other than $0$.
4. Each $\lambda\in\sigma(T)\setminus\Set{0}$ is an eigenvalue of $T\left(\lambda \in \sigma(T)\setminus\Set{0}\right)$ with $\dim\left(\ker(T - \lambda) \right)$
</MathBox> 

## Example: $\ell^p$

Consider the sequence space $X = \ell^p (\N)$. For $\Set{ \lambda_j }_{j\in\N} \subset \mathbb{C}$ with $\sup_{j\in\N}\left| \lambda_j \right| < \infty$ we define a bounded linear operator $T:\ell^p (\N) \to \ell^p (\N)$ given by $(Tx)_j := \lambda_j x_j$. Evidently, the $\lambda_j$ are eigenvalues with corresponding eigenvectors $e_j$ as unit sequences, and the set $\Set{ \lambda_j }_{j\in\N} \subset \mathbb{C}$ forms the point spectrum of $T$

$$
  \Set{ \lambda_j }_{j\in\N} = \sigma_p (T) \subseteq \sigma (T)
$$

Because $\sigma_p (T)$ is infinite it can have accumulation points $\mu \in \mathbb{C}$ with $\mu \in \overline{\sigma_p (T)}$ such that $T - \mu I$ is injective. It can be shown that $T - \mu I$ is not surjective.

Assuming by contradiction that $T - \mu I$ is surjective, implying that it is also bijective. By the bounded inverse theorem, the inverse $(T - \mu I)^{-1}$ is therefore bounded with operator norm

$$
\begin{align*}
  \lVert (T - \mu I)^{-1}\rVert \geq& \lVert (T - \mu I)^{-1} e_j\rVert_{\ell^p (\N)} \\
  =& \lVert (\lambda_j - \mu I)^{-1} e_j\rVert_{\ell^p (\N)} \\
  =& \frac{1}{\left| \lambda_j - \mu \right|}
\end{align*}
$$

Since $\mu$ is an accumulation point we can find a subsequence such that the resulting fraction goes to infinity, so that $(T - \mu I)^{-1}$ cannot be bounded. Thus, by contradiction $T - \mu I$ is not surjective. The spectrum of $T$ becomes

$$
\begin{align*}
  \sigma (T) =& \Set{ \mu \in \mathbb{C} | \mu \notin \Set{\lambda_j}_{j\in\N} \land \mu\in\overline{\Set{\lambda_j }}_{j\in\N}} \\
  =& \sigma_p(T) \cup \sigma_c(T)
\end{align*}
$$

# Analysis in normed spaces

<MathBox title='Bounded multilinear operators' boxType='definition'>
Let $X$ and $Y$ be normed spaces. The space of bounded $k$-linear operators is defined inductively by

$$
  \mathcal{B}(X,Y) = \mathcal{B}(X, \mathcal{B}^{k-1}(X,Y)),\; k\in\N
$$

where $\mathcal{B}^1(X,Y) = \mathcal{B}(X,Y)$. An element $L\in\mathcal{B}^k (X,Y)$ is a $k$-linear operator $L:X^k \to Y$
</MathBox>

<MathBox title='Continuity criterion for bounded multilinear operators' boxType='definition'>
Let $X$ and $Y$ be normed spaces. An $n$-linear operator $L\in\mathcal{B}^n (X,Y)$ is continuous if its norm

$$
  \lVert L \rVert_{X\to\mathcal{B}^{n-1}(X,Y)} = \sup\Set{\frac{\lVert L(h_1,\dots,h_n) \rVert_Y}{\prod_{i=1}^n \lVert h_i \rVert_X} | h_i \in X_i \setminus\Set{0}}
$$

is finite.

<details>
<summary>Proof</summary>

The proof is shown for $n = 2$ (the result follows by induction). An element of $\mathcal{B}^2 (X, Y)$ is a linear tranformation $L:X\to\mathcal{B}(X,Y)$ whose induced norm

$$
  \lVert L \rVert_{X\to\mathcal{B}(X,Y)} = \sup\Set{\frac{\lVert L(h_1) \rVert_{X\to Y}}{\lVert h_1 \rVert_X} | h_1 \in X\setminus\Set{0}}
$$

is finite, where for each $h_1 \in X$, the linear transformation $L(h_1): X \to Y$ is bounded, i.e.

$$
  \lVert L(h_1)\rVert_{X\to Y} = \sup\Set{\frac{\lVert L(h_1)h_2 \rVert_Y}{\lVert h_2 \rVert_X} | h_2 \in X\setminus\Set{0}} < \infty
$$

We combine $\lVert L(h_1) \rVert_{X\to Y} \leq \lVert L \rVert_{X\to\mathcal{B}(X,Y)}\lVert h_1 \rVert_X$ and $\lVert L(h_1)h_2 \rVert_Y \leq \lVert L(h_1) \rVert_{X\to Y} \lVert h_2 \rVert_X$ to get

$$
  \lVert L(h_1)h_2 \rVert_Y \leq \lVert L \rVert_{X\to\mathcal{B}(X,Y)} \lVert h_1 \rVert_X \lVert h_2 \rVert_X
$$

or when both $\lVert h_1 \rVert_X$ and $\lVert h_2 \rVert_X$ are nonzero, that

$$
  \lVert L \rVert_{X\to\mathcal{B}(X,Y)} \geq \frac{\lVert L(h_1)h_2 \rVert_Y}{\lVert h_1 \rVert_X \lVert h_2 \rVert_X}
$$

The upper bound $\lVert L \rVert_{X\to\mathcal{B}(X,Y)}$ on the ratios is the supremum because for $\epsilon > 0$ there exists a nonzero $h_1 \in X$ such that

$$
  \frac{\lVert L(h_1) \rVert_{X\to Y}}{\lVert h_1 \rVert_X} > \lVert L \rVert_{X\to\mathcal{B}(X,Y)} - \frac{\epsilon}{2}
$$

and there exists a nonzero $h_2 \in X$ such that

$$
  \frac{\lVert L(h_1)h_2 \rVert_Y}{\lVert h_2 \rVert_X} > \lVert L(h_1) \rVert_{X\to Y} - \frac{\epsilon\lVert h_1 \rVert_X}{2}
$$

so that

$$
\begin{align*}
  \frac{\lVert L(h_1) h_2 \rVert_Y}{\lVert h_1 \rVert_X \lVert h_2 \rVErt_X} >& \frac{\lVert L(h_1) \rVert_{X\to Y}}{\lVert h_1 \rVert_X} - \frac{\epsilon}{2} \\
  >& \lVert L \rVert_{X\to\mathcal{B}(X,Y)} -\epsilon
\end{align*}
$$

Thus

$$
  \lVert L \rVert_{X\to\mathcal{B}(X,Y)} = \sup\Set{\frac{\lVert L(h_1)h_2 \rVert_Y}{\lVert h_1 \rVert_X \lVert h_2 \rVert_X} | h_1, h_2 \in X\setminus\Set{0}}
$$
</details>
</MathBox>

## Fréchet derivative

<MathBox title='Fréchet derivative' boxType='definition'>
Let $(X, \lVert\cdot\rVert_X)$ and $(Y, \lVert\cdot\rVert_Y)$ be normed spaces. A function $f:X\to Y$ is *Fréchet differentiable* at $x\in X$ if there exists a bounded linear operator $T\in\mathcal{B}(X, Y)$ such that

$$
  \lim_{\lVert h \rVert_X \to 0} \frac{\lVert f(x + h) - f(x) - Th \rVert_Y}{\lVert h \rVert_X} = 0
$$

The operator $T$ is called the *Fréchet derivative* of $F$ at $x$, denoted $T = \d f(x)$. The Fréchet differential $Th = \d f(x) h$ is called the *first order variation* of $f$ at $x$. The function $f$ is *Fréchet differentiable* on $X$ if the function $\d f : X \to \mathcal{B}(X, Y)$ is continuous. The set of continuosly Fréchet differentiable functions from $X$ to $Y$ is denoted $C^1 (X, Y)$.
</MathBox>

<MathBox title='Fréchet partial derivative' boxType='definition'>
Let $X_i$ for $i=1,\dots,n$ be a finite collection of normed spaces. Fix an open set $U \subset X_1 \times\cdots\times X_n$, and an ordered list of $k$ integers $i_1,\dots,i_k$ where $i_j \in\Set{1,\dots,k}$ (not necessarily distinct). The $k$-th order partial derivative of $f\in C^k (U, Y)$ corresponding to $i_1,\dots,i_k$ is the function $\d_{i_1}\cdots\d_{i_k} f \in C(U, \mathcal{B}(X_1,\dots,X_k;Y))$.

When $X_i = \mathbb{F}$ for all $i=1,\dots,n$ and $Y$ the $k$-th order partial derivative $\d_{i_1}\cdots\d_{i_k} f$ is written

$$
  \frac{\partial^k f}{\partial x_{i_1}\cdots\partial x_{i_k}}
$$
</MathBox>

<MathBox title='The Fréchet derivative is unique' boxType='proposition'>
Let $X$ and $Y$ be normed spaces. If a function $f:X\to Y$ is Fréchet differentiable at $x\in X$, then the Fréchet derivative $\d f(x): X\to Y$ is unique.

<details>
<summary>Proof</summary>

Assume $T_1, T_2 \in\mathcal{B}(X,Y)$ are Fréchet derivatives of $f:X\to Y$ at $x\in X$ satisfying

$$
  \lim_{h\to 0} \frac{\lVert f(x + h) - f(x) - T_i h \rVert_Y}{\lVert h\rVert_X} = 0,\; i=1,2
$$

For nonzero $v\in X$ and $\tau\in\mathbb{F}$ we have

$$
\begin{align*}
  \frac{\lVert T_1 v - T_2 v \rVert_Y}{\lVert v \rVert_X} =& \frac{|\tau|}{|\tau|}\frac{\lVert T_1 v - T_2 v\rVert_Y}{\lVert v \rVert_X} \\
  =& \frac{\lVert T_1 (\tau v) - T_2 (\tau v) \rVert_Y}{\lVert \tau v \rVert_X} \\
  =& \frac{\lVert (f(x + \tau v) - f(x) - T_2 (\tau v)) - (f(x + \tau v) - f(x) - T_1 (\tau v)) \rVert_Y}{\lVert \tau v\rVert_X} \\
  \overset{\triangle}{\leq} \frac{\lVert f(x + \tau v) - f(x) - T_2 (\tau v) \rVert_Y}{\lVert \tau v \rVert_X} + \frac{\lVert f(x + \tau v) - f(x) - T_1 (\tau v) \rVert_Y}{\lVert \tau v \rVert_X} \xrightarrow{\tau\to 0} \to 0
\end{align*}
$$

This shows that $T_1 v = T_2 v$ for all $v\in X$, implying that $T_1 = T_2$.
</details>
</MathBox>

<MathBox title='Fréchet differentiability criterion' boxType='proposition'>
Let $X$ and $Y$ be normed spaces. Then the following are equivalent
1. A function $f:X\to Y$ is Fréchet differentiable at $x$ with derivative $T \in\mathcal{B}(X,Y)$
2. For every $\epsilon > 0$ there exists $\delta > 0$ with the open ball $B(x, \delta) \subset X$ such that for all $h\in B(x, \delta)$
$$
  \lVert f(x + h) - f(x) - Th \rVert_Y \leq \epsilon \lVert h \rVert_X
$$
</MathBox>

<MathBox title='Fréchet differentiable functions are Lipschitz' boxType='proposition' tag='proposition-23'>
Let $X$ and $Y$ be normed spaces. If $f:X\to Y$ is Fréchet differentiable at $x_0$, then $f$ is locally Lipschitz at $x_0$. That is, there exists $\delta > 0$ and $L > 0$ such that $B(x_0, \delta) \subset X$ and for all $x \in B(x,\delta)$

$$
  \lVert f(x) - f(x_0) \rVert_Y \leq L\lvert x - x_0 \rVert_X
$$

<details>
<summary>Proof</summary>

Since $f:X\to Y$ is Fréchet differentiable at $x_0 \in X$, then for $\epsilon = 1$ there is $\delta > 0$ such that for all $x\in X$ with $\lVert x - x_0 \rVert_X < \delta$ we have

$$
  \frac{\lVert f(x) - f(x_0) - \d f(x_0) (x - x_0) \rVert_Y}{\lVert x - x_0 \rVert_X} < 0
$$

This can be rewritten as

$$
  \lVert f(x) - f(x_0) - \D f(x_0)(x - x_0) \rVert_Y < \lVert x - x_0 \rVert_X
$$

By the triangle inequality, we have

$$
\begin{align*}
  \lVert f(x) - f(x_0) \rVert_Y =& \lVert f(x) - f(x_0) - \d f(x_0) (x - x_0) + \d f(x_0)(x - x_0) \rVert_Y \\
  \leq& \lVert f(x) - f(x_0) - \d f(x_0) (x - x_0) \rVert_Y + \lVert \d f(x_0)(x - x_0) \rVert_Y \\
  \leq& \lVert x - x_0 \rVert_X + \lVert \d f(x_0)(x - x_0) \rVert_Y 
\end{align*}
$$

Since $\d f(x_0)$ is a bounded linear operator, it follows that

$$
  \lVert \d f(x_0)(x - x_0) \rVert_Y \leq \lVert \d f(x_0) \rVert \cdot \lVert \rVert_X
$$

Thus, we obtain

$$
  \lVert f(x) - f(x_0) \rVert_Y \leq (1 + \lVert \d f(x_0)\rVert)\lVert x - x_0 \rVert_X
$$

Taking $L = 1 + \lVert \d f(x_0) \rVert$ shows that $f$ is locally Lipschitz at $x_0$.
</details>
</MathBox>

<MathBox title='Fréchet linearity' boxType='proposition'>
Let $X$ and $Y$ be normed spaces. If $f,g:X\to Y$ are differentiable at $x \in X$, then for any scalars $\alpha,\beta\in\mathbb{F}$, the linear combination $\alpha f + \beta g$ is differentiable at $x$ with derivative

$$
  \d (\alpha f + \beta g)(x) = \alpha \d f(x) + \beta \d g(x)
$$

<details>
<summary>Proof</summary>

By differentiability of $f$ and $g$ at $x$, for any $\epsilon > 0$ there is $\delta > 0$ such that for all $h \in B(x, \delta) \subset X$ we have

$$
\begin{align*}
  \lVert f(x + h) - f(x) - \d f(x)h \rVert_Y \leq& \frac{\epsilon \lVert h \rVert_X}{2(|\alpha| + 1)} \\
  \lVert g(x + h) - g(x) - \d g(x)h \rVert_Y \leq& \frac{\epsilon \lVert h \rVert_X}{2(|\beta| + 1)}
\end{align*}
$$

Thus

$$
\begin{align*}
  & \lVert \alpha f(x + h) + \beta g(x + h) - \alpha f(x) - \beta g(x) - \alpha\d f(x)h - \beta\d g(x) h \rVert_Y \\
  &\leq |\alpha|\cdot\lVert f(x + h) - f(x) - \d f(x)h \rVert_Y + |\beta|\cdot\lVert g(x + h) - g(x) - \d g(x)h \rVert_Y \\
  &\leq \frac{\epsilon|\alpha|\cdot\lVert h \rVert_X}{2(|\alpha| + 1)} + \frac{\epsilon|\beta|\cdot\lVert h \rVert_X}{2(|\beta| + 1)} = \epsilon \lVert h \rVert_X \left(\frac{|\alpha|}{2(|\alpha| + 1)} + \frac{|\beta|}{2(|\beta| + 1)} \right) \\
  <& \epsilon \lVert h \rVert_X
\end{align*}
$$

Since $\d f(x), \d g(x)\in\mathcal{B}(X,Y)$ it follows that $\alpha \d f(x) + \beta g(x) \in \mathcal{B}(X,Y)$. Hence, $\alpha f + \beta g$ is differentiable at $x$ with Fréchet derivative $\alpha \d f(x) + \beta g (x)$.
</details>
</MathBox>

<MathBox title='Fréchet product rule' boxType='proposition'>
Let $X$ be a normed space and $\mathbb{F}$ a field. If $f,g: X\to\mathbb{F}$ are Fréchet differentiable at $x\in X$, then the product $fg$ is differentiable at $x$ with derivative

$$
  \d (fg)(x) = g(x) \d f(x) + f(x) \d g(x)
$$

<details>
<summary>Proof</summary>

By differentiability of $f$ and $g$ at $x$, for each $\epsilon > 0$ there is $\delta_x > 0$ with $B(x,\delta_x)\subset X$, and a constant $L > 0$ (by Proposition $\ref{proposition-23}$) such that for all $0 < \lVert h\rVert < \delta_x$

$$
  |f(x + h) - f(x)| \leq L \lVert h \rVert_X
$$

and

$$
\begin{align*}
  |f(x + h) - f(x) - \d f(x)h| \leq& \frac{\epsilon \lVert h \rVert_X}{3(|g(x)| + 1)} \\
  |g(x + h) - f(x) - \d f(x)h| \leq& \frac{\epsilon \lVert h \rVert_X}{3(|f(x)| + L)}
\end{align*}
$$

For $\epsilon > 0$ choose

$$
  \delta = \min\Set{1, \delta_x, \frac{\epsilon}{3L(\lVert \d g(x) \rVert + 1)}}
$$

When $0 \leq \lVert h \rVert_X < 0$, we get

$$
\begin{align*}
  & |f(x + h)g(x + h) - f(x)g(x) - g(x) \d f(x) h - f(x) \d g(x) h| \\
  =& |f(x + h)g(x + h) - f(x + h)g(x) + f(x + h)g(x) - f(x)g(x) \\
  &+ f(x + h)\d g(x) h - f(x + h) \d g(x) h - g(x) \d f(x) h - f(x) \d g(x) h| \\
  \leq& |f(x + h)|\cdot |g(x + h) - g(x) - \d g(x)h| \\
  &+ |g(x)|\cdot|f(x + h) - f(x) - \d f(x) h| \\
  &+ |f(x + h) - f(x)|\cdot\lVert \d g(x) \rVert\cdot\lVert h \rVert_X \\
  \leq& (|f(x)| + L)\frac{\epsilon\lVert h \rVert_X}{3(|f(x) + L|)} \\
  &+ |g(x)|\frac{\epsilon\lVert h \rVert_X}{3(|g(x)| + 1)} + \delta L \lVert \d g(x) \rVert\cdot\lVert h \rVert_X \\
  <& \epsilon \lVert h \rVert_X
\end{align*}
$$

Where we have used the Lipschitz implication

$$
  |f(x + h) - f(x)| \leq L\lVert h \rVert_X \implies |f(x + h)| \leq |f(x)| + L\lVert h \rVert_X
$$

Hence, $fg$ is differentiable at $x$ with Fréchet derivative $\d (fg)(x) = g(x) \d f(x) + f(x) \d g(x)$.
</details>
</MathBox>

<MathBox title='Bilinear Fréchet product rule' boxType='proposition' tag='proposition-30'>
Let $X$ and $Y$ be normed spaces. If $L \in\mathcal{L}^2 (X,Y)$ is a bilinear operator, then $L\in C^2 (X^2, Y)$ with

$$
  \d L(x)h = L(h_1, x_2) + L(x_1, h_2)
$$

If $X$ is a normed algebra, and $L(x_1, x_2) = x_1 x_2$ we obtain the usual form of the product rule.

<details>
<summary>Proof</summary>

By bilinearity

$$
  L(x_1 + h_1, x^2 + h_2) - L(x_1, x_2) = L(h_1, x_2) + L(x_1, h_2) + L(h_1, h_2)
$$

Since $|L(h_1,h_2)| \leq \lVert L \rVert\cdot|h_1|\cdot|h_2| = o(|h|^2)$ we get

$$
  L(x_1 + h_1, x^2 + h_2) - L(x_1, x_2) = L(h_1, x_2) + L(x_1, h_2) + O(|h|^2)
$$
</details>
</MathBox>

<MathBox title='Fréchet chain rule' boxType='proposition'>
Let $X, Y, Z$ be normed spaces. If $f:X\to Y$ is Fréchet differentiable at $x\in X$ and $g:Y\to Z$ is Fréchet differentiable at $y = f(x) \in Y$, the composition $g\circ f$ is Fréchet differentiable at $x$ with derivative

$$
  \d (g\circ f)(x) = \d g(f(x)) \d f(x)
$$

<details>
<summary>Proof</summary>

Choose $\epsilon > 0$. By differentiability of $f:X\to Y$ at $x\in X$ and of $g:Y\to Z$ at $y = f(x) \in Y$, there is $\delta_1 > 0$ such that $B(x,\delta_1)\subset X$ and for all $\xi \in X$ satisfying $0 \lVert\xi\rVert_X < \delta_1$ we have

$$
  \lVert f(x + \xi) - f(x) - \d f(x)\xi \rVert_Y \leq \frac{\epsilon\lVert\xi\rVert_X}{2(\lVert \d g(y) \rVert + 1)}
$$

By Proposition $\ref{proposition-23}$, $f$ is locally Lipschitz at $x$, i.e. there is $\delta_2 > 0$ and $L > 0$ such that $B(x,\delta_2)\subset X$ and for all $\xi\in X$ satisfying $0 < \lVert\xi\rVert_X < \delta_2$ we have

$$
  \lVert f(x + \xi) - f(x) \rVert_X \leq L\lVert\xi\rVert_X
$$

Set $\delta_x = \min\Set{\delta_1, \delta_2}$. Since $g$ is differentiable at $y$, there is $\delta_y > 0$ such that $B(y, \delta_y) \subset Y$ and for all $0 \leq \lVert\eta\rVert_Y < \delta_y$ we have

$$
  \lVert g(y + \eta) - g(y) -\d g(y)\eta \rVert_Z \leq \frac{\epsilon\lVert\eta\rVert_Y}{2L}
$$

Set $\delta = \min\Set{\delta_x, \delta_y / L}$ such that $L\delta\leq\delta_y$. If we take $\eta(\xi) = f(x + \xi) - f(x) = f(x + \xi) - y$ then $h = g\circ f$ satisfies

$$
  h(x + \xi) - h(x) = g(f(x + \xi)) - g(f(x)) = g(y + \eta(\xi) - g(y))
$$

Thus for all $\xi\in X$ satisfying $\lVert\xi\rVert_X < \delta$ we have

$$
  \lVert \rVert_Y = \lVert f(x + \xi) - f(x) \rVert_Y \leq L\lVert\xi\rVert_X < L\delta \leq \delta_y
$$

so that

$$
\begin{align*}
  &\lVert h(x + \xi) - h(x) - \d g(y) \d f(x)\xi \rVert_Z \\
  =& \lVert g(y + \eta(\xi)) - g(y) - \d g(y)\eta(\xi) + \d g(y) \eta(\xi) - \d g(y) \d f(x)\xi \rVert_Z \\
  \overset{\triangle}{\leq}& \lVert g(y + \eta(\xi)) - g(y) - \d g(y)\eta(\xi) \rVert_Z \\
  &+ \lVert \d g(y)\eta(\xi) - \d g(y) \d f(x)\xi \rVert_Z \\
  \leq& \lVert g(y + \eta(\xi)) - g(y) - \d g(y)\eta(\xi) \rVert_Z \\
  &+ \lVert \d g(y) \rVert \cdot \lVert \eta(\xi) - \d f(x)\xi \rVert_Y \\
  =& \lVert g(y + \eta(\xi)) - g(y) - \d g(y)\eta(\xi) \rVert_Z \\
  &+ \lVert \d g(y) \rVert \cdot \lVert f(x + \xi) - f(x) - \d f(x)\xi \rVert_Y \\
  \leq& \frac{\epsilon\lVert\eta(\xi)\rVert_Y}{2L} + \lVert \d g(y) \rVert \frac{\epsilon\lVert\xi\rVert_X}{2(\lVert\d g(y)\rVert + 1)} \\
  <& \frac{\epsilon L\lVert\xi\rVert_X}{2 L} + \frac{\epsilon\lVert\xi\rVert_X}{2} = \epsilon\lVert\xi\rVert_X
\end{align*}
$$

Since $\d g(x) \in\mathcal{B}(X,Y)$ and $\d g(y) \in\mathcal{B}(Y,Z)$, it follows that $\d g(y) \d f(x) \in \mathcal{B}(X,Z)$. Hence $g\circ f$ is differentiable at $x$ with Fréchet derivative $\d (g\circ f)(x) = \d g(f(x)) \d f(x)$.
</details>
</MathBox>

<MathBox title='Fréchet mean value theorem (univariate)' boxType='theorem'>
Let $X$ be a normed space. If $f\in C^1 (I,X)$ is continuously differentiable on an interval $I\subseteq\R$, then for $s,t\in I$ with $s\leq t$

$$
\begin{gather*}
  \lVert f(t) - f(s) \rVert_X \leq M |t-s| \\
  M := \sup_{\tau\in[s,t]} \lVert \d f(\tau) \rVert
\end{gather*}
$$

<details>
<summary>Proof</summary>

Fix $\tilde{M} > M\in\R$ and consider $d(\tau) := \lVert f(\tau) - f(s)\rVert_X - \tilde{M}(\tau - s)$ for $\tau\in[s,t]$. Suppose $\tau_0$ is the largest $\tau$ for which $d(\tau) \leq 0$ holds. Then tehre must be a sequence $\epsilon_n \downarrow 0$ such that

$$
\begin{align*}
  0 \leq& d(\tau_0 + \epsilon_n) \\
  \leq& \lVert f(\tau_0 + \epsilon_n) - f(\tau_0) \rVert - \tilde{M}\epsilon_n + d(\tau_0) \\
  =& \lVert \d f(\tau_0)\epsilon_n + o(\epsilon_n) \rVert - \tilde{M}\epsilon_n \\
  \leq& (M - \tilde{M} + o(1))\epsilon_n < 0
\end{align*}
$$

Taking $n\to\infty$ contradicts our assumption.
</details>
</MathBox>

<MathBox title='Matrix representation of Fréchet derivatives' boxType='proposition' tag='proposition-33'>
Let $U\subseteq \R^n$ be open. If a vector function $f:U \to\R^m$ by $f = (f_i:U\to\R)_{i=1}^m$, in the standard basis of $\R^n$, is Fréchet differentiable at $\mathbf{x}\in U$ , then the partial derivatives $\d_j f_i \mathbf{x}$ exist for all $j = 1,\dots,n$ and $i=1,\dots,m$. The matrix representation of $\d f(\mathbf{x})$ in standard coordinates is the Jacobian matrix

$$
  J(\mathbf{x}) = \begin{bmatrix} 
    \d_1 f_1 (\mathbf{x}) & \cdots & \d_n f_1 (\mathbf{x}) \\
    \vdots & \ddots & \vdots \\
    \d_1 f_m (\mathbf{x}) & \cdots & \d_n f_m (\mathbf{x})
  \end{bmatrix}
$$

<details>
<summary>Proof</summary>

Let $J_j$ be the $j$-th column of $J(\mathbf{x})$, i.e. $J_j = \d f(\mathbf{x})\mathbf{e}_j$, and let $J_{ij}$ be the $i$-th entry of the $j$-column of $J(\mathbf{x})$. For $\mathbf{h} = r\mathbf{e}_j$ we have

$$
\begin{align*}
  0 =& \lim_{\mathbf{h}\to\mathbf{0}} \frac{\lVert f(\mathbf{x} + \mathbf{h}) - f(\mathbf{x}) - \d f(\mathbf{x}\mathbf{h}) \rVert}{\lVert \mathbf{h} \rVert} \\
  =& \lim_{r\to 0} \frac{\lVert f(\mathbf{x} + r\mathbf{e}_j) - f(\mathbf{x}) - r\d f(\mathbf{x}) \mathbf{e}_j \rVert}{|r|\cdot\lVert\mathbf{e}_j\rVert} \\
  =& \lim_{r\to 0} \frac{\lVert f(x_1,\dots,x_j + r,\dots,x_n) - f(x_1,\dots,x_n) - rJ_j \rVert}{|r|}
\end{align*}
$$

This implies that for each component of $f$

$$
  \lim_{r\to 0} \frac{|f(x_1,\dots,x_j + r,\dots,x_n) - f(x_1,\dots,x_n) - rJ_{ij} |}{|r|} = 0
$$

showing that the partial derivative $\d_j f_i (\mathbf{x}) = J_{ij}$ exists.
</details>
</MathBox>

### Higher order derivatives

<MathBox title='Higher order Fréchet derivatives' boxType='definition'>
Let $X$ and $Y$ be normed spaces, and suppose $f:X\to Y$ is Fréchet differentiable on $X$. The function $f$ is twice Fréchet differentiable on $X$ if $\d f: X\to \mathcal{X,Y}$ is differentiable on $X$. The second order Fréchet derivative is the map

$$
  \d^2 f = \d (\d f): X\to\mathcal{B}(X,\mathcal{B}(X,Y)) = \mathcal{B}^2 (X,Y)
$$

Inductively, $f$ is $n \geq 2$ times Fréchet differentiable on $X$ if the map $\d^{n-1} f:X\to\mathcal{B}^{n-1} (X,Y)$ is differentiable on $X$. The $n$-the order Fréchet derivative is the map

$$
  \d^n f = \d (\d^{n-1} f)
$$

For each $x\in X$, the $n$-th order derivative $\d^n f(x)\in\mathcal{B}^n (X,Y)$ is a continuous multilinear operator $\d^n f(x): X^n \to Y$.

The set of $n$-times continuously Fréchet differentiable functions from $X$ to $Y$ is denoted $C^n (X, Y) = \Set{f\in C^1 (X, Y) | \d f \in C^{n-1} (X, Y)}$ and is a vector space of function. A function $f$ is *smooth* if $f\in C^n (X,Y)$ for all $n\in\N$. The space of smooth functions from $X$ to $Y$ is denoted $C^\infty (X,Y)$.
</MathBox>

<MathBox title='Symmetry of second order Fréchet derivatives' boxType='proposition'>
Let $X$ and $Y$ be normed spaces with $Y$ finite-dimensional. If $f\in C^2$ (X, Y) is twice differentiable on an open subset $U\subseteq X$, then for all $\mathbf{x}\in U$ and for all $(u, v)\in X\times X$

$$
  \d^2 f(x)(u,v) = \d^2 f(x)(v,u)
$$

When $U$ is an open subset of $X = X_1 \times\cdots\times X_n$ for normed spaces $X_i$ and $f\in C^2 (U, Y)$ for finited dimensional $Y$, then for all $x\in U$ and for all $i,j\in\Set{1,\dots,n}$

$$
  \d_i \d_j f(x) = \d_j \d_i f(x)
$$

When $U$ is an open subset of $X = \mathbb{F}^n$, $Y = \mathbb{F}^m$ and $f = (f_1,\dots,f_m)\in C^2 (U,Y)$, then for all $x\in U$ and for all $i,j\in\Set{1,\dots,n}$ and for all $k\in\Set{1,\dots,m}$ there holds

$$
  \frac{\partial^2 f_k}{\partial x_i \partial x_j} = \frac{\partial^2 f_k}{\partial x_j \partial x_i}
$$

<details>
<summary>Proof</summary>

The hypothesis of finite dimensionality of $Y$ implies that we can assume without loss of generality that $Y = \mathbb{F}^m$ and that $f = (f_1,\dots,f_m)$. Since $\mathbb{C}^m$ and $\R^{2m}$ are isomorphic as Banach spaces (with the standard norms) we can assume without loss of generality that $\mathbb{F} = \R$. As $f_k :U\to\R$, it suffices to show the result for $Y = \R$.

For a fixed $x\in U$ and $u,v\in X$ there exist $t,s > 0$ by the openness of $U$ such that $x + \xi u + \eta v \in U$ for all $\xi, \eta \in [0,\max\Set{s,t}]$. Define $g:[0,t]\to\R$ by

$$
  g_\xi (x) = f(x + \xi u) - f(x)
$$

and $S_{\eta, t}(x):[0,s]\to\R$ by

$$
\begin{align*}
  S_{\eta,t}(x) =& g_t (x + \eta v) - g_t (x) \\
  =& f(x + tu + \eta v) - f(x + \eta v) - f(x + tu) + f(x) 
\end{align*}
$$

Note that $S_{0,t} (x) = 0$ and that with $x$ and $t$ fixed

$$
  \d S_{\eta, t}(x) = \d g_t (x + \eta v)v
$$

The function $S_{\eta,t}$ is continuous on $[0,s]$ and differentiable on $(0,s)$, so by the mean value theorem there is $\sigma_{s,t}\in(0,s)$ such that

$$
\begin{align*}
  S_{s,t}(x) =& S_{s,t}(x) - S_{0,t}(x) \\
  =& \d g_t (x + \sigma_{s,t}v)(v)(s - 0) \\
  =& \d g_t (x + \sigma_{s,t}v)(sv)
\end{align*}
$$

Since $g_t (x + \eta v) = f(x + tu + \eta v) - f(x + \eta v)$ we have

$$
  \d g_t (x + \sigma_{s,t}v)(sv) = \d f(x + tu + \sigma_{s,t} v)(sv) - \d f(x + \sigma_{s,t}v)(sv)
$$

The function

$$
  \xi\mapsto \d f(x + tu + \sigma_{s,t} v)(sv) - \d f(x + \sigma_{s,t}v)(sv)
$$

is zero when $\xi = 0$, is continuous on $[0,t]$ and differentiable on $(0,t)$, so by the mean value theorem there is $\tau_{s,t}\in(0,t)$ such that

$$
  \d f(x + tu + \sigma_{s,t} v)(sv) - \d f(x + \sigma_{s,t}v)(sv) = \d^2 f(x + \tau_{s,t}u + \sigma_{s,t}v)(sv)(tu)
$$

Thus,

$$
  S_{s,t}(x) = \d^2 f(x + \tau_{s,t}u + \sigma_{s,t}v)(sv)(tu)
$$

Switching the roles of $tu$ and $sv$ in the above argument gives the existence of $\tau'_{s,t}$ and $\sigma'_{s,t}$ such that

$$
  S_{s,t}(x) = \d^2 f(x + \sigma'_{s,t}v + \tau'_{s,t}u)(tu, sv)
$$

Equating the two expression for the same quantity $S_{s,t}(x)$ gives

$$
  \d^2 f(x + \tau_{s,t}u + \sigma_{s,t}v)(sv, tu) = \d^2 f(x + \sigma'_{s,t}v + \tau'_{s,t}u)(tu, sv)
$$

Since $\d^2 f(x)$ is multilinear, we can pull out the scalars $s$ and $t$ from the inputs $sv$ and $tu$ from both sides; they cancel, giving

$$
  \d^2 f(x + \tau_{s,t}u + \sigma_{s,t}v)(v,u) = \d^2 f(x + \sigma'_{s,t}v + \tau'_{s,t}u)(u,v)
$$

In the limit $s,t\to 0$, the quantities $\sigma_{s,t}, \sigma'_{s,t}, \tau_{s,t}, \tau'_{s,t}$ all go to zero. The assumed continuity of $\d^2 f$ on $U$ implies as $s,t\to 0$ that $\d^2 f(x)(v,u) = \d^2 f(x)(u,v)$. In the case that $X = X_1 \times\cdots\times X_n$, we take $u_i \in X$ and $v_j \in X_j$ and form the vectors

$$
\begin{align*}
  u =& (\delta_{ik} u_i)_{k=1}^n = (0,\dots,u_i,\dots,0) \\
  v =& (\delta_{jk} v_i)_{k=1}^n = (0,\dots,v_j,\dots,0)
\end{align*}
$$

to get

$$
  \d^2 f(x)(u,v) = \d^2 f(x)(v,u)
$$

which implies that $\d_i \d_j f(x) = \d_j \d_i f(x)$ and 

$$
  \frac{\partial^2 f_k}{\partial x_i \partial x_j} = \frac{\partial^2 f_k}{\partial x_j \partial x_i}
$$

for all $k=1,\dots,m$.
</details>
</MathBox>

<MathBox title='Representation of Fréchet derivatives' boxType='proposition'>
Let $X$ and $Y$ be normed spaces. If $f\in C^r (X,Y)$, then for every $x\in X$ the $k$-th derivative of $f$ at $x$ takes the form

$$
  \d^k f(x)(u_1,\dots,u_k) = \left.\partial_{t_1}\cdots\partial_{t_k} f\left(x + \sum_{i=1}^k t_i u_i \right)\right|_{t_1=\cdots=t_k=0}
$$

Moreover, $\d^k f(x) \in \mathcal{B}^k (X,Y)$ is a bounded symmetric multilinear operator.
</MathBox>

<MathBox title='Hessian matrix' boxType='example'>
For an open subset $U\subseteq\R^n$, suppose $f:U\to\R$ is differentiable on $U$, implying that the derivative $\d f(x) \in\mathcal{B}(\R^n,\R)$ exists at each $\mathbf{x}\in U$. The Banach space $\mathcal{B}(\R^n,\R)$ is the dual space space of $\R^n$, which by the Riesz representation theorem is isomorphic to $\R^n$. This means that for each $L\in\mathcal{B}(\R^n,\R)$ there is a unique $\mathbf{u}\in\R^n$ such that $L(\mathbf{v}) = \langle\mathbf{u},\mathbf{v}\rangle = \mathbf{u}^\top \mathbf{v}$.

The derivative $\d f(\mathbf{x})$ can thus be represented as a row vector, which in the standard basis of $\R^n$ is

$$
  \d f(x) = [\d_i f(\mathbf{x})]_{i=1}^n
$$

where $\d_i f(\mathbf{x}) = \d f(\mathbf{x})\mathbf{e}_i \in \R$ for $i=1,\dots,n$ are the partial derivative.

Suppose $f$ is twice differentiable on $U$, i.e. $\d^2 f(x) \in\mathcal{B}^2 (\R^n,\R)$ exists at each $\mathbf{x}\in U$. Since $\mathcal{B}(\R^2,\R) = \mathcal{B}(\R^n,\mathcal{B}(\R^n,\R)) = \mathcal{B}(\R^n,(\R^n)^*)$, the directional derivative of $\d f(x)$ in the direction $\mathbb{u}\in\R^n$, i.e. $\d^2 f(\mathbf{x})(\mathbf{u})\in(\R^n)^*$, is a row vector. Applying Proposition $\ref{proposition-33}$, gives the matrix form $\d^2 f(\mathbf{x})(\mathbf{u}) = \mathbf{u}^\top \mathbf{H}^\top (\mathbf{x})$, where $H(\mathbf{x})$ is the Hessian matrix of $f$ at $\mathbf{x}$

$$
\begin{align*}
  H(\mathbf{x}) =& \d \begin{bmatrix} \d_1 f(\mathbf{x}) & \cdots & \d_n \mathbf{x} \end{bmatrix}^\top \\
  =& \begin{bmatrix} 
    \d_1 \d_1 f(\mathbf{x}) & \cdots & \d_n \d_1 f(\mathbf{x}) \\ 
    \vdots & \ddots & \vdots \\
    \d_1 \d_n f(\mathbf{x}) & \cdots & \d_n \d_n f(\mathbf{x})  
  \end{bmatrix}
\end{align*}
$$

so that

$$
  \d^2 f(\mathbf{x})(\mathbf{u})(\mathbf{v}) = \mathbf{u}^\top H^\top \mathbf{v} \in\R,\; \mathbf{u},\mathbf{v}\in\R^n
$$

From $\d^2 f(\mathbf{x})(\mathbf{u})(\mathbf{v}) = \mathbf{u}^\top H^\top \mathbf{v} = \mathbf{v}^\top H \mathbf{v}$, we see that $\d^2 f(\mathbf{x}):\R^n \times \R^n \to \R$ is a bilinear map. 
</MathBox>

<MathBox title='Higher order directional derivatives' boxType='example'>
For an open subset $U\subseteq\mathbb{F}^n$ and a $k$ times continuously differentiable function $f\in C^k (U,\mathbb{F}^m)$, the derivative $\d^k f(\mathbf{x})\in \mathcal{B}^k (\mathbb{F}^n,\mathbb{F}^m)$ is a multilinear map $\d^k f(\mathbf{x}): \prod_{i=1}^k \mathbb{F}^n \to \mathbb{F}^n$. The $k$-th directional derivative of $f$ at $\mathbf{x}\in U$ in the direction $\mathbf{v}\in\mathbb{F}^n$ is defined as

$$
  \d_\mathbf{v}^k f(\mathbf{x}) = \d^k f(\mathbf{x})(\mathbf{v},\dots,\mathbf{v})
$$

A vector in $\mathbb{F}^n$ takes the form $\mathbf{v} = \sum_{i=1}^n v_i \mathbf{e}_i \in\mathbb{F}^n$ in standard coordinates. The first-order directional derivative of $f$ at $\mathbf{x}$ in the direction $\mathbf{v}$ is the vector

$$
  \d f(\mathbf{x})\mathbf{x} = \begin{bmatrix} \d_1 f(\mathbf{x}) & \cdots & \d_n f(\mathbf{x}) \end{bmatrix} \mathbf{v} = \sum_{j=1}^n \d_j f(\mathbf{x})v_j \in \mathbb{F}^m
$$

The second-order directional derivative of $f$ at $\mathbf{x}$ in the direction $\mathbf{v}$ is 

$$
\begin{align*}
  \d_\mathbf{v}^2 f(\mathbf{x}) =& \d_\mathbf{v} \sum_{j=1}^n \d_j f(\mathbf{x})v_j \\
  =& \sum_{i=1}^n \sum_{j=1}^n \d_i \d_j f(\mathbf{x})v_i v_j \\
  =& \mathbf{v}^\top H(\mathbf{x})\mathbf{v}
\end{align*}
$$

where $H(\mathbf{x})$ is the Hessian of $f$ at $\mathbf{x}$. Inductively, the $k$th order direction derviative of $f$ at $\mathbf{x}$ in the direction $\mathbf{v}$ is

$$
  \d_\mathbf{v}^k f(\mathbf{x}) = \sum_{i_1,\dots, i_k = 1}^n \d_{i_1} \cdots \d_{i_k} f(\mathbf{x})v_{i_1} \cdots v_{i_k}
$$

Combining the mixed terms gives

$$
  \d_\mathbf{v}^k f(\mathbf{x}) = \sum_{j_1 +\cdots+ j_n = k} \frac{k!}{j_1!\cdots j_n!} \d_1^{j_1} \cdots \d_n^{j_n} f(\mathbf{x}) v_1^{j_1} \cdots v_n^{j_n}
$$

The $k$-th order directional derivative of $f$ at $\mathbf{x}\in$ along $\mathbf{v}$ is also written $\d^k f(\mathbf{x}) \mathbf{v}^{(k)}$ where $\mathbf{v}^{(k)} \in X^k$
</MathBox>

## Gâteaux derivative

<MathBox title='Gâteaux derivative' boxType='definition'>
Let $X$ and $Y$ be locally convex topological vector spaces. A function $f: X \to Y$ is Gâteaux differentiable at $x\in X$ in the direction $h\in X$ if the following limit exists

$$
  \D f(x; z) = \lim_{\tau\to 0} \frac{f(x + \tau v) - f(x)}{\tau} = \left.\frac{\d}{\d\tau} f(x + \tau v)\right|_{\tau=0}
$$

This limit is called the *Gâteaux differential* of $f$ at $x$ in the direction $v$. If the limit exists for all $v\in X$, then $f$ is Gâteaux differentiable at $x$. 

If the Gâteaux differential $\D f(x; \cdot): X\to Y$ is linear and continuous, i.e. there is a linear map $A\in\mathcal{L}(X,Y)$ with $\D f(x; h) = Ah$ for all $h\in X$, then $A$ is called the *Gâteaux derivative* of $F$ at $x$, denoted $\D f(x) = A$.

If $f$ is $n$-times Gâteaux differentiable at $x$, the $n$-th order Gâteaux differential in iterated directions $h_1,\dots,h_n \in X$ is defined recursively as 

$$
  \D^n f(x; h_1,\dots,h_n) = \D(\D^{n-1} f)(x; h_1,\dots,h_{n-1};h_n)
$$

The $n$-th order Gâteaux derivative of $f$ at $x$ in the direction $h\in X$ is defined recursively as

$$
  \D^n f(x; h) := \lim_{\tau\to 0} \frac{\D^{n-1} f(x + \tau h; h) - \D^{n-1} f(x; h)}{\tau^n} = \left.\frac{\d^2}{\d\tau^} f(x + \tau h) \right|_{\tau=0}
$$
</MathBox>

<MathBox title='Gâteaux derivatives are homogeneous' boxType='definition'>
Let $X$ and $Y$ be locally convex topological vector spaces. If a function $f: X \to Y$ is Gâteaux differentiable at $x\in X$, i.e. $\D f(x)$ exists, then for all $\alpha\in\mathbb{F}$

$$
  \D f(x; \alpha h) = \alpha \D f(x; \alpha)
$$

<details>
<summary>Proof</summary>

From the definition of $\D f(x; h)$

$$
\begin{align*}
  \D f(x; \alpha h) =& \lim_{\tau\to 0} \frac{f(x + \tau(\alpha h)) - f(x)}{\tau} \\
  =& \alpha \lim_{\tau\to 0} \frac{f(x + \tau\alpha h) - f(x)}{\alpha\tau} \\
  =& \alpha \D f(x; h)
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Fréchet differentiability implies Gâteaux differentiability' boxType='proposition'>
Let $X, Y$ be normed spaces. If $f:X\to Y$ is Fréchet differentiable at $x\in X$, then $f$ is Gâteaux differentiable at $x$. In this case $\d f(x) = \D f(x)$, where $\d f(x)$ is the Fréchet derivative of $f$ at $x$ and $\D f(x)$ is the Gâteaux derivative of $f$ at $x$.

<details>
<summary>Proof</summary>

Because $f: X\to Y$ is a Fréchet differentiable at $x\in X$, we have

$$
  f(x + h) = f(x) + \d f(x) h + o(h)
$$

For any $v\in X$, the Gâteaux differential limit at $x$ is

$$
  \lim_{t\to 0} \frac{f(x + tv) - f(x)}{t}
$$

Inserting the Fréchet differentiability condition with $h = tv$, we get

$$
\begin{align*}
  \lim_{t\to 0} \frac{\d f(x)(tv) + o(\lVert tv \rVert)}{t} =& \lim_{t\to 0} \frac{t \d f(x)(v) + o(\lVert tv \rVert)}{t} \\
  =& \d f(x)(v) + \lim_{t\to 0} \frac{o(|t|\cdot\lVert v \rVert)}{t} \\
  =& \d f(x)(v)
\end{align*}
$$

Hence, 

$$
  \lim_{t\to 0} \frac{f(x + tv) - f(x)}{t} = \d f(x)(v),\; \forall v\in X
$$

showing that the Gâteaux differential of $f$ at $x$ in the direction $v$ equals the Fréchet differential of $f$ at $x$. Since the Fréchet derivative $\d f(x) \in \mathcal{B}(X,Y)$ is a bounded linear operator, it follows that $f$ is Gâteaux differentiable at $x$ with Gâteaux derivative $\D f(x) = \d f(x)$.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $X$ and $Y$ be normed spaces. If $f:U\subseteq X\to Y$ is Gâteaux differentiable such that the Gâteaux derivative is continuously linear, i.e. $\D f \in C(U, \mathcal{L}(X,Y))$, then $f\in C^1 (U, Y)$ and $\d f = \D f$, where $\d f$ is the Fréchet derivative of $f$.

<details>
<summary>Proof</summary>

Assume $g(t) := f(x + th)$ is in $C^1 ([0,1], Y)$ for $h\in U$ with sufficiently small norm. Moreover, by definition we have

$$
  \dot{g}(t) := \frac{\d }{\d t} g(t) = \D f(x + th)h 
$$

Using the fundamental theorem of calculus, we obtain

$$
\begin{align*}
  f(x + h) - f(x) =& g(1) - g(0) = \int_0^1 \dot{g}(t)\;\d t \\
  =& \int_^1 \D f(x + th)h\;\d t \\
  =& \left(\int_0^1 \D f(x + th)\;\d t \right)h
\end{align*}
$$

where the last equality follows from continuity of the integral since it clearly holds for simple functions. Consequently

$$
\begin{align*}
  |f(x + h) - f(x) - \delta f(x)h| = \left|\left(\int_0^1 (\D f(x + th) - \D f(x))\;\d t \right)h \right| \\
  \leq& \left(\int_0^1 \lVert \D f(x + th) - \D f(x) \rVert\;\d t \right)|h| \\
  \leq& \max_{t\in[0,1]} \lVert \D f(x + th) - \D f(x) \rVert\cdot|h|
\end{align*}
$$

By continuity of $\D f$, the right-hand side is $o(h)$ as required.
</details>
</MathBox>

<MathBox title='Gâteaux mean value theorem' boxType='theorem'>
Let $X$ and $Y$ be normed space and suppose $f:X\to Y$ is Gâteaux differentiable on $X$. If $X$ is convex then for $x, y\in X$

$$
\begin{gather*}
  \lVert f(x) - f(y) \rVert_Y \leq M \lVert x - y \rVert_X \\
  M := \sup{t\in[0,1]} \Set{\D f((1 - t)x + ty), \frac{x - y}{\lVert x - y \rVert_X}}
\end{gather*}
$$

Converserly, if 

$$
  \lVert f(x) - f(y) \rVert_Y \leq M \lVert x - y \rVert_X
$$

then

$$
  \sup_{x\in X, \lVert e\rVert_X} \lVert \D f(x; e) \rVert \leq M
$$

<details>
<summary>Proof</summary>

Define $g(t) = f((1 - t)x + ty)$ with $\D f(t) = \delta g(t) = \delta f((1 - t)x + ty;y - x)$ for $t\in[0,1]$. This implies $\lVert \d g(t) \rVert \leq \tilde{M} := M\lVert x - y \rVert$ by homogeneity of the Gâteaux derivative.  
</details>
</MathBox>

## Taylor's theorem

<MathBox title="Taylor's theorem" boxType='theorem'>
Let $X$ and $Y$ be normed spaces. If $f \in C^k (U,Y)$ is a $k$ times differentiable function on an open subset $U\subseteq X$, then for all $x\in U$ and $h\in X$ such that the line segment $\ell(x,x+h) = \Set{(1-t)x + th | t\in[0,1]}$ lies in $U$, then

$$
\begin{align*}
  f(x + h) =& R_k + \sum_{i=1}^k \frac{\d_h^{k-1} f(x)}{(k-1)!} \\
  =& R_k + \sum_{i=1}^k \frac{\d^{k-1} f(x)h^{(k-1)}}{(k-1)!}
\end{align*}
$$

where $h^{(k) \in X^k}$, and the remainder $R_k$ is given by the integral form

$$
  R_k (x,h) \equiv o(|h|^k) = \int_0^1 \frac{(1 - t)^{k-1}}{(k-1)!}\d^k f(x + th)h^{(k)}\;\d t
$$

Expressing $R_k$ as $o(|h|^k)$ is called the *Peano form* of the remainder.

<details>
<summary>Proof</summary>

We use proof by induction. In the base case $k = 0$, assume $g(t) := f(x + th)$ is in $C^1 ([0,1], Y)$ for $h\in U$ with sufficiently small norm. Moreover, by definition we have

$$
  \dot{g}(t) := \frac{\d }{\d t} g(t) = \D f(x + th)h 
$$

Using the fundamental theorem of calculus, we obtain

$$
\begin{align*}
  f(x + h) - f(x) =& g(1) - g(0) = \int_0^1 \dot{g}(t)\;\d t \\
  =& \int_^1 \D f(x + th)h\;\d t \\
  =& \left(\int_0^1 \D f(x + th)\;\d t \right)h
\end{align*}
$$

For the induction step, we use integration by parts. Let $f_j \in C^1 ([0,1], X_j)$ for $j=1,2$ and suppose $L\in\mathcal{L}^2 (X_1 \times X_2, Y)$ is bilinear. Applying the fundamental theorem of calculus on the bilinear product rule (Proposition $\ref{proposition-30}$) we get

$$
\begin{align*}
  \int_0^1 L(\dot{f}_1 (t), f_2 (t))\;\d t =& L(f_1 (1), f_2 (1)) - L(f_1 (0), f_2 (0)) \\
  &- \int_0^1 L(f_1 (t), \dot{f}_2 (t))\;\d t
\end{align*}
$$

Applying by parts with with $L(y,t) = ty$, $f_1 (t) = \d^k f(x + ht)$ and $f_2 (t) = \frac{(1 - t)^{k}}{(k!)}$ establishes the induction step.

To show the Peano form for the remainder we estimate

$$
\begin{align*}
  &\left| \int_0^1 \frac{(1 - t)^{k-1}}{(k-1)!}\d_h^k f(x + th)\;\d t - \frac{1}{k!} \d^k f(x) \right|h^{k} \\
  &\leq \frac{|h|^k}{(k - 1)!} \int_0^1 (1 - t)^{k-1} \lVert \d^k f(x + th) - d^k f(x) \rVert\;\d t \\
  &\leq \frac{|h|^k}{k!} \sup_{t\in[0,1]} \lVert \d^k f(x + th) - d^k f(x) \rVert
\end{align*}
$$
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $X$ and $Y$ be normed spaces. If $f \in C^k (U,Y)$ is a $k$ times differentiable function on an open subset $U\subseteq X$, then for all $x\in U$ and $h\in X$
</MathBox>

<MathBox title='Upper bound for the Taylor remainders' boxType='corollary'>
If $\lVert \d^k (fx + th) \rVert < M$ for all $t\in[0,1]$, then

$$
  \lVert R_k \rVert_Y \leq \frac{M}{k!} \lVert h \rVert_X^k
$$
</MathBox>

## Banach-valued integrals

### Integrals of step function

<MathBox title='Step function' boxType='definition'>
Let $X$ be a Banach space. A function $f:[a,b]\subset\R\to X$ is a step function if there exists finitely many $t_0,\dots,t_N \in [a,b]$ and finitely many $x_1,\dots,x_N \in X$ such that $a = t_0 < \cdots < t_N = b$ and

$$
  f = \left( \sum_{i=1}^{N-1} x_i \chi_{[t_{i-1}, t_i)} \right) + x_N \chi_{[t_{N-1},t_N]}
$$

where $\chi$ is the characteristic function. The set of all step functions from $[a,b]$ to $X$ is denoted $S([a,b], X)$.
</MathBox>

<MathBox title='' boxType='proposition' tag='proposition-31'>
Let $X$ be a Banach space. The set $S([a,b], X)$ is a subspace of $(L^\infty ([a,b], X), \lVert\cdot\rVert_\infty)$.

<details>
<summary>Proof</summary>

For $f\in S([a,b], X)$ we have

$$
  \lVert f \rVert_\infty = \sup\Set{\lVert x_i \rVert}_{i=1}^N < \infty
$$

It remains to show that $S([a,b], X)$ is closed under linear combinations. For $f,g \in S([a,b],X)$ we have

$$
\begin{align*}
  f =& \left(\sum_{i=1}^{N-1} x_i \chi_{[t_{i-1}, t)} \right) + x_N \chi_{[t_[N-1], t_N]} \\
  g =& \left(\sum_{i=1}^{M-1} y_i \chi_{[s_{i-1}, s)} \right) + y_M \chi_{[s_[M-1], s_M]}
\end{align*}
$$

where both $\Set{t_0,\dots,t_N}$ and $\Set{s_0,\dots,s_M}$ define disjoint partitions of $[a,b]$. Taking the ordered union $\Set{u_0,\dots,u_L} = \Set{t_0,\dots,t_N}\cup\Set{s_0,\dots,s_M}$, then there exists $x'_i, y'_i \in X$ for $i = 1,\dots,L$ such that

$$
\begin{align*}
  f =& \left(\sum_{i=1}^{L-1} x'_i \chi_{[u_{i-1}, u)} \right) + x'_L \chi_{[u_[L-1], u_L]} \\
  g =& \left(\sum_{i=1}^{L-1} y'_i \chi_{[u_{i-1}, u)} \right) + y'_L \chi_{[u_[L-1], u_L]}
\end{align*}
$$

Thus, for scalars $\alpha, \beta\in\mathbb{F}$ we get

$$
  \alpha f + \beta g = \left(\sum_{i=1}^{L-1} (\alpha x'_i + \beta y'_i) \chi_{[u_{i-1}, u)} \right) + (\alpha x'_L + \beta y'_L) \chi_{[u_[L-1], u_L]} \in S([a,b], X)
$$
</details>
</MathBox>

<MathBox title='Regulated integral of step functions' boxType='definition'>
Let $X$ be a Banach space. The integral $I(f)$ of a step function $f\in S([a,b], X)$ is defined as

$$
  I(f) = \sum_{i=1}^N x_i (t_i - t_{i-1}) = \int_a^b f(t)\;\d t \in X
$$

The integral defines a function $I:S([a,b],X)\to X$.
</MathBox>

<MathBox title='Integral of step functions is a bounded linear operator' boxType='proposition' tag='proposition-32'>
Let $X$ be a Banach space. The integral of step functions $I:S([a,b],X)\to X$ is a bounded linear transformation with $\lVert I \rVert = b - a$.

<details>
<summary>Proof</summary>

Linearity of $I$ follow from the proof of Proposition $\ref{proposition-31}$ where we showed how to combine two subdivisions of $[a,b]$. For

$$
  f = \left(\sum_{i=1}^{N-1} x_i \chi_{[t_{i-1}, t_i)]}\right) + x_N \chi_{[t_{N-1}, t_N]}
$$

we have

$$
\begin{align*}
  \lVert I(f) \rVert =& \left\lVert \sum_{i=1}^N x_i (t_i - t_{i-1}) \right\rVert \\
  \leq& \sum_{i=1}^N (t_i - t_{i-1}) \lVert x_i \rVert \\
  \leq& \sum_{i=1}^N (t_i - t_{i-1}) \sup\Set{\lVert x_i \rVert}_{i=1}^N \\
  =& (b - a)\lVert f \rVert_\infty
\end{align*}
$$

This implies for all nonzero $f\in S([a,b], X)$ that

$$
  \frac{\lVert I(f) \rVert}{\lVert f \rVert_\infty} \leq b - a
$$

Thus, $\lVert I \rVert \leq b - a$. The upper bound on $\lVert I \rVert$ is realized for the function $f = \chi_[a,b] \in S([a,b], X)$ and so $\lVert I \rVert = b - a$.
</details>
</MathBox>

### Single-variable Banach-valued integral

<MathBox title='Single-variable Banach-valued integration' boxType='theorem'>
Let $X$ be a Banach space. There is a unique bounded linear extension $\overline{I}: \overline{S([a,b],X)}\to X$ of the integral operator $I:S([a,b], X)\to X$.

<details>
<summary>Proof</summary>

This follows immediately from the continuous linear extension theorem (Theorem $\ref{theorem-12}$).
</details>
</MathBox>

<MathBox title='Single-variable Banach-valued integral' boxType='definition'>
Let $X$ be a Banach space. The integral of $f\in\overline{S([a,b], X)}$ is defined by

$$
  \overline{I}(f) = \lim_{n\to\infty} I(s_n) = \lim_{n\to\infty} \int_a^b s_n (d)\;\d t
$$

where $(s_n)_{n\in\N_+}$ is a sequence of step functions converging uniformly to $f$. For every $f\in \overline{S([a,b], X)}$ we define

$$
  \int_b^a f(t)\;\d t = -\int_a^b f(t)\;\d t
$$
</MathBox>

<MathBox title='' boxType='theorem'>
Let $X$ be a Banach space. Every continuous function from $[a,b]\subset\R$ to $X$ is the uniform limit of step functions, i.e. $C([a,b],X) \subset\overline{S([a,b], X)}$.

<details>
<summary>Proof</summary>

Any $f\in C([a,b], X)$ is uniformly continuous because $[a,b]$ is compact. For $\epsilon > 0$ there is $\delta > 0$ such that $\lVert f(x) - f(y) \rVert < \epsilon$ whenever $x,y \in [a,b]$ satisfy $|x - y| < \delta$. Choose $N\in\N$ such that $(b-a)/N < \delta$. Set $t_i = a + i(b - a)/N$ for $i=0,\dots,N$. Define the step funcion $f_N \in S([a,b],X)$ by

$$
  f_N = \left(\sum_{i=1}^{N} f(t_{i-1})\chi_{[t_{i-1}, t_i)}\right) + f(t_{N-1})\chi_{[t_{N-1},t_N]}
$$

Since any $t\in [t_{i-1}, t_i)$ for $i=1,\dots,N-1$ and $t\in[t_{N-1}, t_N]$ is within $\delta$ of the endpoints, we have $\lVert f(t) - f_N (t) \lVert < \epsilon$. As this holds for all $t\in[a,b]$, we obtain $\lVert f - f_N \rVert < \epsilon$. Since $\epsilon > 0$ is arbitrary, we have that $f$ is a limit point of $S([a,b], X)$. This shows that $C([a,b],X)\subset\overline{S([a,b],X)}$.
</details>
</MathBox>

<MathBox title='Properties of single-variable Banach-valued integrals' boxType='proposition' tag='proposition-37'>
Let $X$ be a Banach space. If $f\in\overline{S([a,b],X)}$ and $\alpha,\beta,\gamma\in[a,b]$ with $\alpha < \gamma < \beta$, then
1. $\lVert \int_a^b f(t)\;\d t \rVert \leq (b-a)\lVert f\rVert_\infty$
2. $\lVert \int_a^b f(t)\;\d t \rVert \leq \int_a^b \lVert f \rVert\;\d t$
3. if $f\chi_{\alpha,\beta} \in\overline{S([a,b],X)}$, then $\int_a^b f(t)\chi_{[\alpha,\beta]}(t)\;\d t = \int_\alpha^\beta f(t)\;\d t$
4. $\int_\alpha^\beta f(t)\;\d t = \int_\alpha^\gamma f(t)\;\d t + \int_\gamma^\beta f(t)\;\d t$
5. The function $F(t) = \int_a^t f(s)\;\d s$ for $s\in[a,b]$ is continuous on $C$.

<details>
<summary>Proof</summary>

**(1):** This is a restatement of $\lVert\overline{I}\rVert = b-a$ in Proposition $\ref{proposition-32}$.

**(2):** The function $t\mapsto \lVert f(t) \rVert$ belongs to $\overline{S([a,b],X)}$, and so $\int_a^b \lVert f(t) \rVert\;\d t$ is defined.

**(3):** Let $(s_k)_{k\in\N_+}$ in $S([a,b],X)$ converge uniformly to $f\in\overline{S([a,b],X)}$. Let $I_{ab}$ be the integral on $S([a,b], X)$ and $I_{\alpha\beta}$ the integral on $S([a,b],X)$. Since $s_k \chi_{[\alpha,\beta]}$ is zero outside of $[\alpha,\beta]$, the value $I_{\alpha\beta} (s_k \chi_{[\alpha,\beta]})$ is the same as that of $I_{ab}(s_k \chi_{[\alpha,\beta]})$. Thus for each $k\in\N_+$ we have

$$
  \int_a^b s_k (t) \chi_{[\alpha,\beta]}\;\d t = \int_\alpha^\beta s_k (t)\;\d t
$$

Since $\overline{I}$ is continuous, the uniformly convergence $s_k \xrightarrow{k\to\infty} f$ implies that $\overline{I}(s_k) \xrightarrow{k\to\infty} \overline{I}(f)$ by continuity of $\overline{I}$.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
If $f\in\overline{S([a,b],\R^n)}$ is written in coordinates as $f(t) = (f_i(t))_{i=1}^n$, then $f_i \in\overline{S([a,b],\R)}$ for each $i$ and

$$
  \int_a^b f(t)\;\d t = \left(\int_a^b f_i (t)\;\d t \right)_{i=1}^n
$$

<details>
<summary>Proof</summary>

Since $\overline{I}:\overline{S([a,b],\R^n)}\to\R^n$ is continuous, we have $\overline{I}(s_k) \xrightarrow{k\to\infty} \overline{I}(f)$ whenever $s_k \xrightarrow{k\to\infty} f$ uniformly. Thus, it suffices to show that the integral can be written as stated for step function. For a step function $s(t) = (s_i(t))_{i=1}^n$ we have

$$
\begin{align*}
  \int_a^b s(t)\;\d t =& \sum_{i=1}^M s(t_i)(t_i - t_{i-1}) \\
  =& \sum_{i=1}^M (s_1 (t_i),\dots,s_n (t_i))(t_i - t_{i-1}) \\
  =& \left(\sum_{i=1}^M s_1 (t_i)(t_i - t_{i-1}),\dots,\sum_{i=1}^M s_n (t_i)(t_i - t_{i-1}) \right) \\
  =& \left(\int_a^b s_j (t)\;\d t \right)_{j=1}^n
\end{align*}
$$
</details>
</MathBox>

### Fundamental theorem of calculus

<MathBox title='Constant functions have zero derivative' boxType='proposition' tag='proposition-29'>
Let $X$ be a normed space and suppose $f \in C^1 (I, X)$ is a continuously differentiable function on an open interval $I\subseteq \R$. If $\d f(t) \equiv 0$ for all $t\in I$, then $f$ is constant.

<details>
<summary>Proof</summary>

Let $a, b \in I$ with $a < b$ and fix an arbitrary $t\in (a, b)$. By hypothesis, for all $\epsilon > 0$ there is $\delta_t > 0$ such that for all $|h| < \delta_t$ we have

$$
\begin{align*}
  \lVert f(t + h) - f(t) \rVert_X =& \lVert f(t + h) - f(t) - 0 \rVert_X \\
  =& \lVert f(t + h) - f(t) - \d f(t)h \rVert_X \leq \epsilon |h|
\end{align*}
$$

The collection of open intervals $(t - \delta_t, t + \delta_t)$ is an open covering of the compact $[a,b]$ so there is a finite subcovering $(t_i - \delta_{t_i}, t_i + \delta_{t_i})$ of $[a,b]$ for $i = 1,\dots,n$. Without loss of generality we can assume taht $a < t_1 < \cdots < t_n < b$. Choose points $x_0,\dots,x_n \in [a,b]$ so that

$$
  a = x_0 < t_1 < x_1 < \cdots < t_n < x_n = b
$$

with $|x_i - t_i| < \delta_{t_i}$ and $|x_i - t_{i-1}| < \delta_{t_i - 1}$. We then have

$$
\begin{align*}
  \lVert f(b) - f(a) \rVert_X =& \left\lVert \sum_{i=1}^n f(x_i) - f(x_{i-1}) \right\rVert_X \\
  =& \left\lVert \sum_{i=1}^n (f(x_i) - f(t_i)) + (f(t_i) - f(x_{i-1})) \right\rVert_X \\
  \overset{\triangle}{\leq}& \sum_{i=1}^n \lVert f(x_i) - f(t_i) \rVert_X + \sum_{i=1}^n \lVert f(t_i) - f(x_{i-1}) \rVert_X \\
  \leq& \sum_{i=1}^n \epsilon((x_i - t_i) + (t_i - x_{i-1})) \\
  =& \epsilon(b - a)
\end{align*}
$$

Since $\epsilon > 0$ is arbitrary, if follows that $f$ is constant on $(a, b)$. Furthermore, $f$ is constant on $I$ because $a, b \in I$ with $a < b$ are arbitrary.
</details>
</MathBox>

<MathBox title='Fundamental theorem of calculus' boxType='theorem'>
Let $X$ be a normed space.
1. If $f\in C([a,b], X)$ is continuous on $[a,b]$, then $t \mapsto \int_a^t f(s)\;\d s$ is differentiable on $(a,b)$ and for all $t\in(a,b)$
$$
  \frac{\d}{\d t} \int_a^t f(s)\;\d s = f(t)
$$
2. If $f\in C^1((a, b), X)$ is continuously differentiable on $(a,b)$, and $\d f(t)$ extends to a continuous function on $[a,b]$, then
$$
  \int_a^b \d f(s)\;\d s = f(b) - f(a)
$$

<details>
<summary>Proof</summary>

**(1):** By Proposition $\ref{proposition-14}$, the function $f$ is uniformly continuous since $f$ is continuous on the compact interval $[a,b]$. Consequently, for every $\epsilon > 0$ there is $\delta > 0$ such that for all $t\in[a,b]$ and all $|h| < \delta$ with $t + h\in[a,b]$

$$
  \lVert f(t + h) - f(t) \rVert_X < \epsilon
$$

From the properties of univariate Banach-valued integral (Proposition $\ref{proposition-37}$)

$$
\begin{align*}
  &\left\lVert \int_a^{t+h} f(x)\;\d s - \int_a^t f(s)\;\d s - f(t)h \right\rVert_X \\
  &= \left\lVert \int_t^{t+h} f(s)\;\d s - f(t)h \right\rVert_X \\
  &= \left\lVert \int_t^{t+h} (f(s) - f(t))\;\d s \right\rVert_X \\
  &\leq \left| \int_t^{t+h} \lVert f(s) - f(t) \rVert_X \;\d s \right| \\
  \leq& \left| \int_t^{t+h} \epsilon\;\d s \right| \\
  &= \epsilon |h|
\end{align*}
$$

This shows that $\int_a^t f(s)\;\d s$ is a differentiable function of $t$ on $(a,b)$, whose derivative is $f(t)$.

**(2):** By hypothesis
- the function $s\mapsto \d f(s)$ from $[a,b]$ to $X$ is continuous
- the function $f:[a,b]\to X$ is continuous

Then the function $G:[a,b]\to X$ defined by

$$
  g(t) = \int_a^t \d f(s)\;\d s - f(t)
$$

is continuous on $[a,b]$ by Proposition $\ref{proposition-29}$, and $g$ is differentiable on $(a,b)$ by part **(1)**. Applying **(1)** we have for $t\in(a,b)$ that

$$
  \d g(t) = \frac{\d}{\d t} \left[\int_a^t \d f(s)\;\d s - f(t) \right] = \d f(t) - \d f(t) = 0
$$

By Proposition $\ref{proposition-29}$, it follows that $g$ is constant on $[a,b]$. Since

$$
\begin{align*}
  g(a) =& \int_a^a \d f(s)\;\d s - f(a) = -f(a) \\
  g(b) =& \int_a^b \d f(s)\;\d s - f(b)
\end{align*}
$$

the equality $g(a) = g(b)$ implies

$$
  \int_a^b \d f(s)\;\d s = f(b) - f(a)
$$
</details>
</MathBox>

<MathBox title='Integral mean value theorem' boxType='theorem'>
Let $X, Y$ be normed spaces. If $f\in C^1 (X,Y)$ is continuous differentiable on $X$, and for $x_0, \x_1$ the line segment $\ell (x_0, x_1) = \Set{(1-t)x_1 + (1 - t)x_0 | t\in [0,1]}$ lies entirely in $X$, then

$$
  f(x_1) - f(x_0) = \int_0^1 \d f (tx + (1 - t)x_0)(x_1 - x_0)\;\d t
$$

Alternatively, with $h = x_1 - x_0$

$$
  f(x_0 + h) - f(x_0) = \int_0^1 \d f(x_0 + th)h\;\d t
$$

Moreover,

$$
  \lVert f(x_1) - f(x_0) \rVert_Y \leq \sup_{c\in\ell(x_0,x_1)} \lVert \d f(c) \rVert\cdot\lVert x_1 - x_0 \rVert
$$
</MathBox>

<MathBox title='Change of variables formula' boxType='theorem'>
Let $X$ be a normed space and $f\in C([a,b], X)$ a continuous function on $[a,b]$. If $g:[c,d]\to[a,b]$ is continuous with $g$ continuously differentiable on $(c, d)$ and $\d g$ continuously extendable to $[c,d]$, then

$$
  \int_c^d f(g(s)) \d g(s)\;\d s = \int_{g(c)}^{g(d)} f(t)\;\d t
$$
</MathBox>

### Uniform convergence and derivatives

<MathBox title='' boxType='definition'>
Let $X$ and $Y$ be Banach spaces. We say that a sequence $(f_n)_{n\in\N_+}$ in $C(X,Y)$
1. is Cauchy in $C(X, Y)$ if for every compact $K\subset U$, the sequence $(f_n |_K )_{n\in\N_+}$ is Cauchy in $(C(K,Y),\lVert\cdot\rVert_\infty)$ and
2. converges uniformly on compact subsets of $X$ to $f\in C(X,Y)$ if for every compact $K\subset U$, the sequence $(f_n |_K)_{n\in\N_+}$ converges to $f|_K$ in $(C(K,Y),\lVert\cdot\rVert_\infty)$.

Note that a Cauchy sequence $(g_n)_{n\in\N_+}$ in $(C(K,Y),\lVert\cdot\rVert_\infty)$ converges uniformly to a unique $g\in C(K,Y)$ because $(C(K,Y), \lVert\cdot\rVert_\infty)$ is a Banach space. 
</MathBox>

<MathBox title='' boxType='proposition'>
Let $X$ and $Y$ be finite-dimensional Banach spaces, and $U = B(x_*, r) \subset X$ an open set for $x_* \in X$ and $r > 0$. For a sequence $(f_n)_{n\in\N_+} \in C^1 (U,Y)$, if
1. $(f_n (x_*))_{n\in\N_+}$ converges in $Y$ and
2. $(\d f_n)_{n\in\N_+}$ converges uniformly on compact subsets of $U$ to $g\in C(U,\mathcal{B}(X,Y))$,

then $(f_n)_{n\in\N_+}$ converges uniformly on compact subsets of $U$ to $f\in C^1 (U,Y)$ for which $\d f = g$.

<details>
<summary>Proof</summary>

By hypothesis, the sequence $(f_n (x_*))_{n\in\N_+}$ converges to, say $z\in Y$. For each $x\in B(x_*, r) = U$ set $h = x - x_*$. A candidate for the limit function $f\in C(U,Y)$ is

$$
  f(x) = z + \int_0^1 g(x_* + th)h\;\d t
$$

where $g\in C(U,\mathcal{B}(X,Y))$ is what $(\d f_n)_{n\in\N_+}$ converges uniformly on compact sets by hypothesis.

Since $U = B(x_*, r)$ is convex, the function $g(x_* + th)$ is defined for all $t\in[0,1]$. When $h = 0$, i.e. $x = x_*$, then

$$
  f(x_*) = z + \int_0^1 g(x_* + t0)0\;\d t = z = \lim_{n\to\infyt} f_n (x_*)
$$

We show that $(f_n)_{n\in\N_+}$ converges uniformly on compact subsets of $B(x_*, r)$. It suffices to show uniform convergence on the arbitrary compact set $K = \overline{B(x_*, \rho)}\subset U$, i.e. $0 < \rho < r$. On $K$ we have, by applying the mean value theorem to $f_n$ and using the definition of $f$ that

$$
\begin{align*}
  &\lVert f_n (x) - f(x) \rVert_Y \\
  =& \left\lVert f_n (x_*) + \int_0^1 \d f_n (x_* + th)h\;\d - z - \int_0^1 g(x_* + th)h\;\d t \right\rVert_Y \\
  \leq& \lVert f_n (x_*) - z \rVert_Y + \left\lVert \int_0^1 (\d f_n (x_* + th) - g(x_* + th))h\;\d t \right\rVert_Y \\
  \leq& \lVert f_n (x_*) - z \rVert_Y + \sup_{c\in K} \lVert \d f_n (c) - g(c) \rVert_{X\to Y} \lVert h \rVert_X
\end{align*}
$$

Since $f_n (x_*) \to z$, for $\epsilon > 0$ there is $N\in\N$ such that for all $n\geq N$ we have

$$
  \lVert f_n (x_*) - z \rVert_Y < \frac{\epsilon}{2}
$$

and since $\d f_n \xrightarrow{n\to\infty} g$ uniformly on the compact $K$, there is M \geq N$ such that for $n\geq M$ we have

$$
  \sup_{c\in K} \lVert \d f_n (c) - g(c) \rVert_Y < \frac{\epsilon}{2r}
$$

Thus, for all a$n\geq M$, and with $\lVert h \rVert_X$, we have

$$
  \lVert f_n (x) - f(x) \rVert_Y < \frac{\epsilon}{2} + \frac{\epsilon}{2r}r = \epsilon
$$

The choice of $N$ and $M$ is independent of the point $x\in K$ chosen, so for all $n\geq N$ we have

$$
  \lVert f_n - f\rVert_\infty = \sup_{x\in K} \lVert f_n (x) - f(x) \rVert_Y \leq \epsilon
$$

Thus, $f_n \xrightarrow{n\to\infty} f$ uniformly on the compact $K$. 

Finally, we show that $\d f(x) = g(x)$ for all $x\in U$. For a fixed $x\in B(x_*, r) = U$, there is $a > 0$ such that $K = \overline{B(x,a)}\subset U$. For $\lVert h \rVert_X \leq a$ we have by the integral mean value theorem that

$$
  f_n (x + h) - f_n (x) = \int_0^1 \d f_n (x + th)h\;\d t
$$

The left-hand side of this converges uniformly on the compact set $K$ to $f(x + h)  - f(x)$, while the integrand on the right-hand side converges uniformly on $K$ to $g(x + th)h$. By commutativity of integration with uniform limits, we get

$$
\begin{align*}
  f(x + h) - f(x) =& \lim_{n\to\infty} (f_n (x + h) - f_n (x)) \\
  =& \lim_{n\to\infty} \int_0^1 \d f_n (x + th)h\;\d t \\
  =& \int_0^1 \lim_{n\to\infty} \d f_n (x + th)h\;\d t \\
  =& \int_0^1 g(x + th)h\;\d t
\end{align*}
$$

Hence,

$$
\begin{align*}
  \lvert f(x + h) - f(x) - g(x)h \rVert =& \left\lVert \int_0^1 (g(x + th)h - g(x)h)\;\d t \right\rVert_Y \\
  =& \left\lVert \int_0^1 (g(x + h) - g(x))h\;\d t \right\rVert_Y
\end{align*}
$$

Continuity of $g$ on the compact $K = \overline{B(x,a)}$ implies $g$ is uniformly continuous on $K$. For $\epsilon > 0 $ there is $0 < \delta < a$ such that for all $|h|_X < \delta$

$$
  \lVert g(x + h) - g(x)\rVert_{X\to Y} < \epsilon
$$

We thus arrive at

$$
  \lVert f(x + h) - f(x) - g(x)h \rVert_Y \leq \epsilon\lVert h \rVert_X
$$

This shows that $f$ is differentiable at $x$ with derivative $\d f(x) = g(x)$.
</details>
</MathBox>
