---
title: 'Functional Analysis'
subject: 'Mathematics'
showToc: true
---

# Metric space

<MathBox title='Metric space' boxType='definition'>
A metric space $(X, d)$ consists of a set $X$ and a metric $d: X \times X \to [0, \infty)$ with the following properties for $x,y,z \in X$:

1. $d(x,y) \geq 0$ and $d(x, y) = 0 \iff x = y$ (positive definite)
2. $d(x, y) = d(y, x)$ (symmetric)
3. $d(x, z) \leq d(x, z) + d(z, y)$ (triangle inequality)
</MathBox>

For $X = \R^n$ the line segment between to points is given by the Euclidean metric $d(x, y) = \sqrt{\sum_{i=1}^n (x_i - y_i)^2}$

## Topology of a metric space

<MathBox title='Open ball' boxType='definition'>
Let $(X, d)$ be a metric space. The open ball about a point $x\in X$ with radius $\epsilon\in(0, \infty)$ is the set

$$
  B_\epsilon (x) := \Set{ y \in X | d(x, y) < \epsilon }
$$
</MathBox>

<MathBox title='Open and closed sets' boxType='definition'>
Let $(X, d)$ be a metric space. A subset $A \subseteq X$ is open if for each $x \in A$ there is an open ball with $B_\epsilon (x) \subseteq A$. Conversely, a subset $B\subseteq X$ is closed if its complement $B^c := X \setminus A$ is open.
<details>
<summary>Proof</summary>

To show that open balls in fact are open, consider $x\in B_{r}(x)\subseteq X$. Let $y\in B_\epsilon(x)$, and let $a = d(x, y) < r$. We need to find $s > 0$ such that $B_s(y)\subseteq B_r (x)$. If $z\in B_{r-a}(y), then $d(y, z) < r - a$, so by the triangle inequality

$$
  d(x, z) < a + (r - a) = r
$$

Thus $z\in B_r (x)$ and $B_{r - a}(y)\subseteq B_r(x)$. Hence $B_r (x)$ is open.
</details>
</MathBox>

<MathBox title='Boundary' boxType='definition'>
Let $(X, d)$ be a metric space. A point $x \in X$ is called a boundary point for $A \subseteq X$ if for all $\epsilon > 0$: 

$$
  B_\epsilon (x) \cap A \neq \emptyset \quad \mathrm{and} \quad  B_\epsilon (x) \cap A^c \neq \emptyset, \quad A^c = X \setminus A
$$

The set of boundary points in $A$ is denoted $\partial A$.
</MathBox>

<MathBox title='Closure' boxType='definition'>
Let $(X, d)$ be a metric space. The closure of an open set $A\subseteq X$ is the union of the set with its boundary, $\bar{A} := A \cup \partial A$, and is always closed.
</MathBox>

<MathBox title='Induced topology' boxType='proposition'>
Let $(X, d)$ be a metric space. The collection $\mathcal{T}_d$ of open subsets of $X$ is a topology.

<details>
<summary>Proof</summary>

We show that $\mathcal{T}_d$ satisfy the axioms of topology.
1. For any $x\in X$, there is $\epsilon > 0$ such that $B_\epsilon (x)\subseteq X$, hence $X$ is open. Also, $\emptyset$ is open by definition. That is, $X, \emptyset\in\mathcal{T}_d$
2. Let $\Set{U_i \subseteq X}_{i\in I}$ be a collection of open sets in $X$ for an arbitrary index set $I$. For any $x\in U = \bigcup_{i\in I} U_i$, clearly $x\in U_j$ for some $j\in I$. Since $U_j$ is open, there is $\epsilon > 0$ such that $B_\epsilon(x)\subseteq U_j\subseteq U$. Hence, $U$ is also open and $\mathcal{T}_d$ is closed under arbitrary unions.
3. Let $\Set{U_i \subseteq X}_{i\in I}$ be a finite collection of open sets in $X$ for a finite index set $I$. Trivially, if $U=\bigcap_{i\in I} U_i = \emptyset$, then $U$ is open. For any $x\in U$, clearly $x\in U_i$ for every $i\in I$. For each $i\in I$ there is $\epsilon_i > 0$ such that $B_{\epsilon_i}(x)\subseteq U_i$. Take $\epsilon = \min\Set{ \epsilon_i }_{i\in I} > 0$. Since $I$ is finite, $B_\epsilon(x) \subseteq B_{\epsilon_i}(x)\subseteq U_i$ for each $i\in I$. Hence $B_\epsilon(x)\subseteq U$, so $U$ is open and $\mathcal{T}_d$ is closed under finite intersection.  
</details>
</MathBox>

<MathBox title='Metric spaces are Hausdorff' boxType='proposition'>
A metric space $(X, d)$ is a Hausdorff space.

<details>
<summary>Proof</summary>

Consider distinct point $x, y\in X$ with $r = d(x, y) > 0$. Note that $B_{\frac{r}{2}}(x)$ and $B_{\frac{r}{2}}(y)$ are open and contain $x$ and $y$, respectively. Suppose $z\in B_{\frac{r}{2}}(x)\cap B_{\frac{r}{2}}(y)$. Applying the triangle inequality

$$
  d(x, y) \leq d(x, z) + d(z, y) < \frac{r}{2} + \frac{r}{2} = r
$$

creates a contradiction. Hence $B_{\frac{r}{2}}(x) \cap B_{\frac{r}{2}}(y) = \emptyset$ (disjoint). Since $x$ and $y$ are arbitrarily chosen, we conclude that $(X, d)$ is a Hausdorff space.
</details>
</MathBox>

## Convergence

<MathBox title='Convergent sequence' boxType='definition'>
Let $(X, d)$ be a metric space. A sequence $\left( x_n \in X \right)_{n \in \N}$ of points in $X$ is convergent if there is $\tilde{x}\in X$ with 

$$
  \forall \epsilon > 0 \quad \exists N \in \N \quad \forall n \geq N : d\left(x_n, \tilde{x} \right) < \epsilon
$$

A limit is denoted $x_n \xrightarrow{n\to\infty} \tilde{x}$ or $\lim_{n\to\infty} x_n = \tilde{x}$. Equivalently this can be stated as

$$
  x_n \xrightarrow{n\to\infty} \tilde{x} \iff d(x_n, \tilde{x})\xrightarrow{n\to\infty} 0
$$
</MathBox>

<MathBox title='Uniqueness of limits' boxType='proposition'>
Let $(X, d)$ be a metric space, and let $\left( x_n \in X \right)_{n \in \N}$ be a sequence of points in $X$. If $x_n \xrightarrow{n\to\infty} x$ and $x_n \xrightarrow{n\to\infty} y$ for $x,y\in X$, then $x = y$. 

<details>
<summary>Proof</summary>

Let $\epsilon > 0$, then

$$
\begin{gather*}
  \exists M \in \N \; \forall n \geq M: \; d(x_n, x) < \frac{\epsilon}{2} \\
  \exists N \in \N \; \forall n \geq N: \; d(x_n, y) < \frac{\epsilon}{2}
\end{gather*}
$$

Let $n \geq \max(N, \tilde{N})$, and apply the triangle inequality

$$
  d(x, y) \leq d(x, x_n) + d(x_n, y) < \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon
$$

Hence $d(x, y) \xrightarrow{n\to\infty} 0$ implying $x = y$
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $(X, d)$ be a metric space. A subset $A \subseteq X$ is closed if and only if for every convergent sequence $\left(a_n \in A \right)_{n \in \N}$ of points in $A$, the limit is in $A$, i.e. $\lim_{n\to\infty} a_n \in A$.

<details>
<summary>Proof</summary>

Suppose $A$ is closed and that $\left(a_n \in A \right)_{n \in \N}$ is a sequence of points in $A$ such that $a_n \xrightarrow{n\to\infty} x \in X$. Suppose $x\in A^c$. Since $A^c$ is open, $x^n \in A^c$ for sufficiently large $n$, giving a contradiction. Hence $x\in A$. Conversely, suppose $A$ is not closed. Then $A^c$ is not open and there exists $x\in A^c$ with the property that every neighbourhood of $x$ has points in $A$. Note that for each $n\in\N$ there exist $x_n \in B_{\frac{1}{n}}(x)$ with $x_n \in A$. However, clearly $x_n \xrightarrow{n\to\infty} x$, giving a contraction.
</details>
</MathBox>

## Completeness (Cauchy sequence)

<MathBox title='Cauchy sequence' boxType='definition'>
Let $(X, d)$ be a metric space. A sequence $\left( x_n \in X \right)_{n \in \N}$ is called a Cauchy sequence if 

$$
  \forall \epsilon > 0 \; \exists N \in \N: \forall n, m \geq N: \; d(x_n, x_m) < \epsilon
$$

A metric space is complete if all Cauchy sequences converge.
</MathBox>

<MathBox title='Convergent sequences are Cauchy' boxType='proposition'>
Let $(X, d)$ be a metric space. If a sequence $\left( x_n \in X \right)_{n \in \N}$ converges, then the sequence is Cauchy.
<details>
<summary>Proof</summary>

Assume the sequence $\left( x_n \in X \right)_{n\in\N}$ converges to $x\in X$. Let $\epsilon > 0$, then there exists $N\in\N$ such that $d(x_n, x) < \frac{\epsilon}{2}$ for every $n \geq N$. If $m, n > N$, then by the triangle inequality

$$
  d(x_m, x_n) \leq d(x_m, x) + d(x_n, x) < \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon
$$

Hence $\left( x_n \right)_{n\in\N}$ is Cauchy.
</details>
</MathBox>

<MathBox title='Complete sets are closed' boxType='proposition'>
Let $(X, d)$ be a metric space. A complete set $A\subseteq X$ is also closed.

<details>
<summary>Proof</summary>

Assume the sequence $\left( a_n \in A \right)_{n\in\N}$ converges to $x \in X$. Then $\left( a_n \in A \right)_{n \in \N}$ is a Cauchy sequence, and by completeness $x\in A$. Hence $A$ is closed.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $(X, d)$ be a metric space, and let $\left( x_n \in X \right)_{n\in\N}$ be a Cauchy sequence. If there is a subsequence $\left( x_{n_k} \right)_{k\in\N}$ with $x_{n_k}\xrightarrow{k\to\infty} x \in X$, then $x_n \xrightarrow{n\to\infty} x$.

<details>
<summary>Proof</summary>

Assume the subsequence $\left( x_{n_k} \in A \right)_{k\in\N}$ converges to $x \in X$. Let $\epsilon > 0$, then there exists $j\in\N$ such that $d\left(x_{n_k}, x) < \frac{\epsilon}{2}$. Since $\left( x_n \right)_{n\in\N}$ is Cauchy, there exists $N\in\N$ such that for $d(x_m, x_n) < \frac{\epsilon}{2}$ for $m, n \geq N$. Choose $k\in\N$ such that $k \geq j$ and $n_k \geq N$. By the triangle inequality

$$
  d(x_m, x) \leq d\left(x_m, x_{n_k} \right) + d\left(x_{n_k}, x \right) \leq \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon
$$

Hence $x_n \xrightarrow{n\to\infty} x$.
</details>
</MathBox>

## Continuity

<MathBox title='Continuity' boxType='definition'>
Given two metric spaces $(X, d_X)$ and $(Y, d_Y)$, a function $f: X \to Y$ is continuous if $f^{-1}[B] \subseteq X$ is open for all open sets $B \subseteq Y$. A function $f$ is called sequentially continuous if for all $\tilde{x} \in X$ and $\left( x_n \right)_{n \in \N} \subseteq X$ with $x_n \xrightarrow{n\to\infty} \tilde{x}$ holds $f(x_n) \xrightarrow{n\to\infty} f(\tilde{x})$. For metric spaces, the two continuity concepts are equivalent. 
</MathBox>

### Uniform continuity

<MathBox title='Point-wise continuity' boxType='definition'>
Given two metric spaces $(X, d_X)$ and $(Y, d_Y)$, a function $f: X \to Y$ is continuous at $x\in X$ if

$$
  \forall \epsilon > 0 \; \exists \delta > 0 : y\in X \land d_X(x, y) < \delta \implies d_Y(f(x), f(y)) < \epsilon
$$

<details>
<summary>Proof</summary>

The definition of point-wise continuity is equivalent with the topoligical definition of continuity. Suppose $f$ is continuous at $x$. For $\epsilon > 0$, the open ball $B_{d_Y}(f(x), \epsilon)$ is a neighbourhood of $f(x)$ and hence $U = f^{-1}\left[ B_{d_Y}(f(x), \epsilon) \right]$ is a neighbourhood of $x$. Thus there exists $\delta > 0$ such that $B_{d_Y}(x, \delta) \subseteq U$. Conversely, suppose $V$ is a neighbourhood of $f(x)$. Then there exists $\epsilon > 0$ such that $B_{d_Y}(f(x), \epsilon)\subseteq V$. By assumption, there exists $\delta > 0$ such that if $y\in B_{d_X}(x, \delta)$, then $f(y) \in B_{d_Y}(f(x), \epsilon)\subseteq V$. Hence $f^{-1}[V]$ is a neighbourhood of $x$.
</details>
</MathBox>

<MathBox title='Uniform continuity' boxType='definition'>
Given two metric spaces $(X, d_X)$ and $(X, d_Y)$, a map $f: X \to Y$ is uniformly continuous if for all $\epsilon > 0$ there exists a $\delta > 0$ such that for every $x, y \in X$ with $d_X(x, y) < \delta$, we have that $d_Y (f(x), f(y)) < \epsilon$. If $f$ is uniformly continuous, then $f$ is continuous (on $X$).
</MathBox>

### Equicontinuity

<MathBox title='Equicontinuity' boxType='definition'>
Let $F \subseteq C(X, Y)$ be a family of continuous maps from $X$ to $Y$. Then $F$ is uniformly equicontinuous if for all $\epsilon > 0$, there exists a $\delta > 0$ such that $d_Y (f(x), f(y)) < \epsilon$ for all $f \in F$ and all $x, y \in X$ such that $d_X (x, y) < \delta$. Equivalently 

$$
  \sup_{f \in F} d_Y (f(x), f(y)) \xrightarrow{d_X (x, y)\to 0} 0
$$
</MathBox>

### Hölder continuity

<MathBox title='Hölder continuity' boxType='definition'>
Given two metric spaces $\left( X, d_X \right)$ and $\left( Y, d_Y \right)$, a function $f: X\to Y$ is called Hölder continuous with exponent $\alpha\in(0,\infty)$ if there is $C\in(0, \infty)$ such that, for all $x, \tilde{x}\in X$

$$
  d_Y\left( f(x), f\left(\tilde{x}\right) \right) \leq Cd_X\left( x, \tilde{x} \right)^\alpha
$$

For any $\alpha > 0$, the Hölder condition implies that $f$ is uniformly continuous. If $\alpha = 1$, then function satisfies the Lipschitz condition.
</MathBox>

<MathBox title='Contraction' boxType='definition'>
Given two metric spaces $\left( X, d_X \right)$ and $\left( Y, d_Y \right)$, a function $f: X\to Y$ is a contraction if there is $C\in(0, 1)$ such that, for all $x, \tilde{x}\in X$

$$
  d_Y\left( f(x), f\left(\tilde{x}\right) \right) \leq Cd_X\left( x, \tilde{x} \right)
$$

Contractions have Hölder exponent $\alpha = 1$, and are thus uniformly continuous.
</MathBox>

<MathBox title='Banach fixed-point theorem' boxType='theorem'>
Let $\left( X, d \right)$ be a complete metric space. Then any contraction $f:X\to X$ has a unique fixed point. That is, there exists $x*\in X$ with $f(x*) = x*$. Alternatively, let $x_0\in X$ and recursively define $x_n = f(x_{n-1})$ for $n\in\N$. Then $x_n \xrightarrow{n\to\infty} x*$.  
</MathBox>

### Lipschitz continuity

<MathBox title='Lipschitz continuity' boxType='definition'>
Given two metric spaces $\left( X, d_X \right)$ and $\left( Y, d_Y \right)$, a function $f: X\to Y$ is called Lipschitz continuous if there exists $C\in (0, \infty)$ such that, for all $x, \tilde{x}\in X$

$$
  d_Y\left( f(x), f\left(\tilde{x}\right) \right) \leq Cd_X\left( x, \tilde{x} \right)
$$

The Lipschitz condition ensures uniform continuity, as it corresponds with a Hölder exponent $\alpha > 0$.
</MathBox>

## Compactness and boundedness

<MathBox title='Bounded set' boxType='definition'>
Let $(X, d)$ be a metric space. A subset $A \subseteq X$ is bounded if there exists $r > 0$ such that $d(x, y) \leq r$ for all $x, y \in X$. The diameter of $A$ is defined as

$$
  \mathrm{diam}(A) := \inf\Set{ r > 0 | d(x, y) < r \; \forall x,y \in A }
$$

So $A$ is bounded if and only if $\mathrm{diam}(A) < \infty$.
</MathBox>

<MathBox title='Totally bounded set' boxType='definition'>
Let $(X, d)$ be a metric space. A subset $A \subseteq X$ is totally bounded if for every $r > 0$ there is a finite cover of $A$ with open balls of radius $r$.
</MathBox>

<MathBox title='' boxType='proposition'>
Let $(X, d)$ be a metric space. If $A \subseteq X$ is totally bounded, then $A$ is bounded.
<details>
<summary>Proof</summary>

If $A$ is totally bounded, there exists a finite cover of $A$ with open unit balls (radius $1$). Let $C$ denote the center of the balls, and let $c = \max\Set{ d(u, v) | u, v\in C }$ be the maximum distance between two centers. Since $C$ is finite, $c<\infty$. Now let $x, y\in A$. Since the balls cover $A$, there exists $u, v\in C$ with $x\in B_1(u)$ and $y\in B_1(v)$. By the triangle inequality

$$
  d(x, y) \leq d(x, u) + d(u, v) + d(v, y) \leq 2 + c
$$

This shows that $\mathrm{diam}(A) < \infty$, hence $A$ is bounded.
</details>
</MathBox>

<MathBox title='Compact set' boxType='definition'>
Let $(X, d)$ be a metric space. A subset $A \subseteq X$ is (sequentially) compact if for each sequence $\left( x_n \right)_{n \in \N} \subseteq A$ there is a convergent subsequence $\left( x_{n_k} \right)_{k \in \N}$ with $\tilde{x} := \lim_{k\to\infty} x_{n_k} \in A$.
</MathBox>

<MathBox title='' boxType='proposition'>
If a subspace $A \subseteq X$ is compact, then it is also closed and bounded.

<details>
<summary>Proof</summary> 

Let $A \subseteq X$ be compact and let $\left( x_n \right)_{n \in \N} \subseteq A$ be convergent with limit $\tilde{x} \in X$. Because $A$ is compact, there is a convergent subsequence $\left( x_{n_k} \right)_{k \in \N}$ with limit $\tilde{\tilde{x}} \in A$. Furthermore, since limits are unique, then $\tilde{x} = \tilde{\tilde{x}} \in A$. This shows that $A$ is closed.

Proving that compactness implies boundedness can be done by contraposition. Assuming that $A$ is not bounded, then for a given $a \in A$, there are $x_n \in A$ with $d(a, x_n) > n$. For any subsequence $\left( x_{n_k} \right)_{k \in \N}$ and any point $b \in A$ we have

$$
\begin{gather*}
  n_k < d(a, x_{n_k}) \leq d(a, b) + d(b, x_{n_k}) \\
  \implies n_k - d(a, b) \leq d(b,  x_{n_k}) \\
  \implies \lim_{k \to \infty} d(b,  x_{n_k}) \neq 0 \quad \forall b \in A
\end{gather*}
$$

Hence, $A$ is not compact. 
</details>
</MathBox>

<MathBox title='Heine-Borel theorem' boxType='theorem'>

Let $A\subseteq\R^n$ be subset of Euclidean space. Then $A$ is compact if and only if $A$ is closed and bounded.

</MathBox>

## Isometry

<MathBox title='Isometry' boxType='definition'>
Let $(X, d_X)$ and $(Y, d_Y)$ be two metric spaces. A function $f: X \to Y$ is called an isometry if for any $a, b \in X$

$$
  d_Y \left( f(a), f(b) \right) = d_X \left(a, b \right)
$$

That is, $f$ preserves distance. The metric spaces $(X, d_X)$ and $(Y, d_Y)$ are isometric if there exists a bijective isometry $f:X\to Y$.
</MathBox>

<MathBox title='' boxType='proposition'>
Let $(X, d_X)$ and $(Y, d_Y)$ be two metric spaces, and consider a function $f:X\to Y$. If function $f$ is isometric, then $f$ is injective and Lipschitz continuous.
<details>
<summary>Proof</summary> 

By definition, an isometry $f$ is Hölder continuous with $\alpha = 1$ and $C = 1$, and hence $f$ is also Lipschitz continuous. To show injectivity, note that for any $x, y\in X$ with $x \neq y$, then $d_Y(f(x), f(y)) = d(x, y) > 0$. Hence $f(x)\neq f(y)$, showing that $f$ is injective (one-to-one). 
</details>
</MathBox>

<MathBox title='Isometry is an equivalence relation' boxType='proposition'>
Isometry is an equivalence relation on metric spaces that satisifies, for metric spaces $(X, d_X)$, $(Y, d_Y)$ and $(Z, d_Z)$
1. reflexivity: $X$ is isometric to $X$
2. symmetry: if $X$ is isometric to $Y$, then $Y$ is isometric to $X$
3. transitivity: if $X$ is isometric to $Y$ and $Y$ is isometric to $Z$, then $X$ is isometric to $Z$
<details>
<summary>Proof</summary> 

1. The identity function $\operatorname{id}:X\to X$ is an isometry from $X$ to $X$.
2. if $X$ is isometric to $Y$, there is a bijective isometry $f:X\to Y$ with an isometric inverse $f^{-1}: Y\to X$. Hence $Y$ is isometric to $X$.
3. if $X$ is isometric to $Y$ and $Y$ is isometric to $Z$, there are two bijective isometries $f: X\to Y$ and $g:Y\to Z$. Then the composition $g\circ f: X\to Z$ is an isometry from $X$ to $Z$.
</details>
</MathBox>

# Normed space

<MathBox title='Normed space' boxType='definition'>
Let $\mathbb{F} \in \Set{\R, \mathbb{C} }$ be a field of numbers and let $X$ be a $\mathbb{F}$-vector space. A map $\|\cdot\| : X \mapsto [0, \infty)$ is called a norm if it satisfies

1. $\lVert x\rVert = 0 \iff x = 0$ (positive definite)
2. $\lVert \lambda x\rVert = |\lambda| \lVert x\rVert, \quad \lambda \in \mathbb{F}$ (absolutely homogenous)
3. $\lVert x + y\rVert \leq \lVert x\rVert + \lVert y\rVert$ (triangle inequality)

The pair $(X, \lVert\cdot\|)$ is called a normed space. The norm $\| \cdot\rVert$ induces a metric by

$$
  d_{\lVert \cdot\rVert}(x, y) := \lVert x - y\rVert
$$

which makes the normed space $(X, \|\cdot\|)$ into a metric space $(X, d_{\|\cdot\|})$.
</MathBox>

The norm $\lVert \cdot\rVert$ is a continuous map. Considering the sequence $\left( x_n \right)_{n\in\N} \subseteq X$ with limit $\tilde{x} \in X$, then by the triangle inequality

$$
\begin{gather*}
  \lVert x_n\rVert = \lVert \left(x_n - \tilde{x}\right) + \tilde{x}\rVert \leq \lVert x_n - \tilde{x}\rVert + \lVert \tilde{x}\rVert = d \left(x_n, \tilde{x} \right) + \lVert \tilde{x}\rVert \\
  \implies \lim_{n\to\infty} \lVert x_n\rVert \leq \lVert \tilde{x}\rVert
\end{gather*}
$$

Conversely,

$$
\begin{gather*}
  \lVert \tilde{x}\rVert = \lVert \left(\tilde{x} - x_n \right) + x_n\rVert \leq \lVert \tilde{x} - x_n\rVert + \lVert x_n\rVert = d(\tilde{x}, x_n) +  \lVert x_n\rVert \\
  \implies \lVert \tilde{x}\rVert \leq \lim_{n\to\infty}  \lVert x_n\rVert
\end{gather*}
$$

<MathBox title='Banach space' boxType='definition'>
A normed space $(X,\lVert\cdot\|)$ is called a Banach space if $\left(X, d_{\| \cdot\rVert} \right)$ is a complete metric space. A Banach space is a complete normed space.
</MathBox>

## Asymptotic behaviour

<MathBox title='Bachmann-Landau notation' boxType='definition'>
Let $X, Y$ be normed spaces, and let $f:X\to Y$ and $\alpha: X\to [0,\infty)$ be functions. We write

$$
  f(x) = O(\alpha(x)),\; x\to a \in X
$$

if there exists some constants $C > 0$ and $r > 0$ such that for $a\in X$

$$
  \lVert f(x) \rVert_Y \leq C\alpha (x),\; \forall x\in B(a,r) \subset X
$$

We write

$$
  f(x) = o(\alpha(x)),\; x\to a \in X
$$

if for every $\epsilon > 0$ there is $\delta > 0$ such that

$$
  \frac{\lVert f(x) \rVert_Y}{\alpha (x)} \leq \epsilon,\; \forall x\in B(a,\delta) \subset X
$$

or equivalently

$$
  \lim_{x\to a} \frac{\lVert f(x) \rVert_X}{\alpha (x)} = 0
$$

Similarly, we write

$$
  f(x) = O(\alpha(x)),\; x\to\infty
$$

if there exists constants $C > 0$ and $K > 0$ such that $\lVert f(x) \rVert_Y < C \alpha(x)$ for all $x\in X$ with $\lVert x \rVert_X > K$.

We write 

$$
  f(x) = o(\alpha(x)),\; x\to\infty
$$

if for every $\epsilon > 0$ there is $K > 0$ such that

$$
  \frac{\lVert f(x) \rVert_Y}{\alpha (x)} \leq \epsilon
$$

for all $x \in X$ with $\lVert x \rVert$, or equivalently

$$
  \lim_{\lVert x \rVert_X \to\infty} \frac{\lVert f(x) \rVert_Y}{\alpha(x)} = 0
$$
</MathBox>

## Dual space

<MathBox title='Dual space' boxType='definition'>
The dual space $X'$ of a normed space $X$ is the space of all continuous (bounded) linear operators (functionals) $\ell : X \to \mathbb{F}$. 
</MathBox>

<MathBox title='Dual spaces are Banach spaces' boxType='proposition'>
Let $X'$ be the dual space of a normed space $X$. The dual space itself is a normed space $\left(X', \lVert \cdot\rVert_{X \to \mathbb{F}}\right)$, which is also a Banach space.

<details>
<summary>Proof</summary> 

Let $\left( \ell_k \right)_{k\in\N} \subseteq X'$ be a Cauchy sequence, i.e.

$$
  \forall \epsilon > 0 \quad \exists N \in \N \quad \forall n, m \geq N : \lVert \ell_n - \ell_m\rVert_{X\to\mathbb{F}} < \epsilon
$$

Since the operator norm $\lVert \cdot\rVert_{X\to\mathbb{F}}$ is defined by a supremum, we know that for all $x \in X$ with $x\neq 0$

$$
\begin{gather*}
  \frac{1}{\lVert x\rVert_X} \left| \ell_n - \ell_m \right| \leq \lVert \ell_n - \ell_m\rVert_{X\to\mathbb{F}} < \epsilon \\
  \left| \ell_n - \ell_m \right| < \epsilon \lVert x\rVert_X
\end{gather*}
$$

Hence, $\left( \ell_k (x) \right)_{k\in\N} \subseteq \mathbb{F}$ is a Cauchy sequence for all $x \in X$, with the limit

$$
  \ell(x) := \lim_{k\to\infty} \ell_k (x)
$$

We then have to show that $\ell: X \to \mathbb{F}$ is linear and bounded and that $\lVert \ell_k - \ell\rVert_{X \to \mathbb{F}} \xrightarrow{k\to\infty} 0$. Boundedness follows from the Cauchy property of $\left( \ell_k \right)_{k\in\N}$:

$$
\begin{gather*}
  \lVert \ell_n\rVert_{X\to\mathbb{F}} \overset{\triangle}{\leq} \underbrace{\lVert \ell_n - \ell_N\rVert_{X\to\mathbb{F}}}_{< \epsilon} + \underbrace{\lVert \ell_N\rVert_{X\to\mathbb{F}}}_{:= C} \leq \epsilon + C \quad \forall n\geq N \\
  \implies \left| \ell(x) \right| = \left| \lim_{k\to\infty} \ell_k (x) \right| = \lim_{k\to\infty} \left| \ell_k (x) \right| \leq \lim_{k\to\infty} \underbrace{\lVert \ell_{k}\rVert_{X\to\mathbb{F}}}_{\leq \tilde{C}} \lVert x\rVert_X \\
  \implies \lVert \ell\rVert_{X\to\mathbb{F}} = \frac{1}{\lVert x\rVert_X} \left| \ell(x) \right| \leq \tilde{C} < \infty
\end{gather*}
$$

To show the convergence of the operator norm $\lVert \cdot\rVert_{X\to\mathbb{F}}$, we have the for $\epsilon > 0$ there is a $N \in \N$ such that for all $n, m \geq N$

$$
\begin{gather*}
  \frac{1}{\lVert x\rVert_X} \left| \ell_n (x) - \ell_m (x) \right| < \epsilon \\
  \implies \sup_{\overset{x\in X}{x\neq 0}} \frac{1}{\lVert x\rVert_X}  \left| \ell_n (x) - \lim_{m\to\infty} \ell_m (x) \right| = \sup_{\overset{x\in X}{x\neq 0}} \frac{1}{\lVert x\rVert_X}  \left| \ell_n (x) - \ell (x) \right| \leq \epsilon \\
  \implies \lVert \ell_n - \ell\rVert_{X\to\mathbb{F}} \leq \epsilon
\end{gather*}
$$
</details>
</MathBox>

<MathBox title='Hahn-Banach theorem' boxType='theorem'>
Let $\left(X, \lVert \cdot\rVert_X \right)$ be a normed space, $U \subset X$ a subspace and $\ell \in U^*$ a continuous linear functional on $U$. Then there exists a linear functional $\tilde{\ell} \in X^*$ that extends $\ell$ with $\tilde{\ell}(u) = \ell(u)$ for all $u \in U$ and $\lVert \tilde{\ell}\rVert_{X^*} = \lVert \ell\rVert_{U^*}$.
</MathBox>

## Arzelà-Ascoli theorem

<MathBox title='Arzelà-Ascoli theorem' boxType='theorem'>
Consider the Banach space $\left(C([a, b]), \lVert \cdot\rVert_\infty \right)$ of continuous functions on an interval $I = [a, b] \in \R$ with the supremum norm $\lVert f\rVert_\infty := \sup \Set{ |f(t)| | t\in[a, b] }$. A subset $F \in C([a, b])$ is compact if and only if $F$ is closed, bounded and uniformly equicontinuous.
</MathBox>

## Lebesgue space ($L^p$)

<MathBox title='$L^p$ space' boxType='definition'>
Let $(X,\mathcal{A},\mu)$ be a measure space and suppose $f: X\to\R$ is measurable. For $p\in(0,\infty)$ we define the $p$-norm

$$
  \lVert f\rVert_p := \left( \int_X |f|^p \;\d\mu \right)^{1/p}
$$

and for $p=\infty$ we define the essential supremum norm

$$
  \lVert f\rVert_\infty := \inf\Set{b\in[0,\infty] : |f|\overset{\textrm{a.e.}}{\leq} b }
$$

The function $f$ is called $p$-integrable if $\| f\|_p < \infty$. The set of all $p$-integrable functions is denoted

$$
  L^p (X,\mathcal{A},\mu) := \Set{ f:X\to\R: \| f\|_p < \infty } \subseteq \mathcal{M}(X,\R)
$$

and is called a Lebesgue space. Here $\mathcal{M}(X,\R)$ denotes the collection of all measurable functions from $X$ into $\R$.
</MathBox>

<MathBox title='$L^p$ space are normed vector spaces' boxType='proposition'>
For $p\in[1,\infty]$, let $\mathcal{L}^p = \Set{[f] : f\in L^p }$ and define $\lVert[f]\|_p =\rVert f \|_p$ for $f\in\mathcal{M}(X,\R)$ where $\mathcal{M}(X,\R)$ is the collection of measurable functions $f:X\to\R$. Then $\mathcal{L}^p \subseteq\mathcal{M}\setminus\equiv$ and $\|\cdot\|$ is a norm on $\mathcal{L}^p$ that satisfies for $f,g\in L^p$ and $c\in\mathcal{R}$
1. $\| f\|_p \geq 0$ and $\| f\|_p = 0$ if and only if $f\equiv 0$ (positive definite)
2. $\| cf\|_p = |c|\cdot\| f\|_p$ (absolutely homogenous)
3. $\lVert f+g\|_p \leq\rVert f\lVert_p +\rVert p\|_p$ (triangle inequality)

For every $p\in[0,\infty]$, the Lebesgue space $L^p$ is a Banach space.
<details>
<summary>Proof</summary> 

That $\mathcal{L}^p \subseteq\mathcal{M}\setminus\equiv$ follows immediately from that $L^p\subseteq\mathcal{M}$. It remains to show that $\|\cdot\|_p$ satisfies the norm properties.
1. The predicate $\lVert f\|_p \geq 0$ follows from the definitions. For the second part, we first consider the case $p\in(0,\infty)$. Trivially, $\| 0\|_p = 0$. Conversely, if $\| f\|_p = 0$ then $\int_X\rVert f\|^p\;\d\mu = 0$ and hence $|f|^p \overset{\textrm{a.e.}}{=} 0$ and $f \overset{\textrm{a.e.}}{=} 0$. For the case $p = \infty$, it follows trivially that $\| 0\|_\infty = 0$. Conversely, suppose $\| f\|_\infty = 0$. Then for each $n\in\N$ there exists $b_n\in[0,\infty)$ with $b_n \xrightarrow{n\to\infty}0$ and $|f\overset{\textrm{a.e.}\leq b_n$, resulting in $f\overset{\textrm{a.e.}}{=}0$.
2. For $p\in(0,\infty)$ we have

$$
\begin{gather*}
  \| cf\|^p \int_X |cf|^p\;\d\mu = |c|^p \int_X |f|^p\;\d\mu = |c|^p\cdot\| f\|^p \\
  \iff \| cf\| = |c|\cdot\| f\|_p
\end{gather*}
$$

For $p=\infty$, the result is trivially true if $c=0$. For $c\neq 0$, note that $b\in[0,\infty]$ is an essential bound of $|f|$  if and only if f $|c|b$ is an essential bound of $|cf|$.
3. This follows from the Minkowski inequality.
</details>
</MathBox>

### Hölder's inequality

<MathBox title='Conjugate indices' boxType='definition'>
Two indices $p,q\in(1,\infty)$ are called conjugate if $\frac{1}{p} + \frac{1}{q} = $. Note that

$$
  q = \frac{p}{p-1} = \frac{1}{1 - 1/p}
$$

so that $q\uparrow\infty$ as $p\downarrow 1$. Hence $1$ and $\infty$ are conjugate indices.
</MathBox>

<MathBox title="Young's inequality" boxType='theorem'>
If $x, y\in(0, \infty)$ and $p, q\in(1,\infty)$ are conjugate indices, then 

$$
  xy \leq \frac{1}{p}x^p + \frac{1}{q}y^q
$$

Equality occurs if and only if $x^p = y^p$

<details>
<summary>Proof</summary>

Fix $y\geq 0$ and define $f:(0,\infty)\to\R$ by

$$
\begin{align*}
  f(x) =& \frac{x^p}{p} + \frac{y^q}{q} - xy \quad x\geq 0 \\
  f'(x) =& x^{p-1} - y \\
  f''(x) =& (p-1)x^{p-2}
\end{align*}
$$

Solving for $f'(x) = 0$ shows that $f$ has a minimum at $x_0 = y^{1/(p-1)} = y^{q/p}$ as $f''(x_0) > 0$. Noticing that $q = \frac{p}{p-1}$, we get

$$
\begin{align*}
  f(x_0) =& \frac{y^{p/(p-1)}}{p} + \frac{y^q}{q} - y^{1/(p-1)}y \\
  =& y^q \left(\frac{1}{p} + \frac{1}{q}) - y^q \\
  =& y^q - y^q = 0
\end{align*}
$$

Since $x_0$ is a minima it follows that $xy\leq \frac{x^p}{p} + \frac{y^q}{p}$. 
</details>
</MathBox>

<MathBox title="Hölder's inequality" boxType='theorem'>
Suppose $f,g:X\to\R$ are measurable on $(X,\mathcal{A},\mu)$ and that $p$ and $q$ are conjugate indices. Then

$$
  \lVert fg\|_1 = \int_X |fg|\;\d\mu\leq\| f\rVert_p \cdot \lVert g\rVert_q
$$

If $(X,\mathcal{P}(X),\#)$ is a discrete measure, Hölder's inequality reduces to

$$
  \lVert xy\rVert_1 = \left(\sum_{i\in X} |x_i + y_i| \right) \leq \lVert x\rVert_p \cdot \lVert y\rVert_q
$$

<details>
<summary>Proof</summary>

The result follows trivially if $\| f\|_p = \infty$ or $\| g\|_\infty = \infty$, so assume $f\in L^p$ and $g\in L^q$. For the case $p = 1$ and $q = \infty$, note that $|g|\overset{\mu\textrm{-a.e.}{\leq}}\| g\|_\infty$, hence

$$
  \int_X |fg|\;\d\mu = \int_X |f|\cdot|g|\;\d\mu \leq\lVert g\|_\infty \int_X |f|\;\d\mu =\rVert f\lVert_1 \cdot\rVert g\|_\infty
$$

Consider the case where $p,q\in(1,\infty)$. By positive definiteness, the result holds if $\| f\|_p = 0$ or $\| g\|_q = 0$, so assume $\| f\|_p\geq 0$ and $\| g\|_q\geq 0$. By Young's inequality

$$
  |fg|\leq\frac{1}{p}|f|^p + \frac{1}{q}|g|^q
$$

Suppose first that $\lVert f\|_p =\rVert g\|_q = 1$. From the increasing monotonicity and linearity properties of the integral

$$
  \int_X |fg|\;\d\mu\leq \frac{1}{p}\int_X |f|^p \;\d\mu + \frac{1}{q}\int_X |g|^q \;\d\mu = \frac{1}{p} + \frac{1}{q} = 1
$$

For the general case where $\lVert f\|_p > 0$ and $\| g\|_q > 0$, let $f_1 = \frac{f}{\| f\|_p}$ and $g_1 = \frac{g}{\| g\|_q}$. Then $\| f_1\|_p =\rVert g_1\|_q = 1$ and $\| f_1g_1\|_q\leq 1$. By the scaling property of the norm

$$
  \| f_1g_1\|_1 = \frac{\| fg\|_1}{\| f\|_q\cdot\| g\|_q}\leq 1
$$
</details>
</MathBox>

### Minkowski's inequality

Minkowski's inequality produces the triangle inequality for the $p$-norm and establishes that $L^p$-spaces are normed vector spaces.

<MathBox title="Minkowski's inequality" boxType='theorem'>
Suppose $f,g:X\to\R$ are measurable on $(X,\mathcal{A},\mu)$ and that $p\in[1,\infty]$. Then

$$
  \lVert f + g\|_p = \left(\int_X |f + g|^p \;\d\mu\right)^{1/p} \leq\rVert f \lVert_p +\rVert g \|_p
$$

If $(X,\mathcal{P}(X),\#)$ is a discrete measure, Minkowski's inequality reduces to

$$
  \lVert x + y\rVert_p = \left(\sum_{i\in X} |x_i + y_i|^p\right)^{1/p} \leq \lVert x\rVert_p + \lVert y\rVert_p \quad
$$

<details>
<summary>Proof</summary>

The result is trivial if $\| f\|_p = \infty$ or $\| g\|_\infty = \infty$, so assume $f,g\in L^p$. When $p = 1$, the result is the triangle inequality

$$
\begin{align*}
  \lVert f + g\rVert_p =& \int_X |f + g|\;\d\mu \leq\int_X (|f| + |g|)\;\d\mu \\
  =& \int_X |f|\;\d\mu + \int_X |g|\;\d\mu = \lVert f\|_1 +\rVert g\|_1
\end{align*}
$$

For the case $p = \infty$, note that if $a,b\in[0,\infty]$ are an essential bounds for $f$ and $g$, respectively, then $a+b$ is an essential bound for $f + g$. Hence $\lVert f + g\Vert_\infty \leq\| f\|_\infty +\rVert g\|_\infty$.

Consider the case where $p\in(1,\infty)$ and let $q$ be the conjugate index to $p$. Then

$$
\begin{align*}
  |f+g|^p =& |f+g|^{p-1}\cdot|f+g| \leq |f+g|^{p-1}(|f|+|g|) \\
  =& |f+g|^{p-1}\cdot|f| + |f+g|^{p-1}\cdot|g|
\end{align*}
$$

Integrating over $X$ gives

$$
  \| f + g\|_p^p \leq \int_X |f+g|^{p-1}|f|\;\d\mu + \int_X |f+g|^{p-1}|g|\;\d\mu
$$

By Hölder's inequality

$$
\begin{align*}
  \int_X |f+g|^{p-1}|f|\;\d\mu &\leq \left\lVert |f+g|^{p-1} \right\|_q \cdot\rVert f\|_p \\
  \int_X |f+g|^{p-1}|g|\;\d\mu &\leq \left\lVert |f+g|^{p-1} \right\|_q \cdot\rVert g\|_p
\end{align*}
$$

Combining the inequalities gives

$$
  \lVert f + g\|_p^p \leq \left\| |f+g|^{p-1} \right\|_q (\| f\|_p +\rVert g\|_p)
$$

Since $\frac{1}{q} = \frac{p-1}{p}$ we get

$$
\begin{align*}
  \left\| |f+g|^{p-1} \right\|_q =& \left(\int_X |f+g|^{p-1}q \;\d\mu \right)^{1/q} \\
  =& \left(\int_X |f+g|^p \;\d\mu \right)^{(p-1)/p} = \lVert f+g\rVert_p^{p-1}
\end{align*}
$$

Hence we get

$$
\begin{gather*}
  \lVert f+g\rVert_p^p = \lVert f+g\rVert_p^{p-1}\left(\lVert f\|_p +\rVert g\|_p \right) \\
  \iff \lVert f + g\|_p \leq\rVert f \lVert_p +\rVert g \|_p
\end{gather*}
$$

**Discrete case:**
For $p = 1$, the ordinary triangle inequality applies

$$
  \lVert x + y\rVert_1 = \sum_{j=1}^\infty \left| x_j + y_j \right| \leq \sum_{j=1}^\infty \left| x_j \right| + \left| y_j \right| = \lVert x\rVert_1 + \lVert y\rVert_1
$$

For $p \in (1, \infty)$, we note $q = \frac{p}{p-1}$

$$
\begin{align*}
  \lVert x + y\rVert_p^p =& \sum_{j=1}^\infty \left| x_j + y_j \right|^p = \lim_{n\to\infty} \sum_{j=1}^n \left| x_j + y_j \right|^p \\
  \leq& \lim_{n\to\infty} \sum_{j=1}^n \left(\left| x_j \right| + \left| y_j \right| \right)^p
\end{align*}
$$

The summand can be written as

$$
\begin{align*}
  \left(\left| x_j \right| + \left| y_j \right| \right)^p =& \left(\left| x_j \right| + \left| y_j \right| \right) \left(\left| x_j \right| + \left| y_j \right| \right)^{p-1} \\
  =& \left| x_j \right| \left(\left| x_j \right| + \left| y_j \right| \right)^{p - 1} = \left| y_j \right| \left(\left| x_j \right| + \left| y_j \right| \right)^{p-1} \\
  \equiv& a_j b_j + c_j b_j
\end{align*}
$$

Applying Hölder's inequality $\lVert a b\rVert_1 + \lVert c b\rVert_1 \leq \lVert a\rVert_p \cdot \lVert b\rVert_{q} + \lVert c\rVert_p \cdot \lVert b\rVert_{q}$ with

$$
  \lVert b\rVert_{q} = \left( \sum_{j=1}^n \left| \left( \left| x_j \right| + \left| y_j \right| \right)^{p-1} \right|^{q} \right)^{\frac{1}{q}} = \left( \sum_{j=1}^n\left( \left| x_j \right| + \left| y_j \right| \right)^p \right)^{\frac{1}{q}}
$$

We get

$$
\begin{align*}
  \sum_{j=1}^n \left(\left| x_j \right| + \left| y_j \right| \right)^p \leq& \lVert a\rVert_p \cdot \lVert b\rVert_{q} + \lVert c\rVert_p \cdot \lVert b\rVert_{q} \\
  =& \left( \lVert a\rVert_p + \lVert c\rVert_p \right) \left( \sum_{j=1}^n \left( \left| x_j \right| + \left| y_j \right| \right)^p \right)^{\frac{1}{q}} \\
  \implies \left( \sum_{j=1}^n \left( \left| x_j \right| + \left| y_j \right| \right)^p \right)^{1 - \frac{1}{q}} =& \left( \sum_{j=1}^n \left( \left| x_j \right| + \left| y_j \right| \right)^p \right)^{\frac{1}{p}} \\
  \leq& \lVert a\rVert_p + \lVert c\rVert_p = \left( \sum_{j=1}^n \left| x_j \right|^p \right)^{\frac{1}{p}} + \left( \sum_{j=1}^n \left| y_j \right|^p \right)^{\frac{1}{p}}
\end{align*}
$$

In the limit $n \to \infty$ we retrieve the Minkowski inequality.
</details>
</MathBox>

### Sequence space ($\ell^p$)

Discrete measure spaces $(X,\mathcal{P}(X),\#)$ gives a special case for $L^p$ called sequence spaces. For a measurable function $x:X\to\R$ we introduce the notation $x_i = x(i)$.

<MathBox title='Sequence space' boxType='definition'>
Let $(X,\mathcal{P}(X),\#)$ be a discrete measure space and let $x:X\to\R$ be measurable. For $p \in [1, \infty)$, we define the $p$-norm

$$
  \lVert x\rVert_p := \left( \sum_{i\in X} |x_i|^p \right)^(1/p)
$$

and for $p=\infty$ we define $\| x\|_\infty = \sup\Set{x_i | i\in X}$. The Lebesgue space takes the form

$$
  L^p(X,\mathcal{P}(X),\#) = \Set{ (x_i)_{i\in X} \in\R^n : \lVert p\rVert_p < \infty}
$$

When $X = \N$ the $L^p$ space is usually denoted $\ell^p$.
</MathBox>

<MathBox title='Sequence spaces are Banach spaces' boxType='definition'>
The normed space $\left( \ell^p, \lVert \cdot\rVert_p \right)$ is a Banach space.

<details>
<summary>Proof</summary> 

To show the completeness of $\left( \ell^p, \lVert \cdot\rVert_p \right)$ we consider a Cauchy sequence $\left( x^{(k)} \right)_{k \in \N}$ in $\ell^p$. 

$$
\begin{align*}
  x^{(1)} =& \left( x_1^{(1)}, x_2^{(1)}, \dots \right) \\
  x^{(2)} =& \left( x_1^{(2)}, x_2^{(2)}, \dots \right) \\
  \vdots
\end{align*}
$$

The elementwise difference between two points is

$$
  \left\lvert x_m^{(k)} - x_m^{(l)} \right\rvert^p \leq \sum_{n=1}^\infty \left\lvert x_n^{(k)} - x_n^{(l)} \right\rvert^p = \lVert x^{k} - x^{l}\rVert_p^p
$$

Because $\left( x^{(k)} \right)_{k \in \N}$ is a Cauchy sequence, it follows that $\forall \epsilon > 0 \; \exists N \in \N:\; \left\| x^{(k)} - x^{(l)} \right\|^p$. Hence, $\left( x_m^{(k)} \right)_{k \in \N}$ is a Cauchy sequence in $\mathbb{F}$ converging to a limit $\tilde{x}_m \in \mathbb{F}$. This suggests that $\left( x^{(k)} \right)_{k \in \N}$ converges to a limit $\tilde{x} := \left( \tilde{x}_i \right)_{i \in \N}$, which we will now show.

Let $\epsilon > 0$ and choose $K \in \N$ such that $\forall k, l \geq K:\; \left\| x^{(k)} - x^{(l)} \right\|_p < \epsilon'$

$$
\begin{align*}
  \left\| x^{(k)} - \tilde{x} \right\|_p^p =& \sum_{n=1}^\infty \left\lvert x^{(k)} - \tilde{x} \right\rvert^p = \lim_{N \to \infty} \sum_{n=1}^N \left\lvert x^{(k)} - \tilde{x} \right\rvert^p \\
  =& \lim_{N \to \infty} \lim_{l\to\infty} \sum_{n=1}^N \left\lvert x^{(k)} - x^{(l)} \right\rvert^p \leq (\epsilon')^p
\end{align*}
$$

Thus $\forall k \geq K: \left\| x^{(k)} - \tilde{x} \right\|_p \leq \epsilon' < \epsilon$, since $\epsilon'$ can be choosen arbitrarily small. Furthermore, it follows that $\tilde{x} = \left(\tilde{x} - x^{(k)} \right) + x^{(k)} \in \ell^p$ since both $\tilde{x} - x^{(k)}$ and $x^{(k)}$ are in $\ell^p$ and $\ell^p$ is a vector space and thus closed under addition.
</details>
</MathBox>

<MathBox title='Isomorphism of $\ell^p$-dual space' boxType='proposition'>
The dual space of $\ell^p (\N)$ has a natural isomorphism with $\ell^q (\N)$ where $q$ is the Hölder conjugate satisfying $\frac{1}{p} + \frac{1}{q} = 1$. This isomorphism associates $x \in \ell^q$ with the functional $T: \ell^q(\N) \to \ell^p (\N)^*$ given by

$$
\begin{gather*}
  (Tx)(y) := \sum_{j=1}^\infty x_j y_j \quad \forall y \in \ell^p (\N) \\
  x \mapsto \langle \bar{x}, \cdot \rangle_{\ell^2 (\N)}
\end{gather*}
$$

<details>
<summary>Proof</summary>

Since the inner product by definition is linear in its second argument, it follows that $T$ is linear. By Hölder's inequality we have

$$
  \lvert (Tx)(y) \rvert \leq \lim_{n\to\infty} \sum_{j=1}^n \left| y_j x_j \right| \leq \lVert y\rVert_p \cdot \lVert x\rVert_q < \infty
$$

which shows that $Tx$ is bounded for all $x \in \ell^q(\N)$, hence it is well-defined. The operator norm for $T$ is

$$
\begin{gather*}
  \lVert Tx\rVert_{\ell^p (\N) \to \mathbb{F}} = \sup\Set{ \left| (Tx)(y) \right| | \lVert y\rVert_p = 1 } \leq \sup\Set{\lVert y\rVert_p \cdot \lVert x\rVert_q | \lVert y\rVert_p = 1} \leq \lVert x\rVert_q \\
  \implies \lVert T\rVert = \frac{\lVert Tx\rVert_{\ell^p (\N)}}{\lVert x\rVert_q} \leq 1
\end{gather*}
$$

Let $y' \in \ell^p (\N)^*$ and define $x_j := y'(e_j)$ and $x := \left( x_j \right)_{j \in \N}$ with $e_j = \left(\delta_{jk}\right)_{k\in\N}$ (unit sequence). To prove that $T$ is surjective, we need to show that $x \in \ell^q (\N)$ and $Tx = y'$.

$$
\begin{align*}
  \sum_{j=1}^n \left| x_j \right|^{q} =& \sum_{j=1}^n x_j t_j \quad\quad t_j = \begin{cases} \frac{\left| x_j \right|^q}{x_j}, & x_j \neq 0 \\ 0, & x = 0 \end{cases} \\
  =& \sum_{j=1}^n t_j y'(e_j) = y'\sum_{j=1}^n t_j e_j \\
  \leq& \lVert y'\rVert_{\ell^p (\N) \to \mathbb{F}} \lVert \sum_{j=1}^n t_j e_j\rVert_p = \lVert y'\rVert_{\ell^p (\N) \to \mathbb{F}} \left( \sum_{j=1}^n \left| t_j \right|^p \right)^{1/p}
\end{align*}
$$

since

$$
  \left| t_j \right|^p = \left(\frac{\left| x_j \right|^q}{\left| x_j \right|}\right)^p = \left| x_j \right|^{(q - 1)p} = \left| x_j \right|^q
$$

we get

$$
\begin{gather*}
  \sum_{j=1}^n \left| x_j \right|^{q} \leq \lVert y'\rVert_{\ell^p (\N) \to \mathbb{F}} \left( \sum_{j=1}^n \left| x_j \right|^q \right)^{1/p} \\
  \implies \left( \sum_{j=1}^n \left| x_j \right|^{q} \right)^{1 - 1/p} = \left( \sum_{j=1}^n \left| x_j \right|^{q} \right)^{1/q} \leq \lVert y'\rVert_{\ell^p (\N) \to \mathbb{F}} \\
  \xRightarrow{n\to\infty} \lVert x\rVert_q \leq \lVert y'\rVert_{\ell^p (\N) \to \mathbb{F}} \implies x \in \ell^q (\N)
\end{gather*}
$$

For $y \in \ell^p (\N)$ we have

$$
\begin{gather*}
\begin{aligned}
  (Tx - y')(y) =& (Tx - y')\left(\lim_{n\to\infty} \sum_{j=1}^n y_j e_j \right) \\
  =& \lim_{n\to\infty} (Tx - y') \left(\sum_{j=1}^n y_j e_j \right) \\
  =& \lim_{n\to\infty} \sum_{j=1}^n y_j (Tx - y')(e_j) = 0
\end{aligned} \\
  \implies Tx = y'
\end{gather*}
$$

To prove that $T$ is injective and thus bijective, it suffices to show that $T$ is isometric. We already know that 

$$
  \lVert Tx\rVert_{\ell^p (\N) \to \mathbb{F})} \leq \lVert x\rVert_q \leq \lVert y\rVert_{\ell^p (\N) \to \mathbb{F})}
$$

At the same time $\lVert Tx\rVert_{\ell^p (\N) \to \mathbb{F})} = \lVert y\rVert_{\ell^p (\N) \to \mathbb{F})}$, so that $\lVert Tx\rVert_{\ell^p (\N) \to \mathbb{F})} = \lVert x\rVert_q$ which shows that $T$ is ismotric.
</details>
</MathBox>

# Inner product space

<MathBox title='Inner product space' boxType='definition'>
Let $\mathbb{F} \in \Set{\R, \mathbb{C} }$ be a field of numbers and let $X$ be a $\mathbb{F}$-vector space. A map $\langle \cdot, \cdot \rangle : X \times X \to \mathbb{F}$ is called an inner product if it satisfies

1. Positive definite $\langle x, x \rangle \geq 0 \quad \forall x \in X$ and $\langle x, x \rangle = 0 \iff x = 0$
2. Conjugate symmetry
    - $\langle x, y \rangle = \langle y, x\rangle$ for $\mathbb{F} = \R$
    - $\langle x, y \rangle = \overline{\langle y, x \rangle}$ for $\mathbb{F} = \mathbb{C}$
3. Linearity in the 2nd argument:
    - $\langle x, y_1 + y_2\rangle = \langle x, y_1 \rangle + \langle x, y_2 \rangle$
    - $\langle x, \lambda y \rangle = \lambda \langle x, y \rangle, \quad \lambda \in \mathbb{F}$
4. Conjugate linearity in the 1st argument
    - $\langle \lambda x, y \rangle = \bar{\lambda} \langle x, y \rangle, \quad \lambda \in \mathbb{F}$
    
If $\langle \cdot, \cdot \rangle$ is an inner product, then $\lVert x\rVert_{\langle \cdot, \cdot \rangle} = \sqrt{\langle x, x \rangle}$ defines a norm. 
</MathBox>

The inner product $\langle \cdot, \cdot \rangle$ is a continuous map. Considering the sequence $\left( x_n \right)_{n\in\N}$ with limit $\tilde{x} \in X$ and a fixed $x_0 \in X$, then by the Cauchy-Schwartz inequality

$$
\begin{align*}
  \lvert \langle x_0, x_n \rangle - \langle x_0, \tilde{x} \rangle \rvert =& \lvert \langle x_0, x_n - \tilde{x} \rangle\rvert \\
  \leq& \lVert x_0\rVert \cdot \lVert x_n - \tilde{x}\rVert \xrightarrow{n\to\infty} 0 \\
  \implies f(x_n) \xrightarrow{n\to\infty} f(\tilde{x})
\end{align*}
$$

## Hilbert space

<MathBox title='Hilbert space' boxType='definition'>
If $\left(X, \lVert \cdot\rVert_{\langle \cdot, \cdot \rangle} \right)$ defines a Banach space, then $\left(X, \langle \cdot, \cdot \rangle \right)$ is called a Hilbert space 
</MathBox>

## $\ell^2$-space

The set of square summable sequences $\ell^2 (\N, \mathbb{F})$ endowed with the inner product  

$$
  \langle x, y \rangle = \sum_{i=1}^\infty \bar{x}_i y_i
$$

is a Hilbert space because $\ell^p$ is a Banach space. The map $\langle \cdot, \cdot \rangle: \ell^2 \times \ell^2 \to \mathbb{F}$ is indeed an inner product because it satisfies

1. Positivity: 
$$
\begin{gather*}
  \langle x, x \rangle = \sum_{i=1}^\infty \bar{x}_i x_i = \sum_{i=1}^\infty \lvert x_i \rvert^2 \geq 0 \\
  \langle x, x \rangle = 0 \implies \lvert x_i \rvert^2 = 0\; \forall i \in \N \implies x = 0
\end{gather*}
$$
2. Conjugate symmetry:
$$
  \overline{\langle y, x \rangle} = \sum_{i=1}^\infty \overline{\bar{y}_i x_i} = \sum_{i=1}^\infty y_i \bar{x}_i = \langle x, y \rangle
$$
3. Linearity in the 2nd argument: 
$$
\begin{align*}
  \langle x, \alpha y + \beta z \rangle =& \sum_{i=1}^\infty \bar{x}_i \left(\alpha y_i + \beta z_i \right) \\
  =& \alpha \sum_{i=1}^\infty \bar{x}_i y_i + \beta \sum_{i=1}^\infty \bar{x}_i z_i \\
  =& \alpha \langle x, y \rangle + \beta \langle x, z \rangle
\end{align*}
$$

## Cauchy-Schwarz inequality

<MathBox title='Cauchy-Scharz inequality' boxType='theorem'>
Let $\left(X, \langle \cdot, \cdot \rangle \right)$ be an inner product space with norm $\lVert x\rVert = \sqrt{\langle x, x \rangle}$. Then for all $x, y \in X$

$$
  \lvert \langle x, y \rangle \rvert \leq \lVert x\rVert \cdot \lVert y\rVert
$$

where the equality holds if and only if $x$ and $y$ are linearly dependent, ie. $y = \lambda x$. The triangle inequality follows from the Cauchy-Schwarz inequality

$$
\begin{align*}
  \lVert x + y\rVert^2 =& \langle x + y, x + y \rangle = \lVert x\rVert^2 + 2\lvert \langle x, y \rangle \rvert + \lVert y\rVert^2 \\
  \leq& \lVert x\rVert^2 + 2\lVert x\rVert \cdot \lVert y\rVert + \lVert y\rVert^2 = \left( \lVert x\rVert + \lVert y\rVert \right)^2
\end{align*}
$$

Taking the square root we obtain the triangle inequality $\lVert x + y\rVert \leq \lVert x\rVert + \lVert y\rVert$.

<details>
<summary>Proof</summary> 

Considering the non-trivial case $x \neq 0$, we define the unit vector $\hat{x} := \frac{x}{\lVert x\rVert}$ and take the orthogonal projection of $y$ onto $\hat{x}$ (parallel component) given by $y_\parallel = \langle \hat{x}, y \rangle \hat{x}$. The Cauchy-Schwarz inequality can be obtained from taking the norm of the orthogonal component $y_\perp = y - y_\parallel$

$$
\begin{align*}
  0 \leq \lVert y_\perp\rVert^2 =& \lVert y - y_\parallel\rVert^2 = \langle y - \langle \hat{x}, y \rangle \hat{x}, y - \langle \hat{x}, y \rangle \hat{x} \rangle \\
  =& \langle y - \langle \hat{x}, y  \rangle \hat{x}, y \rangle - \langle y - \langle \hat{x}, y  \rangle \hat{x}, \langle \hat{x}, y  \rangle \hat{x} \rangle \\
  =& \langle y, y \rangle - \langle \langle \hat{x}, y  \rangle \hat{x}, y \rangle - \langle y,  \langle \hat{x}, y  \rangle \hat{x} \rangle + \langle \langle \hat{x}, y  \rangle \hat{x}, \langle \hat{x}, y  \rangle \hat{x}  \rangle \\
  =& \lVert y\rVert^2 - \overline{\langle \hat{x}, y \rangle}\langle \hat{x}, y \rangle - \langle \hat{x}, y \rangle \overline{\langle \hat{x}, y \rangle}  + \lvert \langle \hat{x}, y  \rangle \rvert^2 \underbrace{\lVert \hat{x}\rVert^2}_{=1} \\
  =& \lVert y\rVert^2 - \lvert \langle \hat{x}, y  \rangle \rvert^2
\end{align*}
$$

rearranging the inequality we get

$$
\begin{gather*}
  \lVert y\rVert^2 \geq \lvert \langle \hat{x}, y  \rangle \rvert^2 = \left\lvert \left\langle \frac{x}{\lVert x\rVert}, y  \right\rangle \right\rvert^2 = \frac{1}{\lVert x\rVert^2} \lvert \langle x, y \rangle \rvert^2 \\
  \implies \lvert \langle x, y \rangle \rvert \leq \lVert x\rVert \cdot \lVert y\rVert
\end{gather*}
$$
</details>
</MathBox>

## Orthogonality

<MathBox title='Orthogonality' boxType='definition'>
Let $\left(X, \langle \cdot, \cdot \rangle \right)$ be an inner product space. Two vectors $x, y \in X$ are orthogonal if $\langle x, y \rangle = 0$, denoted $x \perp y$. Two sets $U, V \in X$ are orthogonal, denoted $U \perp V$, if $u \perp v$ for all $u \in U$ and $v \in V$. The orthogonal component of a set $U \subseteq X$ is

$$
  U^\perp = \Set{ x \in X | \langle x, u \rangle = 0 \; \forall u\in U }
$$
</MathBox>

If $x \perp y$, then the triangle inequality reduces to the Pytagorean theorem

$$
\begin{align*}
  \lVert x + y\rVert^2 =& \langle x + y, x + y \rangle \\
  =& \lVert x\rVert^2 + 2\lvert \langle x, y \rangle \rvert + \lVert y\rVert^2 \\
  =& \lVert x\rVert^2 + \lVert y\rVert^2
\end{align*}
$$

<MathBox title='Orthogonal components' boxType='proposition'>
Let $(X, \langle\cdot,\cdot\rangle)$ be an inner product space and let $U^\perp$ be the orthogonal component of $U\subseteq X$. Then $U^perp$ is closed. 

<details>
<summary>Proof</summary>

This is proved by considering a sequence $\left( x_n \right)_{n\in\N} \subseteq U^\perp$ showing that the limit $\tilde{x} \in X$ is also in $U^\perp$. It follows that for all $u \in U$

$$
  \langle x_n, u \rangle = 0 \implies \lim_{n\to\infty} \langle x_n, u \rangle = 0 \implies \langle \tilde{x}, u \rangle = 0 \implies \tilde{x}\in U^\perp
$$

Hence, $U^\perp$ is closed. Here we applied the continuity property of the inner product resulting in $\langle x_n, 0 \rangle \xrightarrow{n\to\infty} \langle \tilde{x}, u \rangle$.

If $U \subseteq V$ then $U^\perp \supseteq V^\perp$. For $x \in V^\perp$ we have $\langle x, v \rangle = 0$ for all $v \in V$. Because $U \subseteq V$, we have $\langle x, u \rangle = 0$ for all $u \in U$. This implies that $x \in U^\perp$, hence $U^\perp \supseteq V^\perp$. 
</details>
</MathBox>

## Riesz representation theorem

<MathBox title='Riesz representation theorem' boxType='theorem'>
For each continuous linear functional $\ell: X \to \mathbb{F}$ there is exactly one $x_\ell \in X$ such that $\ell(x) = \langle x_\ell, x \rangle$ for all $x \in X$ and $\lVert \ell\rVert = \lVert x_\ell\rVert_X$.

<details>
<summary>Proof</summary>

To prove the existence of $x_\ell$ we consider the kernel of $\ell$ defined as $\ker(\ell) := \Set{ x \in X | \ell(x) = 0 }$. In the trivial case $\ker (\ell) = X$ it follows that $x_\ell = 0$, so we assume $\ker(\ell) \neq X$. Evidently $\ell$ is continuous because $\ker(\ell) = \ell^{-1}(\Set{ 0 })$. Thus $\ker(\ell)$ is a closed subset of $X$. Consider $\hat{x} \in \ker(\ell)^\perp$ with $\lVert \hat{x}\rVert_X = 1$ and set $x_\ell := \overline{\ell(\hat{x})}\hat{x}$, then

$$
\begin{align*}
  \ell(x) =& \ell\left(x - \frac{\ell(x)}{\ell(\hat{x})}\hat{x} + \frac{\ell(x)}{\ell(\hat{x})}\hat{x} \right) = \ell\left(x - \frac{\ell(x)}{\ell(\hat{x})}\hat{x} \right) + \ell\left( \frac{\ell(x)}{\ell(\hat{x})}\hat{x} \right) \\
  =& \ell(x) - \frac{\ell(x)}{\ell(\hat{x})} \ell(\hat{x}) + \ell(\lambda \hat{x}) \\
  =& \lambda \ell(\hat{x}) \langle \hat{x}, \hat{x} \rangle = \lambda \langle \overline{\ell(\hat{x})}\hat{x}, \hat{x} \rangle = \langle x_\ell, \lambda\hat{x} - x + x\rangle \\
  =& \langle x_\ell, \lambda \hat{x} - x \rangle + \langle x_\ell, x \rangle \\
  =& \langle x_\ell, x \rangle
\end{align*}
$$

To prove the uniqueness of $x_\ell$, we assume $x_\ell, \tilde{x} \in X$ fulfil 

$$
\begin{align*}
  \ell(x) = \langle x_\ell, x \rangle = \langle \tilde{x}_\ell, x \rangle \\
  \implies \langle x_\ell - \tilde{x}_\ell, x \rangle = 0, \; \forall x \in X \\
  \implies \langle x_\ell - \tilde{x}_\ell, x_\ell - \tilde{x}_\ell \rangle = 0
\end{align*}
$$

Because the inner product is positive definite we conclude that $x_\ell = \tilde{x}_\ell$.

To prove that the operator norm is $\lVert \ell\rVert = \lVert x_\ell\rVert$, we have

$$
  \lVert \ell \rVert = \sup\Set{ \lvert \ell(x) \rvert : \lVert x \rVert_X = 1 } = \sup\Set{ \underbrace{\lvert \langle x_\ell, x\rangle \rvert}_{\leq \lVert x_\ell \rVert_X \cdot \lVert x \rVert_X } : \lVert x \rVert_X \leq 1 } \leq \lVert x_\ell \rVert
$$

Similarly

$$
  \lVert \ell\rVert \geq \left\lvert \ell \left( \frac{x_\ell}{\lVert x_\ell\rVert} \right) \right\rvert = \left\lvert \left\langle x_\ell, \frac{x_\ell}{\lVert x_\ell\rVert} \right\rangle \right\rvert = \lVert x_\ell\rVert
$$

Hence, we conclude that $\lVert \ell\rVert = \lVert x_\ell\rVert$.
</details>
</MathBox>

# Linear operators

<MathBox title='Linear operator' boxType='definition'>
Let $(X, \lVert \cdot\rVert_X)$ and $(Y, \lVert \cdot\rVert_Y)$ be two normed spaces. An operator $T: X \to Y$ is linear if it is closed under addition and scalar multiplication

$$
\begin{gather*}
  T(x + \tilde{x}) = Tx + T\tilde{x} \\
  T(\lambda x) = \lambda T x
\end{gather*}
$$

for all $x, \tilde{x} \in X$ and $\lambda \in \mathbb{F}$. The operator norm of $T$ is defined as

$$
  \lVert T\rVert = \lVert T\rVert_{X\to Y} := \sup\Set{ \frac{\lVert T_X\rVert_Y}{\lVert X\rVert_X} | x \neq 0 \in X }
$$
</MathBox>

## Bounded operators

<MathBox title='Bounded operator' boxType='definition'>
Let $T$ be a linear operator. If there exists some $M > 0$ such that $\lVert T x\rVert_Y \leq M\lVert x\rVert_X  < \infty$ for all $x \in X$, then $T$ is a bounded operator with operator norm $M$. The set of all linear and bounded operators $T: X \to Y$ is denoted $\mathcal{B}(X, Y)$.
</MathBox>

<MathBox title='' boxType='proposition'>
A linear normed operator is bounded if and only if and only if it is continuous.

<details>
<summary>Proof</summary> 

Suppose $T$ is bounded. Then for a sequence $\left(x_n \right)_{n\in\N} \subseteq X$ with limit $\tilde{x} \in X$ we have

$$
  \lVert Tx_n - T\tilde{x}\rVert_Y = \lVert T(x_n - \tilde{x})\rVert_Y \leq M \lVert x_n - \tilde{x}\rVert_X \xrightarrow{n\to\infty} 0
$$

which shows that $T$ is continuous. Conversly, suppose $T$ is continuous. Then there exists $\delta > 0$ such that $\lVert Tx\rVert_Y < 1$ for all $x \in X$ with $\lVert x\rVert_X < \delta$, thus

$$
  \lVert Tx\rVert_Y = \left\lVert \frac{\| x\rVert}{\delta} T\left( \delta \frac{x}{\lVert x\rVert} \right) \right\lVert = \frac{\| x\rVert}{\delta} \left\lVert T\left( \delta \frac{x}{\| x\rVert} \right) \right\lVert \leq \frac{\| x\rVert}{\delta} < \infty
$$
</details>
</MathBox>

## Isomorphism

<MathBox title='Isomorphism' boxType='definition'>
A map linear and bijective map $f: X \to Y$ is an isometric isomorphism if $\lVert f(x)\rVert_Y = \lVert x\rVert_X$ for any $x \in X$.
</MathBox>

## Uniform boundedness principle

<MathBox title='Banach-Steinhaus theorem' boxType='theorem'>
Let $X$ and $Y$ be normed spaces, where $X$ is also a Banach space. For every subset $\mathcal{M} \subseteq \mathcal{B}(X, Y)$, then $\mathcal{M}$ is bounded pointwise on $X$ if and only if $\mathcal{M}$ is uniformly bounded. Pointwise boundedness implies that for all $x\in X$ there is a $C_x \geq 0$ such that $\lVert Tx\rVert_Y \leq C_x$, while uniformly boundedness implies that for all $T \in \mathcal{M}$ there is a $C \geq 0$ such that $\lVert T\rVert_{X \to Y} \leq C$.
</MathBox>

## Open mapping theorem

<MathBox title='Banach-Schauder theorem' boxType='theorem'>
Let $X$ and $Y$ be Banach spaces, and $T \in \mathcal{B}(X, Y)$ a bounded linear operator $T: X \to Y$. The operator $T$ is surjective if and only if $T$ is an open map.
</MathBox>

## Bounded inverse theorem

<MathBox title='Bounded inverse theorem' boxType='theorem'>
Let $X$ and $Y$ be Banach spaces, and $T \in \mathcal{B}(X, Y)$ a bounded linear operator $T: X \to Y$. If $T$ is bijective then $T^{-1} \in \mathcal{B}(X, Y)$, i.e. continuous.
</MathBox>

## Neumann series

<MathBox title='Neumann series' boxType='definition'>
A Neumann series is a geometric series for an operator $T$

$$
  \sum_{k=0}^\infty T^k
$$

where $T^k = T^{k-1} \circ T$. If $T$ is a bounded linear operator on a normed space $X$, and the Neumann series converges in the operator norm then $I - T$ is invertible with the inverse as the series

$$
  (I - T)^{-1} = \sum_{k=0}^\infty
$$

In analogy to a geometric series $\sum_{k=0}^n = x^k = 1 - x^{n+1}$, we have that

$$
\begin{align*}
  \lim_{n\to\infty} (I - T)\sum_{k=0}^\infty T^k =& \lim_{n\to\infty} \left(\sum_{k=0}^\infty T^k - \sum_{k=1}^\infty T^{k + 1} \right) \\
  =& \lim_{n\to\infty} I - T^{n + 1} = I
\end{align*}
$$

Convergence is guaranteed when $X$ is a Banach space with $\lVert T\rVert < 1$.
</MathBox>

# Spectral theory

<MathBox title='Spectrum and resolvent set' boxType='definition'>
Let $X$ be a complex Banach space and $T: X \to X$ be a bounded linear operator. The spectrum of $T$ is defined by

$$
  \sigma(T) := \Set{ \lambda \in \mathbb{C} | (T - \lambda I) \textrm{ not bijective}  }
$$

The resolvent set of $T$ is defined by

$$
  \rho(T) := \Set{ \lambda \in \mathbb{C} | (T - \lambda I) \textrm{ bijective and } (T - \lambda I)^{-1} \textrm{ bounded}  }
$$

The bounded inverse theorem implies that $\sigma(T) = \mathbb{C}\setminus \rho(T)$. The spectrum can be split into disjoint sets $\sigma(T) = \sigma_p(T) \cup \sigma_c(T) \cup \sigma_r(T)$ where

- Point spectrum (eigenvalues): $\sigma_p(T) := \Set{ \lambda \in \mathbb{C} | (T - \lambda I) \textrm{ not injective}  }$
- Continuous spectrum: $\sigma_c(T) := \Set{ \lambda \in \mathbb{C} | (T - \lambda I) \textrm{ injective and not surjective with } \overline{\operatorname{ran}(T - \lambda I)} = X }$
- Residual spectrum: $\sigma_c(T) := \Set{ \lambda \in \mathbb{C} | (T - \lambda I) \textrm{ injective and not surjective with } \overline{\operatorname{ran}(T - \lambda I)} \neq X }$
</MathBox>

<MathBox title='Properties of spectra and resolvent sets' boxType='proposition'>
The spectrum $\sigma(T)$ and the resolvent $\rho(T)$ has the following properties
1. The resolvent $\rho(T)$ is open, while the spectrum $\sigma(T)$ is closed
2. The map $\rho(T): \to \mathcal{B}(X)$ given by $\lambda \mapsto (T - \lambda)^{-1}$ is analytical, i.e. it can be locally expressed as a Taylor series
3. The inverse of the distance of any $\lambda \in \rho(T)$ to the spectrum, written $d(\lambda, \sigma(T))$, satisfies

$$
  \lVert (T - \lambda)^{-1}\rVert \geq \frac{1}{d(\lambda, \sigma(T))}
$$

<details>
<summary>Proof</summary>

The first property can be shown by choosing $\lambda_0 \in \rho(T)$ and setting $c := \lVert (T - \lambda_0)^{-1}\rVert$ and $\epsilon := \frac{1}{c}$. Consider any $\lambda \in \mathbb{C}$ with $\left| \lambda - \lambda_0 \right| < \epsilon$, then

$$
  T - \lambda = (T - \lambda_0) - (\lambda - \lambda_0) = (T - \lambda_0) \left( I - (\lambda - \lambda_0)(T - \lambda_0)^{-1} \right) = (T - \lambda_0) (I - S)
$$

Furthermore, $\lVert S\rVert = \lVert \lambda - \lambda_0\rVert \cdot \lVert (T - \lambda_0)^{-1}\rVert < \epsilon c = 1$. This implies that $I - S$ is a convergent Neumann series, and thus invertible. Since, $T - \lambda$ is product a two invertible operators, it is also invertible. This shows that $\lambda \in \rho(T)$, meaning that $\rho(T)$ is open. Thus, $\sigma(T)$ is closed.

The second property follows from the results above, by writing out the inverse of $T - \lambda$

$$
\begin{align*}
  (T - \lambda)^{-1} =& (I - S)^{-1} (T - \lambda)^{-1} = \sum_{k=0}^\infty S^k (T - \lambda_0)^{-1} \\
  =& \sum_{k=0}^\infty (\lambda - \lambda_0 )^k (T - \lambda_0)^{-k} (T - \lambda_0)^{-1} \\
  =& \sum_{k=0}^\infty (T - \lambda_0)^{-(k + 1)} (\lambda - \lambda_0)^k
\end{align*}
$$

which takes the form of a Taylor series.

The third property also follows from the results above as

$$
  |\lambda - \lambda_0 | \geq \epsilon \implies \frac{1}{| \lambda - \lambda_0 |} \leq C = \lVert (T - \lambda)^{-1}\rVert
$$

Hence, we get

$$
  \frac{1}{d(\lambda, \sigma(T))} = \frac{1}{\inf_{\lambda \in \sigma(T)} | \lambda - \lambda_0 |} = \sup_{\lambda \in \sigma(T)} \frac{1}{| \lambda - \lambda_0 |} \leq \lVert (T - \lambda)^{-1}\rVert
$$
</details>
</MathBox>

## Spectral radius

<MathBox title='Properties of spectra and spectral radi' boxType='proposition'>
Let $X$ be a complex Banach space and $T: X \to X$ be a bounded linear operator. The spectrum $\sigma(T)$ has the following properties

1. $\sigma(T) \subseteq \mathbb{C}$ is compact (closed and bounded).
2. $\sigma(T) \neq \emptyset$ for $X \neq \Set{ 0 }$
3. $r(T) := \sup_{\lambda\in\sigma(T)}|\lambda| = \lim_{k\to\infty} \lVert T^k\rVert^{1/k} = \inf_{k\in\N} \lVert T^k\rVert^{1/k} \leq \lVert T\rVert < \infty$

where $r(T)$ denotes the spectral radius. 

<details>
<summary>Proof</summary>

The boundedness implied by the first and last property can be proved by choosing $\lambda \in \mathbb{C}$ with $|\lambda| > \lVert T\rVert$ and consider $\frac{T}{\lambda}$ as a Neumann series

$$
\begin{gather*}
  \left( I - \frac{T}{\lambda} \right)^{-1} = \sum_{k=0}^\infty \left( \frac{T}{\lambda} \right)^k \\
  \implies (T - \lambda)^{-1} = -\frac{1}{\lambda} \left(I - \frac{T}{\lambda} \right)^{-1} = -\frac{1}{\lambda} \sum_{k=0}^\infty \left( \frac{T}{\lambda} \right)^k \\
  \sup_{\lambda \in \sigma(T)} |\lambda | \leq \lVert T\rVert
\end{gather*}
$$

which shows that $\sigma(T)$ is bounded. 

The second property can be proved by contraposition, assuming $\sigma(T) = \emptyset \implies \rho(T) = \mathbb{C}$. Recall that the map $\rho(T) \to \mathcal{B}(X)$ given by $\lambda \mapsto (T - \lambda)^{-1}$ is analytic. Thus, we can take any linear functional $\ell \in \mathcal{B}(X)^*$ and apply it on the map above resulting in the map $f_\ell: \mathbb{C} \to \mathbb{C}$ given by $\lambda \mapsto \ell\left[ (T - \lambda)^{-1} \right]$. Then for any $\lambda \geq \lVert T\rVert$ we have

$$
  \left| f_\ell(\lambda) \right| \leq \lVert \ell\rVert \cdot \lVert (T - \lambda)^{-1}\rVert \leq \lVert \ell\rVert \frac{1}{|\lambda|}\sum_{k=0}^\infty \left\lVert \frac{T}{\lambda} \right\|^k \leq \frac{\| \ell\rVert}{\lVert T\rVert}
$$

which shows that $f_\ell$ is a bounded entire function. Applying Liouville's theorem implies that $f_\ell$ is constant. We can then proceed to calculate this constant for $\lambda_0 = 0$

$$
  f_\ell (0) = \ell(T^{-1})
$$

This should hold for all $\lambda \in \mathbb{C}$

$$
  f_\ell (\lambda) = \ell\left[ (T - \lambda)^{-1} \right] = \ell\left( \sum_{k=0}^\infty (T - \lambda_0)^{-(k + 1) (\lambda - \lambda_0)^k} \right) = \sum_{k=0}^\infty \ell \left( T^{-(k+1)} \right)\lambda^k
$$

Consequently $\ell\left(T^{-2}\right) = 0$ for all $\ell \in \mathcal{B}(X)^*$. By the Hahn-Banach theorem, we must have $T^{-2} = 0$, but this implies that $X = \Set{ 0 }$ since $T$ is bijective. Hence $\sigma(T) \neq \emptyset$ for  $X \neq \Set{0}$.
</details>
</MathBox>

## Adjoint operators

<MathBox title='Adjoint operator' boxType='definition'>
Let $X$ be a Hilbert space and $T: X \to X$ a bounded linear operator. The adjoint operator of $T$, denoted $T^\dagger: X \to X$, satisfies

$$
  \langle y, Tx \rangle = \langle T^* y, x \rangle \quad \forall x, y \in X
$$

A bounded linear operator $T$ is called
1. self-adjoint if $T^* = T$
2. skew-adjoint if $T^* = -T$
3. normal if $T^* T = T T^*$

If $T$ is normal then the spectral radius of $T$ is equal to the operator norm, i.e. $r(T) = \lVert T\rVert$.
</MathBox>

## Compact operators

<MathBox title='Compact operator' boxType='proposition'>
Let $\left(X ,\lVert \cdot\rVert_X\right)$ and $\left(X ,\lVert \cdot\rVert_X\right)$ be normed spaces. A bounded linear operator $T: X \to Y$ is *compact* if $\overline{T\left[\mathcal{B}_1(0)\right]}$ is compact.
</MathBox>

<MathBox title='' boxType='proposition'>
Suppose $\left(X ,\lVert \cdot\rVert_X\right)$ is a Banach space, and $T: X \to X$ a compact operator, then
1. $\sigma(T)$ is a countable set
2. $\dim(X) = \infty \implies 0 \in \sigma(T)$
3. $\sigma(T)\setminus\Set{0}$ could be empty or finite. Otherwise $\sigma(T)\setminus\Set{0} = \Set{ \lambda_n }_{n\in\N}$ with no accumulation points other than $0$.
4. Each $\lambda\in\sigma(T)\setminus\Set{0}$ is an eigenvalue of $T\left(\lambda \in \sigma(T)\setminus\Set{0}\right)$ with $\dim\left(\ker(T - \lambda) \right)$
</MathBox> 

## Example: $\ell^p$

Consider the sequence space $X = \ell^p (\N)$. For $\Set{ \lambda_j }_{j\in\N} \subset \mathbb{C}$ with $\sup_{j\in\N}\left| \lambda_j \right| < \infty$ we define a bounded linear operator $T:\ell^p (\N) \to \ell^p (\N)$ given by $(Tx)_j := \lambda_j x_j$. Evidently, the $\lambda_j$ are eigenvalues with corresponding eigenvectors $e_j$ as unit sequences, and the set $\Set{ \lambda_j }_{j\in\N} \subset \mathbb{C}$ forms the point spectrum of $T$

$$
  \Set{ \lambda_j }_{j\in\N} = \sigma_p (T) \subseteq \sigma (T)
$$

Because $\sigma_p (T)$ is infinite it can have accumulation points $\mu \in \mathbb{C}$ with $\mu \in \overline{\sigma_p (T)}$ such that $T - \mu I$ is injective. It can be shown that $T - \mu I$ is not surjective.

Assuming by contradiction that $T - \mu I$ is surjective, implying that it is also bijective. By the bounded inverse theorem, the inverse $(T - \mu I)^{-1}$ is therefore bounded with operator norm

$$
\begin{align*}
  \lVert (T - \mu I)^{-1}\rVert \geq& \lVert (T - \mu I)^{-1} e_j\rVert_{\ell^p (\N)} \\
  =& \lVert (\lambda_j - \mu I)^{-1} e_j\rVert_{\ell^p (\N)} \\
  =& \frac{1}{\left| \lambda_j - \mu \right|}
\end{align*}
$$

Since $\mu$ is an accumulation point we can find a subsequence such that the resulting fraction goes to infinity, so that $(T - \mu I)^{-1}$ cannot be bounded. Thus, by contradiction $T - \mu I$ is not surjective. The spectrum of $T$ becomes

$$
\begin{align*}
  \sigma (T) =& \Set{ \mu \in \mathbb{C} | \mu \notin \Set{\lambda_j}_{j\in\N} \land \mu\in\overline{\Set{\lambda_j }}_{j\in\N}} \\
  =& \sigma_p(T) \cup \sigma_c(T)
\end{align*}
$$

# Analysis in normed spaces

## Fréchet derivative

<MathBox title='Fréchet derivative' boxType='definition'>
Let $(X, \lVert\cdot\rVert_X)$ and $(Y, \lVert\cdot\rVert_Y)$ be normed spaces. A function $f:X\to Y$ is *Fréchet differentiable* at $x\in X$ if there exists a bounded linear operator $T\in\mathcal{B}(X, Y)$ such that

$$
  \lim_{\lVert h \rVert_X \to 0} \frac{\lVert f(x + h) - f(x) - Th \rVert_Y}{\lVert h \rVert_X} = 0
$$

The operator $T$ is called the *Fréchet derivative* of $F$ at $x$, denoted $T = \d f(x)$. The Fréchet differential $Th = \d f(x) h$ is called the *first order variation* of $f$ at $x$.

If $f$ is $m$ times differentiable at $x$, the $m$-th order Fréchet derivative at $x$ is the $m$-linear map $\d^m f : X^m \to Y$ defined recursively as 

$$
  \d^m f (x) := \d(\d^{m-1} f(x))
$$

The $m$-th order Fréchet differential $\d^m f(x) (h_1,\dots,h_m)$ is called the *m*-th order variation of $f$ at $x$.

The function $f$ is $m$ times *Fréchet differentiable* for any point of $X$ if the function $\d^m f : X^m \to \mathcal{L}^m(X, Y)$ is continuous, where $\mathcal{L}^m(X, Y)$ is the space of $m$-linear functions from $X$ to $Y$. The set of $m$ times Fréchet differentiable functions from $X$ to $Y$ is denoted $C^m (X, Y) = \Set{f\in C^1 (X, Y) | \d f \in C^{m-1} (X, Y)}$.
</MathBox>

<MathBox title='The Fréchet derivative is unique' boxType='proposition'>
Let $X$ and $Y$ be normed spaces. If a function $f:X\to Y$ is Fréchet differentiable at $x\in X$, then the Fréchet derivative $\d f(x): X\to Y$ is unique.

<details>
<summary>Proof</summary>

Assume $T_1, T_2 \in\mathcal{B}(X,Y)$ are Fréchet derivatives of $f:X\to Y$ at $x\in X$ satisfying

$$
  \lim_{h\to 0} \frac{\lVert f(x + h) - f(x) - T_i h \rVert_Y}{\lVert h\rVert_X} = 0,\; i=1,2
$$

For nonzero $v\in X$ and $\tau\in\mathbb{F}$ we have

$$
\begin{align*}
  \frac{\lVert T_1 v - T_2 v \rVert_Y}{\lVert v \rVert_X} =& \frac{|\tau|}{|\tau|}\frac{\lVert T_1 v - T_2 v\rVert_Y}{\lVert v \rVert_X} \\
  =& \frac{\lVert T_1 (\tau v) - T_2 (\tau v) \rVert_Y}{\lVert \tau v \rVert_X} \\
  =& \frac{\lVert (f(x + \tau v) - f(x) - T_2 (\tau v)) - (f(x + \tau v) - f(x) - T_1 (\tau v)) \rVert_Y}{\lVert \tau v\rVert_X} \\
  \overset{\triangle}{\leq} \frac{\lVert f(x + \tau v) - f(x) - T_2 (\tau v) \rVert_Y}{\lVert \tau v \rVert_X} + \frac{\lVert f(x + \tau v) - f(x) - T_1 (\tau v) \rVert_Y}{\lVert \tau v \rVert_X} \xrightarrow{\tau\to 0} \to 0
\end{align*}
$$

This shows that $T_1 v = T_2 v$ for all $v\in X$, implying that $T_1 = T_2$.
</details>
</MathBox>

<MathBox title='Fréchet differentiability criterion' boxType='proposition'>
Let $X$ and $Y$ be normed spaces. Then the following are equivalent
1. A function $f:X\to Y$ is Fréchet differentiable at $x$ with derivative $T \in\mathcal{B}(X,Y)$
2. For every $\epsilon > 0$ there exists $\delta > 0$ with the open ball $B(x, \delta) \subset X$ such that for all $h\in B(x, \delta)$
$$
  \lVert f(x + h) - f(x) - Th \rVert_Y \leq \epsilon \lVert h \rVert_X
$$
</MathBox>

<MathBox title='Fréchet differentiable functions are Lipschitz' boxType='proposition' tag='proposition-23'>
Let $X$ and $Y$ be normed spaces. If $f:X\to Y$ is Fréchet differentiable at $x_0$, then $f$ is locally Lipschitz at $x_0$. That is, there exists $\delta > 0$ and $L > 0$ such that $B(x_0, \delta) \subset X$ and for all $x \in B(x,\delta)$

$$
  \lVert f(x) - f(x_0) \rVert_Y \leq L\lvert x - x_0 \rVert_X
$$

<details>
<summary>Proof</summary>

Since $f:X\to Y$ is Fréchet differentiable at $x_0 \in X$, then for $\epsilon = 1$ there is $\delta > 0$ such that for all $x\in X$ with $\lVert x - x_0 \rVert_X < \delta$ we have

$$
  \frac{\lVert f(x) - f(x_0) - \d f(x_0) (x - x_0) \rVert_Y}{\lVert x - x_0 \rVert_X} < 0
$$

This can be rewritten as

$$
  \lVert f(x) - f(x_0) - \D f(x_0)(x - x_0) \rVert_Y < \lVert x - x_0 \rVert_X
$$

By the triangle inequality, we have

$$
\begin{align*}
  \lVert f(x) - f(x_0) \rVert_Y =& \lVert f(x) - f(x_0) - \d f(x_0) (x - x_0) + \d f(x_0)(x - x_0) \rVert_Y \\
  \leq& \lVert f(x) - f(x_0) - \d f(x_0) (x - x_0) \rVert_Y + \lVert \d f(x_0)(x - x_0) \rVert_Y \\
  \leq& \lVert x - x_0 \rVert_X + \lVert \d f(x_0)(x - x_0) \rVert_Y 
\end{align*}
$$

Since $\d f(x_0)$ is a bounded linear operator, it follows that

$$
  \lVert \d f(x_0)(x - x_0) \rVert_Y \leq \lVert \d f(x_0) \rVert \cdot \lVert \rVert_X
$$

Thus, we obtain

$$
  \lVert f(x) - f(x_0) \rVert_Y \leq (1 + \lVert \d f(x_0)\rVert)\lVert x - x_0 \rVert_X
$$

Taking $L = 1 + \lVert \d f(x_0) \rVert$ shows that $f$ is locally Lipschitz at $x_0$.
</details>
</MathBox>

<MathBox title='Fréchet linearity' boxType='proposition'>
Let $X$ and $Y$ be normed spaces. If $f,g:X\to Y$ are differentiable at $x \in X$, then for any scalars $\alpha,\beta\in\mathbb{F}$, the linear combination $\alpha f + \beta g$ is differentiable at $x$ with derivative

$$
  \d (\alpha f + \beta g)(x) = \alpha \d f(x) + \beta \d g(x)
$$

<details>
<summary>Proof</summary>

By differentiability of $f$ and $g$ at $x$, for any $\epsilon > 0$ there is $\delta > 0$ such that for all $h \in B(x, \delta) \subset X$ we have

$$
\begin{align*}
  \lVert f(x + h) - f(x) - \d f(x)h \rVert_Y \leq& \frac{\epsilon \lVert h \rVert_X}{2(|\alpha| + 1)} \\
  \lVert g(x + h) - g(x) - \d g(x)h \rVert_Y \leq& \frac{\epsilon \lVert h \rVert_X}{2(|\beta| + 1)}
\end{align*}
$$

Thus

$$
\begin{align*}
  & \lVert \alpha f(x + h) + \beta g(x + h) - \alpha f(x) - \beta g(x) - \alpha\d f(x)h - \beta\d g(x) h \rVert_Y \\
  &\leq |\alpha|\cdot\lVert f(x + h) - f(x) - \d f(x)h \rVert_Y + |\beta|\cdot\lVert g(x + h) - g(x) - \d g(x)h \rVert_Y \\
  &\leq \frac{\epsilon|\alpha|\cdot\lVert h \rVert_X}{2(|\alpha| + 1)} + \frac{\epsilon|\beta|\cdot\lVert h \rVert_X}{2(|\beta| + 1)} = \epsilon \lVert h \rVert_X \left(\frac{|\alpha|}{2(|\alpha| + 1)} + \frac{|\beta|}{2(|\beta| + 1)} \right) \\
  <& \epsilon \lVert h \rVert_X
\end{align*}
$$

Since $\d f(x), \d g(x)\in\mathcal{B}(X,Y)$ it follows that $\alpha \d f(x) + \beta g(x) \in \mathcal{B}(X,Y)$. Hence, $\alpha f + \beta g$ is differentiable at $x$ with Fréchet derivative $\alpha \d f(x) + \beta g (x)$.
</details>
</MathBox>

<MathBox title='Fréchet product rule' boxType='proposition'>
Let $X$ be a normed space and $\mathbb{F}$ a field. If $f,g: X\to\mathbb{F}$ are Fréchet differentiable at $x\in X$, then the product $fg$ is differentiable at $x$ with derivative

$$
  \d (fg)(x) = g(x) \d f(x) + f(x) \d g(x)
$$

<details>
<summary>Proof</summary>

By differentiability of $f$ and $g$ at $x$, for each $\epsilon > 0$ there is $\delta_x > 0$ with $B(x,\delta_x)\subset X$, and a constant $L > 0$ (by Proposition $\ref{proposition-23}$) such that for all $0 < \lVert h\rVert < \delta_x$

$$
  |f(x + h) - f(x)| \leq L \lVert h \rVert_X
$$

and

$$
\begin{align*}
  |f(x + h) - f(x) - \d f(x)h| \leq& \frac{\epsilon \lVert h \rVert_X}{3(|g(x)| + 1)} \\
  |g(x + h) - f(x) - \d f(x)h| \leq& \frac{\epsilon \lVert h \rVert_X}{3(|f(x)| + L)}
\end{align*}
$$

For $\epsilon > 0$ choose

$$
  \delta = \min\Set{1, \delta_x, \frac{\epsilon}{3L(\lVert \d g(x) \rVert + 1)}}
$$

When $0 \leq \lVert h \rVert_X < 0$, we get

$$
\begin{align*}
  & |f(x + h)g(x + h) - f(x)g(x) - g(x) \d f(x) h - f(x) \d g(x) h| \\
  =& |f(x + h)g(x + h) - f(x + h)g(x) + f(x + h)g(x) - f(x)g(x) \\
  &+ f(x + h)\d g(x) h - f(x + h) \d g(x) h - g(x) \d f(x) h - f(x) \d g(x) h| \\
  \leq& |f(x + h)|\cdot |g(x + h) - g(x) - \d g(x)h| \\
  &+ |g(x)|\cdot|f(x + h) - f(x) - \d f(x) h| \\
  &+ |f(x + h) - f(x)|\cdot\lVert \d g(x) \rVert\cdot\lVert h \rVert_X \\
  \leq& (|f(x)| + L)\frac{\epsilon\lVert h \rVert_X}{3(|f(x) + L|)} \\
  &+ |g(x)|\frac{\epsilon\lVert h \rVert_X}{3(|g(x)| + 1)} + \delta L \lVert \d g(x) \rVert\cdot\lVert h \rVert_X \\
  <& \epsilon \lVert h \rVert_X
\end{align*}
$$

Where we have used the Lipschitz implication

$$
  |f(x + h) - f(x)| \leq L\lVert h \rVert_X \implies |f(x + h)| \leq |f(x)| + L\lVert h \rVert_X
$$

Hence, $fg$ is differentiable at $x$ with Fréchet derivative $\d (fg)(x) = g(x) \d f(x) + f(x) \d g(x)$.
</details>
</MathBox>

<MathBox title='Fréchet chain rule' boxType='proposition'>
Let $X, Y, Z$ be normed spaces. If $f:X\to Y$ is Fréchet differentiable at $x\in X$ and $g:Y\to Z$ is Fréchet differentiable at $y = f(x) \in Y$, the composition $g\circ f$ is Fréchet differentiable at $x$ with derivative

$$
  \d (g\circ f)(x) = \d g(f(x)) \d f(x)
$$

<details>
<summary>Proof</summary>

Choose $\epsilon > 0$. By differentiability of $f:X\to Y$ at $x\in X$ and of $g:Y\to Z$ at $y = f(x) \in Y$, there is $\delta_1 > 0$ such that $B(x,\delta_1)\subset X$ and for all $\xi \in X$ satisfying $0 \lVert\xi\rVert_X < \delta_1$ we have

$$
  \lVert f(x + \xi) - f(x) - \d f(x)\xi \rVert_Y \leq \frac{\epsilon\lVert\xi\rVert_X}{2(\lVert \d g(y) \rVert + 1)}
$$

By Proposition $\ref{proposition-23}$, $f$ is locally Lipschitz at $x$, i.e. there is $\delta_2 > 0$ and $L > 0$ such that $B(x,\delta_2)\subset X$ and for all $\xi\in X$ satisfying $0 < \lVert\xi\rVert_X < \delta_2$ we have

$$
  \lVert f(x + \xi) - f(x) \rVert_X \leq L\lVert\xi\rVert_X
$$

Set $\delta_x = \min\Set{\delta_1, \delta_2}$. Since $g$ is differentiable at $y$, there is $\delta_y > 0$ such that $B(y, \delta_y) \subset Y$ and for all $0 \leq \lVert\eta\rVert_Y < \delta_y$ we have

$$
  \lVert g(y + \eta) - g(y) -\d g(y)\eta \rVert_Z \leq \frac{\epsilon\lVert\eta\rVert_Y}{2L}
$$

Set $\delta = \min\Set{\delta_x, \delta_y / L}$ such that $L\delta\leq\delta_y$. If we take $\eta(\xi) = f(x + \xi) - f(x) = f(x + \xi) - y$ then $h = g\circ f$ satisfies

$$
  h(x + \xi) - h(x) = g(f(x + \xi)) - g(f(x)) = g(y + \eta(\xi) - g(y))
$$

Thus for all $\xi\in X$ satisfying $\lVert\xi\rVert_X < \delta$ we have

$$
  \lVert \rVert_Y = \lVert f(x + \xi) - f(x) \rVert_Y \leq L\lVert\xi\rVert_X < L\delta \leq \delta_y
$$

so that

$$
\begin{align*}
  &\lVert h(x + \xi) - h(x) - \d g(y) \d f(x)\xi \rVert_Z \\
  =& \lVert g(y + \eta(\xi)) - g(y) - \d g(y)\eta(\xi) + \d g(y) \eta(\xi) - \d g(y) \d f(x)\xi \rVert_Z \\
  \overset{\triangle}{\leq}& \lVert g(y + \eta(\xi)) - g(y) - \d g(y)\eta(\xi) \rVert_Z \\
  &+ \lVert \d g(y)\eta(\xi) - \d g(y) \d f(x)\xi \rVert_Z \\
  \leq& \lVert g(y + \eta(\xi)) - g(y) - \d g(y)\eta(\xi) \rVert_Z \\
  &+ \lVert \d g(y) \rVert \cdot \lVert \eta(\xi) - \d f(x)\xi \rVert_Y \\
  =& \lVert g(y + \eta(\xi)) - g(y) - \d g(y)\eta(\xi) \rVert_Z \\
  &+ \lVert \d g(y) \rVert \cdot \lVert f(x + \xi) - f(x) - \d f(x)\xi \rVert_Y \\
  \leq& \frac{\epsilon\lVert\eta(\xi)\rVert_Y}{2L} + \lVert \d g(y) \rVert \frac{\epsilon\lVert\xi\rVert_X}{2(\lVert\d g(y)\rVert + 1)} \\
  <& \frac{\epsilon L\lVert\xi\rVert_X}{2 L} + \frac{\epsilon\lVert\xi\rVert_X}{2} = \epsilon\lVert\xi\rVert_X
\end{align*}
$$

Since $\d g(x) \in\mathcal{B}(X,Y)$ and $\d g(y) \in\mathcal{B}(Y,Z)$, it follows that $\d g(y) \d f(x) \in \mathcal{B}(X,Z)$. Hence $g\circ f$ is differentiable at $x$ with Fréchet derivative $\d (g\circ f)(x) = \d g(f(x)) \d f(x)$.
</details>
</MathBox>

<MathBox title='Fréchet mean value theorem (univariate)' boxType='theorem'>
Let $X$ be a normed space. If $f\in C^1 (I,X)$ is continuously differentiable on an interval $I\subseteq\R$, then for $s,t\in I$ with $s\leq t$

$$
\begin{gather*}
  \lVert f(t) - f(s) \rVert_X \leq M |t-s| \\
  M := \sup_{\tau\in[s,t]} \lVert \d f(\tau) \rVert
\end{gather*}
$$

<details>
<summary>Proof</summary>

Fix $\tilde{M} > M\in\R$ and consider $d(\tau) := \lVert f(\tau) - f(s)\rVert_X - \tilde{M}(\tau - s)$ for $\tau\in[s,t]$. Suppose $\tau_0$ is the largest $\tau$ for which $d(\tau) \leq 0$ holds. Then tehre must be a sequence $\epsilon_n \downarrow 0$ such that

$$
\begin{align*}
  0 \leq& d(\tau_0 + \epsilon_n) \\
  \leq& \lVert f(\tau_0 + \epsilon_n) - f(\tau_0) \rVert - \tilde{M}\epsilon_n + d(\tau_0) \\
  =& \lVert \d f(\tau_0)\epsilon_n + o(\epsilon_n) \rVert - \tilde{M}\epsilon_n \\
  \leq& (M - \tilde{M} + o(1))\epsilon_n < 0
\end{align*}
$$

Taking $n\to\infty$ contradicts our assumption.
</details>
</MathBox>

<MathBox title='Matrix representation of Fréchet derivatives' boxType='proposition'>
Let $U\subseteq \R^n$ be open. If a vector function $f:U \to\R^m$ by $f = (f_i:U\to\R)_{i=1}^m$, in standard coordinates, is Fréchet differentiable at $\mathbf{x}\in U$ , then the partial derivatives $\d_j f_i \mathbf{x}$ exist for all $j = 1,\dots,n$ and $i=1,\dots,m$. The matrix representation of $\d f(\mathbf{x})$ in standard coordinates is the Jacobian matrix

$$
  J(\mathbf{x}) = \begin{bmatrix} 
    \d_1 f_1 (\mathbf{x}) & \cdots & \d_n f_1 (\mathbf{x}) \\
    \vdots & \ddots & \vdots \\
    \d_1 f_m (\mathbf{x}) & \cdots & \d_n f_m (\mathbf{x})
  \end{bmatrix}
$$

<details>
<summary>Proof</summary>

Let $J_j$ be the $j$-th column of $J(\mathbf{x})$, i.e. $J_j = \d f(\mathbf{x})\mathbf{e}_j$, and let $J_{ij}$ be the $i$-th entry of the $j$-column of $J(\mathbf{x})$. For $\mathbf{h} = r\mathbf{e}_j$ we have

$$
\begin{align*}
  0 =& \lim_{\mathbf{h}\to\mathbf{0}} \frac{\lVert f(\mathbf{x} + \mathbf{h}) - f(\mathbf{x}) - \d f(\mathbf{x}\mathbf{h}) \rVert}{\lVert \mathbf{h} \rVert} \\
  =& \lim_{r\to 0} \frac{\lVert f(\mathbf{x} + r\mathbf{e}_j) - f(\mathbf{x}) - r\d f(\mathbf{x}) \mathbf{e}_j \rVert}{|r|\cdot\lVert\mathbf{e}_j\rVert} \\
  =& \lim_{r\to 0} \frac{\lVert f(x_1,\dots,x_j + r,\dots,x_n) - f(x_1,\dots,x_n) - rJ_j \rVert}{|r|}
\end{align*}
$$

This implies that for each component of $f$

$$
  \lim_{r\to 0} \frac{|f(x_1,\dots,x_j + r,\dots,x_n) - f(x_1,\dots,x_n) - rJ_{ij} |}{|r|} = 0
$$

showing that the partial derivative $\d_j f_i (\mathbf{x}) = J_{ij}$ exists.
</details>
</MathBox>

## Gâteaux derivative

<MathBox title='Gâteaux derivative' boxType='definition'>
Let $X$ and $Y$ be locally convex topological vector spaces. A function $f: X \to Y$ is Gâteaux differentiable at $x\in X$ in the direction $v\in X$ if the following limit exists

$$
  \D f(x; z) = \lim_{t\to 0} \frac{f(x + tv) - f(x)}{t} = \left.\frac{\d}{\d t} f(x + tv)\right|_{t=0}
$$

This limit is called the *Gâteaux differential* of $f$ at $x$ in the direction $v$. If the limit exists for all $v\in X$, then $f$ is Gâteaux differentiable at $x$. 

If the Gâteaux differential $\D f(x; \cdot): X\to Y$ is linear and continuous, i.e. there is a linear map $A\in\mathcal{L}(X,Y)$ with $\D f(x; v) = Av$ for all $v\in X$, then $A$ is called the *Gâteaux derivative* of $F$ at $x$, denoted $\D f(x) = A$.

If $f$ is $m$ times Gâteaux differentiable at $x$, the $m$-th order Gâteaux differential in iterated directions $v_1,\dots,v_m \in X$ is defined recursively as 

$$
  \D^m f(x; v_1,\dots,v_m) = \D(\D^{m-1} f)(x; v_1,\dots,v_{m-1};v_m)
$$ 
</MathBox>

<MathBox title='Fréchet differentiability implies Gâteaux differentiability' boxType='proposition'>
Let $X, Y$ be normed spaces. If $f:X\to Y$ is Fréchet differentiable at $x\in X$, then $f$ is Gâteaux differentiable at $x$. In this case $\d f(x) = \D f(x)$, where $\d f(x)$ is the Fréchet derivative of $f$ at $x$ and $\D f(x)$ is the Gâteaux derivative of $f$ at $x$.

<details>
<summary>Proof</summary>

Because $f: X\to Y$ is a Fréchet differentiable at $x\in X$, we have

$$
  f(x + h) = f(x) + \d f(x) h + \mathcal{O}(h)
$$

For any $v\in X$, the Gâteaux differential limit at $x$ is

$$
  \lim_{t\to 0} \frac{f(x + tv) - f(x)}{t}
$$

Inserting the Fréchet differentiability condition with $h = tv$, we get

$$
\begin{align*}
  \lim_{t\to 0} \frac{\d f(x)(tv) + \mathcal{O}(\lVert tv \rVert)}{t} =& \lim_{t\to 0} \frac{t \d f(x)(v) + \mathcal{O}(\lVert tv \rVert)}{t} \\
  =& \d f(x)(v) + \lim_{t\to 0} \frac{\mathcal{O}(|t|\cdot\lVert v \rVert)}{t} \\
  =& \d f(x)(v)
\end{align*}
$$

Hence, 

$$
  \lim_{t\to 0} \frac{f(x + tv) - f(x)}{t} = \d f(x)(v),\; \forall v\in X
$$

showing that the Gâteaux differential of $f$ at $x$ in the direction $v$ equals the Fréchet differential of $f$ at $x$. Since the Fréchet derivative $\d f(x) \in \mathcal{B}(X,Y)$ is a bounded linear operator, it follows that $f$ is Gâteaux differentiable at $x$ with Gâteaux derivative $\D f(x) = \d f(x)$.
</details>
</MathBox>

<MathBox title='Gâteaux mean value theorem' boxType='theorem'>
Let $X$ and $Y$ be normed space and suppose $f:X\to Y$ is Gâteaux differentiable on $X$. If $X$ is convex then for $x, y\in X$

$$
\begin{gather*}
  \lVert f(x) - f(y) \rVert_Y \leq M \lVert x - y \rVert_X \\
  M := \sup{t\in[0,1]} \Set{\D f((1 - t)x + ty), \frac{x - y}{\lVert x - y \rVert_X}}
\end{gather*}
$$

Converserly, if 

$$
  \lVert f(x) - f(y) \rVert_Y \leq M \lVert x - y \rVert_X
$$

then

$$
  \sup_{x\in X, \lVert e\rVert_X} \lVert \D f(x; e) \rVert \leq M
$$

<details>
<summary>Proof</summary>

Define $g(t) = f((1 - t)x + ty)$ with $\D f(t) = \delta g(t) = \delta f((1 - t)x + ty;y - x)$ for $t\in[0,1]$. This implies $\lVert \d g(t) \rVert \leq \tilde{M} := M\lVert x - y \rVert$ by homogeneity of the Gâteaux derivative.  
</details>
</MathBox>


## Integral on normed spaces

<MathBox title='Constant functions have zero derivative' boxType='proposition' tag='proposition-29'>
Let $X$ be a normed space and suppose $f \in C^1 (I, X)$ is a continuously differentiable function on an open interval $I\subseteq \R$. If $\d f(t) \equiv 0$ for all $t\in I$, then $f$ is constant.

<details>
<summary>Proof</summary>

Let $a, b \in I$ with $a < b$ and fix an arbitrary $t\in (a, b)$. By hypothesis, for all $\epsilon > 0$ there is $\delta_t > 0$ such that for all $|h| < \delta_t$ we have

$$
\begin{align*}
  \lVert f(t + h) - f(t) \rVert_X =& \lVert f(t + h) - f(t) - 0 \rVert_X \\
  =& \lVert f(t + h) - f(t) - \d f(t)h \rVert_X \leq \epsilon |h|
\end{align*}
$$

The collection of open intervals $(t - \delta_t, t + \delta_t)$ is an open covering of the compact $[a,b]$ so there is a finite subcovering $(t_i - \delta_{t_i}, t_i + \delta_{t_i})$ of $[a,b]$ for $i = 1,\dots,n$. Without loss of generality we can assume taht $a < t_1 < \cdots < t_n < b$. Choose points $x_0,\dots,x_n \in [a,b]$ so that

$$
  a = x_0 < t_1 < x_1 < \cdots < t_n < x_n = b
$$

with $|x_i - t_i| < \delta_{t_i}$ and $|x_i - t_{i-1}| < \delta_{t_i - 1}$. We then have

$$
\begin{align*}
  \lVert f(b) - f(a) \rVert_X =& \left\lVert \sum_{i=1}^n f(x_i) - f(x_{i-1}) \right\rVert_X \\
  =& \left\lVert \sum_{i=1}^n (f(x_i) - f(t_i)) + (f(t_i) - f(x_{i-1})) \right\rVert_X \\
  \overset{\triangle}{\leq}& \sum_{i=1}^n \lVert f(x_i) - f(t_i) \rVert_X + \sum_{i=1}^n \lVert f(t_i) - f(x_{i-1}) \rVert_X \\
  \leq& \sum_{i=1}^n \epsilon((x_i - t_i) + (t_i - x_{i-1})) \\
  =& \epsilon(b - a)
\end{align*}
$$

Since $\epsilon > 0$ is arbitrary, if follows that $f$ is constant on $(a, b)$. Furthermore, $f$ is constant on $I$ because $a, b \in I$ with $a < b$ are arbitrary.
</details>
</MathBox>

<MathBox title='Fundamental theorem of calculus' boxType='theorem'>
Let $X$ be a normed space.
1. If $f\in C([a,b], X)$ is continuous on $[a,b]$, then $t \mapsto \int_a^t f(s)\;\d s$ is differentiable on $(a,b)$ and for all $t\in(a,b)$
$$
  \frac{\d}{\d t} \int_a^t f(s)\;\d s = f(t)
$$
2. If $f\in C^1((a, b), X)$ is continuously differentiable on $(a,b)$, and $\d f(t)$ extends to a continuous function on $[a,b]$, then
$$
  \int_a^b \d f(s)\;\d s = f(b) - f(a)
$$

<details>
<summary>Proof</summary>

**(1):** Note that $f$ is uniformly continuous, since $f$ is continuous on the compact interval $[a,b]$. Consequently, for every $\epsilon > 0$ there is $\delta > 0$ such that for all $t\in[a,b]$ and all $|h| < \delta$ with $t + h\in[a,b]$

$$
  \lVert f(t + h) - f(t) \rVert_X < \epsilon
$$

From the properties of univariate Banach-valued integral

$$
\begin{align*}
  &\left\lVert \int_a^{t+h} f(x)\;\d s - \int_a^t f(s)\;\d s - f(t)h \right\rVert_X \\
  &= \left\lVert \int_t^{t+h} f(s)\;\d s - f(t)h \right\rVert_X \\
  &= \left\lVert \int_t^{t+h} (f(s) - f(t))\;\d s \right\rVert_X \\
  &\leq \left| \int_t^{t+h} \lVert f(s) - f(t) \rVert_X \;\d s \right| \\
  \leq& \left| \int_t^{t+h} \epsilon\;\d s \right| \\
  &= \epsilon |h|
\end{align*}
$$

This shows that $\int_a^t f(s)\;\d s$ is a differentiable function of $t$ on $(a,b)$, whose derivative is $f(t)$.

**(2):** By hypothesis
- the function $s\mapsto \d f(s)$ from $[a,b]$ to $X$ is continuous
- the function $f:[a,b]\to X$ is continuous

Then the function $G:[a,b]\to X$ defined by

$$
  g(t) = \int_a^t \d f(s)\;\d s - f(t)
$$

is continuous on $[a,b]$ and $g$ is differentiable on $(a,b)$ by part **(1)**. Applying **(1)** we have for $t\in(a,b)$ that

$$
  \d g(t) = \frac{\d}{\d t} \left[\int_a^t \d f(s)\;\d s - f(t) \right] = \d f(t) - \d f(t) = 0
$$

By Proposition $\ref{proposition-29}$, it follows that $g$ is constant on $[a,b]$. Since

$$
\begin{align*}
  g(a) =& \int_a^a \d f(s)\;\d s - f(a) = -f(a) \\
  g(b) =& \int_a^b \d f(s)\;\d s - f(b)
\end{align*}
$$

the equality $g(a) = g(b)$ implies

$$
  \int_a^b \d f(s)\;\d s = f(b) - f(a)
$$
</details>
</MathBox>

<MathBox title='Integral mean value theorem' boxType='theorem'>
Let $X, Y$ be normed spaces. If $f\in C^1 (X,Y)$ is continuous differentiable on $X$, and for $x_0, \x_1$ the line segment $\ell (x_0, x_1) = \Set{(1-t)x_1 + (1 - t)x_0 | t\in [0,1]}$ lies entirely in $X$, then

$$
  f(x_1) - f(x_0) = \int_0^1 \d f (tx + (1 - t)x_0)(x_1 - x_0)\;\d t
$$

Alternatively, with $h = x_1 - x_0$

$$
  f(x_0 + h) - f(x_0) = \int_0^1 \d f(x_0 + th)h\;\d t
$$

Moreover,

$$
  \lVert f(x_1) - f(x_0) \rVert_Y \leq \sup_{c\in\ell(x_0,x_1)} \lVert \d f(c) \rVert\cdot\lVert x_1 - x_0 \rVert
$$
</MathBox>

<MathBox title='Change of variables formula' boxType='theorem'>
Let $X$ be a normed space and $f\in C([a,b], X)$ a continuous function on $[a,b]$. If $g:[c,d]\to[a,b]$ is continuous with $g$ continuously differentiable on $(c, d)$ and $\d g$ continuously extendable to $[c,d]$, then

$$
  \int_c^d f(g(s)) \d g(s)\;\d s = \int_{g(c)}^{g(d)} f(t)\;\d t
$$
</MathBox>
