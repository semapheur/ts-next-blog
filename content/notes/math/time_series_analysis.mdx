---
title: 'Time Series Analysis'
subject: 'Mathematics'
showToc: true
---

# Hurst exponential

Let $X$ be a stochastic process. Then $X$ is self-similar (self-affine) if

$$
  X(t) \simeq \lambda^{-H} X(\lambda x)\quad\forall \lambda < 0
$$

where the Hurst exponent $H\in[0, 1]$ expresses the tendency for $\mathrm{d}X = \frac{\mathrm{d}X(t)}{\mathrm{d}t}\mathrm{d}t$ to change sign
- when $H = \frac{1}{2}$, the sign changes randomly. In this case $X$ is a Brownian motion.
- when $\frac{1}{2}< H\leq 1$, the sign tends not to change. In this case $X$ is called persistent.
- when $0\leq H < \frac{1}{2}$, the sign tends to change (anti-correlation). In this case $X$ is called anti-persistent.

The unconditional moments of $X(t)$, if they exist, behave as power laws of time

$$
  \mathrm{E}\left[ |X(t) | \right]^q = \mathrm{E}\left[ |X(1)|^q \right]|t|^{qH}
$$

The Hurst exponent $H$ of a time series is defined in terms of the asymptotic behaviour of the rescaled range as a function of the time span $n$

$$
  \lim_{n\to\infty} \mathrm{E}\left[ \frac{R(n)}{S(n)} \right] = Cn^H
$$

where
- $R(n)$ is the range of the first $n$ cumulative deviations from the mean
- $S(n)$ is the sum of the first $n$ standard deviations
- $C$ is a constant

## Estimation techniques

### Rescaled range analysis
The rescaled range is calculated as follows
1. Calculate the mean $m = \frac{1}{n}\sum_{i=1}^n X_i$
2. Create a mean-adjusted series $Y_t = X_t - m$ for $t \in [1, n]$
3. Calculate the cumulative deviate series $Z_t = \sum_{i=1}^t Y_i$
4. Compute the range $R(n) = \max[(Z_i)_{i\in[1, n]}] - \min[(Z_i)_{i\in[1, n]}]$
5. Compute the standard deviation $S(n) = \sqrt{\frac{1}{n}\sum_{i=1}^n \left(X_i - m \right)^2}$

The Hurst exponent is estimated by

$$
  H = \frac{\ln\left[ R(n)/S(n) \right]}{\ln(Cn)}
$$

Generalized Hurts exponent for a time series $x(t)$

$$
  \langle \left| g(t + \tau) - g(t) \right|^q \rangle_t \sim \tau^{qH(q)}
$$

### Moment-based estimators

Another method for estimating the global Hurst exponent utilizes absolute moments of a Gaussian random variable. Assuming that we have $N$ observations of an MBM with equal spacing in time, the $k$-th moment in window of size $\epsilon_N \leq N$ is given by

$$
  M_k (t) = \frac{1}{\epsilon_N} \sum_{i=0}^{\epsilon_N - 1} \left| X\left(t - \frac{i}{N} \right) - X\left(t - \frac{i + 1}{N} \right) \right|^k
$$

which is an estimate for $E\left[ \left| X(t) - X\left(t - \frac{1}{N} \right) \right|^k \right]$ given by

$$
  E\left[ \left| X(t) - X\left(t - \frac{1}{N} \right) \right|^k \right] = \frac{2^{k/2}\Gamma\left( \frac{k + 1}{2} \right)}{\Gamma\left( \frac{1}{2} \right)}\sigma^k N^{-kH(t)}
$$

This leads to the following estimate of $H(t)$

$$
  \hat{H}(t) = -\frac{\ln\left[ \sqrt{\pi}M_k (t) \right] - \ln\left[ 2^{k/2} \Gamma\left( \frac{k + 1}{2} \right)\sigma^k \right]}{k \ln(N)}
$$

which converges towards $H(t)$ at the rate $\mathcal{O}\left[ \left( \sqrt{\epsilon_N} \ln(N) \right)^{-1}\right]$. For $k = 1$, the estimator becomes

$$
  \hat{H}(t) = -\frac{\ln\left( \sqrt{\frac{\pi}{2}} \frac{M_1 (t)}{\sigma} \right)}{\ln(N)}
$$

To get rid of the parameter $\sigma$, the ratio of two moments can be calculated for observations in a common window at different resolutions. For $k = 2$, the average quadratic variation $M_2 (t)$ can be compared with a statistic $M'_2 (t)$ with halved resolution

$$
  M'_2 (t) = \frac{2}{\epsilon_N} \sum_{i=0}^{\epsilon_N / 2 - 1} \left| X\left( t - \frac{2i}{N} \right) - X\left( t - \frac{2(i + 1)}{N} \right) \right|^2
$$

The ratio $M'_2(t)/M_2(t)$ converges towards $2^{2H(t)}$, resulting in the estimator

$$
  \hat{H}(t) = \frac{1}{2}\log_2 \left( \frac{M'_2 (t)}{M_2 (t)} \right)
$$

Estimators can be derived from more general filters

$$
  V^a_{t, i} (X) = \frac{j+1}{d} a_j X\left( t - \frac{i + j - 1}{N} \right)
$$

The moment-based approach is retrieved for $a = (1, -1)$.

### Averaged wavelet coefficient method

Applying the wavelet transform to a self-similar stochastic process $X(t)$ gives

$$
\begin{align*}
  \mathcal{W}\left[ X(t) \right](a, b) &\simeq \mathcal{W}\left[ \lambda^{-H} X(\lambda t) \right](a, b) \\
  &= \frac{1}{\sqrt{a}}\int_{-\infty}^\infty \lambda^{-H} X(\lambda t) \overline{\psi\left( \frac{t - b}{a} \right)}\mathrm{d}t \\
  &= \lambda^{-\frac{1}{2}-H} \frac{1}{\sqrt{\lambda a}} \int_{-\infty}^\infty X(\lambda t') \overline{\psi\left( \frac{t' - \lambda b}{\lambda a} \right)}\mathrm{d}t' \\
  &= \lambda^{-\frac{1}{2}-H} \mathcal{W}\left[ X(t) \right] (\lambda a, \lambda b)
\end{align*}
$$

This shows that if we rescale the wavelet domain with the factor $\lambda$, the wavelet coefficient of the original domain rescales by a factor $\lambda^{\frac{1}{2}+H}$. The average wavelet coefficient at a given scale $a$ can be found by averaging out the translation parameter $b$

$$
  W[X(t)](a) := \langle | \mathcal{W}[X(t)](a, b) | \rangle_b
$$

where $\langle\cdot\rangle_b$ is the arithmic mean operator with respect to the translation parameter $b$. The rescaling relation becomes

$$
  W[X(t)](\lambda a) \simeq \lambda^{\frac{1}{2}+H}W[h](a)
$$

### Variational smoothing

The variational smoothing $\mathcal{H}$ of a noisy parameter $\hat{H}$ aims at minimizing the quadratic distance between $\mathcal{H}$ and $\hat{H}$ 

$$
  \mathcal{H} = \underset{h \in C^2([0, 1], \R)}{\mathrm{argmin}} \int_0^1 \left[ \left( h(t) - \hat{H}(t) \right)^2 + \lambda h'(t)^2 \right]\mathrm{d}t = \underset{h \in C^2([0, 1], \R)}{\mathrm{argmin}} \int_0^1 \mathcal{L} \left[ t, h(t), h'(t)\right]\mathrm{d}t
$$

where $\lambda \geq 0$ is a smoothness factor. The Euler-Lagrange equation becomes

$$
  \frac{\partial}{\partial h}  \mathcal{L} \left[ t, h(t), h'(t)\right] = \frac{\mathrm{d}}{\mathrm{d}}\frac{\partial}{\partial h'} \mathcal{L} \left[ t, h(t), h'(t)\right]
$$

For $\lambda > 0$, we get the second order linear differential equation

$$
  \mathcal{H}''(t) - \frac{\mathcal{H}(t)}{\lambda} = -\frac{\hat{H}(t)}{\lambda}
$$

with the general solution

$$
  \mathcal{H}(t) = e^{t/\sqrt{\lambda}} \left( A - \frac{1}{2\sqrt{\lambda}} \int_0^t e^{-s/\sqrt{\lambda}} \hat{H}(s) \mathrm{d}s \right) + e^{-t/\sqrt{\lambda}} \left( B + \frac{1}{2\sqrt{\lambda}} \int_0^t e^{s/\sqrt{\lambda}} \hat{H}(s) \mathrm{d}s \right)
$$

The unique solution that minimizes $\int_0^1 \left[ \mathcal{H}(t) - \hat{H}(t) \right]^2 \mathrm{d}t$ is 

$$
  \mathcal{H}(t) = \Phi \hat{H}(t) + Ae^{\frac{t}{\sqrt{\lambda}}} +  Be^{-\frac{t}{\sqrt{\lambda}}}
$$

where the operator $\Phi$ is given by

$$
  \Phi h(t) = \frac{e^{t/\sqrt{\lambda}}}{2\sqrt{\lambda}} \int_0^t e^{-s/\sqrt{\lambda}} h(s) \mathrm{d}s + \frac{e^{-t/\sqrt{\lambda}}}{2\sqrt{\lambda}} \int_0^t e^{s/\sqrt{\lambda}}h(s) \mathrm{d}s
$$

and where 

$$
\begin{align*}
  \mathcal{G}(t) &= \hat{H} - \Phi\hat{H}(t) \\
  C &= \sqrt{\frac{2}{\sqrt{\lambda}\left( e^{2/\sqrt{\lambda} - 1} \right)}} \\
  D &= C\left( e^{-2/\sqrt{\lambda}} - C^4 \right)^{-1/2} \\
  A &= C^2 \left( 1 + C^2 D^2 \right) \int_0^1 \mathcal{G}(s) e^{s/\sqrt{\lambda}}\mathrm{d}s - C^2 D^2 \int_0^1 \mathcal{G}(s) e^{-s/\sqrt{\lambda}}\mathrm{d}s \\
  B &= -C^2 D^2 \int_0^1 \mathcal{G}(s) e^{s/\sqrt{\lambda}}\mathrm{d}s + D^2 \int_0^1 \mathcal{G}(s) e^{-s/\sqrt{\lambda}}\mathrm{d}s
\end{align*}
$$

When $\lambda$ is close to zero, the exponentials in the closed form of $\mathcal{H}$ blow up, giving inaccuracies in the discrete approximation of the integrals for the coefficients $A$ and $B$. In a discrete framework the coefficients can be calculted using least-squares regression

$$
\begin{align*}
  A &= \frac{1}{\left( \sum_{i=1}^n e^{2t_i /\sqrt{\lambda}} \right)\left( \sum_{i=1}^n e^{-2t_i /\sqrt{\lambda}} \right) - n^2} \left[ \left( \sum_{i=1}^n e^{-2t_i /\sqrt{\lambda}} \right) \left( \sum_{i=1}^n e^{t_i /\sqrt{\lambda}} \left| 1 - \Phi^d \right| \hat{H}(t_i) \right) - n \left( \sum_{i=1}^n e^{-t_i /\sqrt{\lambda}} \left| 1 - \Phi^d \right| \hat{H}(t) \right) \right] \\
  B &= \frac{1}{\left( \sum_{i=1}^n e^{2t_i /\sqrt{\lambda}} \right)\left( \sum_{i=1}^n e^{-2t_i /\sqrt{\lambda}} \right) - n^2} \left[ \left( \sum_{i=1}^n e^{2t_i /\sqrt{\lambda}} \right) \left( \sum_{i=1}^n e^{-t_i /\sqrt{\lambda}} \left| 1 - \Phi^d \right| \hat{H}(t_i) \right) - n \left( \sum_{i=1}^n e^{t_i /\sqrt{\lambda}} \left| 1 - \Phi^d \right| \hat{H}(t) \right) \right]
\end{align*}
$$

In the limit $\lambda \to 0$, there exists $\xi \in [0, 1]$ such that

$$
  \min_{(A, B) \in \R^2} \int_0^1 \left[ \Phi H(t) + Ae^{t/\sqrt{\lambda}} + Be^{-t/\sqrt{\lambda}} - H(t) \right]^2 \mathrm{d}t \leq 4\lambda^2 H''(\xi)^2 + \mathcal{O}(\lambda^2)
$$