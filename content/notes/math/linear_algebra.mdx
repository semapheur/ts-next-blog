---
title: 'Linear Algebra'
subject: 'Mathematics'
showToc: true
---


# Matrices

The set of $m\times n$ matrices with entries in a field $\mathbb{F}$ is denoted $\mathcal{M}_{m,n}(\mathbb{F})$. For $n\times n$ matrices this set is denoted $\mathcal{M}_{n,n}(\mathbb{F})$. The $(i, j)$-th entry of an $m\times n$ matrix $A\in\mathcal{M}_{m,n}(\mathbb{F})$ is denoted $[A]_{i,j} = a_{i,j}$.

The *main diagonal* of an $m\times n$ matrix $A$ is sequence of entries $(a_{i,i})_{i=1}^{\min\Set{m,n}}$. An $m\times n$ matrix $A$ is called diagonal if its off-diagonal entries are zero, and denoted $A = \mathrm{diag}(a_{i,i})_{i=1}^n$. A square matrix is *upper triangular* if all of its entries below the main diagonal are $0$. Similarly, a square matrix is *lower triangular* if all of its entries above the main diagonal are $0$. 

The transpose of $A\in\mathcal{M}_{m,n}$ is the matrix $A^T$ defined by $[A^T]_{i,j} = [A]_{j,i}$. A matrix is symmetric if $A = A^T$ and skew-symmetric if $A^T = -A$. All $n\times n$ diagonal matrices are symmetric.

The $n\times n$ identity matrix is defined as $I_n := \mathrm{diag}(1)_{i=1}^n$ with $[I_n]_{i,j} = \delta_{i,j}$ where $\delta_{i,j}$ is the Kronecker delta

$$
  \delta_{i,j} = \begin{cases} 1,\quad& i=j \\ 0,\quad& i\neq j \end{cases}
$$

<MathBox title='Properties of the transpose' boxType='proposition'>
The transpose operation has the following properties for $A,B\in\mathcal{M}_{m,n}$ and $\lambda\in\mathbb{F}$
1. $(A^T)^T = A$ **(injection)**
2. $(A + B)^T = A^T + B^T$
3. $(\lambda A)^T = \lambda A^T$
4. $(AB)^T = B^T A^T$
5. $\det(A^T) = \det(A)$
</MathBox>

## Matrix multiplication

If $A$ is an $m\times n$ matrix and $B$ and $p\times q$ matrix, the matrix product $AB$ is defined if $n = p$, resulting in an $m \times q$ matrix given by the dot product of the corresponding row of $A$ and the corresponding column of $B$

$$
  [AB]_{i,j} = \sum_{r=1}^n a_{i,r} b_{r,j}
$$

The matrix product $BA$ is only defined if $m = q$ resulting in an $p \times n$ matrix. If both products are defined, they generally need not be equal, meaning that matrix multiplication is not commutative.

Matrix multiplication has the following properties for matrices $A, B, C$ with appropriate sizes
- $cA = Ac$ for a scalar $c$ **(scalar commutativity)**
- $c(AB) = (cA)B$ **(left scalar associativity)**
- $(AB)c = A(Bc)$ **(right scalar associativity)**
- $(AB)C = A(BC)$ **(associativity)**
- $C(A + B) = CA + CB$ **(left distributivity)**
- $(A + B)C = AC + BC$ **(left distributivity)**

## Partioning and matrix multiplication

Let $M$ be an $m\times n$ matrix. If $B\subseteq \Set{1,\dots,m }$ and $C\subseteq\Set{1,\dots,n}$ then the submatrix $M[B,C]$ is the matrix obtained from $M$ by keeping only the rows index in $B$ and the columns with index in $C$ such that $M[B,C]$ has size $|B|\cdot|C|$.

Suppose that $M\in\mathcal{M}_{m,n}$ and $N\in\mathcal{M}_{n,k}$. For
1. $\mathcal{P} = \Set{B_1,\dots, B_p}$ be a partition of $\Set{1,\dots,m}$
2. $\mathcal{Q} = \Set{C_1,\dots, C_q}$ be a partition of $\Set{1,\dots,n}$
3. $\mathcal{R} = \Set{D_1,\dots, D_r}$ be a partition of $\Set{1,\dots,k}$

we have

$$
  [MN][B_i, D_i] = \sum_{C_h \in\mathcal{Q}} M[B_i, C_h] N[C_h, D_j]
$$

When the partions contain only single-element block, this reduces to the usual formula for matrix multiplication

$$
  [MN]_{i,j} = \sum_{h=1}^m M_{i,h} N_{h,j}
$$

## Block matrices

If $B_{i,j}$ are submatrices of $M\in\mathcal{M}_{m,n}$ with appropriate sizes, then $M$ can be written as the block matrix

$$
  M = \begin{bmatrix} 
    B_{1,1} & \cdots & B_{1,n} \\
    \vdots & \ddots & \vdots \\
    B_{m,1} & \cdots & B_{m,n}
  \end{bmatrix}
$$

A square matrix of the form

$$
  M = \begin{bmatrix} 
    B_1 & 0 &\cdots & 0 \\
    0 & \ddots & \ddots & \vdots \\
    \vdots & \ddots & \ddots & 0 \\
    0 & \cdots & 0 & B_n
  \end{bmatrix}
$$

where each $B_i$ is square and $0$ is a zero submatrix, is called a *block diagonal matrix*.

## Elementary row operations

There are three elementary row operations on matrices
- a row within the matrix can be switched with another row (row switching)
- each element in a row can be multiplied by a non-zero constant (row scaling)
- a row can be replaced by the sum of that row and a multiple of another row (row addition)

<MathBox title='Reduced row echelon form' boxType='definition'>
A matrix $R$ is said to be in *reduced row echelon* form if
1. All zero rows appear at the bottom of the matrix.
2. In any nonzero row, the first nonzero entry is $1$, called a leading entry.
3. For any two consecutive rows, the leading entry of the lower is to the right of the leading entry of the upper row.
4. Any column that contains a leading entry has zeroes in all other positions.
</MathBox>

<MathBox title='Row equivalence' boxType='proposition'>
Two matrices $A, B\in\mathcal{M}_{m,n}$ are row equivalent, denoted $A\sim B$, if eiter one can be obtained from the other by a series of elementary row operations, i.e. there is an invertible matrix $P$ such that $A = PB$.

A matrix $A$ is row equivalent to one and only one matrix $R$ in reduced row echelon form, i.e. $A = E_1\cdots E_k R$ where $E_i$ are elementary matrices required to reduce $A$ to reduced row echelon form.

A matrix $A$ is invertible if and only if its reduced row echelon form is an identity matrix. Hence a matrix is invertible if and only if it is the product of elementary matrices.
</MathBox>

An $m\times n$ matrix $R$ that is in both reduced row echelon form and reduced column echelon form must have the block form

$$
  J_k = \begin{bmatrix} 
    I_k & 0_{k,n-k} \\
    0_{m-k, k} & 0_{m-k, n-k}
  \end{bmatrix}
$$

<MathBox title='Similar matrices' boxType='definition'>
Two matrices $A, B\in\mathcal{M}_{m,n}$ are *similar* if there exists an invertible matrix $P$ such that 

$$
  A = PBP^{-1}
$$
</MathBox>

<MathBox title='Congruent matrices' boxType='definition'>
Two matrices $A, B\in\mathcal{M}_{m,n}$ are *congruent* if there exists an invertible matrix $P$ such that 

$$
  A = PBP^T
$$
</MathBox>

### Row-switching transformations

The corresponding elementary matrix for row switching is obtained by swapping row $i$ and row $j$ of the identity matrix.
$$
  T_{i,j} = \begin{bmatrix}
    1 & & & & & & \\
    & \ddots & & & & & \\
    & & 0 & & 1 & & \\
    & & & \ddots & & & \\
    & & 1 & & 0 & & \\
    & & & & & \ddots & \\
    & & & & & & 1
  \end{bmatrix}
$$

### Row scaling transformation

The corresponding elementary matrix for scaling the $i$th row by $m \neq 0$ is a diagonal matrix with diagonal entries $1$ except in the $i$th position

$$
  D_i (m) =   T_{i,j} = \begin{bmatrix}
    1 & & & & & & \\
    & \ddots & & & & & \\
    & & 1 & & & & \\
    & & & m & & & \\
    & & & & 1 & & \\
    & & & & & \ddots & \\
    & & & & & & 1
  \end{bmatrix}
$$

The entries of $D_i(m)$ are given by

$$
  [D_i(m)]_{k,l} = \begin{cases} 
    0 \quad& k\neq l \\
    1 \quad& k = l, k\neq i \\
    m \quad& k = l, k = i
  \end{cases}
$$

### Row addition transformation

The corresponding elementary matrix for adding the $j$th row multiplied by a scalar $m$ to the $i$th row is the identity matrix with an $m$ in the $(i,j)$th entry

$$
  L_{i,j} (m) =   T_{i,j} = \begin{bmatrix}
    1 & & & & & & \\
    & \ddots & & & & & \\
    & & 1 & & & & \\
    & & & \ddots & & & \\
    & & m & & 1 & & \\
    & & & & & \ddots & \\
    & & & & & & 1
  \end{bmatrix}
$$

The entries of $L_{i,j}(m)$ are given by

$$
  [L_{i,j}(m)]_{k,l} = \begin{cases} 
    0 \quad& k\neq l, k\neq i, l\neq j \\
    1 \quad& k = l \\
    m \quad& k = i, l = j
  \end{cases}
$$

## Determinants

<MathBox title='Determinant' boxType='proposition'>
The determinant is a function $\mathrm{det}:\mathcal{M}_{n}(\mathbb{F}) \to\mathbb{F}$ defined by

$$
  \mathrm{det}(A) = \sum_{\pi\in S_n} \left( \mathrm{sgn}(\sigma) \prod_{i=1}^n A_{i,\sigma(i)} \right)
$$

Where $\sigma$ is a permutation bijection and $S_n$ is the set of all permutations of $\Set{1,\dots,n}$. The signature of a permutation $\sigma$ is given by

$$
  \mathrm{sgn}(\sigma) = \begin{cases} 
    +1,\quad& \sigma \text{ is even} \\
    -1,\quad& \sigma \text{ is odd}
  \end{cases}
$$
</MathBox>

<MathBox title='Properties of the determinant' boxType='proposition'>
The determinant has the following properties for $A,B\in\mathbb{M}_{n}(\mathbb{F})$
1. $\mathrm{det}(AB) = \mathrm{det}(A)\mathrm{det}(B)$
2. $A$ is invertible (nonsingular) if and only if $\mathrm{det}(A) \neq 0$
3. if $A$ is an upper/lower triangular matrix, then $\mathrm{det}(A) = \prod_{i=1}^n A_{i,j}$
4. if a square matrix $M$ has the block diagonal form

$$
  M = \begin{bmatrix} 
    B_1 & 0 &\cdots & 0 \\
    0 & \ddots & \ddots & \vdots \\
    \vdots & \ddots & \ddots & 0 \\
    0 & \cdots & 0 & B_n
  \end{bmatrix}
$$

then $\mathrm{det}(M) = \prod_{i=1}^n B_i$.
</MathBox>

## Trace

<MathBox title='Trace' boxType='definition'>
The trace of an $n\times n$-matrix $\mathbf{A}$, denoted $\mathbf{tr}(\mathbf{A})$ is the sum of the elements on the main diagonal of $\mathbf{A}$

$$
  \mathrm{tr}(\mathbf{A}) = \sum_{i_1}^n a_{i,i}
$$
</MathBox>

<MathBox title='Properties of trace' boxType='proposition'>
1. $\mathrm{tr}(\lambda\mathbf{A}) = \lambda\mathrm{tr}(\mathbf{A})$
2. $\mathrm{tr}(\mathbf{A} + \mathbf{B}) = \mathrm{tr}(\mathbf{A}) + \mathrm{tr}(\mathbf{B})$
3. $\mathrm{tr}(\mathbf{A}\mathbf{B}) = \mathrm{tr}(\mathbf{B}\mathbf{A})$
4. $\mathrm{tr}(\mathbf{A}\mathbf{B}\mathbf{C}) = \mathrm{tr}(\mathbf{C}\mathbf{A}\mathbf{B}) = \mathrm{tr}(\mathbf{B}\mathbf{C}\mathbf{A})$
</MathBox>

# Vector space

<MathBox title='Vector space' boxType='definition'>
A vector space is a set $V$ over a field $\mathbb{F}$ equipped with the two closed operations 
- $+: V \times V \to V$ **(vector addition)**
- $\cdot: \mathbb{F} \times V \to V$ **(scalar multiplication)**

and has the following properties

- $V$ is an abelian group under vector addition, i.e. for all $\mathbf{u}, \mathbf{v}, \mathbf{w} \in V$
  - $\mathbf{u} + (\mathbf{v} + \mathbf{w}) = (\mathbf{u} + \mathbf{v}) + \mathbf{w}$ **(associativity)**
  - $\exists \mathbf{0} \in V : \mathbf{v} + \mathbf{0} = \mathbf{v}$ **(identity element)**
  - $\forall \mathbf{v} \; \exists -\mathbf{v} : \mathbf{v} + (-\mathbf{v}) = 0$ **(additive inverse)**
  - $\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$ **(commutativity)**
- Scalar multiplication is compatible, satisfying for all $\alpha, \beta\in \mathbb{F}$
  - $\alpha(\beta \mathbf{v}) = (\alpha\beta)\mathbf{v}$
  - $1\mathbf{v} = \mathbf{v}$
- Vector addition and scalar multiplication are related by distributivity
  - $\alpha (\mathbf{u} + \mathbf{v}) = \alpha \mathbf{u} + \alpha \mathbf{v}$
  - $(\alpha + \beta)\mathbf{v} = \alpha \mathbf{v} + \beta \mathbf{v}$

The elements of $V$ are called vectors, while the elements of $\mathbb{F}$ are called scalars.
</MathBox>

<MathBox title='Linear combination' boxType='definition'>
A linear combination of vectors $\Set{ \mathbf{v}_i }_{i=1}^k \subseteq V$ for $k\in\N_+$ is a vector of the form

$$
  \sum_{i=1}^k \lambda_i \mathbf{v}_i,\; \lambda_i \in\mathbb{F}
$$

If at least one of the scalars $\lambda_i$ is nonzero, the linear combination is nontrivial.
</MathBox>

<MathBox title='Linear subspace' boxType='definition'>
A linear subspace of an $\mathbb{F}$-vector space $V$ is a set $U\subseteq V$ that is itself an $\mathbb{F}$-vector. This means that $U$ must satisfy
- $\mathbf{0}\in U$
- $\mathbf{u}, \mathbf{v}\in U \implies \mathbf{u} + \mathbf{v} \in U$ (closed under vector addition)
- $\mathbf{u}\in U, \alpha\in\mathbb{F} \implies \alpha \mathbf{u} \in U$ (closed under scalar multiplication)

The conditions imply that $U$ is closed under linear combinations, i.e. $\alpha \mathbf{u} + \beta \mathbf{v} \in U$ for all $\alpha,\beta\in\R$ and all $\mathbf{u}, \mathbf{v} \in U$.
</MathBox>

## Direct sums and products

### External direct sums and products
<MathBox title='External direct sum' boxType='definition'>
Let $V_1,\dots,V_n$ be $\mathbb{F}$-vector spaces. The external direct sum of $V_1,\dots,V_n$ denoted

$$
  V = V_1 \boxplus \cdots \boxplus V_n
$$

is the vector space $V$ whose elements are ordered $n$-tuples

$$
  V = \Set{ (\mathbf{v}_i)_{i=1}^n | \mathbf{v}_i \in V_i }
$$

and whose vector operations apply componentwise

$$
\begin{gather*}
  (\mathbf{u}_i)_{i=1}^n + (\mathbf{v}_i)_{i=1}^n = (\mathbf{u}_i + \mathbf{v}_i)_{i=1}^n,\; \mathbf{u}_i, \mathbf{v}_i\in V_i \\
  \lambda (\mathbf{u}_i)_{i=1}^n = (\lambda \mathbf{u}_i)_{i=1}^n,\; \lambda\in\mathbb{F}
\end{gather*}
$$
</MathBox>

<MathBox title='Direct product' boxType='definition'>
Let $\mathcal{F} = \Set{ V_i }_{i\in I}$ be a collection of $\mathbb{F}$-vector spaces for an index set $I$. The direct product of $\mathcal{F}$ is the vector space

$$
  \prod_{i\in I} V_i = \Set{ f: I\to\bigcup_{i\in I} V_i | f(i) \in V_i }
$$

which is subspace of the vector space of all functions from $I$ to $\bigcup_{i\in I} V_i$.
</MathBox>

<MathBox title='Generalized external direct sum' boxType='definition'>
Let $\mathcal{V} = \Set{ V_i }_{i\in I}$ be a collection of $\mathbb{F}$-vector spaces for an index set $I$. The external direct sum of $\mathcal{F}$ is the vector space

$$
  \bigoplus_{i\in I} V = \Set{ f: I\to\bigcup_{i\in I} V_i | f(i) \in V_i, f \text{ has finite support} }
$$

which is subspace of the vector space of all functions from $I$ to $\bigcup_{i\in I} V_i$. The support of $f:I\to\bigcup_{i\in I} V_i$ is the set

$$
  \mathrm{supp}(f) = \Set{i\in I | f(i) \neq 0}
$$

Thus, $f$ has finite support if $f(i) = 0$ for all but a finite number of $i\in I$.
</MathBox>

An important case occurs when $V_i = V$ for all $i\in I$. If $V^I$ denotes the set of all functions from $I$ to $V$ and $(V^I)_0$ denote the set of all functions in $I$ that have finite support then

$$
\begin{align*}
  \prod_{i\in I} V =& V^I \\ 
  \bigoplus_{i\in I} V =& (V^I)_0
\end{align*}
$$

### Internal direct sums

<MathBox title='Internal direct sum' boxType='definition'>
An $\mathbb{F}$-vector space $V$ is the (internal) direct sum of a collection $\mathcal{U} = \Set{ U_i }_{i\in I}$ of subspaces of $V$ if every vector $\mathbf{v}\in V$ can be uniquely written (except for order) as a finite sum of vectors from the subspaces in $\mathcal{F}$. That is, if for all $\mathbf{v}\in V$

$$
  v = \sum_{i=1}^{n\in I} \mathbf{u}_i = \sum_{i=1}^{m\in I} \mathbf{w}_j,\; \mathbf{u}_i, \mathbf{w}_i \in U_i
$$

then $m = n$ and $\mathbf{u}_i = \mathbf{w}_i$ (after reindexing if necessary) for all $i=1,\dots,n$.

If $V$ is the direct sum of $\mathcal{F}$ we write

$$
  V = \bigoplus_{i\in I} U_i
$$

where each $U_i$ is a direct summand of $V$. If $V = U \oplus W$, then $W$ is called a complement of $U$ in $V$.
</MathBox>

<MathBox title='All subspaces have a complement' boxType='proposition'>
Any subspace $U$ of a vector space $V$ has a complement $W$ for which $V = U \oplus W$.
</MathBox>

<MathBox title='' boxType='proposition'>
Let $I\subseteq\N_+$ be an index set. A vector space $V$ is the direct sum of a collection of subspaces $\mathcal{U} = \Set{ U_i }_{i\in I}$ if and only if
1. $V = \sum_{i\in I} U_i$
2. $U_i \cap \left(\sum_{j\neq i} U_i \right) = \Set{\mathbf{0}}$

<details>
<summary>Proof</summary>

Suppose first that $V$ is the direct sum of $\mathcal{U}$. Then $(1)$ clearly holds and if

$$
  \mathbf{v} \in U_i \cap \left(\sum_{j\neq i} U_i \right)
$$

then $\mathbf{v} = \mathbf{u}_i$ for some $\mathbf{u}_i \in U_i$ and

$$
  \mathbf{v} = \sum_{k=1}^n \mathbf{u}_{j_k},\, \mathbf{u}_{j_k}\in U_{j_k}
$$

where $j_k \neq 1$ for all $k=1,\dots,n$. Hence, by the uniqueness of direct sum representations, $\mathbf{u}_i = \mathbf{0}$ and $\mathbf{v} = \mathbf{0}$.

Conversely, suppose that $(1)$ and $(2)$ hold. We need only verify the uniqueness condition. If

$$
  \mathbf{v} = \sum_{i=1}^m \mathbf{u}_{j_i}\ = \sum_{i=1}^n \mathbf{w}_{k_i},\; \mathbf{u}_{j_i}\in U_{j_i}, \mathbf{w}_{k_i}\in U_{k_i}
$$

then by including additional zero terms we may assume that the index sets $\Set{j_i}_{i=1}^m$ and $\Set{k_i}_{i=1}^n$ are the same set $\Set{ i_j }_{j=1}^p$, giving

$$
  \sum_{j=1}^p (\mathbf{u}_{i_j} - \mathbf{w}_{i_j}) = 0
$$

Thus each term $\mathbf{u}_{i_j} - \mathbf{t}_{i_j}\in S_{i_j}$ is a sum of vectors from subspaces other than $S_{i_j}$, which can happen only if $\mathbf{u}_{i_j} - \mathbf{t}_{i_j} = 0$. Hence $\mathbf{u}_{i_j} = \mathbf{t}_{i_j}$ for all $i_j$ and $V$ is a direct sum of $\mathcal{U}$.
</details>
</MathBox>

## Span

<MathBox title='Linear dependence and independence' boxType='definition'>
A set of vectors $S =\Set{ v_i }_{i=1}^k$ for $k\in\N_+$ of an $\mathbb{F}$-vector space $V$ is linearly dependent if there is a non-trivial linear combination for $0\in V$. That is, there is a sequence of scalars $(\lambda_i \in\mathbb{F})_{i=1}^k$ that are not all equal to zero such that

$$
\begin{gather*}
  \sum_{i=1}^k \lambda_i \mathbf{v}_i = \mathbf{0} \\
  \iff \mathbf{v}_j = \sum_{i=1, i\neq j}^k \tilde{\lambda}_i \mathbf{v}_i,\; 1 \leq j \leq k, \lambda_j \neq 0, \tilde{\lambda}_i = \frac{-\lambda_i}{\lambda_j}
\end{gather*}  
$$

Equivalently, $S$ is linearly dependent if and only if one of its vectors is a linear combination of the others. The set $S$ is linearly independent if it is not linearly dependent, i.e. 

$$
  \sum_{i=1}^k \lambda_i \mathbf{v}_i = \mathbf{0} \implies \lambda_i = 0
$$
</MathBox>

<MathBox title='Span' boxType='definition'>
Given a subset $U$ of a $\mathbb{F}$-vector space $V$, the subspace spanned by $U$ is the the smallest set of all linear combinations of vectors in $U$. If $U$ is finite, i.e. for $k\in\N_+$ we can write $U = \Set{ \mathbf{u}_i }_{i=1}^k$, the span of $U$ is the set

$$
  \mathrm{span}(U) := \Set{ \mathbf{v}\in V | \exists (\lambda_i)_{i=1}^k \in\mathbb{F}^k : \mathbf{v} = \sum_{i=1}^k \lambda_i \mathbf{u}_i }
$$

In particular $\mathrm{span}(\emptyset) := \Set{\mathbf{0}}$. 

The subset $U$ is said to span (generate) $V$ if every $\mathbf{v}\in V$ is a linear combination of vectors in $U$, in which case we write $\mathrm{span}(U) = V$.
</MathBox>

## Basis

<MathBox title='Basis' boxType='definition'>
A basis $B$ of an $\mathbb{F}$-vector space $V$ is a linearly independent subset of $V$ spanning $V$. 
</MathBox>

<MathBox title='' boxType='proposition'>
A finite subset $U = \Set{\mathbf{u}_i}_{i=1}^n$ of an $\mathbb{F}$-vector space $V$ for $n\in\N$ is a basis for $V$ if and only if

$$
  V = \bigoplus_{i=1}^n \mathrm{span}(\Set{\mathbf{u}_i})
$$
</MathBox>

<MathBox title='Basis properties' boxType='proposition'>
If $B$ is a subset of an $\mathbb{F}$-vector space $V$, the following are equivalent
1. $B$ is a basis of $V$ 
2. $S$ is a minimal spanning set, i.e. $B$ spans $V$ while no proper subsets of $B$ does
3. $S$ is a maximal linearly independent set, i.e. $S$ is linearly independent while no proper supersets of $S$ is linearly dependent.

<details>
<summary>Proof</summary>

**$(1)\iff(2)$**
Suppose that $B$ is a basis of $V$, i.e. $B$ is linearly independent with $\mathrm{span}(B) = V$. If $\mathrm{\tilde{B}} = V$ for some $\tilde{B}\subset B$, then any vector in $B\setminus \tilde{B}$ should be a linear combination of the vectors in $\tilde{B}$, contradicting the fact that the vectors in $B$ are linearly independent. Hence $B$ must be a minimal spanning set.

Conversely, if $B$ is a minimal spanning set, then it must be linearly independent. If not some $b\in B$ would be a linear combination of the other vectors in $B$ and so $B\setminus\Set{b}$ would be a proper spanning subset of $B$, which is a contradiction. Hence $B$ must be a basis. 

**$(1)\iff(3)$**
Suppose that $B$ is a bsis of $V$. If $B$ is not maximal, there should be a vector $\mathbf{v}\in V\setminus{B}$ for which the set $B\cup\Set{\mathbf{v}}$ is linearly independent. However, then $\mathbf{v}\notin \mathrm{span}(B)$, contradicting the fact that $B$ is a spanning set. Hence, $B$ is a maximal linearly independent set.

Conversely, if $B$ is a maximal linearly independent set then $\mathrm{span}(B) = V$. If not, we could find a vector $\mathbf{v}\in V\setminus{B}$ that is not a linear combination of the vectors in $B$. In this case, $B\cup\Set{\mathbf{v}}$ would be a linearly independent proper superset of $B$, which is a contradiction. Hence, $B$ must be a basis.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V$ be a nonzero vector space. Let $I$ be a linearly independent set in $V$ and let $S$ be a spanning set in $V$ containing $I$. Then there is a basis $B$ for $V$ for which $I\subseteq B\subseteq S$. In particular
1. any vector space, except $\Set{\mathbf{0}}$ has a basis
2. any linearly independent set in $V$ is contained in a basis
4. any spanning set in $V$ contains a basis

<details>
<summary>Proof</summary>

Consider the collection $\mathcal{A}$ of all linearly independent subsets of $V$ containing $I$ and contained in $S$. Clearly, $\mathcal{A}$ is not empty since $I\in\mathcal{A}$. If $\mathcal{C} = \Set{ I_j}_{j\in J}$ for some index set $J$ is a chain in $\mathcal{A}$ then the union $U = \bigcup_{j\in J} I_j$ is linearly independent and satisfies $I\subseteq U\subseteq S$, i.e. $U\in\mathcal{A}$. Thus, every chain in $\mathcal{A}$ has an upper bound in $\mathcal{A}$ and by Zorn's lemma, $\mathcal{A}$ must contain a maximal element $B$, which is linearly independent.

The set $B$ is a basis for the vector space $\mathrm{span}(S) = V$, for if any $s\in S$ is not a linear combination of the elements of $B$, then $B\cup\Set{s} \subseteq S$ is linearly independent, contradicting the maximality of $B$. Hence $S\subseteq\mathrm{span}(B)$ and so $V = \mathrm{span}(S) \subseteq\mathrm{span}(B)$. 
</details>
</MathBox>

## Dimension

<MathBox title='' boxType='proposition'>
Let $V$ be a vector space and assume that the vectors $\Set{\mathbf{v}_i}_{i=1}^n$ for $n\in\N_+$ are linearly independent and the vectors $\Set{\mathbf{s}_i}_{i=1}^m$ for $m\in\N_+$ span $V$. Then $n \leq m$. 

<details>
<summary>Proof</summary>

List the two set of vectors with the spanning set followed by the linearly independent set

$$
  \mathbf{s}_1,\dots,\mathbf{s}_m;\mathbf{v}_1,\dots,\mathbf{v}_n
$$

Move the first vector $\mathbf{v}_1$ to the front of the list

$$
  \mathbf{v}_1, \mathbf{s}_1,\dots,\mathbf{s}_m;\mathbf{v}_2,\dots,\mathbf{v}_n
$$

Since $\mathrm{span}\Set{ \mathbf{s}_i }_{i=1}^m = V$, it follows that $\mathbf{v}_1$ is a linear combination of the $\mathbf{s}_i$'s. This implies that we may remove one the $\mathbf{s}_i$'s, which by reindexing if necessary can be $\mathbf{s}_1$, from the list and still have a spanning set

$$
  \mathbf{v}_1, \mathbf{s}_2,\dots,\mathbf{s}_m;\mathbf{v}_2,\dots,\mathbf{v}_n
$$

Note that the first set of vectors still spans $V$ and the second set is still linearly independent. Repeat the process, moving $\mathbf{v}_2$ from the second list to the first list

$$
  \mathbf{v}_1, \mathbf{v}_2, \mathbf{s}_2,\dots,\mathbf{s}_m;\mathbf{v}_3,\dots,\mathbf{v}_n
$$

As before, the vectors in the first list are linearly independent, since they spanned $V$ before the inclusion of $\mathbf{v}_2$. However, since the $\mathbf{v}_i$'s are linearly independent, any nontrivial linear combination of the vectors in the first list that equals 0 must involve at least one of the $\mathbf{s}_i$'s. Thus, we may remove that vector, which by reindexing if necessary can be $\mathbf{s}_2$, and still have a spanning set

$$
  \mathbf{v}_1, \mathbf{v}_2, \mathbf{s}_3,\dots,\mathbf{s}_m;\mathbf{v}_3,\dots,\mathbf{v}_n
$$

If $m < n$, this process will eventually exhaust the $\mathbf{s}_i$'s and lead to the list

$$
  \mathbf{v}_1,\dots,\mathbf{v}_m;\mathbf{v}_{m+1},\dots,\mathbf{v}_n
$$

where $\mathrm{span}\Set{\mathbf{v}_i}_{i=1}^m = V$, which is contradictory since any $\mathbf{v}_i$ for $i > m$ is not in the span of $\Set{\mathbf{v}_i}_{i=1}^m$. Hence, $n\leq m$. 
</details>
</MathBox>

<MathBox title='All bases have same cardinality' boxType='theorem'>
All bases of an $\mathbb{F}$-vector space $V$ have the same cardinality, called the dimension of $V$, denoted $\dim(V)$.

<details>
<summary>Proof</summary>

For an index set $I\in\N$, let $B = \Set{\mathbf{b}_i}_{i\in I}$ be a basis for $V$ and suppose that $C$ is another basis for $V$. Then any vector $\mathbf{c}\in C$ can be written as finite linear combination of the vectors in $B$

$$
  \mathbf{c} = \sum_{i\in U_C} \lambda_i \mathbf{b}_i,\; \lambda_i\in\mathbb{F}\setminus\Set{0}
$$

Because $C$ is basis, we must have $\bigcup_{\mathbf{c}\in C} U_{\mathbf{c}} = I$. If the vectors in $C$ can be expressed as finite linear combinations of the vectors in a proper subset $B' \subset B$ then $\mathrm{span}(B') = V$, which is contradictory.

Since $U_{\mathbf{c}}$ is finite, i.e. $|U_{\mathbf{c}}| < \aleph_0$, for all $\mathbf{c}\in C$, it follows that 

$$
  |B| = |I| \leq \aleph_0 |C| = |C|
$$

Reversing the roles of $B$ and $C$, we may also conclude that $|C| \leq |B|$ and so $|B| = |C|$ by the SchrÃ¶der-Bernstein theorem.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
A vector space $V$ is *finite-dimensional* if it is the zero space $\Set{\mathbf{0}}$ or if it has a finite basis. Otherwise, $V$ is infinite-dimensional. If $V$ has a basis of cardinality $n\in\N$ we say that $V$ is $n$-dimensional and write $\dim(V) = n$. In particular $\dim(\Set{\mathbf{0}}) = 0$.
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V$ be a vector space, then
1. if $B$ is a basis for $V$ and if $B = B_1 \cup B_2$ with $B_1 \cap B_2 = \emptyset$ then

$$
  V = \mathrm{span}(B_1) \oplus\mathrm{span}(B_1)
$$
2. let $V = S\oplus T$. If $B_1$ is a basis for $S$ and $B_2$ is a basis for $T$ then $B_1 \cap B_2 = \emptyset$ and $B = B_1 \cup B_2$ is a basis for $V$. 
</MathBox>

<MathBox title='' boxType='proposition'>
Let $S$ and $T$ be subspaces of a vector space $V$. Then

$$
  \dim(S) + \dim(T) = \dim(S + T) + \dim(S \cap T)
$$

In particular, if $T$ is any complement of $S$ in $V$, i.e. $S\oplus T = V$, then

$$
  \dim(S\oplus T) + \dim(S) + \dim(T) = dim(V)
$$

<details>
<summary>Proof</summary>

Suppose that $B = \Set{b_i}_{i\in I}$ is a basis for $S\cap T$. Extend this to a basis $A\cup B$ for $S$ where $A = \Set{a_j}_{j\in J}$ is disjoint from $B$. Also, extend $B$ to a basis $B\cup C$ for $T$ where $C = \Set{c_k}_{k\in K}$ is disjoint from $B$. We claim that $A\cup B\cup C$ is a basis for $S+T$. It is clear that $\mathrm{span}(A\cup B\cup C) = S + T$.

To see that $A\cup B\cup C$ is linearly independent, suppose the opposite that

$$
  \sum_{i=1}^n \alpha_i \mathbf{v}_i = 0,\;\alpha_i \in\mathbb{F}\setminus\Set{0}, \mathbf{v}_i\in A\cup\B\cup C
$$

There must be vectors $\mathbf{v}_i$ in this expression from $A$ and $C$ since $A\cup B$ and $B\cup C$ are linearly independent. Isolating the terms involving the vectors from $A$ on one side of the equality shows that there is a nonzero vector $\mathbf{x}\in\mathrm{span}(A) \cap \mathrm{span}(B\cup C)$. However, then $\mathbf{x}\in S\cap T$ and so $\mathbf{x}\in \mathrm{span}(A) \cap \mathrm{span}(B)$, which implies that $\mathbf{x} = \mathbf{0}$, a contradiction. Hence $A\cup B\cup C$ is linearly independent and a basis for $S + T$, giving

$$
\begin{align*}
  \dim(S) + \dim(T) =& |A\cup B| + |B\cup A| \\
  =& |A| + |B| + |B| + |C| \\
  =& |A| + |B| + |C| + \dim(S\cap T) \\
  =& \dim(S + T) + \dim(S\cap T)
\end{align*}
$$
</details>
</MathBox>

## Coordinates

<MathBox title='Ordered basis' boxType='definition'>
An ordered basis for an $n$-dimensional vector space $V$ is an ordered $n$-tuple $(\mathbf{v}_i)_{i=1}^n$ of vectors for which the set $\Set{\mathbf{v}_i}_{i=1}^n$ is a basis for $V$.
</MathBox>

<MathBox title='Coordinate map' boxType='definition'>
If $B = \Set{\mathbf{b}_i}_{i=1}^n$ is an ordered basis for a vector space $V$ over $\mathbb{F}$ then for each $\mathbf{v}\in V$ there is a unique ordered $n$-tuple $(\lambda_i)_{i=1}^n \in \mathbb{F}^n$ such that $\mathbf{v} = \sum_{i=1}^n \lambda_i \mathbf{b}_i$.

Accordingly we can define the *coordinate map* $\varphi_B: V\to\mathbb{F}^n$ by

$$
  \varphi_B (\mathbf{v}) = [\mathbf{v}]_B := \begin{bmatrix} \lambda_i \\ \vdots \\ \lambda_n \end{bmatrix}
$$

where the column matrix $[\mathbf{v}]_B$ is known as the *coordinate matrix* of $\mathbf{v}$ with respect to the ordered basis $B$. The coordinate is an isomorphism (bijection) with inverse $\varphi_B^{-1}:\mathbb{F}^n \to V$ defined by

$$
  (\lambda_i)_{i=1}^n \mapsto \sum_{i=1}^n \lambda_i \mathbf{b}_i
$$
</MathBox>

## Row and column spaces of matrices

Let $\mathbf{A}\in\mathcal{M}_{m,n}(\mathbb{F})$ be and $m\times n$ matrix. The rows of $\mathbf{A}$ span a subspace of $\mathbb{F}^n$ called the *row space* of $\mathbf{A}$, dented $\mathrm{rs}(\mathbf{A})$, and the columns of $\mathbf{A}$ span a subspace of $\mathbb{F}^m$ called the *column space* of $\mathbf{A}$, denoted $\mathrm{cs}(\mathbf{A})$. The dimensions of these spaces are called the *row rank* and *column rank*, denoted $\mathrm{rrk}(\mathbf{A})$ and $\mathrm{crk}(\mathbf{A})$, respectively.

<MathBox title='' boxType='proposition'>
Let $\mathbf{A}$ be an $m\times n$ matrix. Then elementary column operations do not affect the row rank of $\mathbf{A}$. Similarly, elementary row operations do not affect the column rank of $\mathbf{A}$.

<details>
<summary>Proof</summary>

The rowspace of $\mathbf{A}$ is 

$$
  \mathrm{rs}(\mathbf{A}) = \mathrm{span}(\mathbf{e}_i \mathbf{A})_{i=1}^n
$$

where $\mathbf{e}_i$ are the standard basis vectors in $\mathbb{F}$. Perferming an elementary column operation on $\mathbf{A}$ is equivalent to multiplying $\mathbf{A}$ on the right by an elementary matrix $\mathbf{E}$. Thus, the row space of $\mathbf{A}\mathbf{E}$ is

$$
  \mathrm{rs}(\mathbf{A}) = \mathrm{span}(\mathbf{e}_i \mathbf{E}\mathbf{A})_{i=1}^n
$$

and since $\mathbf{E}$ is invertible

$$
  \mathrm{rrk}(\mathbf{A}) = \dim(\mathrm{rs}(\mathbf{A})) = \dim(\mathrm{rs}(\mathbf{A}\mathbf{E})) = \mathrm{rrk}(\mathbf{A}\mathbf{E})
$$

The second statement follows from the first by taking transposes. 
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
If $\mathbf{A}\in\mathcal{M}_{m,n}$, then $\mathrm{rrk}(\mathbf{A}) = \mathrm{crk}(\mathbf{A})$. This number is called the rank of $\mathbf{A}$ and is denoted by $\mathrm{rank}(\mathbf{A})$.

<details>
<summary>Proof</summary>
 
According to the previous result, $\mathbf{A}$ can be transformed into a reduced column echelon form without affecting the row rank. This reduction does not affect the column either. The matrix $\mathbf{A}$ can be further transformed into a reduced row echelon form without affecting either rank. The resultiing matrix $\mathbf{M}$ has the same row and column ranks ars $\mathbf{A}$. However, $\mathbf{M}$ is a matrix with $1$'s followed by $0$' on the main diagonals entries and $0$'s elsewhere. Hence

$$
  \mathrm{rrk}(\mathbf{A}) = \mathrm{rrk}(\mathbf{M}) = \mathrm{crk}(\mathbf{M}) = \mathrm{crk}(\mathbf{A})
$$
</details>
</MathBox>

# Linear transformations

<MathBox title='Linear transformation' boxType='definition'>
Let $V$ and $W$ be vector spaces over the same field $\mathbb{F}$. A function $\mathrm{T}: V \to W$ is called a *linear transformation* if it preserve vector space operations, i.e.

$$
  \mathrm{T}(\alpha \mathbf{v} + \beta \mathbf{w}) = \alpha \mathrm{T}(\mathbf{v}) + \beta \mathrm{T}(\mathbf{w})
$$

for all $\alpha, \beta\in\mathbb{F}$ and $\mathbf{v}, \mathbf{w}\in V$. A linear transformation $\mathrm{T}: V\to V$ is called a *linear operator* on $V$. The set of all linear transformations from $V$ to $W$ is denoted $\mathcal{L}(V, W)$ and the set of all linear operators on $V$ is denoted $\mathcal{L}(V)$. 

The following terms are used to classify linear transformations and operators
- **homomorphism:** linear transformation
- **endomorphism:** linear operator
- **monomorphism (embedding):** injective linear transformation
- **epimorphism**: surjective linear transformation
- **isomorphism**: bijective linear transformation
- **automorphism**: bijective linear operator
</MathBox>

<MathBox title='' boxType='proposition'>
1. The set $\mathcal{L}(V, W)$ is a vector space under ordinary addition of funtions and scalar multiplication of functions by elements of $\mathcal{F}$.
2. If $\mathrm{T}\in\mathcal{L}(U, V)$ and $\mathrm{S}\in\mathcal{L}(V, W)$ then the composition $\mathrm{S}\circ \mathrm{T}: V\to W$
3. If $\mathrm{T}\in\mathcal{L}(V, W)$ is bijective, then $\mathrm{T}^{-1}\in\mathcal{L}(W, V)$
4. The vector space $\mathcal{L}$ is an algebra, where multiplication is composition of functions. The identity map $\mathrm{id}\in\mathcal{L}(V)$ is the multiplicative identity, and the zero map $0\in\mathcal{L}(V)$ is the additive identity.

<details>
<summary>Proof</summary>

Let $\mathrm{T}:V\to W$ be a bijective linear transformation. Then $\mathrm{T}^{-1}: W\to V$ is well-defined and since any two vectors $\mathbf{w}_1, \mathbf{w}_2\in W$ have the form $\mathbf{w}_1 = \mathrm{T}(\mathbf{v}_1)$ and $\mathbf{w}_2 = \mathrm{T}(\mathbf{v}_2)$ we have for $\alpha, \beta\in\mathbb{F}$

$$
\begin{align*}
  \mathrm{T}^{-1}(\alpha \mathbf{w}_1 + \beta \mathbf{w}_2) =& \mathrm{T}^{-1}[\alpha \mathrm{T}(\mathbf{v}_1) + \beta \mathrm{T}(\mathbf{v}_2)] \\
  =& \mathrm{T}^{-1}[\mathrm{T}(\alpha \mathbf{v}_1 + \beta \mathbf{v}_2)] \\
  =& \alpha \mathbf{v}_1 + \beta \mathbf{v}_2 \\
  =& \alpha \mathrm{T}^{-1}(\mathbf{w}_1) + \beta \mathrm{T}^{-1}(\mathbf{w}_2)
\end{align*}
$$
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V$ and $W$ be vector spaces and let $B = \Set{v_i}_{i\in I}$ be a basis for $V$. Then we can define a linear transformation $\mathrm{T}\in\mathcal{L}(V,W)$ by specfiying the values of $\mathrm{T}(\mathbf{v}_i)\in W$ arbitrarily for all $v_i\in B$ and extending the domain of $\mathrm{T}$ to $V$ using linearity, i.e.

$$
  \mathrm{T}(\lambda_i \mathbf{v}_i)_{i\in I} = \sum_{i\in I} \lambda_i \mathrm{T}(\mathbf{v}_i)
$$

This process uniquely defines a linear transformation. If $\mathrm{T},\mathrm{S}\in\mathcal{L}(V,W)$ satisfy $\mathrm{T}(\mathbf{v}_i) = \mathrm{S}(\mathbf{v}_i)$ for all $\mathbf{v}_i\in B$ then $\mathrm{T} = \mathrm{S}$.

<details>
<summary>Proof</summary>

</details>
</MathBox>

<MathBox title='Kernel and range' boxType='definition'>
A linear transformation $\mathrm{T}\in\mathcal{L}(V, W)$ has the following two principal subspaces:
- the *kernel* (null space) of $\mathrm{T}$ is the set $\mathrm{ker}(\mathrm{T}):= \Set{\mathbf{v}\in V | \mathrm{T}(\mathbf{v}) = \mathbf{0}}$
- the *range* (image) of $\mathrm{T}$ is the set $\mathrm{ran}(\mathrm{T}):= \Set{\mathrm{T}(\mathbf{v}) | \mathbf{v}\in V}$

The dimension of $\mathrm{ker}(\mathrm{T})$ is called the *nullity* of $\mathrm{T}$ and is denoted $\mathrm{null}(\mathrm{T})$. The dimension of $\mathrm{ran}(\mathrm{T})$ is called the *rank* of $\mathrm{T}$ is denoted $\mathrm{rank}(\mathrm{T})$.
</MathBox>

<MathBox title='' boxType='proposition'>
Let $\mathrm{T}\in\mathcal{L}(V, W)$, then
1. $\mathrm{T}$ is surjective if and only if $\mathrm{ran}(\mathrm{T}) = W$
2. $\mathrm{T}$ is injective if and only if $\mathrm{ker}(\mathrm{T}) = \Set{\mathbf{0}}$

<details>
<summary>Proof</summary>

The first statement is merely a restatement of the definition of surjectivity. To show the second statement, note that

$$
  \mathrm{T}(u) = \mathrm{T}(v) \iff \mathrm{T}(u - u) = 0 \iff u - v \in\mathrm{ker}(\mathrm{T})
$$

Thus, if $\mathrm{ker}(\mathrm{T}) = \Set{0}$ then $\mathrm{T}(u) = \mathrm{T}(v) \iff u = v$, showing that $f$ is injective. Conversely, if $\mathrm{T}$ is injective and $u\in\mathrm{ker}(\mathrm{T})$, then $\mathrm{T}(u) = \mathrm{T}(0) \iff u = 0$. Hnece $\mathrm{ker}(\mathrm{T}) = \Set{\mathbf{0}}$.
</details>
</MathBox>

## Isomorphism

<MathBox title='Isomorphism' boxType='definition'>
A bijective linear transformation $\mathrm{T}:V\to W$ is an isomorphism from $V$ to $W$. The vector space $V$ and $W$ are isomorphic, denoted $V \cong W$, if there is an isomorphism from $V$ to $W$.
</MathBox>

For any ordered basis $B$ of an $\mathbb{F}$-vector space $V$ with $\dim(V) = n$, the coordinate map $\varphi_B: V\to\mathbb{F}^n$ is an isomorphism. Hence, any $n$-dimensional vector space over $\mathbb{F}$ is isomorphic to $\mathbb{F}^n$.

An isomorphism can be characterized as a linear transformation $\mathrm{T}:V\to W$ that maps a basis for $V$ to a basis for $W$.

<MathBox title='Properties of isomorphisms' boxType='definition'>
Let $\mathrm{T}\in\mathcal{L}(V, W)$ be an isomorphism. For $S\subseteq V$, then
1. $S$ spans $V$ if and only if $\mathrm{T}(S)$ spans $W$.
2. $S$ is linearly independent in $V$ if and only if $\mathrm{T}(S)$ is linearly independent in $W$.
3. $S$ is a basis for $V$ if and only if $\mathrm{T}(S)$ is a basis for $W$.
</MathBox>

<MathBox title='Isomorphic spaces have same dimension' boxType='definition'>
Let $V$ and $W$ be vector spaces, then $V\cong W$ if and only if $\dim(V) = \dim(W)$.
</MathBox>

<MathBox title='' boxType='definition'>
For $n\in\N$, any $n$-dimensinoal vector space over $\mathbb{F}$ is isomorphic to $\mathbb{F}^n$. If $B$ is a set of cardinality $|B| = \kappa$, then any $\kappa$-dimensional vector space over $\mathbb{F}$ is isomorphic to the vector space $(\mathbb{F}^B)_0$ of all functions from $B$ to $\mathbb{F}$ with finite support.
</MathBox>

## Rank-nullity theorem

<MathBox title='' boxType='proposition'>
Let $\mathrm{T}\in\mathcal{L}(V,W)$. Any complement of $\mathrm{ker}(\mathrm{T})$ is isomorphic to $\mathrm{ran}(\mathrm{T})$.

<details>
<summary>Proof</summary>

Let $\mathrm{T}\in\mathcal{L}(V,W)$. Since any subspace of $V$ has a complement, we can write

$$
  V = \mathrm{ker}(\mathrm{T}) \oplus\mathrm{ker}(\mathrm{T})^c
$$

where $\mathrm{ker}(\mathrm{T})^c$ is the complement of $\mathrm{ker}(\mathrm{T})$. The restriction of $\mathrm{T}$ to $\mathrm{ker}(\mathrm{T})^c$, denoted $\mathrm{T}^c:\mathrm{ker}(\mathrm{T})^c \to W$ is injective since

$$
  \mathrm{ker}(\mathrm{T}^c) = \mathrm{ker}(\mathrm{T}) \cap\mathrm{ker}(\mathrm{T})^c = \Set{\mathbf{0}}
$$

Also, $\mathrm{ran}(\mathrm{T}^c)\subseteq \mathrm{ran}(\mathrm{T})$. For the reverse inclusion, if $\mathrm{T}\in\mathrm{ran}(f)$ then since $\mathbf{v} = \mathbf{u} + \mathbf{w}$ for $\mathbf{u}\in\mathrm{ker}(\mathrm{T})$ and $\mathbf{w}\in\mathrm{ker}(\mathrm{T})^c$, we have

$$
  \mathrm{T}(\mathbf{v}) = \mathrm{T}(\mathbf{u}) + \mathrm{T}(\mathbf{w}) = \mathrm{T}(\mathbf{w}) = \mathrm{T}^c(\mathbf{w}) \in\mathrm{ran}(\mathrm{T}^c)
$$

Thus $\mathrm{ran}(\mathrm{T}^c) = \mathrm{ran}(\mathrm{T})$ and it follows that $\mathrm{ker}(\mathrm{T})^c \cong \mathrm{ran}(\mathrm{T})$.
</details>
</MathBox>

<MathBox title='Rank-nullity theorem' boxType='theorem'>
For any $\mathrm{T}\in\mathcal{L}(V,W)$

$$
\begin{gather*}
  \dim(\mathrm{ker}(\mathrm{T})) + \dim(\mathrm{ran}(\mathrm{T})) = \dim(V) \\
  \mathrm{rank}(\mathrm{T}) + \mathrm{null}(\mathrm{T}) = \dim(V)
\end{gather*}
$$
</MathBox>

<MathBox title='' boxType='corollary'>
Let $\mathrm{T}\in\mathcal{L}(V,W)$, where $\dim(V) = \dim(W) < \infty$. Then $\mathrm{T}$ is injective if and only if it is surjective.
</MathBox>

## Finite-dimensional linear transformations

Any $m\times n$ matrix $\mathbf{A}$ over $\mathbb{F}$ defines a linear transformation $\mathrm{T}_{\mathbf{A}}:\mathbb{F}^n\to\mathbb{F}^m$ in the form of the multiplication map $\mathbf{v}\mapsto \mathbf{A}\mathbf{v}$. 

<MathBox title='' boxType='lemma'>
1. If $\mathbf{A}$ is an $m\times n$ matrix over $\mathbb{F}$, then the multiplication function $\mathrm{T}_\mathbf{A}:\mathbb{F}^n \to\mathbb{F}^m$ defined by $\mathbf{v} \mapsto \mathbf{A}\mathbf{v}$ is a linear map, i.e. $\mathrm{T}_\mathbf{A} \in\mathcal{L}(\mathbb{F}^n,\mathbb{F}^m)$.
2. If $\mathrm{T}\in\mathcal{L}(\mathbb{F}^n,\mathbb{F}^m)$ then $\mathrm{T} = \mathrm{T}_\mathbf{A}$ where for the standard basis $E = \Set{ \mathbf{e}_i }_{i=1}^n$

$$
  \mathbf{A} = \begin{bmatrix} \mathrm{T}(\mathbf{e}_1) & \cdots & \mathrm{T}(\mathbf{e}_n) \end{bmatrix} \in\mathcal{M}_{m,n}(\mathbb{F})
$$

is the matrix of $T$.

<details>
<summary>Proof</summary>

**(1)**: For a matrix $\mathbf{A}\in\mathcal{M}_{m,n}(\mathbb{F})$, vectors $\mathbf{v}, \mathbf{w} \in \mathbb{F}^n$ and scalars $\alpha, \beta\in\mathbb{F}$ the associativity and distributivity properties of matrix multiplication gives

$$
  \mathbf{A}(\alpha\mathbf{v} + \beta\mathbf{w}) = \mathbf{A}(\alpha\mathbf{v}) + \mathbf{A}(\beta\mathbf{v}) = \alpha \mathbf{A}\mathbf{v} + \beta\mathbf{A}\mathbf{w}
$$

showing that $\mathrm{T}_{\mathbf{A}} \in\mathcal{L}(\mathbb{F}^n,\mathbb{F}^m)$.

**(2)**: Let $E = \Set{ \mathbf{e}_i }_{i=1}^n$ be the standard basis of $\mathbb{F}^n$. If a vector $\mathbf{v}\in V$ has coordinates $[\mathbf{v}]_E = \left[(\beta_i)_{i=1}^n\right]^T \in\mathbb{F}^n$ then $\mathbf{v}$ can be written as the linear combination

$$
  \mathbf{v} = \sum_{i=1}^n \beta_i \mathbf{e}_i  
$$

By the linearity of $\mathrm{T}$

$$
\begin{align*}
  \mathrm{T}(\mathbf{v}) =& \mathrm{T} \left(\sum_{i=1}^n \beta_i \mathbf{e}_i \right) = \sum_{i=1}^n \beta_i \mathrm{T}(\mathbf{e}_i) \\
  =& \begin{bmatrix} \mathrm{T}(\mathbf{e}_1) & \cdots & \mathrm{T}(\mathbf{e}_n) \end{bmatrix} [\mathbf{v}]_E \\
  =& \mathbf{A}[\mathbf{v}]_E = \mathrm{T}_\mathbf{A} (\mathbf{v})
\end{align*}
$$

Hence $\mathbf{A} = \begin{bmatrix} \mathrm{T}(\mathbf{e}_1) & \cdots & \mathrm{T}(\mathbf{e}_n) \end{bmatrix}$
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $\mathbf{A}$ be an $m\times n$ matrix over $F$.
1. $\mathrm{T}_\mathbf{A}:\mathbb{F}^n \to\mathbb{F}^m$ is injective if and only if $\mathrm{rank}(\mathbf{A}) = n$.
2. $\mathrm{T}_\mathbf{A}:\mathbb{F}^n \to\mathbb{F}^m$ is surjective if and only if $\mathrm{rank}(\mathbf{A}) = m$.
</MathBox>

### Change of basis matrices

<MathBox title='Change of basis operator' boxType='definition'>
Let $B $ and $C$ be ordered bases for an $n$-dimensional vector space $V$. For any $\mathbf{v}\in V$, the map $\varphi_{B,C} = \varphi_C \varphi_B^{-1}$ given by $[\mathbf{v}]_B \mapsto [\mathbf{v}]_C$ is called the change of basis operator. 
</MathBox>

<MathBox title='' boxType='proposition'>
Let $B = \Set{\mathbf{b}_i}_{i=1}^n$ and $C$ be ordered bases for an $n$-dimensional vector space $V$. The change of basis operator $\varphi_{B,C} = \varphi_C \varphi_B^{-1}$ from $B$ to $C$ is an automorphism of $\mathbb{F}^n$ whose standard matrix is

$$
  \mathbf{M}_{B,C} = \begin{bmatrix} [\mathbf{b}_1]_C & \cdots & [\mathbf{b}_n]_C \end{bmatrix}
$$

Hence $[\mathbf{v}]_C = \mathbf{M}_{B,C}[\mathbf{v}]_B$ and $\mathbf{M}_{C,B} = \mathbf{M}_{B,C}^{-1}$.

<details>
<summary>Proof</summary>

Since $\varphi_{B,C}$ is an operator on $\mathbb{F}^n$ it has the form $\mathrm{T}_\mathbf{M}$ where $\mathbf{M}\in\mathcal{M}_n$

$$
\begin{align*}
  \mathbf{M} =& \begin{bmatrix} \varphi_{B,C}(\mathbf{e}_1) & \cdots & \varphi_{B,C}(\mathbf{e}_n) \end{bmatrix} \\
  =& \begin{bmatrix} \varphi_C \varphi_B^{-1}([\mathbf{b}_1]_B) & \cdots & \varphi_C \varphi_B^{-1}([\mathbf{b}_n]_B) \end{bmatrix} \\
  =& \begin{bmatrix} [\mathbf{b}_1]_C & \cdots & [\mathbf{b}_n]_C \end{bmatrix}
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Functional dependency of bases and transformation matrices' boxType='proposition'>
If given any two of the following
1. an invertible $n\times n$ matrix $\mathbf{A}$
2. an ordered basis $B$ for $\mathbb{F}^n$
3. an ordered basis $C$ for $\mathbb{F}^n$

then the third is uniquely determined by the equation

$$
  \mathbf{A} = \mathbf{M}_{B,C}
$$

<details>
<summary>Proof</summary>

The result is clear if $B$ and $C$ are given or if $\mathbf{A}$ and $C$ are given. If $\mathbf{A}$ and $B$ are given, then there is a unique $C$ for which $\mathbf{A}^{-1} = \mathbf{M}_{C,B}$ and so there is a unique $C$ for which $\mathbf{A} = \mathbf{M}_{B,C}$.
</details>
</MathBox>

### The matrix of a linear transformation

<MathBox title='' boxType='proposition'>
Let $\mathrm{T}:V\to W$ be a linear transformation, where $\dim(V) = n$ and $\dim(W) = m$, and let $B = \Set{\mathbf{b}_i}_{i=1}^n$ be an ordered basis for $V$ and $C$ and ordered basis for $W$. Then $\mathrm{T}$ can be represented with respect to $B$ and $C$ as the matrix product $[\mathrm{T}(\mathbf{v})]_C = [\mathrm{T}]_{B,C}[\mathbf{v}]_B$ where

$$
  [\mathrm{T}]_{B,C} = \begin{bmatrix} [\mathrm{T}(\mathbf{b}_1)]_C & \cdots & [\mathrm{T}(\mathbf{b}_n)]_C \end{bmatrix}
$$

is called the matrix of $\mathrm{T}$ with respect to $B$ and $C$. If $V = W$ and $B = C$ we denote $[\mathrm{T}]_{B,B}$ by $[\mathrm{T}]_{B}$ and so

$$
  [\mathrm{T}(\mathbf{v})]_B = [\mathrm{T}]_B [\mathbf{v}]_B
$$

<details>
<summary>Proof</summary>

Let $\mathrm{T}\in\mathcal{L}(V, W)$ and let $B = \Set{b_i}_{i=1}^n$ and $C$ be ordered bases for $V$ and $W$, respectively. Then the map $\theta:[\mathbf{v}]_B \mapsto [\mathrm{T}(v)]_C$ is a representation of $\mathrm{T}$ as a linear transformation from $\mathbb{F}^n$ to $\mathbb{F}^m$ in the sense that knowing $\theta$ (along with $B$ and $C$) is equivalent to knowing $\mathrm{T}$.

Since $\theta$ is a linear transformation from $\mathbb{F}^n$ to $\mathbb{F}^m$, it is simply multiplication by an $m \times n$ matrix $\mathbf{A}$, i.e. $[\mathrm{T}(v)]_C = \mathbf{A}[\mathbf{v}]_B$. Since $[\mathbf{b}_i]_B = \mathbf{e}_i$ we get the columns of $\mathbf{A}$

$$
  \mathbf{A}[\mathbf{v}]_B = \begin{bmatrix} [\mathrm{T}(\mathbf{b}_i)]_C & \cdots & [\mathrm{T}(\mathbf{b}_n)]_C  \end{bmatrix} [\mathbf{v}]_B
$$
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $\mathrm{T}\in\mathcal{L}(V, W)$, and let $V$ and $W$ be vector spaces over $\mathbb{F}$, with ordered bases $B = \Set{\mathbf{b}_i}_{i=1}^n$ and $C = \Set{\mathbf{c}_i}_{i=1}^m$, respectively.
1. The map $\mathrm{S}:\mathcal{L}(V, W)\to\mathcal{M}_{m,n}(\mathbb{F})$ defined by $\mathrm{S}(\mathrm{T}) = [\mathrm{T}]_{B,C}$ is an isomorphism and so $\mathcal{L}(V,W)\cong\mathcal{M}_{m,n}(\mathbb{F})$.
2. If $\mathrm{R}\in\mathcal{L}(U,V)$ and $\mathrm{T}\in\mathcal{L}(V,W)$ and if $B$, $C$ and $D$ are ordered bases for $U$, $V$ and $W$, respectively, then $[\mathrm{T}\mathrm{R}]_{B,C} = [\mathrm{T}]_{C,D} [\mathrm{R}]_{B,C}$. Thus, the matrix of the product (composition) $\mathrm{T}\mathrm{R}$ is the product of the matrices of $\mathrm{T}$ and $\mathrm{R}$ respectively.

<details>
<summary>Proof</summary>

**(1)**: To see that $S$ is linear, note that for all $i\in\Set{1,\dots,n}$

$$
\begin{align*}
  [\alpha\mathrm{R} + \beta T]_{B,C} [\mathbf{b}_i]_B =& [(\alpha\mathrm{R} + \beta T)(\mathbf{b}_i)]_C \\
  =& [\alpha\mathrm{R}(\mathbf{b}_i) + \beta\mathrm{T}(\mathbf{b}_i)]_C \\
  =& \alpha[\mathrm{R}(\mathbf{b}_i)]_C + \beta[\mathrm{T}(\mathbf{b}_i)]_C \\
  =& \alpha[\mathrm{R}]_{B,C}[\mathbf{b}_i]_B + \beta[\mathrm{T}]_{B,C}[\mathbf{b}_i]_B \\
  =& (\alpha[\mathrm{R}]_{B,C} + \beta[\mathrm{T}]_{B,C})[\mathbf{b}_i]_B
\end{align*}
$$

since $[\mathbf{b}_i]_B = \mathbf{e}_i$ is a standard basis vector, it follows that

$$
  [\alpha\mathrm{R} + \beta \mathrm{T}]_{B,C} = \alpha[\mathrm{R}]_{B,C} + \beta[\mathrm{T}]_{B,C}
$$

showing that $\mathrm{S}$ is linear. If $\mathbf{A}\in\mathcal{M}_{m,n}$, we define $\mathrm{T}$ by the condition $[\mathrm{T}(\mathbf{b}_i)]_C = \mathbf{A}_i$ giving $\mathrm{S}(\mathrm{T}) = \mathbf{A}$ which is surjective. Since $\dim(\mathcal{L}(V,W)) = \dim(\mathcal{M}_{m,n})$, the map $\mathrm{S}$ is an isomorphism.

**(2)**: We have

$$
\begin{align*}
  [\mathrm{T}\mathrm{R}]_{B,D}[\mathbf{v}]_B =& [\mathrm{T}(\mathrm{R}(\mathbf{v}))]_D = [\mathrm{T}]_{C,D}[\mathrm{R}(\mathbf{v})]_{C} \\
  =& [\mathrm{T}]_{C,D}[\mathrm{R}]_{B,C}[\mathbf{v}]_B
\end{align*}
$$
</details>
</MathBox>

### Change of bases for linear transforms

<LatexFig width={50} src='/fig/change_of_basis_linear_transformation.svg' alt=''
  caption='Change of basis isomorphisms'
>
```tex
\documentclass[tikz]{standalone}
\usepackage{amssymb}
\usepackage{tikz}
\usetikzlibrary{cd}

\begin{document}

\begin{tikzcd}[
  row sep=huge, 
  column sep=8em, 
  every label/.append style = {font=\scriptsize}
]
  \mathbb{F}^n \arrow[r, "\psi_{C'} \circ \mathrm{T} \circ \phi_{B'}^{-1}"] & \mathbb{F}^m \\
  V 
    \arrow[u, "\phi_{B'}"] \arrow[d, "\phi_B"]  \arrow[r, "\mathrm{T}"] & 
  W 
    \arrow[u, "\psi_{C'}"] \arrow[d, "\psi_C"] \\
  \mathbb{F}^n \arrow[r, "\psi_C \circ \mathrm{T} \circ \phi_B^{-1}"] & \mathbb{F}^m
\end{tikzcd}

\end{document}
```
</LatexFig>

<MathBox title='Change of bases equivalence' boxType='proposition'>
Let $\mathrm{T}\in\mathcal{L}(V,W)$ and let $(B,C)$ and $(B',C')$ be pairs of ordered bases of $V$ and $W$, respectively, then

$$
  [\mathrm{T}]_{B',C'} = \mathbf{M}_{C,C'}[\mathrm{T}]_{B,C}\mathbf{M}_{B',B}
$$

in which case $[\mathrm{T}]_{B',C'} \sim [\mathrm{T}]_{B,C}$.

If $\mathrm{T}\in\mathcal{L}(V)$ and $B$ and $C$ are ordered bases for $V$, then the matrix of $T$ reduce to

$$
  [\mathrm{T}]_C = \mathbf{M}_{B,C}[\mathrm{T}]_B \mathbf{M}_{B,C}^{-1}
$$

in which case $[\mathrm{T}]_C \sim [\mathrm{T}]_B$.

<details>
<summary>Proof</summary>

Multiplication by $[\mathrm{T}]_{B',C'}$ sends $[\mathbf{v}]_{B'}$ to $[\mathrm{T}(v)]_{C'}$. This can be reproduced by first switching from $B'$ to $B$, then applying $[\mathrm{T}]_{B,C}$, and finally switching from $C$ to $C'$, i.e.

$$
\begin{align*}
  [\mathrm{T}]_{B',C'} =& \mathbf{M}_{C,C'}[\mathrm{T}]_{B,C} \mathbf{M}_{B',B} \\
  =& \mathbf{M}_{C,C'} [\mathrm{T}]_{B,C} \mathbf{M}_{B,B'}^{-1}
\end{align*}
$$
</details>
</MathBox>

### Equivalence of matrices

<MathBox title='Equivalence of matrices' boxType='definition'>
Two matrices $\mathbf{A}$ and $\mathbf{B}$ are *equivalent* if there exist invertible matrices $\mathbf{P}$ and $\mathbf{Q}$ for which

$$
  \mathbf{B} = \mathbf{P}\mathbf{A}\mathbf{Q}^{-1}
$$
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V$ and $W$ be vector spaces with $\dim(V) = n$ and $\dim(W) = m$. Then two $m\times n$ matrices $\mathbf{A}$ and $\mathbf{B}$ are equivalent if and only if they represent the same linear transformation $\mathrm{T}\in\mathcal{L}(V,W)$, possibly with respect to different ordered bases. In this case, $\mathbf{A}$ and $\mathbf{B}$ represent exactly the same set of linear transformation in $\mathcal{L}(V,W)$.

<details>
<summary>Proof</summary>

If $\mathbf{A}$ and $\mathbf{B}$ represent $\mathrm{T}$, i.e. if $\mathbf{A} = [\mathrm{T}]_{B,C}$ and $\mathbf{B} = [\mathrm{T}]_{B',C'}$ for ordered bases $B$, $C$, $B'$ and $C'$, then by the change of basis equivalence, $\mathbf{A}$ and $\mathbf{B}$ are equivalent. 

Conversely, suppose that $\mathbf{A}$ and $\mathbf{B}$ are eqiuvalent, i.e. $\mathbf{B} = \mathbf{P}\mathbf{A}\mathbf{Q}^{-1}$ where $\mathbf{P}$ an $\mathbf{Q}$ are invertible. Suppose that $\mathbf{A}$ represents a linear transformation $\mathrm{T}\in\mathcal{L}(V,W)$ for some ordered basis $B$ and $C$, i.e.

$$
  \mathbf{A} = [\mathrm{T}]_{B,C}
$$

By the functional dependency property, there is a unique ordered basis $B'$ for $V$ for which $\mathbf{Q} = \mathbf{M}_{B,B'}$ and a unique ordered basis $C'$ for $W$ for which $\mathbf{P} = \mathbf{M}_{C,C'}$. Thus

$$
  \mathbf{B} = \mathbf{M}_{C,C'}[\mathrm{T}]_{B,C} \mathbf{M}_{B', B} = [\mathrm{T}]_{B',C'}
$$

showing that $\mathbf{B}$ also represents $\mathrm{T}$. By symmetry, we se that $\mathbf{A}$ and $\mathbf{B}$ represent the same set of linear transformation.
</details>
</MathBox>

### Similarity of matrices

<MathBox title='Similarity of matrices' boxType='proposition'>
Two matrices are *similar* if there exists an invertible matrix $\mathbf{P}$ for which $\mathbf{B} = \mathbf{P}\mathbf{A}\mathbf{P}^{-1}$. The equivalence classes associated with similarity are called similarity classes.
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V$ be a vector spaces with $\dim(V) = n$. Then two $n\times n$ matrices $\mathbf{A}$ and $\mathbf{B}$ are similar if and only if they represent the same linear transformation $\mathrm{T}\in\mathcal{L}(V)$, possibly with respect to different ordered bases. In this case, $\mathbf{A}$ and $\mathbf{B}$ represent exactly the same set of linear transformation in $\mathcal{L}(V)$.

<details>
<summary>Proof</summary>

If $\mathbf{A}$ and $\mathbf{B}$ represent $\mathrm{T}\in\mathcal{L}(V)$, i.e. if $\mathbf{A} = [\mathrm{T}]_B$ and $\mathbf{B} = [\mathrm{T}]_C$ for ordered bases $B$ and $C$, then by the change of bases equivalence $\mathbf{A}$ and $\mathbf{B}$ are similar.

Conversely, suppose that $\mathbf{A}$ and $\mathbf{B}$ are similar, i.e. $\mathbf{B} = \mathbf{P}\mathbf{A}\mathbf{P}^{-1}$. Suppose that $\mathbf{A}$ represents a linear operator $\mathcal{L}(V)$ for some ordered basis $B$, i.e. $\mathbf{A} = [\mathrm{T}]_B$. By the functional dependency relation, there is a unique ordered basis $C$ for $V$ for which $\mathbf{P} = \mathbf{M}_{B,C}$. Thus 

$$
  \mathbf{B} = \mathbf{M}_{B,C}[\mathrm{T}]_{B}\mathbf{M}_{B,C}^{-1} = [\mathrm{T}]_C
$$

Hence, $\mathbf{B}$ also represents $\mathrm{T}$. By symmetry, it follows that $\mathbf{A}$ and $\mathbf{B}$ represent the same set of linear operators.
</details>
</MathBox>

### Similarity of operators

<MathBox title='Similarity of operators' boxType='proposition'>
Two linear operators $\mathrm{T}, \mathrm{S}\in\mathcal{L}(V)$ are *similar* if there exists an automorphism $R\in\mathcal{L}(V)$ for which 

$$
  \mathrm{S} = \mathrm{R}\mathrm{T}\mathrm{R}^{-1}
$$

The equivalence classes associated with similarity are called *similarity classes*.
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V$ be a vector spaces with $\dim(V) = n$. Then two linear operators $\mathrm{T},\mathrm{S}\in\mathcal{L}(V)$ are similar if and only if there is a matrix $\mathbf{A}\in\mathcal{M}_n$ that represents both operators with respect to possibly different ordered bases. In this case, $\mathrm{T}$ and $\mathrm{S}$ are represented by exactly the same set of matrices in $\mathcal{M}_n$.

<details>
<summary>Proof</summary>

If $\mathrm{T}$ and $\mathrm{S}$ are represented by $\mathbf{A}\in\mathcal{M}_{n}$, i.e. if $[\mathrm{T}]_B = \mathbf{A} = [\mathrm{S}]_C$ for ordered bases $B$ and $C$ then

$$
  [\mathrm{S}]_C = [\mathrm{T}]_B = \mathbf{M}_{C,B}[\mathrm{T}]_C \mathbf{M}_{B_C}
$$

Let $R\in\mathcal{L}(V)$ be the automorphism of $V$ defined by $\mathrm{T}(\mathbf{c}_i) = \mathbf{b}_i$ where $B = \Set{\mathbf{b}_i}_{i=n}$ and $C = \Set{\mathbf{c}_i}_{i=1}^n$, then

$$
\begin{align*}
  [\mathrm{R}]_C =& \begin{bmatrix} [\mathrm{R}(\mathbf{c}_1)]_C & \cdots & [\mathrm{R}(\mathbf{c}_n)]_C \end{bmatrix} \\
  =& \begin{bmatrix} [\mathbf{b}_1]_C & \cdots & [\mathbf{b}_n]_C \end{bmatrix} \\
  =& \mathbf{M}_{B,C}
\end{align*}
$$

and so

$$
  [\mathrm{S}]_C = [\mathrm{R}]_C^{-1}[\mathrm{T}]_C [\mathrm{R}]_C = [\mathrm{R}^{-1}\mathrm{T}\mathrm{R}]_C
$$

from which it follows that $\mathrm{T}$ and $\mathrm{S}$ are similar.

Conversely, suppose that $\mathrm{T}$ and $\mathrm{S}$ are similar, i.e. $\mathrm{S} = \mathrm{R}\mathrm{T}\mathrm{R}^{-1}$. Suppose also that $\mathrm{T}$ is represented by the matrix $\mathbf{A}\in\mathcal{M}_n$, i.e. $\mathbf{A} = [\mathrm{T}]_B$ for some ordered basis $B$. Then

$$
  [\mathrm{S}]_B = [\mathrm{R}\mathrm{T}\mathrm{R}^{-1}]_B = [\mathrm{R}]_B [\mathrm{T}]_B [\mathrm{R}]_B^{-1}
$$

Setting $\mathbf{c}_i = h(\mathbf{b}_i)$ then $C = \Set{ \mathbf{c}_i }_{i=1}^n$ is an ordered basis for $V$ and

$$
\begin{align*}
  [\mathrm{R}]_B =& \begin{bmatrix} [\mathrm{R}(\mathbf{b}_1)]_B & \cdots & [\mathrm{R}(\mathbf{b}_n)]_B \end{bmatrix} \\
  =& \begin{bmatrix} [\mathbf{c}_1]_B & \cdots & [\mathbf{c}_n]_C \end{bmatrix} \\
  =& \mathbf{M}_{C,B}
\end{align*}
$$

Thus, $[\mathrm{S}]_B = \mathbf{M}_{C,B}[\mathrm{T}]_B \mathbf{M}_{C,B}^{-1}$ and it follows that

$$
  \mathbf{A} = [\mathrm{T}]_B = \mathbf{M}_{B,C}[\mathrm{S}]_B \mathbf{M}_{B,C}^{-1} = [\mathrm{S}]_C
$$

and so $A$ also represents $S$. By symmetry, it follows that $\mathrm{T}$ and $\mathrm{S}$ are represented by the same set of matrices.
</details>
</MathBox>

## Invariant subspaces and reducing pairs

The restriction of a linear operator $\mathrm{T}\in\mathcal{L}(V)$ to a subspace $S\subseteq V$ is not necessarily a linear operator on $S$.

<MathBox title='Invatiant subspaces' boxType='definition'>
Let $\mathrm{F}\in\mathcal{L}(V)$. A subspace $S\subseteq V$ is *invariant* under $\mathrm{F}$ if $\mathrm{F}(S) \subseteq S$. That is, $S$ is invariant under $\mathrm{F}$ if the restriction $\mathrm{F}|_S$ is a linear operator on $S$.
</MathBox>

If $V = S \oplus T$ then the fact that $S$ is $\mathrm{F}$-invariant does not imply that the complement $T$ is also $\mathrm{F}$-invariant.

<MathBox title='Reducing pair' boxType='definition'>
Let $\mathrm{F}\in\mathcal{L}(V)$. If $V = S\oplus T$ and if both $S$ and $T$ are $\mathrm{F}$-invariant, we say that the pair $(S,T)$ *reduces* $\mathrm{T}$.
</MathBox>

<MathBox title='Direct sum of linear operators' boxType='definition'>
Let $\mathrm{F}\in\mathcal{L}(V)$. If $(S,T)$ reduces $\mathrm{F}$ we write

$$
  \mathrm{F} = \mathrm{F}|_S \oplus \mathrm{F}|_U
$$

and call $\mathrm{F}$ the *direct sum* of $\mathrm{F}|_S$ and $\mathrm{F}|_T$. Thus, the expression $\mathrm{H} = \mathrm{G} \oplus \mathrm{F}$ means that there exists subspaces $S$ and $T$ of $V$ for which $(S,T)$ reduces $H$ with $\mathrm{G} = \mathrm{R}|_S$ and $\mathrm{F} = \mathrm{R}|_T$.
</MathBox>

# Topological vector space

The standard topology on $\R^n$ is the topology induced by the Euclidean metric on $\R^n$ for which the set of open rectangles

$$
  B = \Set{ \prod_{i=1}^n I_i | I_i \text{ is open an open interval in } \R }
$$

is a basis, i.e. a subset of $\R^n$ is open if and only if it is a union of sets in $B$. The standard topology on $\R^n$ has the properties that the any linear functional $f:\R^n \to\R^n$ together with the addition function

$$
  \mathcal{A}:\R^n\times\R^n \ni (v,w) \mapsto v + w \in\R^n
$$

and the scalar multiplication function

$$
  \mathcal{M}:\R\times\R^n \ni (\lambda,\mathbf{v}) \mapsto\lambda \mathbf{v} \in\R^n
$$

are continuous. As such $\R^n$ is a topological vector space.

Generally, any real vector space $V$ endowed with a topology $\mathcal{T}$ is called a *topological vector space* if the operations of addition $\mathcal{A}:V\times V\to V$ and scalar multiplication $\mathcal{M}:\R\times V\to V$ are continuous under $\mathcal{T}$.

<MathBox title='Topological vector space' boxType='proposition'>
Let $V$ be a real vector space with $\dim(V) = n$. There is a unique topology $\mathcal{T}$ on $V$, called the *natural topology* for which $V$ is a topological vector space and for which all linear functionals on $V$ are continuous. This topology is determined by the fact that the coordinate map $\varphi: V\to\R^n$ is a homeomorphism.

<details>
<summary>Proof</summary>

Let $V$ be any real vector space with $\dim(V) = n$ and fix and ordered basis $B = \Set{ \mathbf{v}_i }_{i=1}^n$ for $V$. Consider the coordinate map

$$
  \varphi = \varphi_B: V\ni \mathbf{v} \mapsto [\mathbf{v}]_B \in \R^n
$$

and its inverse

$$
  \psi_B = \varphi_B^{-1}:\R^n \ni (\lambda_i)_{i=1}^n \mapsto sum_{i=1}^n \lambda_i \mathbf{v}_i
$$

We need to show that there is a unique topology $\mathcal{T}$ on $V$ for which $\varphi_B$ (and thus $\psi_B$) is a homeomorphism, i.e. a bijection that is continuous with a continuous inverse.

**$\psi$ is continuous under $\mathcal{T}$**<br/>
First we show that if $V$ is a topological vector space under a topology $\mathcal{T}$ then $\psi$ is continuous. Since $\psi = \sum_{i=1}^n \psi$ where $\psi_i:\R\to V$ is defined by $\psi_i (\lambda_i)_{i=1}^n = \lambda_i \mathbf{v}_i$, it is sufficient to show that these maps continuous, as the sum of continuous maps is continuous.

Let $O$ be an open set in $\mathcal{T}$. Then the inverse scalar multiplication operation

$$
  \mathcal{M}^{-1}(O) = \Set{(\alpha, \mathbf{v})\in\R\times V | \alpha}
$$

is an open set in $\R\times V$. We need to show that the set

$$
  \psi_i^{-1}(O) = \Set{(\lambda_i)_{i=1}^n \in\R^n | \lambda_i \mathbf{v}_i \in O }
$$

is open in $\R^n$. Let $(\lambda_i)_{i=1}^n \in\psi_i^{-1}(O)$ so that $\lambda_i \mathbf{v}_i \in O$. It follows that $(\lambda_i, \mathbf{v}_i)\in\mathcal{M}^{-1}(O)$, which is open. Thus, there is an open interval $I\subseteq\mathbb{R}$ and an open set $B\in\mathcal{T}$ of $V$ for which

$$
  (\lambda_i, \mathbf{v}_i) \in I \times B \subseteq\mathcal{M}^{-1}(O)
$$

Then the open set $U = \R\times\cdot\R\times I \times\R\times\cdots\times\R$ where $I$ is in the $i$th position, has the property that $\psi_i(U)\subseteq O$. Thus

$$
  (\lambda_i)_{i=1}^n \in U \subseteq \psi_i^{-1}(O)
$$

and so $\psi_i^{-1}(O)$ is open. Hence, $\psi_i$, and therefore $\psi$, is continuous.

**$\varphi$ is continuous under $\mathcal{T}$**<br/>
Next we show that if every linear functional on $V$ is continuous under a topology $\mathcal{T}$ on $V$ then the coordinate map $\varphi$ is continuous. For $\mathbf{v}\in V$ let $[\mathbf{v}]_{B,i}$ denote the $i$th coordinate of $[\mathbf{v}]_B$. The map $\mu:V \to\R$ defined by $\mu(\mathbf{v}) = [\mathbf{v}]_{B,i}$ is a linear functional and so is continuous by assumption. Thus, for any open interval $I_i \in\R$ the set

$$
  A_i = \Set{ \mathbf{\mathbf{v}}\in V | [\mathbf{v}]_{B,i} \in I_i }
$$

is open. If $I_i$ are open intervals in $\R$ then

$$
  \varphi^{-1}(\prod_{i=1}^n I_i) = \Set{ \mathbf{\mathbf{v}}\in V | [\mathbf{v}]_B \in \prod_{i=1}^n I_i } = \bigcap_{i=1}^n A_i
$$

is open. Hence $\varphi$ is continuous.

**$\mathcal{T}$ is unique topology on $V$**
If a topology $\mathcal{T}$ has the property that $V$ is a topological vector space and every linear functional is continuous, then $\varphi$ and $\psi = \varphi^{-1}$ are homeomorphisms. This means that $\mathcal{T}$, if it exists, must be unique.

It remains to prove that the topology $\mathcal{T}$ on $V$ that makes $\varphi$ a homeomorphism has the property that $V$ is a topological space under $\mathcal{T}$ and that any linear functional $f$ on $V$ is continuous.

**Addition $\mathcal{A}$ is continuous under $\mathcal{T}$**
As to addition, the maps $\varphi: V\to\R^n$ and $(\varphi\times\varphi):V\times V\to\R^n \times\R^n$ are homeomorphisms and the map $\mathcal{A}':\R^n\times \R^n \to\R^n$ is continuous and so the map $\mathcal{A}:V\times V\to V$, being equal to $\varphi^{-1}\circ\mathcal{A}'\circ(\varphi\times\varphi)$,, is also continuous.

**Scalar multiplication $\mathcal{M}$ is continuous under $\mathcal{T}$**
As to scalar multiplication, the maps $\varphi:V\to\R^n$ and $(\iota\times\varphi):\R\times V\to \R\times\R^n$ are homeomorphisms and the map $\mathcal{M}:V\times V\to V$, being equal to $\varphi^{-1}\circ\mathcal{M}'\circ(\iota\times\varphi)$, is also continuous.

**Any linear functional $f:V\to\R$ is continuous**<br/>
Let $f:V\to\R$ be a linear functional. Since $\varphi$ is continuous if and only if $f\circ\varphi^{-1}$ is continuous, we can confine attention to $V=\R^n$. In this case, if $\Set{\mathbf{e}_i}_{i=1}^n$ is the standard basis for $\R^n$ and $|f(\mathbf{e}_i)|\leq M$, then for any $x=(\lambda_i)\in\R^n$ we have

$$
\begin{align*}
  |f(x)| =& \left| \sum_{i=1}^n \lambda_i f(\mathbf{e}_i) \right| \\
  \leq& \sum_{i=1}^n |\lambda_i|\cdot|f(\mathbf{e}_i)| \leq M\sum_{i=1}^n |\lambda_i|
\end{align*}
$$

If $|x| < \frac{\varepsilon}{Mn}$ then $|\lambda_i| < \frac{\varepsilon}{Mn}$ and so $|f(x)| < \varepsilon$, which implies that $f$ is continuous.

By the Riesz representation theorem and the Cauchy-Schwarz inequality we have

$$
  \lVert f(x) \rVert \leq \lVert \mathcal{R}_f \rVert \cdot \lVert x\rVert
$$

Hence, $x_n \to 0$ implies $f(x_n)\to 0$ and so by linearity, $x_n \to x$ implies $f(x_n)\to x$ and so $f$ is continuous.
</details>
</MathBox>

# Quotient space

<MathBox title='Quotient space' boxType='definition'>
Let $S$ be a subspace of a $\mathbb{F}$-vector space $V$. The binary relation on $V$ defined by

$$
  \mathbf{u} \sim \mathbf{v} \iff \mathbf{u} - \mathbf{v} \in S,\; \mathbf{u},\mathbf{\mathbf{v}}\in V
$$

is an equivalence relation. When $\mathbf{u}\sim \mathbf{v}$, we say that $\mathbf{u}$ and $\mathbf{v}$ are *congruent modulo* $S$ written as

$$
  \mathbf{u}\sim \mathbf{v} \mod S
$$

The equivalence class

$$
  [\mathbf{v}]_\sim = \mathbf{v} + S = \Set{\mathbf{v}+\mathbf{s} | \mathbf{s}\in S}
$$

is called a *coset* of $S$ in *V* and $\mathbf{v}$ is called a *coset representative* for $\mathbf{v} + S$. The set of all cosets of $S$ in $V$ is denoted

$$
  V/S = \Set{\mathbf{v} + S | \mathbf{\mathbf{v}}\in V}
$$

and is called the *quotient space* of $V$ modulo $S$. The quotient space is a vector space under the operations
- $\lambda \mathbf{u}_1 + S = \lambda \mathbf{u}_2 + S$ for $\mathbf{u}_1\in V$ and $\lambda\in\mathbb{F}$
- $(\mathbf{u}_1 + \mathbf{v}_1) + S = (\mathbf{u}_2 + v_2) + S$ for $\mathbf{u}_1, \mathbf{u}_2, \mathbf{v}_1, \mathbf{v}_2\in V$

The zero vector in $V/S$ is the coset $\mathbf{0} + S = S$

<details>
<summary>Details</summary>

The equivalence classes on $\sim$ take the form

$$
\begin{align*}
  [\mathbf{v}]_\sim =& \Set{\mathbf{u}\in V | \mathbf{u}\sim \mathbf{v} } \\
  =& \Set{\mathbf{u}\in V | \mathbf{u}-\mathbf{v} \in S } \\
  =& \Set{ \mathbf{u}\in V | \mathbf{u} = \mathbf{v} + \mathbf{s},\; \mathbf{s}\in S } \\
  =& \Set{ \mathbf{v} + \mathbf{s} | \mathbf{s}\in S } \\
  =& \mathbf{v} + S
\end{align*}
$$

If $\mathbf{u}_1 \sim \mathbf{v}_1$ and $\mathbf{u}_2 \sim \mathbf{v}_2$, then 

$$
\begin{align*}
  \mathbf{u_1} - \mathbf{v}_1 \in S, \mathbf{u}_2 - \mathbf{v}_2 \in \implies& \alpha(\mathbf{u}_1 - \mathbf{v}_1) + \beta(\mathbf{u}_2 - \mathbf{v}_2) \in S \\
  \implies& (\alpha\mathbf{u}_1 + \beta\mathbf{u}_2) - (\alpha\mathbf{v}_1 + \beta \mathbf{v}_2)\in S \\
  \implies& \alpha\mathbf{u}_1 + \beta\mathbf{u}_2 \sim \alpha\mathbf{v}_1 + \beta \mathbf{v}_2
\end{align*}
$$

showing that congruence modulo $S$ is preserved under the vector space operations.
</details>
</MathBox>

## Natural projection

<MathBox title='Natural projection' boxType='proposition'>
The natural (canonical) projection $\pi_S: V\to V/S$ defined by

$$
  \pi_S (\mathbf{v}) = \mathbf{v} + S
$$

is a surjective linear transformation with $\mathrm{ker}(\pi_S) = S$.

<details>
<summary>Details</summary>

The natural projection $\pi_S$ is clearly surjective.

For $\mathbf{u},\mathbf{\mathbf{v}}\in V$ and $\alpha,\beta\in\mathbb{F}$ we have
$$
\begin{align*}
  \pi_S(\alpha\mathbf{u} + \beta\mathbf{v}) =& (\alpha\mathbf{u} + \beta\mathbf{v}) + S \\
  =& \alpha(\mathbf{u} + S) \beta(\mathbf{v} + S) \\
  =& \alpha\pi_S(\mathbf{u}) + \beta\pi_S(\mathbf{v})
\end{align*}
$$

showing that $\pi_S$ is linear.

To determine the kernel of $\pi_S$, note that

$$
\begin{align*}
  \mathbf{v}\in \mathrm{ker}(\pi_S) \iff& \pi_S(\mathbf{v}) = \mathbf{0} \\
  \iff& \mathbf{v} + S = S \iff \mathbf{v}\in S
\end{align*}
$$

Hence $\mathrm{ker}(\pi_S) = S$.
</details>
</MathBox>

<MathBox title='The correspondence theorem' boxType='theorem'>
Let $S$ be a subspace of $V$. Then the function that assigns to each intermediate subspace $S\subseteq T\subseteq V$ the subspace $T/S$ of $V/S$ is an order preserving (with respect to set inclusion) one-to-one correspondence between the set of all subspaces of $V$ containing $S$ and the set of all subspaces of $V/S$.
</MathBox>

## The first isomorphism theorem

<LatexFig width={50} src='/fig/universal_property_linear_transformation.svg' alt=''
  caption='Universal property for linear transformation'
>
```tex
\documentclass[tikz]{standalone}
\usepackage{amssymb}
\usepackage{tikz}
\usetikzlibrary{cd}

\begin{document}

\begin{tikzcd}[
  row sep=large, column sep=huge, 
  every label/.append style={font=\scriptsize}
]
  V \arrow[d, "\mathrm{P}_S" swap] \arrow[r, "\mathrm{T} = \mathrm{T}' \circ \mathrm{P}_S"] & W \\
  V/S \arrow[ru, "\mathrm{T}'" swap] &
\end{tikzcd}

\end{document}
```
</LatexFig>

<MathBox title='Universal property' boxType='theorem'>
Let $S$ be a subspace of $V$ and let $\mathrm{T}\in\mathcal{L}(V,W)$ satisfy $S\subseteq\mathrm{ker}(\mathrm{T})$. Then there is a unique linear transformation $\mathrm{T}':V/S\to W$ with the property that 

$$
  \mathrm{T}' \circ \pi_S = \mathrm{T}
$$

and $\mathrm{ker}(\mathrm{T}') = \mathrm{ker}(\mathrm{T})/S$ and $\mathrm{ran}(\mathrm{T}') = \mathrm{ran}(\mathrm{T})$.

<details>
<summary>Details</summary>

There is no other choice but to define $\mathrm{T}'$ by the condition $\mathrm{T}' = \pi_S = \mathrm{T}$, i.e.

$$
  \mathrm{T}'(\mathbf{v} + S) = \mathrm{\mathrm{T}}(\mathbf{v})
$$

This function is well-defined if and only if

$$
  \mathbf{v} + S = \mathbf{u} + S \implies \mathrm{T}'(\mathbf{v} + S) = \mathrm{T}'(\mathbf{u} + S)
$$

which is equivalent to each of the following statements

$$
\begin{align*}
  \mathbf{v} + S = \mathbf{u} + S \implies& \mathrm{T}(\mathbf{v}) = \mathrm{T}(\mathbf{u}) \\
  \mathbf{v} - \mathbf{u} \in S \implies& \mathrm{T}(\mathbf{v} - \mathbf{u}) = \mathbf{0} \\
  \mathbf{x}\in S \implies& \mathrm{T}(\mathbf{x}) = \mathbf{0} \\
  S \subseteq& \mathrm{ker}(\mathrm{T})
\end{align*}
$$

Thus, $\mathrm{T}':V/S\to W$ is well defined. Also

$$
\begin{align*}
  \mathrm{ran}(\mathrm{T}') =& \Set{ \mathrm{T}'(\mathbf{v} + S) | \mathbf{\mathbf{v}}\in V } \\
  =& \Set{ \mathrm{T}(\mathbf{v}) | \mathbf{v}\in V } \\
  =& \mathrm{ran}(\mathrm{T})
\end{align*}
$$

and

$$
\begin{align*}
  \mathrm{ker}(\mathrm{T}') =& \Set{\mathbf{v} + S | \mathrm{T}'(\mathbf{v} + S) = \mathbf{0} } \\
  =& \Set{\mathbf{v} + S | \mathrm{T}(\mathbf{v}) = \mathbf{0} } \\
  =& \Set{\mathbf{v} + S | \mathbf{v}\in\mathrm{ker}(\mathrm{T}) } 
  =& \mathrm{ran}(\mathrm{T})/S
\end{align*}
$$

The uniqueness of $\mathrm{T}'$ is evident.
</details>
</MathBox>

<MathBox title='First isomorphism theorem' boxType='theorem'>
For $\mathrm{T}\in\mathcal{L}(V,W)$, the linear transformation $\mathrm{T}':V/\mathrm{ker}(\mathrm{T}) \to W$ defined by

$$
  \mathrm{T}'(\mathbf{v} + \mathrm{ker}(\mathrm{T})) = \mathrm{T}(\mathbf{v})
$$

is injective and 

$$
  V/\mathrm{ker}(\mathrm{T}) \cong \mathrm{ran}(\mathrm{T})
$$

<details>
<summary>Details</summary>

The first isomorphism theorem follows from the universal property.
</details>
</MathBox>

According to the first isomorphism theorem, the range of any linear transformation on $V$ is isomorphic to a quotient space of $V$. Conversely, any quotient space $V/S$ of $V$ is the range the natural projection $\pi_S$. Thus, up to isomorpisms, quotient spaces are equivalent to homomorphic images.

If $V = S\oplus T$, then the first isomorphism theorem implies that $T \cong V/S$. This can be written as

$$
  (S\oplus T)/T \cong S/(S\cap T)
$$

<MathBox title='Projection operator' boxType='definition'>
Let $S$ be a subspace of $V$ and let $T$ be a complement of $S$, i.e. $V = S\oplus T$. The linear operator $\mathrm{P}_T:V\to V$ defined by

$$
  \mathrm{P}_T(\mathbf{s} + \mathbf{t}) = \mathbf{t},\; \mathbf{s}\in S,\mathbf{t}\in T
$$

is called the projection onto $T$ along $S$ and satisfies

$$
\begin{align*}
  \mathrm{ran}(\mathrm{P}_T) =& T \\
  \mathrm{ker}(\mathrm{P}_T) =& \Set{ \mathbf{s}+\mathbf{t}\in V | \mathbf{t} = \mathbf{0} } = S
\end{align*}
$$
</MathBox>

## Codimension

<MathBox title='' boxType='proposition'>
Let $S$ be a subspace of $V$. All complements of $S$ in $V$ are isomorphic to $V/S$ and hence to each other.

<details>
<summary>Details</summary>

This follows directly from the first isomorphism theorem.
</details>
</MathBox>

If $A$, $B$ and $C$ are subspaces of the vector space then by the previous proposition

$$
  A\oplus B = A\oplus C \implies B\cong C
$$

However, note that the following cases showcasing that quotients and complements do not always behave nicely with respect to isomorphisms:
1. It is possible that

$$
  A\oplus B = C\oplus D
$$

with $A\cong C$ but $B\not\cong D$. Hence $A\cong C$ does not imply that a complement of $A$ is isomorphic to a complement of $C$.

2. It is possible that $V\cong W$ along with $V = S\oplus B$ and $W = S\oplus D$, yet $B \not\cong$. Hence, $V\cong W$ does not imply that $V/S\not\cong W/S$. However, if $V = W$ then $B \cong D$.

<MathBox title='Codimension' boxType='definition'>
If $S$ is a subspace of $V$, then $\dim(V/S)$ is called the *codimension* of $S$ in $V$ and denoted by $\mathrm{codim}_V(S)$.
</MathBox>

<MathBox title='Relation between dimension and codimension' boxType='proposition'>
Let $S$ be a subspace of $V$. Then

$$
  \dim(V) = \dim(S) + \dim(V/S) = \mathrm{codim}_V(S)
$$

If $V$ is finite-dimensional then

$$
  \mathrm{codim}_V(S) = \dim(V) - \dim(S)
$$
</MathBox>

<MathBox title='Second isomorphism theorem' boxType='theorem'>
Let $S$ and $T$ be subspaces of a vector space $V$. Then

$$
  (S + T)/T \cong S/(S\cap T)
$$

<details>
<summary>Proof</summary>

The function $\mathrm{T}: (S+T) \to S/(S\cap T)$ defined by

$$
  \mathrm{T}(\mathbf{s} + \mathbf{t}) = \mathbf{s} + (S \cap T)
$$

is a well-defined surjective linear transformation with $\mathrm{ker}(\mathrm{T}) = T$. Hence, by the first isomorphism theorem $(S + T)/T \cong S/(S\cap T)$.
</details>
</MathBox>

<MathBox title='Third isomorphism theorem' boxType='theorem'>
Let $V$ be a vector space and suppose that $S\subseteq T\subseteq V$ are subspaces of $V$. Then

$$
  (V/S)/(T/S) \cong V/T
$$

<details>
<summary>Proof</summary>

The function $\mathrm{T}: V/S \to V/S$ defined by

$$
  \mathrm{T}(\mathbf{s} + S) = \mathbf{s} + T
$$

is a well-defined surjective linear transformation with $\mathrm{ker}(\mathrm{T}) = T/S$. Hence, by the first isomorphism theorem $\mathrm{T}(\mathbf{s} + S) = \mathbf{s} + T$.
</details>
</MathBox>

<MathBox title='Third isomorphism theorem' boxType='theorem'>
Let $S$ be a subspace of the vector space $V$. Suppose that $V = V_1 \oplus V_2$ and $S = S_2 \oplus S_2$ with $S_i \subseteq V_i$ for $i\in\Set{1,2}$. Then

$$
  (V/S) = (V_1\oplus V_2)/(S_1\oplus S_2) \cong V_1/S_1 \boxplus V_2/S_2
$$

<details>
<summary>Proof</summary>

The function $\mathrm{T}: V\to V_1/S_1 \boxplus V_2/S_2$ defined by

$$
  \mathrm{T}(\mathbf{v_1} + \mathbf{v_2}) \cong (\mathbf{v_1} + S, \mathbf{v_2} + S_2)
$$

is a well-defined since $V = V_1 \oplus V_2$ is a direct sum. It is also a surjective linear transformation with $\mathrm{ker}(\mathrm{T}) = S_1 \oplus S_2$. Hence, by the first isomorphism theorem $\mathrm{T}(\mathbf{v_1} + \mathbf{v_2}) \cong (\mathbf{v_1} + S, \mathbf{v_2} + S_2)$.
</details>
</MathBox>

# Dual space

## Linear functionals (covectors)

<MathBox title='Linear functional and dual space' boxType='definition'>
Let $V$ be a vector space over $\mathbb{F}$. A linear transformation $f\in\mathcal{L}(V,\mathbb{F})$ taking values in $\mathbb{F}$ is a *linear functional* on $V$. The vector space of all linear functionals on $V$ is denoted by $V^*$ and is called the *algebraic dual space* of $V$. The elements of $V^*$ are called *covectors*.
</MathBox>

<MathBox title='' boxType='proposition'>
1. For any nonzero vector $\mathbf{v}\in V\setminus\Set{\mathbf{0}}$, there exists a linear functional $f\in V^*$ for which $f(\mathbf{v}) \neq 0$.
2. A vector $\mathbf{\mathbf{v}}\in V$ is zero if and only if $f(\mathbf{v}) = 0$ for all $f\in V^*$
3. Let $f\in V^*$. If $f(\mathbf{x}) \neq 0$ then $V = \mathrm{span}\Set{\mathbf{v}} \oplus \mathrm{ker}(f)$.
4. Two nonzero linear functionals $f,g\in V^*$ have the same kernel if and only if there is a nonzero scalar $\lambda$ such that $f = \lambda g$.

<details>
<summary>Proof</summary>

**(3):** If $\mathbf{0}\neq\mathbf{v} \in \mathrm{span}\Set{\mathbf{x}}\cap\mathrm{ker}(f)$ then $f(\mathbf{v}) = 0$ and $\mathrm{v} = \lambda\mathbf{x}$ for $0 \neq \lambda a\in\mathbb{F}$ implying $f(\mathbf{x}) = 0$, which is false. Thus, $\mathrm{span}\Set{\mathbf{x}}\cap\mathrm{ker}(f) = \Set{\mathbf{0}}$ and the direct sum $S = \mathrm{ker}(\mathbf{x})\oplus\mathrm{ker}(f)$ exists. Also, for any $\mathbf{v}\in V$ we have

$$
  \mathbf{v} = \frac{f(\mathbf{v})}{f(\mathbf{x})}\mathbf{x} + \left(\mathbf{v} - \frac{f\mathbf{v}}{f(\mathbf{x})}\mathbf{x} \right) \in\mathrm{span}\Set{\mathbf{x}} + \mathrm{ker}(f)
$$

and so $V = \mathrm{span}\Set{\mathbf{x}}\oplus\mathrm{ker}(f)$.

**(4):** If $f = \lambda g$ for $\lambda\neq 0$ then $\mathrm{ker}(f) = \mathrm{ker}(g)$. Conversely, if $K = \mathrm{ker}(f) = \mathrm{ker}(g)$ then for $\mathbf{x}\notin K$, then by **(3)**

$$
  V = \mathrm{span}\Set{\mathbf{x}}\oplus K
$$

Note that $f|_K = \lambda g|_K$ for any $\lambda$. Thus, if $\lambda = \frac{f(\mathbf{x})}{g\mathbf{x}}$, it follows that $\lambda g(\mathbf{x}) = f(\mathbf{x})$ and hence $f = \lambda g$. 
</details>
</MathBox>

## Dual basis

<MathBox title='Dual basis' boxType='proposition'>
Let $V$ be a vector space with basis $B = \Set{\mathbf{b}_i }_{i\in I}$ for some index set $I\subseteq\N$.
1. The set $B^* = \Set{b_i}_{i\in I}$ is linearly independent
2. If $V$ is finite-dimensional then $B^*$ is a basis for $V^*$ called the dual basis of $B$

<details>
<summary>Proof</summary>

**(1):** For each $i \in I$ we can define a linear functional $b_i \in V^*$ by the orthogonality condition $b_i (\mathbf{b}_j) = \delta_{ij}$ where $\delta_{ij}$ is the Kronecker delta function. Then the set $B^* = \Set{b_i}_{i\in I}$ is linearly independent since applying the equation

$$
  0 = \sum_{j=1}^n \lambda_{i_j} b_{i_j}
$$

to the basis vectors $\mathbf{b}_{i_k}$ gives

$$
  0 = \sum_{j=1}^k \lambda{i_j} b_{i_j} (\mathbf{b}_{i_k}) = \sum_{j=1}^k \lambda_{i_j}\delta_{i_j, i_k} = \lambda_{i_k}
$$

for all $i_k$.

**(2):** For any $f\in V^*$ we have

$$
  \sum_{i\in I} f(\mathbf{b}_i)b_i (\mathbf{b}_j) = \sum_{i\in I} f(\mathbf{b}_j)\delta_{ij} = f(\mathbf{b}_j)
$$

showing that $f = \sum_{i\in I} f(\mathbf{b}_i)b_i$ is in the span of $B^*$. Hence $B^*$ is a basis for $V^*$.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
If $\dim(V) < \infty$ then $\dim(V^*) = \dim(V)$.
</MathBox>

## Reflexivity

If $V$ is a vector space, then so is the dual space $V^*$. Hence, we may form the *double dual space* $V^{**}$ which consists of all linear functionals $\gamma: V^* \to\mathbb{F}$. In other words, an element of $V^{**}$ is a linear map that assigns scalar to each linear function on $V$.

<MathBox title='Double dual space' boxType='definition'>
Let $V$ be a vector space over $\mathbb{F}$. The *double dual space* of $V$, denoted $V^{**}$, is the set of all linear functionals $\mathrm{f}: V^* \to\mathbb{F}$. The elements of $V^{**}$ take the form of a linear function $\bar{\mathbf{v}}:V^{*}\to\mathbf{F}$ defined as for $\mathbf{v}\in V$

$$
  f \mapsto f(\mathbf{v})
$$

The function $\bar{\mathbf{v}}$ is called *evaluation at* $\mathbf{v}$.

<details>
<summary>Details</summary>

For $f,g\in V^*$ and $\alpha,\beta\in\mathbb{F}$ then

$$
\begin{align*}
  \bar{\mathbf{v}}(\alpha f + \beta g) =& (\alpha f + \beta g)(\mathbf{v}) \\
  =& \alpha f(\mathbf{v}) + \beta g(\mathbf{v}) \\
  =& \alpha\bar{\mathbf{v}}(f) + \beta\bar{\mathbf{v}}(g)
\end{align*}
$$

showing that $\bar{\mathbf{v}}$ is linear.
</details>
</MathBox>

<MathBox title='Canonical map' boxType='proposition'>
The canonical map $\mathrm{T}:V\to V^{**}$ defined by $\mathbf{v}\mapsto \bar{\mathbf{v}}$ where $\bar{\mathbf{v}}: V^* \to \mathbb{F}$ is the evaluation at $\mathbf{v}$, is a monomorphism. If $V$ is finite-dimensional then $\mathrm{T}$ is an isomorphism.

<details>
<summary>Proof</summary>

The function $\mathrm{T}$ is linear since for $\alpha, \beta\in\mathbb{F}$ and $\mathbf{u},\mathbf{v}\in V$

$$
\begin{align*}
  \overline{\alpha\mathbf{u} + \beta\mathbf{v}}(f) =& f(\alpha\mathbf{u} + \beta\mathbf{v}) \\
  =& \alpha f(\mathbf{u}) + \beta f(\mathbf{v}) \\
  =& (\alpha\bar{\mathbf{u}} + \beta\bar{\mathbf{v}})(f)
\end{align*}
$$

for all $f\in V^*$. To determine the kernel of $\mathrm{T}$, note that

$$
\begin{align*}
  \mathrm{T}(\mathbf{v}) = 0 \implies& \bar{\mathbf{v}} = 0 \\
  \implies& \bar{\mathbf{v}}(f) = 0,\; \forall f\in V^* \\
  \implies& f(\mathbf{v}) = 0,\; \forall f\in V^* \\
  \implies& \mathbf{v} = \mathbf{0}
\end{align*}
$$

showing that $\mathrm{ker}(\mathrm{T}) = \Set{\mathbf{0}}$.

In the finite-dimensional case, since $\dim(V^{**}) = \dim(V^*) = \dim(V)$, it follows that $\mathrm{T}$ is also surjective, hence an isomorphism.
</details>
</MathBox>

The canonical map implies that $V\cong V^{**}$ if $V$ is a finite-dimensional vector space. Since the canonical map is an isomorphism, then $V$ is called *algebraically reflexive*.

## Annihilators

<MathBox title='Annihilator' boxType='definition'>
Let $M$ be a nonempty subset of a vector space $V$. The *annihilator* $M^0$ of $M$ is the set

$$
  M^0 = \Set{ f\in V^* | f(M) } = \Set{ 0 }
$$

where $f(M) = \Set{ f(\mathbf{v}) | \mathbf{v}\in M }$
</MathBox>

<MathBox title='Properties of annihilators' boxType='proposition'>
1. **(Order reversing)** For any subset $M$ and $N$ of $V$

$$
  M\subseteq N \implies N^0 \subseteq M^0
$$

2. If $\dim(V) < \infty$ then $M^{00} \cong \mathrm{span}(M)$ under the canonical map. In particular, if $S$ is a subspace of $V$, then $S^{00}\cong S$.
3. If $\dim(V) < \infty$ and $S$ and $T$ are subspaces of $V$, then

$$
\begin{align*}
  (S\cap T)^0 =& S^0 + T^0 \\
  (S + T)^0 =& S^0 \cap T^0
\end{align*}
$$
</MathBox>

<MathBox title='Extension by 0' boxType='definition'>
Let $S$ and $T$ be complement subspaces of the vector space $V$ with $V = S\oplus T$. Any linear functional $f\in T^*$ can be extended to a linear functional $\bar{f}$ on $V$ by setting $f(S) = 0$. The linear functional $\bar{f}$ is called the extension by $0$ of $f$. Clearly, $\bar{f}\in S^0$.
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V = S \oplus T$.
1. The extension by zero map is an isomorphism from $T^*$ to $S^0$ and so $T^* \cong S^0$.
2. If $V$ is finite-dimensional then

$$
  \dim(S^0) = \mathrm{codim}_V (S) = \dim(V) - \dim(S)
$$
</MathBox>

<MathBox title='' boxType='proposition'>
A linear functional on the direct sum $V = S\oplus T$ can be written as a direct sum of a linear functional that annihilates $S$ and a linear functional that annihilates $T$, i.e.

$$
  (S\oplus T)^* = S^0 \oplus T^0
$$

<details>
<summary>Proof</summary>

Clearly $S^0 \cap T^0 = \Set{ \mathbf{0} }$, since any functional that annihilates both $S$ and $T$ must annihilate $S \oplus T$. Hence, the sum $S^0 + T^0$ is direct. If $f\in V^*$ then we can write

$$
  f = (f \circ \rho_T) + (f + \rho_S) \in S^0 \oplus T^0
$$

Hence $V = S^0 \oplus T^0$
</details>
</MathBox>

## Operator adjoints

<MathBox title='Operator adjoint' boxType='definition'>
The operator adjoint of a linear transformation $\mathrm{T}\in\mathcal{L}(V,W)$, is the function $\mathrm{T}^\times: W^* \to V^*$ defined by

$$
\begin{gather*}
  T^\times (f) = f\circ\mathrm{T} = f\mathrm{T},\; f\in W^* \\
  [T^\times (f)](\mathbf{v}) = f[\mathrm{T}(\mathbf{v})],\; \mathbf{v}\in V
\end{gather*}
$$
</MathBox>

<MathBox title='Properties of the operator adjoint' boxType='proposition'>
Let $V$ be a vector space over $\mathbb{F}$, thenand $\alpha,\beta \in\mathbb{F}$, then
1. $(\alpha\mathrm{T} + \beta\mathrm{S})^\times = \alpha\mathrm{T}^\times + \beta\mathrm{S}^\times$ for $\mathrm{T},\mathrm{S}\in\mathcal{L}(V,W)$ and $\alpha,\beta\in\mathbb{F}$.
2. $(\mathrm{T}\mathrm{S})^\times = \mathrm{S}^\times \mathrm{T}^\times$ for $\mathrm{S}\in\mathcal{L}(V,W)$ and $\mathrm{T}\in\mathcal{L}(W,U)$.
3. If $\mathrm{T}\in\mathcal{L}(V)$ is invertible, then $(\mathrm{T}^{-1})^\times = (\mathrm{T}^\times)^{-1}$

<details>
<summary>Proof</summary>

**(1):** For all $f\in V^*$

$$
\begin{align*}
  (\alpha\mathrm{T} + \beta\mathrm{S})^\times (f) =& f(\alpha\mathrm{T} + \beta\mathrm{S}) \\
  =& \alpha f(\mathrm{T}) + \beta f(\mathrm{S}) \\
  =& \alpha \mathrm{T}^\times (f) + \beta \mathrm{S}^\times (f)
\end{align*}
$$

**(2):** For all $f\in U^*$

$$
\begin{align*}
  (\mathrm{T}\mathrm{S})^\times (f) =& f(\mathrm{T}\mathrm{S}) = \mathrm{S}^\times (f\mathrm{T}) \\
  =& \mathrm{T}(\mathrm{S}^\times(f)) = (\mathrm{T}^\times \mathrm{S}^\times)(f)
\end{align*}
$$

**(3):** Evaluating $\mathrm{T}^\times (\mathrm{T}^\times)^\times$ using property (2) gives

$$
  \mathrm{T}^\times (\mathrm{T}^\times)^\times = (\mathrm{T}^{-1} \mathrm{T})^\times = \mathrm{id}^\times = \mathrm{id}
$$

Doing the same for $(\mathrm{T}^{-1})^{-1} \mathrm{T}^\times$ gives

$$
  (\mathrm{T}^{-1})^{-1} \mathrm{T}^\times = (\mathrm{T}\mathrm{T}^{-1})^\times = \mathrm{id}^\times = \mathrm{id}
$$

Hence, $(\mathrm{T}^{-1})^\times = (\mathrm{T}^\times)^{-1}$.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V$ and $W$ be finite-dimensional $\mathbb{F}$-vector spaces and let $\mathrm{T}\in\mathcal{L}(V,W)$. If we identify $V^{**}$ with $V$ and $W^{**}$ with $W$ using the canonical maps, then $\mathrm{T}^{\times\times}$ is identified with $\mathrm{T}$.

<details>
<summary>Proof</summary>

For any $\mathbf{x}\in V$ let the corresponding element of $V^{**}$ be denoted $\bar{\mathbf{x}}:V^* \to\mathbb{F}$ and similarly for $W$, then for all $f\in W^*$

$$
\begin{align*}
  \mathrm{T}^{\times\times}(\bar{\mathbf{x}})(f) =& \bar{\mathbf{v}}[\mathrm{T}^\times (f)] = \bar{\mathbf{v}}(f\mathrm{T}) \\
  =& f(\mathrm{T}(\mathbf{v})) = \overline{\mathrm{T}(\mathbf{v})}(f)
\end{align*}
$$

and so

$$
\begin{align*}
  \mathrm{T}^{\times\times}(\bar{\mathbf{v}})(f) =& \overline{\mathrm{T}(\mathbf{v})} \in W^{**}
\end{align*}
$$

Using the canonical maps for both $V^{**}$ and $W^{**}$ we have

$$
  \mathrm{T}^{\times\times}(\mathbf{v}) = \mathrm{T}(\mathbf{v})
$$

for all $\mathbf{v}\in V$.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $\mathrm{T}\in\mathcal{L}(V,W)$, then
1. $\mathrm{ker}(\mathrm{T}^\times) = \mathrm{ran}(\mathrm{T})^0$
2. $\mathrm{ran}(\mathrm{T}^\times) = \mathrm{ker}(\mathrm{T})^0$

<details>
<summary>Proof</summary>

**(1):**

$$
  \mathrm{ker}(\mathrm{T}^\times) =& \Set{ f\in W^* | \mathrm{T}^\times = 0 } \\
  =& \Set{ f\in W^* | f(\mathrm{T}(V)) = \Set{0} } \\
  =& \Set{ f\in W^* | f(\mathrm{ran}(\mathrm{T})) = \Set{0} } \\
  =& \mathrm{ran}(\mathrm{T})^0
$$

**(2):** If $f = g\mathrm{T} = \mathrm{T}^\times g\in\mathrm{ran}(\mathrm{T}^\times)$ then $\mathrm{ker}(\mathrm{T})\subseteq \mathrm{ker}(f)$ and so $f\in\mathrm{ker}(\mathrm{T})^0$.

For the reverse inlcusion, let $f\in\mathrm{ker}(\mathrm{T})^0\subseteq V^*$. On $K = \mathrm{ker}(\mathrm{T})$, there is no problem since $f$ and $\mathrm{T}^\times g = g\mathrm{T}$ agree on $K$ for any $g\in W^*$. Let $S$ be a complement of $\mathrm{ker}(\mathrm{T})$. Then $\mathrm{T}$ maps a basis $B = \Set{\mathbf{b}_i }_{i\in I}$ for $S$ to a linearly independent set $\mathrm{T}(B) = \Set{ \mathrm{T}(\mathbf{b}_i) }_{i\in I}\in W$, so we can define $g\in W^*$ any way we want on $\mathrm{T}(B)$. In particular, let $g\in W^*$ be defined by setting

$$
  g(\mathrm{T}(\mathbf{b}_i)) = f(\mathbf{b}_i),\; i\in I
$$

and extending in any manner to all of $W^*$. Then $f = g\mathrm{T} = \mathrm{T}^\times g$ on $B$ and therefore on $S$. Hence $f = \mathrm{T}^\times g\in\mathrm{ran}(\mathrm{T}^\times)$.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $\mathrm{T}\in\mathcal{L}(V,W)$, where $V$ and $W$ are finite-dimensional vector spaces. Then $\mathrm{rank}(\mathrm{T}) = \mathrm{rank}(\mathrm{T}^\times)$.

<details>
<summary>Proof</summary>


</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $\mathrm{T}\in\mathcal{L}(V,W)$, where $V$ and $W$ are finite-dimensional vector spaces. If $B$ and $C$ are ordered bases for $V$ and $W$, respectively, and $B^*$ and $C^*$ are corresponding dual bases, then

$$
  [\mathrm{T}^\times]_{C^*, B^*} = ([\mathrm{T}]_{B,C})^T
$$

<details>
<summary>Proof</summary>

The $(i,j)$th entry of $[\mathrm{T}]_{B,C}$ is

$$
  ([\mathrm{T}]_{B,C})_{i,j} = ([\mathrm{T}(\mathbf{b}_j)]_C)_i = c_i[\mathrm{T}(\mathbf{b}_j)],\; \mathbf{b}_i \in B, c_i \in C^*
$$

and the $(i,j)$th entry of $[\mathrm{T}^\times]_{C^*,B^*}$ is

$$
\begin{align*}
  ([\mathrm{T}^\times]_{C^*,B^*})_{i,j} =& ([\mathrm{T}^\times(c_j)]_{B^*})_i = \mathbf{b}_i^{**}[\mathrm{T}^\times(c_j)] \\
  =& \mathrm{T}^\times (c_j)(\mathbf{b}_i) = c_j (\mathrm{T}(\mathbf{b}_i))
\end{align*}
$$

Comparing the expression, we see that $[\mathrm{T}^\times]_{C^*, B^*} = ([\mathrm{T}]_{B,C})^T$.
</details>
</MathBox>

# Modules

# Linear operators

<MathBox title='Eigenvalue, eigenvector and spectrum' boxType='proposition'>
Let $V$ be a vector space over $\mathbf{F}$.
1. A scalar $\lambda\in\mathbb{F}$ is an *eigenvalue* of an operator $\mathrm{T}\in\mathcal{L}(V)$ if there exists a nonzero vector $\mathbf{v}\in V$ for which

$$
  \mathrm{T}(\mathbf{v}) = \lambda\mathbf{v}
$$

In this case, $\mathbf{v}$ is an *eigenvector* of $\mathrm{T}$ associated with $\lambda$.
2. A scalar $\lambda\in\mathbb{F}$ is an eigenvalue for a matrix $\mathbf{A}\in\mathcal{M}_{n}(\mathbb{F})$ if there exists a nonzero column vector $\mathbf{x}$ for which

$$
  \mathbf{A}\mathbf{x} = \lambda\mathbf{x}
$$

In this case, $\mathbf{x}$ is an eigenvector for $\mathbf{A}$ associated with $\lambda$.
3. The set of all eigenvectors associated with a given eigenvalue $\lambda$, together with the zero vector, forms a subspace of $V$, called the *eigenspace* of $\lambda$, denoted by $E_\lambda$.
4. The set of all eigenvalues of an operator or matrix is called the *spectrum* of the operator or matrix.
</MathBox>

# Inner product space

# Metric vector space

# Tensor product

## Universality for bilinearity

<MathBox title='Bilinear function' boxType='definition'>
Let $U$, $V$ and $W$ be vector spaces over $\mathbb{F}$. A function $F:U\times V\to W$, where $U\times V$ is the cartesian product of sets, is bilinear if it is linear in both variables separately

$$
\begin{align*}
  F(\alpha\mathbf{u}_1 + \beta\mathbf{u}_2, \mathbf{v}) =& \alpha F(\mathbf{u}_1,\mathbf{v}) + \beta F(\mathbf{u}_2, \mathbf{v}) \\
  F(\mathbf{u}, \alpha\mathbf{v}_1 + \beta\mathbf{v_2}) =& \alpha F(\mathbf{u},\mathbf{v}_1) + \beta F(\mathbf{u}, \mathbf{v}_2) 
\end{align*}
$$

The set of all bilinear functions from $U\times V$ to $W$ is denoted $\hom_{\mathbb{F}}(U\times V, W)$. A bilinear function $F:U\times V\to\mathbb{F}$ taking values in the base field $\mathbb{F}$ is called a *bilinear form* on $U\times V$.
</MathBox>

Note the difference between bilinear functions $\hom_{\mathbb{F}}(V\times V;W)$, mapping from cartesian products of vector spaces, and linear functions $\mathcal{L}(V\times V,W)$, mapping from direct products of vector spaces. Unlike direct products, cartesion products of sets are not associated with any algebraic structure.

Using category theory, the tensor product can be defined as a universal for bilinearity. Let $U$ and $V$ be vector spaces over $\mathbf{F}$, and consider the functor

$$
  \mathbf{Bilin}(U,V;-): \mathbf{Vector}_{\mathbb{F}}\to\mathbf{Set} \\
  W \mapsto \hom_{\mathbb{F}}(U\times V, W)
$$

mapping a vector space $W$ to the set of bilinear maps $U\times V\to W$. It can be shown that this functor is representable, i.e. there is a vector space $S$ such that $\mathbf{Bilin}(U,V;W)\cong\mathbf{Vect}_{\mathbb{F}}(T,W) = \mathcal{L}(T,W)$ naturally in $W$. 

In other words, there is a universal pair $(T, B:U\times V \to T)$ of a vector space $T$ and a bilinear map $B$ such that for any pair $(W, F:U\times V \to W)$ of a vector space $W$ and a bilinear map $F$ there is a unique linear map $\mathrm{L}:T\to W$ with $F = \mathrm{L} \circ B$ according to the diagram below.

<LatexFig width={50} src='/fig/universal_property_bilinearity.svg' alt=''
  caption='Universal property for bilinearity'
>
```tex
\documentclass[tikz]{standalone}
\usepackage{amssymb}
\usepackage{tikz}
\usetikzlibrary{cd}

\begin{document}

\begin{tikzcd}[every label/.append style={font=\scriptsize}]
  U\times V \arrow[r, "B"] \arrow[dr, "\forall F" swap] & 
  T \arrow[d, "\exists!\mathrm{L}", dashed] \\
  & \forall W
\end{tikzcd}

\end{document}
```
</LatexFig>

This property, called the universal property, uniquely determines $(T, B)$ up to isomorphism. The vector space $T$ is called the tensor product of $U$ and $V$, denoted $U \otimes V$ and the bilinear map $B$ is written

$$
\begin{gather*}
  \otimes: U\times V \to U\otimes V \\
  (\mathbf{u}, \mathbf{v}) \mapsto \mathbf{u}\otimes\mathbf{v}
\end{gather*}
$$

Since a bilinear map out of $U\times V$ is essentially the same as linear map out of $U\times V$, tensor products reduce the study of bilinear functions to the study of linear transformations. 

<MathBox title='Tensor product' boxType='definition'>
The tensor product of two vector spaces $U$ and $V$ is a vector space denoted $U\otimes V$ together with a bilinear map $\otimes:U\times V\to U\otimes V$ given by $(\mathbf{u},\mathbf{v})\mapsto \mathbf{u}\otimes \mathbf{v}$ satisfying the universal property. That is for every for every bilinear map $F:U\times V\to W$ there is a unique linear map $\mathcal{L}:U\otimes V\to W$ such that $F = \mathcal{L} \circ\otimes$, i.e. $f(\mathbf{u},\mathbf{v}) = \mathcal{L}(\mathbf{u}\otimes\mathbf{v})$ for every $\mathbf{u}\in U$ and $\mathbf{v}\in V$.

<LatexFig width={50} src='/fig/universal_property_tensor_product.svg' alt=''
  caption='Universal property for the tensor product'
>
```tex
\documentclass[tikz]{standalone}
\usepackage{amssymb}
\usepackage{tikz}
\usetikzlibrary{cd}

\begin{document}

\begin{tikzcd}[every label/.append style={font=\scriptsize}]
  U\times V \arrow[r, "\otimes"] \arrow[dr, "\forall F" swap] & 
  U\otimes V \arrow[d, "\exists!\mathrm{L}", dashed] \\
  & \forall W
\end{tikzcd}

\end{document}
```
</LatexFig>
</MathBox>

## Construction from bases

Let $U$ and $V$ be vector spaces with respective bases $B = \Set{\mathbf{c}_j }_{j\in J}$. The tensor product from $U\otimes V$ can be constructed as vector space $U\otimes V$ with basis $D = \Set{ \mathbf{b}_i \otimes \mathbf{c}_j | \mathbf{b}_i \in B, \mathbf{c}_j \in C }$ where $\otimes:U\times V \to U\otimes V$ maps $(\mathbf{b}_i, \mathbf{c}_j)\mapsto \mathbf{b}_i \otimes\mathbf{c}_i$.

For any bilinear function $F:U\times V \to W$, then the universal condition $F = \mathrm{L}\circ\otimes$ is equivalent to

$$
  \mathrm{L}(\mathbf{b}_i \otimes \mathbf{c}_j) = F(\mathbf{b}_i, \mathbf{c}_i)
$$

which uniquely defines a linear map $\mathbf{L}: U\otimes V\to W$. Hence, $(U\otimes V, \otimes)$ has the universal property for bilinearity.

If $U\ni \mathbf{u} = \sum_{i\in J}^n \alpha_i \mathbf{b}_i$ and $V\ni \mathbf{v} = \sum_{j\in J} \beta\mathbf{c}_i$ for $\alpha_i, \beta_j\in \mathbb{F}$ then the elements of $U\otimes V$, called tensors take the form

$$
\begin{align*}
  \mathbf{u}\otimes\mathbf{v} &= \left(\sum_{i\in I} \alpha_i\mathbf{b}_i \right) \otimes \left( \sum_{j\in J} \beta_j\mathbf{c}_j \right) \\
  &= \sum_{i\in I}\sum_{j\in J} \alpha_i \beta_j \left( \mathbf{b}_i \otimes \mathbf{c}_j \right)
\end{align*}
$$

## Construction from quotient space

Let $S_{U\times V}$ be the vector space over $\mathbb{F}$ with basis $U\times V$. Let $R$ be the subspace of $S_{U\times V}$ generated by all vectors of the form

$$
\begin{gather}
  \alpha(\mathbf{u}, \mathbf{v}) + \beta(\mathbf{u}', \mathbf{w}) - (\alpha\mathbf{u} + \beta\mathbf{u}', \mathbf{v}),\; \mathbf{u},\mathbf{u}'\in U, \mathbf{v}\in V \\
  \alpha(\mathbf{u}, \mathbf{v}) + \beta(\mathbf{u}, \mathbf{w}') - (\alpha\mathbf{u}, \alpha\mathbf{v} + \beta\mathbf{v}'),\; \mathbf{u}\in U, \mathbf{v},\mathbf{v}'\in W
\end{gather}
$$

where $\alpha, \beta\in\mathbb{F}$. The quotient space $U\otimes V = \mathbf{F}_{U\times V}/S$ is also called the *tensor product* of $U$ and $V$. The elements of the tensor product have the form

$$
  \left(\sum_{i\in I} \alpha_i(\mathbf{u}_i, \mathbf{v}_i) \right) + R = \sum_{i\in I} \alpha_i[(\mathbf{u}_i, \mathbf{v}_i) + R],\; \alpha_i\in\mathbb{F}, \mathbf{u}_i \in U, \mathbf{v}_i \in V
$$

Because $\alpha(\mathbf{u}, \mathbf{v}) - (\alpha\mathbf{u}, \mathbf{v})\in R$ and $\alpha(\mathbf{u},\mathbf{v}) - (\mathbf{u},\alpha\mathbf{v})\in R$, we can absorb the scalar in either coordinate, i.e.

$$
  \alpha[(\mathbf{u}, \mathbf{v}) + R] = (\alpha\mathbf{u},\mathbf{v}) + S = (\mathbf{u}, \alpha\mathbf{v}) + R
$$

such that the tensors of $U\otimes V$ can be written simply as

$$
  \sum_{i\in I} [(\mathbf{u}_i, \mathbf{v}_i) + R] = \sum_{i\in I} \mathbf{u}_i \otimes \mathbf{v}_i
$$

To show that the pair $(U\times V, \otimes: U\times V\to U\otimes V)$ has the universal property for bilinearity, consider the following diagram.

<LatexFig width={50} src='/fig/tensor_product_isomorphism.svg' alt=''
  caption='Tensor product diagram'
>
```tex
\begin{tikzcd}[every label/.append style={font=\scriptsize}]
  U\times V 
    \arrow[rr, bend left, "\otimes"] 
    \arrow[r, "G"] 
    \arrow[dr, "F" swap] 
  & L_{U\times V}
    \arrow[r, "\mathrm{R}"]
    \arrow[d, dashed, "\mathrm{S}"]
  & U\otimes V
    \arrow[dl, dashed, "\mathrm{T}"]
  \\ & W &
\end{tikzcd}
```
</LatexFig>

Since $\otimes(\mathbf{u},\mathbf{v}) = \mathbf{u}\times\mathbf{v} = \mathrm{R}\circ G(\mathbf{u}, \mathbf{v})$ it follows that $\otimes = \mathrm{R}\circ G$. The universal property of vector spaces implies that there is a unique linear transformation $\mathrm{S}: L_{U\times V}\to W$ for which $\mathrm{S}\circ G = F$.

Note that $\mathrm{S}$ sends any of the vectors $(1)$ and $(2)$ that generate $S$ to the zero vector, i.e. $S\subseteq \mathrm{ker}(\mathrm{S})$

$$
\begin{align*}
  &\mathrm{S}[\alpha(\mathbf{u}, \mathbf{v}) + \beta(\mathbf{u}',\mathbf{v}) - (\alpha\mathbf{u} + \beta\mathbf{u}', \mathbf{v})] \\
  =& \mathrm{S}[\alpha G(\mathbf{u}, \mathbf{v}) + \beta G(\mathbf{u}', \mathbf{v}) - G(\alpha\mathbf{u} + \beta\mathbf{u}',\mathbf{v})] \\
  =& \alpha\mathrm{S}h(\mathbf{u}, \mathrm{v}) + \beta\mathrm{S}h(\mathbf{u}',\mathbf{v}) - \mathrm{S}G(\alpha\mathbf{u} + \beta\mathbf{u}',\mathbf{v}) \\
  =& \alpha f(\mathbf{u},\mathbf{v}) + \beta F(\mathbf{u}', \mathbf{v}) - F(\alpha\mathbf{u} + \beta\mathbf{u}', \mathbf{v}) \\
  =& \alpha F(\mathbf{u},\mathbf{v}) + \beta F(\mathbf{u}', \mathbf{v}) - \alpha F(\mathbf{u}, \mathbf{v}) - \beta F(\mathbf{u}', \mathbf{v})
  =& \mathbf{0}
\end{align*}
$$

and similarly for the second coordinate. Hence by the universal property for linear transformations, there is a unique linear transformation $T:U\otimes V\to W$ such that $\mathrm{T}\circ \mathrm{R} = \mathrm{S}$. Thus

$$
  \mathrm{T}\circ \otimes = \mathrm{T} \circ \mathbf{R}\circ h = \mathrm{S}\circ h 0 f
$$

As to uniqueness, if $\mathrm{T}' \circ \otimes = F$, then $\mathrm{S} = \mathrm{T}' \circ\mathrm{R}$ satisfies

$$
  \mathrm{S}' \circ G = \mathrm{T}' \circ \mathrm{R} = \mathrm{T}' \circ \otimes = F
$$

The uniqueness of $\mathrm{S}$ then implies that $\mathrm{S}' = \mathrm{S}$, which in turn implies that 

$$
  \mathrm{T}' \circ \mathrm{R} = \mathrm{S}' = \mathrm{S} = \mathrm{T}\circ\mathrm{R}
$$

Finally, the uniqueness of $\mathrm{T}$ implies that $\mathrm{T}' = \mathrm{T}$.

## Basis

<MathBox title='Linear independence of tensor products' boxType='proposition'>
Let $U\otimes V$ be the tensor product of $n$-dimensional vector spaces $U$ and $V$. If $\Set{\mathbf{u}_i}_{i=1}^n$ are are linearly independent vectors in $U$ and $\Set{\mathbf{v}_i}_{i=1}^n$ arbitrary vectors in $V$, then

$$
  \sum_{i=1}^n \mathbf{u}_i \otimes \mathbf{v}_i = \mathbf{0} \implies \mathbf{v}_i = \mathbf{0}\; \forall i
$$

In particular $\mathbf{u}\otimes\mathbf{v}=\mathbf{0}$ if and only if $\mathbf{u}=\mathbf{0}$ or $\mathrm{v} = \mathbf{0}$.

<details>
<summary>Proof</summary>

By the bilinearity of the tensor product

$$
\begin{gather*}
  \mathbf{0}\otimes\mathbf{v} = (\mathbf{0} + \mathbf{0})\otimes\mathbf{v} = \mathbf{0}\otimes\mathbf{v} + \mathbf{0}\otimes\mathbf{0} \\
  \implies \mathbf{0}\otimes\mathbf{v} = \mathbf{0}
\end{gather*}
$$

and similarly $\mathbf{u}\otimes\mathbf{0} = \mathbf{0}$.

Suppose that $\sum_{i=1}^n \mathbf{u}_i \otimes \mathbf{v}_i = \mathbf{0}$ assuming none of $\mathbf{u}_i$ and $\mathbf{v}_i$ are zero. According to the universal property of the tensor product, for any bilinear function $F:U\times V\to W$, there is a unique linear transformation $\mathrm{L}: U\otimes V\to W$ for which $\mathrm{L}\circ \otimes = F$. Hence

$$
\begin{align*}
  \mathbf{0} =& \mathrm{T}\left(\sum_{i=1}^n \mathbf{u}_i \otimes \mathbf{v}_i \right) \\
  =& \sum_{i=1}^n (\mathrm{T}\circ t)(\mathbf{u}_i, \mathbf{v}_i) \\
  =& \sum_{i=1}^n F(\mathbf{u}_i, \mathbf{v}_i)
\end{align*}
$$

One possibility for $F$, which can take any form, is to mulitply two linear functionals $a\in U^*$ and $b\in V^*$, i.e. $f(\mathbf{u}, \mathbf{v}) = a(\mathbf{u})b(\mathbf{v})$. This is easily seen to be bilinear and gives

$$
  \sum_{i=1}^n u(\mathbf{u}_i)v(\mathbf{v}_i)
$$

where all $\mathbf{u}_i$ and $\mathbf{v}_i$ are nonzero. If all $\mathbf{u}_i$ are linearly independent, then we can find dual vectors $u_i$ such that $u_i(\mathbf{u}_j) = \delta_{i,j}$. Setting $a = u_j$ gives

$$
  \mathbf{0} = \sum_{i=1}^n u_j (\mathbf{u}_i)b(\mathbf{v}_i) = a(\mathbf{v}_j)
$$

for all linear functionals $b\in V^*$, implying that $\mathbf{v}_i = \mathbf{0}$ for all $i$.
</details>
</MathBox>

<MathBox title='Basis of tensor products' boxType='proposition'>
Let $B = \Set{\mathbf{b}_i}_{i\in I}$ be a basis for a vector space $U$ and $C = \Set{\mathbf{c}_i}_{j\in J}$ be a basis for a vector space $V$. Then the set

$$
  D = \Set{\mathbf{b}_i \otimes \mathbf{v}_j | i\in I, j\in J}
$$

is a basis for $U\otimes V$.

<details>
<summary>Proof</summary>

To show that $D$ is linearly independent, suppose that

$$
  sum_{i\in I, j\in J} \alpha_{i,j}(\mathbf{b}_i \otimes \mathbf{c}_j) = \mathbf{0},\; \alpha_{i,j}\in\mathbb{F}
$$

This can be written

$$
  \sum_{i\in I}\mathbf{b}_i \otimes \left(\sum_{j\in J} \alpha_{i,j}\mathbf{c}_j \right) = \mathbf{0}
$$

Since $\mathbf{b}_i$ are linearly independent, it follows that $\sum_{j\in J} \alpha_{i,j}\mathbf{v}_i = 0$ for all $i\in I$ and thus $\alpha_{i,j} = 0$ for all $i$ and $j$.

To show that $D$ spans $U\times V$ let $\mathbf{u}\otimes\mathbf{v}\in U\otimes V$. Since $\mathbf{u} = \sum_{i\in I}\alpha_i \mathbf{b}_i$ and $\mathbf{v} = \sum_{j\in J}\beta_j \mathbf{c}_j$ we have

$$
\begin{align*}
  \mathbf{u}\otimes\mathbf{v} =& \sum_{i\in I}\alpha_i \mathbf{b}_i \otimes \sum_{j\in J} \beta_j \mathbf{c}_j \\
  =& \sum_{i\in I} \alpha_i \left( \mathbf{b}_i \otimes \sum_{j\in J} \beta \mathbf{c}_j \right) \\
  =& \sum_{i\in I} r_i \left(\sum_{j\in J} (\mathbf{b}_i \otimes \mathbf{c}_j) \right) \\
  =& \sum_{i\in I, j\in J} \alpha_i \beta_j (\mathbf{b}_i \otimes \mathbf{c}_j)
\end{align*}
$$

Hence, any sum of elements of the form $\mathbf{u}\otimes\mathbf{v}$ is a linear combination of the vectors $\mathbf{b}_i \otimes \mathbf{c}_j$.
</details>
</MathBox>

<MathBox title='Tensor product dimension' boxType='proposition'>
For finite dimensional vector spaces

$$
  \dim(U\otimes V) = \dim(U)\cdot\dim(V)
$$
</MathBox>

## Coordinate matrices and rank

If $U$ and $V$ are vector spaces over $\mathbb{F}$ with respective bases $B = \Set{\mathbf{b}_i}_{i\in I}$ and $C = \Set{\mathbf{c}_j}_{j\in J}$, then any vector $\mathbf{w}\in U\otimes V$ can be expressed as a summand

$$
  \mathbf{w} = \sum_{i\in I, i\in J} \rho_{i,j}(\mathbf{b}_i\otimes \mathbf{c}_j)
$$

where only a finite number of the scalars $\rho_{i,j}\in\mathbf{F}$ are nonzero. In fact, for a fixed $\mathbf{w}\in U\otimes V$, we can reindex the bases so that

$$
  \mathbf{w} = \sum_{i=1}^a \sum_{j=1}^b \rho_{i,j}(\mathbf{b}_i\otimes \mathbf{c}_j)
$$

where none of the rows or columns of the matrix $\mathbf{R} = (\rho_{i,j})$ consists of only zeroes. The matrix $\mathbf{R}$ is called a *coordinate matrix* of $\mathbf{w}$ with respect to bases $B$ and $C$.

Suppose that $X = \Set{\mathbf{x}_i}_{i\in I}$ and $Y = \Set{\mathbf{y}_j}{j\in J}$ also are bases for $U$ and $V$, respectively and that

$$
  \mathbf{w} = \sum_{i=1}^c \sum_{j=1}^d \sigma_{i,j}(\mathbf{x}_i\otimes \mathbf{y}_j)
$$

where $\mathbf{S} = (\sigma_{i,j})$ is a coordinate matrix of $\mathbf{w}$ with respect to the bases. We claim that the coordinate matrices $\mathbf{R}$ and $\mathbf{S}$ have the same rank, which defines the rank of the tensor $\mathbf{w}\in U\otimes W$.

Each $\mathbf{x}_1,\dots,\mathbf{x}_c$ is a finite linear combination of basis vectors in $B$. We can reindex $B$ so that each $\mathbf{w}_k$ is a linear combination of the vectors $\Set{\mathbf{b}_i}_{i=1}^n$ where $a\leq n$ generating the subspace

$$
  U_n = \mathrm{span}\Set{\mathbf{b}_i}_{i=1}^n
$$

Next, extend $\Set{\mathbf{x}_i}_{i=1}^c$ to a basis $X' = \Set{\mathbf{x}_1,\dots,\mathbf{x}_c,\mathbf{x}_{c+1},\dots,\mathbf{x}_n}$ for $U_n$. Thus

$$
  \mathbf{x}_i = \sum_{h=1}^n \alpha_{i,h}\mathbf{u}_h,\; i=1,\dots,n
$$

where $\mathbf{A} = (\alpha_{i,h})$ is an invertible $n\times n$ matrix.

Repeating the same process for $V$, where we reindex $C$ so that the subspace $V_m = \mathrm{span}\Set{\mathbf{v}_j}_{j=1}^m$ contains $\mathbf{y}_1,\dots,\mathbf{y}_d$ and extend to a basis $Y' = \Set{\mathbf{y}_1,\dots,\mathbf{y}_d,\mathbf{y}_{d+1},\dots,\mathbf{y}_m}$ for $V_m$. Then

$$
  \mathbf{y}_j = \sum_{k=1}^m \beta_{j,k}\mathbf{v}_k,\; j=1,\dots,m
$$

where $\mathbf{B} = (\beta_{j,k})$ is an invertible $n\times n$ matrix. 

Next, write

$$
  \mathbf{w} = \sum_{i=1}^n \sum_{j=1}^m \rho_{i,j}(\mathbf{u}_i\otimes \mathbf{v}_j)
$$

by setting $\rho_{i,j} = 0$ for $i > a$ or $j > b$. Thus, the $n\times m$ matrix $\mathbf{R}_1 = (\rho_{i,j})$ comes from $\mathbf{R}$ by adding $n-a$ rows of zeroes to the bottom and the $m-b$ columns of zeroes. In particular $\mathbf{R}_1$ and $\mathbf{R}$ have the same rank.

The expression for $\mathbf{w}$ in terms of the bases $X$ and $Y$ can also be extended by zero scalars to

$$
  \mathbf{w} = \sum_{i=1}^n \sum_{j=1}^m \sigma_{i,j}(\mathbf{x}_i\otimes\mathbf{y}_j)
$$

where the $n\times m$ matrix $\mathbf{S}_1 = (\sigma_{i,j})$ has the same rank as $\mathbf{S}$.

Finally, we can compute

$$
\begin{align*}
  \sum_{i=1}^n \sum_{j=1}^m \mathbf{\sigma}(\mathbf{x}_i \otimes\mathbf{y}_j) =& \sum_{i=1}^n \sum_{j=1}^m \sigma_{i,j}\left(\sum_{h=1}^n \alpha_{i,h}\mathbf{b}_h \otimes \sum_{k=1}^m \beta_{j,k}\mathbf{c}_k \right) \\
  =& \sum_{i=1}^n \sum_{j=1}^m \sum_{h=1}^n \sum_{k=1}^m \alpha_{i,j}\sigma_{i,j}\beta_{j,k}(\mathbf{b}_h \otimes \mathbf{c}_k) \\
  =& \sum_{h=1}^n \sum_{k=1}^m \sum_{j=1}^m \sum_{i=1}^n \left(\alpha_{h,i}^T \sigma_{i,j}\right)\beta_{j,k}(\mathbf{b}_h \otimes \mathbf{c}_k) \\
  =& \sum_{h=1}^n \sum_{k=1}^m \sum_{j=1}^m [\mathbf{A}^T \mathbf{S}_1]_{h,j} \beta_{j,k} (\mathbf{b}_h \otimes \mathbf{c}_k) \\
  =& \sum_{h=1}^n \sum_{k=1}^m [\mathbf{A}^T \mathbf{S}_1 \mathbf{B}]_{h,k} (\mathbf{b}_h \otimes \mathbf{c}_k)
\end{align*}
$$

yielding

$$
  \sum_{i=1}^n \sum_{j=1}^m \rho_{i,j}(\mathbf{b}_i \otimes\mathbf{c}_j) = \sum_{h=1}^n \sum_{k=1}^m [\mathbf{A}^T \mathbf{S}_1 \mathbf{B}]_{h,k} (\mathbf{b}_h \otimes \mathbf{c}_k)
$$

It follows that $\mathbf{R}_1 = \mathbf{A}^T \mathbf{S_1} \mathbf{B}$. Since $A$ and $B$ are invertible, we deduce that

$$
  \mathrm{rank}(\mathbf{R}) = \mathrm{rank}(\mathbf{R}_1) = \mathrm{rank}(\mathbf{S}_1) = \mathrm{rank}(\mathbf{S})
$$

In block terms

$$
  \mathbf{R_1} = \begin{bmatrix} \mathbf{R} & 0 \\ 0 & 0 \end{bmatrix} \quad\text{and}\quad \mathbf{S_1} = \begin{bmatrix} \mathbf{S} & 0 \\ 0 & 0 \end{bmatrix}
$$

and

$$
  \mathbf{A}^T = \begin{bmatrix} \mathbf{A}_{a,c}^T & - \\ - & - \end{bmatrix} \quad\text{and}\quad \mathbf{B} = \begin{bmatrix} \mathbf{B}_{d,b} & - \\ - & - \end{bmatrix}
$$

Then $\mathbf{R}_1 = \mathbf{A}^T \mathbf{S_1} \mathbf{B}$ implies that

$$
  \mathbf{R} = \mathbf{A}_{a,c}^T \mathbf{S_1} \mathbf{B}_{d,b}
$$

where $\mathrm{rank}(\mathbf{A}_{c,d}^T)\geq\mathrm{rank}(\mathbf{R})$ and $\mathrm{rank}(\mathbf{B})\geq\mathrm{rank}(\mathbf{R})$.

In the special case

$$
  \mathbf{w} = \sum_{i=1}^r \mathbf{b}_i \otimes \mathbf{c}_j = \sum_{i=1}^r \mathbf{x}_i \otimes\mathbf{y}_i
$$

it follows that $a=b=c=d=r$ and $\mathbf{R} = \mathbf{S} = \mathbf{I}_r$ with

$$
  \mathbf{R_1} = \begin{bmatrix} \mathbf{I}_r & 0 \\ 0 & 0 \end{bmatrix} \quad\text{and}\quad \mathbf{S_1} = \begin{bmatrix} \mathbf{I}_r & 0 \\ 0 & 0 \end{bmatrix}
$$

Thus, the equation  $\mathbf{R} = \mathbf{A}_{a,c}^T \mathbf{S} \mathbf{B}_{d,b}$ becomes

$$
  \mathbf{I}_r = \mathbf{A}_{r,r}^t \mathbf{B}_{r,r}
$$

such that for $\mathbf{A}_{r,r} = (\alpha_{i,h})$

$$
  \mathbf{x}_i = \sum_{i=1}^r \alpha_{i,h}\mathbf{b}_h,\; i=1,\dots,r
$$

and for $\mathbf{B}_{r,r} = (\beta_{j,k})$

$$
  \mathbf{y}_j = \sum_{k=1}^r \beta_{j,k} \mathbf{c}_k,\; j=1,\dots,r
$$

### The rank of a decomposable tensor

A tensor of the form $\mathbf{u}\otimes\mathbf{v}\in U\otimes V$ for called decomposable. If $B = \Set{\mathbf{b}_i}_{i\in I}$ is a basis for $U$ and $C = \Set{\mathbf{c}_j}_{j\in J}$ is a basis for $V$, then any decomposable tensor has the form

$$
  \mathbf{u}\otimes\mathbf{v} = \sum_{i\in I, j\in J} \rho_i \sigma_j (\mathbf{b}_i \otimes\mathbf{c}_j)
$$

Hence, the rank of a decomposable tensor is $1$.

## Characterizing vectors in a tensor product

<MathBox title='Representations of a tensor product' boxType='proposition'>
Let $U$ and $V$ be vector spaces over $\mathbb{F}$ with respective bases $B = \Set{\mathbf{b}_i}_{i\in I}$ and $C = \Set{\mathbf{c}_j}_{j\in J}$. Then every $\mathbf{w} \in U\otimes V$ has a unique expression as a finite sum (up to order of zero terms) of the forms

$$
\begin{align}
  \sum_{i\in I,j\in J}& \rho_{i,j} \mathbf{b}_i \times \mathbf{c}_j,\; \rho_{i,j}\in\mathbb{F} \\
  \sum_{i\in I,j\in J}& \mathbf{b}_i \times \mathbf{v}_j,\; \mathbf{v}_j \in V
  \sum_{i\in I}& \mathbf{u}_i \otimes \mathbf{c}_i,\; \mathbf{u}_i \in U
\end{align}
$$

**(4)** Every nonzero $\mathbf{w}\in U\otimes V$ has an expression of the form

$$
  \sum_{i=1}^n \mathbf{u}_i \otimes\mathbf{v}_i
$$

where $\Set{\mathbf{u}_i}_{i\in I}\subseteq U$ and $\Set{\mathbf{v}_j}_{j\in J}\subseteq V$ are linearly independent sets. As to uniqueness, $n$ is the rank of $\mathbf{w}$ and so it is unique. Also, we have

$$
  \sum_{i=1}^r \mathbf{u}_i \otimes\mathbf{v}_i = \sum_{i=1}^r \mathbf{x}_i \otimes\mathbf{y}_i
$$

where $\Set{\mathbf{x}_i}_{i\in I}\subseteq U$ and $\Set{\mathbf{y}_j}_{j\in J}\subseteq V$, if and only if there exists $r \times r$ matrices $\mathbf{A} = (\alpha_{i,j})$ and $\mathbf{B} = (\beta_{i,j})$ for which $\mathbf{A}^T \mathbf{B} = \mathbf{I}_r$ such that for $j=1,\dots,r$

$$
\begin{align*}
  \mathbf{x}_i =& \sum_{j=1}^r \alpha_{i,j}\mathbf{u}_j \\
  \mathbf{y}_i =& \sum_{j=1}^r \alpha_{i,j}\mathbf{v}_j
\end{align*}
$$

<details>
<summary>Proof</summary>

**(1):** The first sum merely expresses the fact that $D = \Set{ \mathbf{b}_i \otimes\mathbf{c}_i | i\in I, j\in J }$ is a basis for $U\otimes V$

**(2):** From the first sum, we write

$$
\begin{align*}
  \sum_{i\in I, j\in J} \rho_{i,j}\mathbf{b}_i \otimes\mathbf{c}_j =& \sum_{i\in I} \left( \mathbf{b}_i \otimes \sum_{j\in J} \rho_{i, j} \mathbf{c}_j \right) \\
  =& \sum_{i\in I} \mathbf{b}_i \otimes \mathbf{v}_i
\end{align*}
$$

The uniqueness of the sum follows from the linear independence of $C$.

**(3):** Similarly, we write the first sum as

$$
\begin{align*}
  \sum_{i\in I, j\in J} \rho_{i,j}\mathbf{b}_i \otimes\mathbf{c}_j =& \sum_{j\in J} \left( \sum_{i\in I} \rho_{i,j} \mathbf{b}_i \right) \otimes \mathbf{c}_j \right \\
  =& \sum_{j\in J} \mathbf{u}_j \otimes \mathbf{c}_j
\end{align*}
$$

The uniqueness of the sum follows from the linear independence of $B$.


**(4)**: Assume from the second sum that none of $\mathbf{v}_i$ are zero. If the set $\Set{\mathbf{v}_i}$ is linearly independent, we are done. If not, then we may suppose (after reindexing if necessary) that

$$
  \mathbf{y}_n = \sum_{i=1}^{n-1}\rho_i \mathbf{v}_i
$$

Then

$$
\begin{align*}
  \sum_{i=1}^n \mathbf{b}_i \otimes\mathbf{v}_i =& \sum_{i=1}^{n-1}\mathbf{b}_i \otimes\mathbf{v}_i + \left( \mathbf{b}_n \otimes \sum_{i=1}^{n-1} \rho_i \mathbf{v}_i \right) \\
  =& \sum_{i=1}^{n-1} \mathbf{b}_i \otimes\mathbf{v}_i + \sum_{i=1}^{n-1} (\rho_i \mathbf{v}_i) \\
  =& \sum_{i=1}^{n-1} (\mathbf{v}_i + \rho_i \mathbf{v}_n) \otimes\mathbf{v}_i
\end{align*}
$$

The vectors $\Set{\mathbf{b}_i + \rho_i \mathbf{b}_n | 1\leq i \leq n-1}$ are linearly independent. This reduction can be repeated until the second coordinates are linearly independent. Also, the identity matrix $\mathbf{I}_n$ is a coordinate matrix for $\mathbf{w}$ giving $n = \mathrm{rank}(\mathbf{I}_n) = \mathrm{rank}(\mathbf{w})$.
</details>
</MathBox>

## The tensor product of linear transformations

<MathBox title='Dual tensor product isomorphism' boxType='proposition'>
If $U$ and $V$ are finite-dimensional vector spaces over $\mathbb{F}$, then

$$
  U^* \otimes V^* \cong (U \otimes V)^*
$$

via the isomorphism $\mathrm{T}:U^* \otimes V^* \to (U\otimes V)^*$ given by

$$
  \mathrm{T}(f\otimes g)(\mathbf{u}\otimes\mathbf{v}) = f(\mathbf{u})g(\mathbf{v})
$$

Since $(U \times V)^* \cong \hom(U\times V; \mathbb{F})$ it follows that

$$
  U^* \otimes V^* \cong (U\otimes V)^* \cong \hom(U\times V, \mathrm{F})
$$

<details>
<summary>Proof</summary>

For fixed $f$ and $g$, the map $F_{f,g}: U\times V \to \mathbb{F}$ defined by $F_{f,g} = f(\mathbf{u})g(\mathbf{v})$ is bilinear. By the universal property of tensor products, there exists a unique linear functional $\ell_{f, g}(\mathbf{u}\otimes\mathbf{v}) = F_{f,g} (\mathbf{u},\mathbf{v}) = f(\mathbf{u})g(\mathbf{v})$.

Next, the map $G: U^* \to V^* \to (U\otimes V)^*$ defined by $G(f,g) = \ell_{f,g}$ is bilinear since for $\alpha,\beta\in\mathbb{F}$ and $h\in V^*$

$$
\begin{align*}
  G(\alpha f + \beta, g, h)(\mathbf{u}\otimes\mathbf{v}) =& \ell_{\alpha f+ \beta g, h} (\mathbf{u}\otimes\mathbf{v}) \\
  =& (\alpha f + \beta g)(\mathbf{u})\cdot h(\mathbf{v}) \\
  =& \alpha f(\mathbf{u}) h(\mathbf{v}) + \beta g(\mathbf{u})h(\mathbf{v}) \\
  =& (\alpha\ell_{f,h} + \beta\ell_{g,h})(\mathbf{u}, \mathbf{v}) \\
  =& (\alpha G(f,h) + \beta G(g,h))(\mathbf{u}\otimes\mathbf{v})
\end{align*}
$$

showing that $G$ is linear in the first coordinate, and similar for the second. By the universal property, there exists a unique linear map $\mathrm{T}: U^* \otimes V^* \to (U\otimes V)^*$ for which

$$
  \mathrm{T}(f\otimes g) = G(f,g) = \ell_{f,g}
$$

that is,

$$
  \mathrm{T}(f\otimes g) = (\mathbf{u}\otimes\mathbf{v}) = \ell_{f,g}(\mathbf{u}\otimes\mathbf{v}) = f(\mathbf{u})g(\mathbf{v})
$$

It remains to show that $\mathrm{T}$ is bijective. Let $\Set{\mathbf{b}_i}_{i\in I}$ be a basis for $U$, with dual basis $\Set{b_i}_{i\in I}$ and let $\Set{\mathbf{c}_j}_{j\in J}$ be a basis for $V$ with dual basis $\Set{ c_j}_{j\in J}$. Then

$$
\begin{align*}
  \mathrm{T}(b_i \otimes c_i)(\mathbf{b}_u \otimes \mathbf{c}_v) = b_i(\mathbf{b}_u) c_j (\mathbf{c}_v) = \delta_{i,u}\delta_{j,v} = \delta_{(i,j),(u,v)}
\end{align*}
$$

showing that $\Set{\mathrm{T}(b_i \otimes c_i) }\subseteq (U\otimes V)^*$ is the dual basis to basis $\Set{\mathbf{b}_u \otimes \mathbf{c}_v}$ for $U\otimes V$. Thus, $\mathrm{T}$ takes the basis $\Set{ b_i \otimes c_j }$ for $U^* \otimes V^*$ to the basis $\Set{\mathrm{T}(b_i \otimes c_j}$ and is therefore bijective.
</details>
</MathBox>

<MathBox title='Linear transformation of tensor products' boxType='proposition'>
The linear transformation

$$
  \Theta: \mathcal{L}(U, U') \otimes \mathcal{L}(V, V') \to\mathcal{L}(U\otimes V, U' \otimes V') 
$$

defined by $\Theta(\mathrm{T}\otimes\mathrm{S}) = \mathrm{T}\odot\mathrm{S}$ where

$$
  (\mathrm{T}\odot\mathrm{S})(\mathbf{u}\otimes\mathbf{v}) = \mathrm{T}(\mathbf{u})\mathrm{S}(\mathbf{v})
$$

is an embedding (injective linear transformation), and is an isomorphism if all vector spaces are finite-dimensional. Thus, the tensor product $\mathrm{T}\otimes\mathrm{S}$ of all linear transformation is a linear transformation on tensor products.

<details>
<summary>Proof</summary>

For fixed $\mathrm{T}:U\to U'$ and $\mathrm{S}:V\to V'$ the tensor product defined by $f(\mathbf{u}, \mathbf{v}) = \mathrm{T}(\mathbf{u}\otimes\mathrm{S}(\mathbf{v})$ is bilinear. By the universal property for bilinear transformations, there is a unique linear map $(\mathrm{T}\odot\mathrm{S}):U\otimes V \to U' \otimes V'$ for which

$$
  (\mathrm{T}\odot\mathrm{S})(\mathbf{u}\otimes\mathrm{v}) = \mathrm{T}(\mathbf{u})\mathrm{S}(\mathbf{v})
$$

Since $\mathrm{T}\odot\mathrm{S}\in\mathcal{L}(U,U') \times \mathcal{L}(V,V') \to \mathcal{L}(U\otimes V, U' \otimes V')$, there is a function

$$
  F:\mathcal{L}(U, U')\times\mathcal{L}(V,V')\to \mathcal{L}(U\otimes V, U'\otimes V')
$$

defined by

$$
  F(\mathrm{T}, \mathrm{S}) = \mathrm{T}\odot\mathrm{S}
$$

which is linear since for $\alpha,\beta\in\mathbb{F}$ and $\mathrm{R}\in\mathcal{L}(U,U')$

$$
\begin{align*}
  ((\alpha\mathrm{T} + \beta\mathrm{R})\odot\mathrm{S})(\mathbf{u, \mathbf{v}}) =& (\alpha\mathrm{T} + \beta\mathrm{R})(\mathbf{u})\otimes\mathrm{S}(\mathbf{v}) \\
  =& (\alpha\mathrm{T}(\mathbf{u}) + \beta\mathrm{R}(\mathbf{u}))\otimes\mathrm{S}(\mathbf{v}) \\
  =& \alpha[\mathrm{T}(\mathbf{u})\otimes\mathrm{S}(\mathbf{v})] + \beta[\mathrm{R}(\mathbf{u})\otimes\mathbf{v}] \\
  =& \alpha(\mathrm{T}\odot\mathrm{S})(\mathbf{u},\mathbf{v}) + \beta(\mathrm{R},\mathrm{S})(\mathbf{u, \mathbf{v}}) \\
  =& (\alpha(\mathrm{T}\odot\mathrm{S})+ \beta(\mathrm{R}\odot\mathrm{S}))(\mathbf{u},\mathbf{v}) 
\end{align*}
$$

showing that $F$ is linear in the first coordinate, and similarly for the second. By the universal property for bilinear maps there is a unique linear transformation

$$
  \Theta: \mathcal{L}(U,U')\otimes\mathcal{L}(V,V') \to \mathcal{L}(U\otimes V, U'\otimes V')
$$

satisfying $\Theta(\mathrm{T}\otimes\mathrm{S}) = \mathrm{T}\odot\mathrm{S}$, i.e.

$$
  [\Theta(\mathrm{T}\odot\mathrm{S})](\mathbf{u}\odot\mathbf{v}) = \mathrm{T}(\mathbf{u})\mathrm{S}(\mathbf{v})
$$

To see that $\Theta$ is injective, if $\Theta(\mathrm{T}\odot\mathrm{S}) = 0$ then $\mathrm{T}(\mathbf{u})\otimes\mathrm{S}(\mathbf{v}) = 0$ for all $\mathbf{u}\in U$ and $\mathbf{v}\in V$. If $\mathrm{S} = 0$, then $\mathrm{T}\otimes\mathrm{S} = 0$. If $\mathrm{S}\neq 0$, then there is a $\mathbf{v}\in V$ for which $\mathrm{S}(\mathbf{v}) \neq\mathbf{0}$. However, $\mathrm{T}(\mathbf{u})\otimes\mathrm{T}(\mathbf{v}) = \mathbf{0}$ implies that one of the factors is $0$ and so $\mathrm{T}(\mathbf{u}) = \mathbf{0}$ for all $\mathbf{u}\in U$, i.e. $\mathrm{T} = 0$. Hence $\mathrm{T}\otimes\mathrm{S} = 0$. In either case, we see that $\Theta$ is injective.

Thus, $\Theta$ is an embedding and if all vector spaces are finite dimensional then

$$
  \dim(\mathcal{L}(U, U')\otimes\mathcal{L}(V,V')) = \dim(\mathcal{L}(U\otimes V, U'\otimes V'))
$$

showing that $\Theta$ is also subjective and thus an isomorphism.

The embedding of $\mathcal{L}(U,U')\otimes\mathcal{L}(V,V')$ into $\mathcal{L}(U\otimes V, U'\otimes V')$ means that each $\mathrm{T}\otimes\mathrm{S}$ can be thought of as the linear transformation $\mathrm{T}\odot\mathrm{S}$ from $U\otimes V$ to $U' otimes V'$ defined by

$$
  (\mathrm{T}\odot\mathrm{R})(\mathbf{u}\otimes\mathrm{v}) = \mathrm{T}(\mathbf{u}) \odot\mathrm{R}(\mathbf{v})
$$
</details>
</MathBox>

<MathBox title='' boxType='corollary'>
If $X$ an $Y$ are finite-dimensional vector spaces let $X\xhookrightarrow{\cong} Y$ denote that $X$ and $Y$ are isomorphic through an embedding. Consider the following special cases of the linear map for finite-dimensional vector spaces $U$ and $V$ over $\mathbb{F}$

$$
  \Theta: \mathcal{L}(U, U') \otimes \mathcal{L}(V, V') \to\mathcal{L}(U\otimes V, U' \otimes V') 
$$

1. Taking $U' = \mathbb{F}$ gives

$$
  U^* \otimes\mathcal{L}(V,V') \xhookrightarrow{\cong} \mathcal{L}(U\otimes V, V')
$$

where $(f \otimes \mathrm{S})(\mathbf{u}\otimes\mathbf{v}) = f(\mathbf{u})\mathrm{S}(\mathbf{v})$ for $f\in U^*$.
2. Taking $U' = \mathbb{F}$ and $V' = \mathbb{F}$ gives

$$
  U^* \otimes V^* \xhookrightarrow{\cong} (U\otimes V)^*
$$

where $(f\otimes g)(\mathbf{u}\otimes\mathbf{v}) = f(\mathbf{u})g(\mathbf{v})$.
3. Taking $V = \mathbb{F}$ and noting that $\mathcal{L}(\mathbb{F},V') \cong V'$ and $U\otimes\mathbb{F}\cong U$ gives

$$
  \mathcal{L}(U,U') \otimes W \xhookrightarrow{\cong} \mathcal{L}(U, U'\otimes W)
$$

where $(\mathrm{T}\otimes \mathbf{w})(\mathbf{u}) = \mathrm{T}(\mathbf{u})\otimes\mathbf{w}$.
4. Taking $U' = \mathbb{F}$ and $V = \mathbb{F}$ gives (letting $W = V'$)

$$
  U^* \otimes W \xhookrightarrow{\cong} \mathcal{L}(U,W)
$$

where $(f\otimes \mathbf{w})(\mathbf{u}) = f(\mathbf{u})\mathbf{w}$.
</MathBox>

## Change of base field

Let $V_{\mathbb{F}}$ be a vector space over $\mathbb{F}$ and suppose that $\mathbb{K}\supseteq\mathbb{F}$ is an extension field of $\mathbb{K}$. The vector space $V_{\mathbb{F}}$ can be extended over $\mathbb{K}$ via a tensor product of the form $W_{\mathbb{F}} = \mathbb{K}\otimes_{\mathbb{F}} V_{\mathbb{F}}$, where $\otimes_{\mathbb{F}}$ denotes that the tensor product is taken with respect to $\mathbb{F}$. In this case, the subscript can be dropped since the only tensor product that makes sense in $\mathbb{K}\otimes V_{\mathbb{F}}$ is the $\mathbb{F}$-tensor product as $V_{\mathbb{F}}$ is not a $\mathbb{K}$-space.

The vector space $W_{\mathbb{F}}$ is an $\mathbb{F}$-space by definition of the tensor product, but can be extended into a $\mathbb{K}$-space as follows. Intuitively, a scalar $\alpha\in\mathbb{K}$ should be absorbed into the first coordinate of the tensor product, i.e.

$$
  \alpha(\beta\otimes\mathbf{v}) = (\alpha\beta)\otimes\mathbf{v}
$$

and for this to be well-defined we require that

$$
  \beta\otimes\mathbf{v} = \gamma\otimes\mathbf{w} \implies (\alpha\beta)\otimes\mathbf{v} = (\alpha\gamma)\otimes\mathbf{w}
$$

This can be ensured with the universal property for bilinearity by considering the map $F_\alpha: (\mathbb{K}\times V_{\mathbb{F}})\to (\mathbb{K}\otimes V_{\mathbb{F}})$ defined by $F_\alpha (\beta, \mathbf{v}) = (\alpha\beta)\otimes\mathbf{v}$. This map is obviously well-defined and since it is also bilinear, the universal property of tensor products implies that there is a unique $\mathbb{F}$-linear map $\mathrm{T}_\alpha (\mathbb{K}\otimes V_{\mathbb{F}} \to (\mathbb{K}\otimes V_{\mathbb{F}})$ for which

$$
  \mathrm{T}_\alpha (\beta\otimes\mathbf{v}) = (\alpha\beta)\otimes\mathbf{v}
$$

Since $\mathrm{T}_\alpha$ is $\mathbb{F}$-linear, it is additive such that

$$
  \mathrm{T}_\alpha [(\beta\otimes\mathbf{v}) + (\gamma\otimes\mathbf{w})] = \mathrm{T}_\alpha (\beta\otimes\mathbf{v}) + \mathrm{T}_\alpha (\gamma\otimes\mathbf{w})
$$

that is

$$
 alpha [(\beta\otimes\mathbf{v}) + (\gamma\otimes\mathbf{w})] = \alpha (\beta\otimes\mathbf{v}) + \alpha (\gamma\otimes\mathbf{w})
$$

which is one the properties required of a scalar multiplication. Since the other defining properties of scalar multiplication are satisfied, the set $\mathbb{K}\otimes V_{\mathbb{F}}$ is indeed a $\mathbb{K}$-space under this operation (and addition), which we denote by $W_{\mathbf{K}}$.

At this stage, we have have three distinct vector spaces: the $\mathbb{F}$-spaces $V_{\mathbb{F}}$ and $W_{\mathbb{F}} = \mathbb{K}\otimes V_{\mathbb{F}}$ and the $\mathbb{K}$-space $W_{\mathbb{K}} = \mathbb{K}\otimes V_{\mathbb{F}}$, where the tensor product in both cases is with respect to $\mathbb{F}$. The spaces $W_{\mathbb{F}}$ and $W_{\mathbb{K}}$ are identical as sets and abelian groups, and only differ in the base field for scalar multiplication. Accordingly, we can recover $W_{\mathbb{F}}$ from $W_{\mathbb{K}}$ simply by restricting scalar multiplication to scalars from $\mathbb{F}$.

This suggests that there exists $\mathbb{F}$-linear maps $\mathbf{T}:V_{\mathbb{F}}\to W_{\mathbb{K}}$ such that for $\alpha,\beta\in\mathbb{F}$

$$
  \mathrm{T}(\alpha\mathbf{u} + \beta\mathbf{v}) = \alpha\mathrm{T}(\mathbf{u}) + \beta\mathrm{T}(\mathbf{v})
$$

If the dimension of $\mathbb{K}$ as a vector space over $\mathbb{F}$ is $d$ then

$$
\begin{align*}
  \dim(W_{\mathbb{F}}) =& \dim(\mathbb{K}\otimes V_{\mathbb{F}}) \\
  =& \dim(\mathbb{K})\cdot\dim(V_{\mathbb{F}}) = d\cdot\dim(V_{\mathbb{F}})
\end{align*}
$$

As to the dimension of $W_{\mathbb{K}}$ it is not hard to so that if $\Set{\mathbf{b}_i}_{i\in I}$ is a basis for $V_{\mathbb{F}}$ then $\Set{\mathbf{1}\otimes\mathbf{b}_i}_{i\in I}$ is a basis for $W_{\mathbb{K}}$, hence

$$
  \dim(W_{\mathbb{K}}) = \dim(\mathbb{V}_{\mathbb{F}})
$$

even when $V_{\mathbb{F}}$ is finite-dimensional.

The map $\mathrm{F}: V_{\mathbb{F}}\to W_{\mathbb{F}}$ defined by $F(\mathbf{v}) = \mathbf{1}\otimes\mathbf{v}$ is easily seen to be injective and $\mathbb{F}$-linear, and so $W_{\mathbb{F}}$ contains an isomorphic copy of $V_{\mathbb{F}}$. We can also think of $\mathrm{F}$ as mapping $V_{\mathbb{F}}$ into $W_{\mathbb{K}}$, in which case $\mathrm{F}$ is called the $\mathbb{K}$-extension map of $V_{\mathbb{F}}$.

<MathBox title='Universal property for exension maps' boxType='proposition'>
The $\mathbb{K}$-extension map $\mathrm{F}: V_{\mathbb{F}} \to W_{\mathbb{K}}$ has the universal property for the collection of all $\mathbb{F}$-linear maps with domain $V_{\mathbb{F}}$ and codomain a $\mathbb{K}$-space, as measured by $\mathbb{K}$-linear maps. In particular, for any $\mathbb{F}$-linear map $\mathrm{T}: V_{\mathbb{F}} \to Y_{\mathbb{K}}$, there exists a unique $\mathbb{K}$-linear map $\mathrm{K}: W_{\mathbb{K}}\to Y_{\mathbb{K}}$ for which

$$
  \mathrm{K}\circ \mathrm{F} = \mathrm{T}
$$

<details>
<summary>Proof</summary>

If such a map $\mathrm{K}:\mathbb{K}\otimes V_{\mathbb{F}}\to  Y_{\mathbb{K}}$ exists, then it must satisfy

$$
\begin{align*}
  \mathrm{K}(\beta\otimes\mathbf{v}) =& \beta\mathrm{K}(\mathbf{1}\otimes\mathbf{v}) \\
  =& \beta\mathrm{K}\mathrm{F}(\mathbf{v}) = \beta\mathrm{T}(\mathbf{v})
\end{align*}
$$

This shows that if $\mathrm{K}$ exists, it is uniquely determined by $\mathrm{T}$. Let $G: (\mathbb{K}\times V_{\mathbb{F}})\to Y$ be defined by

$$
  G(\beta,\mathbf{v}) = \beta\mathrm{T}(\mathbf{v})
$$

Since this is bilinear, there exists a unique $\mathbb{F}$-linear map $\mathrm{K}$ for which $\mathrm{K}(\beta\otimes\mathbf{v}) = \beta\mathrm{T}(\mathbf{v})$. It is easy to se that $\mathrm{K}$ is also $\mathbb{K}$-linear, since if $\alpha\in\mathbb{K}$ then

$$
\begin{align*}
  \mathrm{K}[\alpha(\beta\otimes\mathbf{v})] =& \mathrm{K}(\alpha\beta\otimes\mathbf{v}) \\
  =& \alpha\beta\mathrm{T}(\mathbf{v}) \\
  =& \alpha\mathrm{K}(\beta\otimes\mathbf{v})
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Extension linear map' boxType='proposition'>
Let $V$ and $W$ be $\mathbb{F}$-spaces with $\mathbb{K}$-extension maps $\mathrm{F}_V$ and $\mathrm{F}_W$, respectively. Then for any $\mathbb{F}$-linear map $\mathrm{K}:V\to W$, the map $\bar{\mathrm{K}} = \mathrm{I}_\mathbb{K} \otimes\mathrm{K}$ is the unique linear map for which

$$
  \mathrm{F}_W \circ \mathrm{K} = \bar{\mathrm{K}} \circ \mathrm{F}_V
$$

according to the following diagram.

$$
\begin{CD}
  V @>\mathrm{K}>> W \\
  @A{\mathrm{F}_V}AA @AA{\mathrm{F}_W}A \\
  \mathbb{K}\otimes V_{\mathbb{F}} @>>\bar{\mathrm{K}}> \mathbb{K}\otimes W_{\mathbb{K}}
\end{CD}
$$

<details>
<summary>Proof</summary>

Consider the $\mathbb{F}$-linear map $\mathrm{L} = (\mathrm{F}_W \circ\mathrm{K}):V\to\mathbb{K}\otimes W_{\mathbb{K}}$. By the universal property of the $\mathbb{K}$-extension map, there is a unique $\mathbb{K}$-linear map $\bar{\mathrm{K}}:\mathbb{K}\otimes V_{\mathbb{K}}\to\mathbb{K}\otimes W_{\mathbb{K}}$ for which $\bar{\mathrm{K}} \circ\mathrm{F}_V = \mathrm{L} = \mathrm{F}_W \circ \mathrm{K}$, satisfying

$$
\begin{align*}
  \bar{\mathrm{K}}(\beta\otimes\mathbf{v}) =& \beta\bar{\mathrm{K}}(\mathbf{1}\otimes\mathbf{v}) \\
  =& \beta(\bar{\mathrm{K}}\circ\mathrm{F}_V)(\mathbf{v}) \\
  =& \beta(\mathrm{F}_W \circ\mathrm{K})(\mathbf{v}) \\
  =& \beta(\mathbf{1}\otimes\mathrm{K}(\mathbf{v})) \\
  =& \beta\otimes\mathrm{K}(\mathbf{v}) \\
  =& (I_{\mathbb{K}}\otimes\mathrm{K})(\beta\otimes\mathbf{v})
\end{align*}
$$
</details>
</MathBox>

## Multilinear maps and iterated tensor products

<MathBox title='Multilinear map' boxType='definition'>
If $V_1,\dots,V_n$ and $W$ are vector spaces over $\mathbb{F}$, a function $F:V_1 \times\cdots\times V_n \to W$ is multilinear if it is linear in each variable separately, i.e. for each $k=1,\dots,n$ and $\alpha,\beta\in\mathbb{F}$

$$
\begin{align*}
  F&(\mathbf{u}_1,\dots,\mathbf{u}_{k-1},\alpha\mathbf{v} + \beta\mathbf{v}',\mathbf{u}_{k+1},\dots,\mathbf{u}_n) = \\
  \alpha F&(\mathbf{u}_1,\dots,\mathbf{u}_{k-1},\mathbf{v},\mathbf{u}_{k+1},\dots,\mathbf{u}_n) + \beta F(\mathbf{u}_1\,dots,\mathbf{u}_{k-1},\mathbf{v}',\mathbf{u}_{k+1},\dots,\mathbf{u}_n)
\end{align*}
$$

A multilinear function of $n$ variable is called an $n$-linear function. The set of all $n$-linear functions is denoted $\hom\left(\prod_{i=1}^n V_i; W\right)$. A multilinear function from $V_1 \times\cdots\times V_n$ to the base field $\mathbb{F}$ is called a multilinear form or $n$-form.
</MathBox>

<MathBox title='Universal property for multilinear maps' boxType='proposition'>
Let $V_1,\dots,V_n$ be vector spaces over $\mathbb{F}$. The pair $(\bigotimes_{i=1}^n V_i, T)$ where

$$
  T: \prod_{i=1}^n V_n \to\bigotimes_{i=1}^n V_i
$$

is the multilinear map defined by

$$
  T(\mathbf{v}_1,\dots,\mathbf{v}_n) = \mathbf{v}_1 \otimes\cdots\otimes\mathbf{v}_n
$$

has the following property. If $W$ is an $\mathbb{F}$-space and $F: \prod_{i=1}^n V_i \to W$ is any multilinear function, then there is a unique linear transformation $\mathrm{L}:\bigotimes_{i=1}^n V_i \to W$ such that $\mathrm{L}\circ T = F$.
</MathBox>

The tensor product construction from bases can be extended to multilinear functions as follows. Let $V_1,\dots,V_n$ be vector spaces with corresponding bases $B_i = \Set{\mathbf{b}_{i,j}}_{j\in J_i}$. For each ordered $n$-tuple $(\mathbf{b}_{k,i_k})_{k=1}^n$, we invent a new formal symbol $\mathbf{b}_{1,i_1}\otimes\cdots\otimes\mathbf{b}_{n,i_n}$ and define a vector space $\bigotimes_{i=1}^n V_i$ with basis

$$
  D = \Set{ \mathbf{b}_{i, i_1} \otimes\cdots\otimes\mathbf{b}_{n,i_n} | \mathbf{b}_{k, i_k}\in B_k, k=1,\dots,n }
$$

Then we define a multilinear map $\otimes:\prod_{i=1}^n \to \bigotimes_{i=1}^n V_i$ by

$$
  \otimes(\mathbf{b}_{i, i_1},\dots,\mathbf{b}_{n,i_n}) = \mathbf{b}_{i, i_1} \otimes\cdots\otimes\mathbf{b}_{n,i_n} 
$$

and extend to multilinearity. This uniquely defines a multilinear map $\otimes$, that is universal for multilinear maps. Indeed, if $F:\prod_{i=1}^n V_i \to W$ is multilinear, the universal condition $F = \mathrm{L}\circ\otimes$ is equivalent To

$$
  \mathrm{L}(\mathbf{b}_{i, i_1} \otimes\cdots\otimes\mathbf{b}_{n,i_n}) = F(\mathbf{b}_{i, i_1},\dots,\mathbf{b}_{n,i_n})
$$

which uniquely defines a linear map $\mathrm{L}:\bigotimes_{i=1}^n V_i \to W$. Hence, the pair $(\bigotimes_{i=1}^n V_i, \otimes)$ has the universal property for multilinearity.

<MathBox title='Iterated tensor product' boxType='definition'>
Let $V_1,\dots,V_n$ be vector spaces over $\mathbb{F}$ and let $T$ be the subspace of the free space $F$ on $V_1 \times\cdots\times V_n$ generated by all vectors of the form

$$
\begin{align*}
  \alpha(\mathbf{v}_1,\dots,\mathbf{v}_{k-1},\mathbf{u},\mathbf{v}_{k+1},\dots,\mathbf{v}) +& \beta(\mathbf{v}_1,\dots,\mathbf{v}_{k-1},\mathbf{u}',\mathbf{v}_{k+1},\dots,\mathbf{v}) \\
  -& (\mathbf{v}_1,\dots,\mathbf{v}_{k-1},\alpha\mathbf{u} + \beta\mathbf{u}',\mathbf{v}_{k+1},\dots,\mathbf{v})
\end{align*}
$$

for all $\alpha,\beta\in\mathbb{F}$, $\mathbf{u},\mathbf{u}' \in U$ and $\mathbf{v}_1,\dots,\mathbf{v}_n \in V$. The quotient space $F/T$ is called the *tensor product* of $V_1,\dots,V_n$, denoted 
  
$$
  \bigotimes_{i=1}^n V_i = V_1 \otimes\cdots\otimes V_n 
$$

The coset $(\mathbf{v}_i)_{i=1}^n + T$ is denoted $\mathbf{v}_1 \otimes\cdots\otimes\mathbf{v}_n$ such that any element of $V_1 \otimes\cdots\otimes V_n$ is a sum of decomposable tensors, i.e.

$$
  \sum_{i} \mathbf{v}_{i_1} \otimes\cdots\otimes\mathbf{v}_{i_n}
$$
</MathBox>

<MathBox title='Properties of tensor products' boxType='proposition'>
The tensor product has the following properties (for vector spaces over $\mathbb{F}$):
1. **(Associativity):** There exists an isomorphism

$$
  \mathrm{T}:\left(\bigotimes_{i=1}^n V_i \right)\otimes\left(\bigotimes_{j=1}^m W_m \right) \to V_1\otimes\cdots\otimes V_n\otimes W_1\otimes\cdots\otimes W_m
$$

for which

$$
  \mathrm{T}[(\mathbf{v}_1)\otimes\cdots\otimes\mathbf{v}_n)\otimes(\mathbf{w}_1\otimes\cdot\otimes\mathbf{w}_n)] = \mathbf{v}_1\otimes\cdots\otimes\mathbf{v}_n\otimes\mathbf{w}_1\otimes\cdots\otimes\mathbf{w}
$$

In particular

$$
  (U\otimes V)\otimes W \cong U\otimes (V\otimes W) \cong U\otimes V\otimes W
$$
2. **(Commutativity):** Let $\pi$ be any permutation of the indices $\Set{1,\dots,n}$. Then there is an isomorphism

$$
  \mathrm{S}: \bigotimes_{i=1}^n V_i \to \bigotimes_{i=1}^n V_{\pi(i)}
$$

for which

$$
  \mathrm{S}(\mathbf{v}_1 \otimes\cdots\otimes\mathbf{v}_n) = \mathbf{v}_{\pi(1)}\otimes\cdots\otimes\mathbf{v}_{\pi(n)}
$$
3. There is an isomorphism $\mathrm{R}_1:\mathbb{F}\otimes V\to V$ for which $\mathrm{R}_1 (\alpha\otimes\mathbf{v} = \alpha\mathbf{v}$, and similarly there is an isomorphism $\mathrm{R}_2:V\otimes\mathbb{F} \to V$ for which $\mathrm{R}_2(\mathbf{v}\otimes\alpha) = \alpha\mathbf{v}$. Hence $\mathbb{F}\otimes V \cong V \cong V\otimes\mathbb{F}$.
</MathBox>

<MathBox title='Tensor product isomorphism' boxType='proposition'>
Let $V_1,\dots,V_n$ and $W$ be vector spaces over $\mathbb{F}$. Then there is an isomorphism $\phi: \hom\left(\prod_{i=1}^n V_i; W \right) \to \mathcal{L}\left(\bigotimes_{i=1}^n V_i, W \right)$ such that

$$
  \hom\left(\prod_{i=1}^n V_i; W \right) \cong \mathcal{L}\left(\bigotimes_{i=1}^n V_i, W \right)
$$

If all vector spaces are finite-dimensional, then

$$
  \dim\left[\hom\left(\prod_{i=1}^n V_i; W\right)\right] = \dim(W) \prod_{i=1}^n \dim(V_i)
$$
</MathBox>

<MathBox title='Linear transformation of tensor products' boxType='proposition'>
The linear transformation

$$
  \Theta: \bigotimes_{i=1}^n \mathcal{L}(U_i, U'_i) \to \mathcal{L}(U_1\otimes\cdots\otimes U_n, U'_1 \otimes\cdot\otimes U'_n)
$$

defined by

$$
  \Theta\left(\bigotimes_{i=1}^n \mathrm{T}_i \right)\left(\bigotimes_{i=1}^n \mathbf{u}_i \right) = \bigotimes_{i=1}^n \mathrm{T}_i (\mathbf{u}_i)
$$

is an embedding (injective linear transformation), and is an isomorphism if all vector spaces are finite-dimensional. Thus, the tensor product $\bigotimes_{i=1}^n \mathrm{T}_i$ of linear transformations is a linear transformation on tensor products.

In particular,

$$
\begin{gather*}
  \bigotimes_{i=1}^n U_i^* \xhookrightarrow{\cong} \left(\bigotimes_{i=1}^n U_i \right)^* \\
  \left(f_i \otimes\cdots\otimes f_n \right)\left(\mathbf{u}_1 \otimes\cdots\otimes\mathbf{u}_n \right) = f_1(\mathbf{u}_1)\cdots f_n(\mathbf{u}_n)
\end{gather*}
$$

and

$$
\begin{gather*}
  \left(\bigotimes_{i=1}^n U_i^* \right)\otimes V \xhookrightarrow{\cong} \mathcal{L}\left(\bigotimes_{i=1}^n U_i, V \right)^* \\
  \left(f_i \otimes\cdots\otimes f_n \otimes\mathbf{v}\right)\left(\mathbf{u}_1 \otimes\cdots\otimes\mathbf{u}_n \right) = f_1(\mathbf{u}_1)\cdots f_n(\mathbf{u}_n)\mathbf{v}
\end{gather*}
$$
</MathBox>

## Tensor spaces

Let $V$ be a finite-dimensional vector space over $\mathbb{F}$. For $p, q\in\mathbb{N}$, the tensor product

$$
  T_q^p (V) = \underbrace{V\otimes\cdots\otimes V}{p\text{ factors}}\otimes\undebrace{V^* \otimes\cdots\otimes V^*}{q\text{ factors}} = V^{\otimes p} \otimes (V^*)^{\otimes q}
$$

is called the space of tensors of rank $(p,q)$, where $p$ is the contravariant type and $q$ is the covariant type. In particaluar
- $(0,0)$-rank tensors are scalars, i.e. $T_0^0 (V) = \mathbb{F}$
- $(p, 0)$-rank tensors are *contravariant tensors*, i.e. $T^p (V) = T_0^p (V) = V^{\otimes p}$
- $(0,q)$-rank tensors are *covariant tensors*, i.e. $T_q (V) = T_q^0 (V) = (V^*)^{\otimes q}$
- tensors with both contravariant and covariant indices are called *mixed tensors*

Since $V$ is finite-dimensional, it follows that $V\cong V^**$ such that

$$
\begin{align*}
  T_q^p (V) =& V^{\otimes p}\otimes (V^*)^{\otimes q} \cong ((V^*)^{\otimes p} \otimes V^{\otimes}q)^* \\
  \cong& \hom((V^*)^{\times p}\times V^{\times q}, \mathbb{F})
\end{align*}
$$

The is the space of all multilinear functionals on $(V^*)^{\times p} \times V^{\times q}$. This is an alternative way of defining tensors of rank $(p,q)$.

Note that $\dim[T_q^p(V)] = \dim(V)^{p+q}$. Since tensor products are associative and commutative it follows that

$$
  T_q^p (V) \otimes T_s^r (V) = T_{q+s}^{p+r} (V)
$$

Generally, tensors can be interpreted in a variety of ways as multilinear maps on a cartesian product, or a linear map on a tensor product. The form of the tensor depends on how many of the contra- and covariant factors are "active", and how many are "passive". More specifically, consider a $(p,q)$-rank tensor written

$$
  \mathbf{v}_1 \otimes\cdots\otimes\mathbf{v}_m\otimes\cdots\otimes\mathbf{v}_p \otimes f_1 \otimes\cdots\otimes f_n \otimes\cdots\otimes f_q \in T_q^p (V)
$$

where $m\leq p$ and $n\leq q$ denote the active vectors and linear functionals, respectively. The map from the cartesian product $(V*)^{\times m} \times V^{\otimes n}$ $V^{\otimes p-m} \times (V^*)^{\otimes q-n}$ of the remaining factors is defined by

$$
\begin{align*}
  $(\mathbf{v}_1 \otimes\cdots\otimes \mathbf{v}_p \otimes f_1 \otimes\cdots\otimes f_q)(h_1,\dots,h_m,\mathbf{x}_1,\dots,\mathbf{x}_n) \\
  =& h_1(\mathbf{v}_1)\cdots h_m(\mathbf{v}_m) f_1 (\mathbf{x}_1) \cdots f_n (\mathbf{x}_n) \mathbf{v_m+1} \otimes\cdots\otimes\mathbf{v}_p otimes f_{n+1} \otimes\cdots\otimes f_q
\end{align*}
$$

The first group $\mathbf{v}_1\otimes\cdots\otimes\mathbf{v}_m$ of (active) vectors interact with the first set $h_1,\cdots,h_m$ of arguments to produce the scalar $h_1(\mathbf{v}_1)\cdots h_m(\mathbf{v}_m)$. The first group $f_1 \otimes\cdots\otimes f_n$ of (active) functionals interact with the second group $\mathbf{x}_1,\dots,\mathbf{x}_n$ of arguments to produce the scalar $f_1(\mathbf{x}_1)\cdots f_n(\mathbf{x}_n)$. The remaining (passive) vectors $\mathbf{v}_{m+1}\otimes\cdots\otimes\mathbf{v}_p$ and functionals $f_{n+1}\otimes\cdots\otimes f_q$ are just "copied" to the image vector.

It is straightforward to show that this map is multilinear. Thus, by the universal property there is a unique linear map from the tensor product $(V^*)^{\otimes m} \otimes V^{\otimes n}$ to the tensor product $V^{\otimes p-m}\otimes (V^*)^{\otimes q-n}$ defined by

$$
  &(\mathbf{v}_1 \otimes\cdots\otimes \mathbf{v}_p \otimes f_1 \otimes\cdots\otimes f_q)(h_1 \otimes\cdot\otimes h_m \otimes\mathbf{x}_1 \otimes\cdots\otimes \mathbf{x}_n) \\
  =& h_1(\mathbf{v}_1)\cdots h_m(\mathbf{v}_m) f_1(\mathbf{x}_1)\mathbf{v}_{m+1} \otimes\cdots\otimes\mathbf{p}\otimes f_{n+1}\otimes\cdots\otimes f_q
$$

In particular
- for a contravariant tensor of rank $(p,0)$, i.e. $\mathbf{v}_1\otimes\cdots\otimes\mathbf{v}_p \in T_0^p (V)$ we get a linear map $(V^*)^{\otimes m} \to V^{\otimes p-m}$ defined by 

$$
  (\mathbf{v}_1 \otimes\cdots\otimes\mathbf{v}_p)(h_1 \otimes\cdots\otimes h_m) = h_1(\mathbf{v}_1)\cdots h_m(\mathbf{v}_m)\mathbf{v}_{m+1}\otimes\cdots\mathbf{v}_p
$$
- for a covariant tensor of type $(0,q)$, i.e. $f_1 \otimes\cdots\otimes f_q \in T_q^0 (V)$ we get a linear map $V^{\otimes n}\to (V^*)^{\otimes q-n}$ defined By

$$
  (f_1\otimes\cdots\otimes f_q)(\mathbf{x}_1 \otimes\cdots\otimes\mathbf{x}_n) = f_1(\mathbf{x}_1)\cdots f_n(\mathbf{x}_n) f_{n+1}\otimes\cdots\otimes f_q
$$

The special case $n=q$ gives a linear functional on $V^{\otimes q}$, i.e. each element of $(V^*)^{\otimes q}$ is a distinct member of $(V^{\otimes q})^*$, producing the embedding $(V^*)^{\otimes q} \xhookrightarrow{\cong} (V^{\otimes q})^*$.

### Contraction

Covarant and contravariant factor can be combined in the following way. Consider the map

$$
  h: V^{\times p} \times (V^*)^{\times q} \to T_{q-1}^{p-1}(V)
$$

defined By

$$
  H(\mathbf{v}_1,\dots,\mathbf{v}_p,f_1,\dots,f_q) = f_1(\mathbf{v}_1)(\mathbf{v}_2 \otimes\cdots\otimes\mathbf{v}_p \otimes f_1\otimes\cdots\otimes f_q)
$$

This is easily seen to be multilinear; by the universal property there is a unique linear map

$$
  \Theta: T_{q}^{p} (V) \to T_{q-1}^{p-1} (V)
$$

defined by

$$
  \Theta(\mathbf{v}_1 \otimes\cdots\otimes\mathbf{v}_p \otimes f_1 \otimes\cdots\otimes f_q) = f_1(\mathbf{v}_1)(\mathbf{v}_1\otimes\cdots\otimes\mathbf{v}_p \otimes f_1\otimes\cdots\otimes f_q)
$$

This is called the contraction in the contravariant index $1$ and covariant index $1$. Contractions in other indices can be defined similarly.

For a vector space $V$, the outer product of its basis $B = \Set{e_i}_{i \in I \subset \N}$ and its dual basis $B^* = \Set{ e^i }_{i \in I \subset \N}$ spans all endomorphic linear transforms. Every linear transform $L : V \to V$ can be writen as a linear combination of vector-covector pairs 

$$
  L = L_j^i \left(e_i \otimes e^j \right)
$$

The outer product itself forms a linear map

$$
\begin{align*}
  w &= L(v) = L_j^i \left(e_i \otimes e^j \right)(v^k e_k) \\
  &= L_j^i v^k e_i e^j (e_k) \\
  &= L_j^i v^k e_i \delta_k^j \\
  &= L_j^i v^j e_i
\end{align*}
$$

## Matrices

Every linear map between finite-dimensional vector spaces can be represented by a matrix. This can be shown as follows for a transform $L : V \subseteq \mathbb{F}^n \to W \subseteq \mathbb{F}^m$. For simplicity, it is assumed that the bases $B^V = \Set{\hat{v}_i}_{i \in I \subseteq \N}$ and $B^W = \Set{\hat{w}_i}_{i \in I \subseteq \N}$ are orthonormal

$$
\begin{align*}
  L(v) &= L\left( \sum_{i=1}^n _ \hat{v}_i \right) = \sum_{i=1}^n _ L(\hat{v}_i) \\
  &= \sum_{i=1}^n _ \sum_{j=1}^m \left( \sum_{k=1}^n L_k^j \hat{v}^k \right)\hat{w}_j, \quad \sum_{k=1}^n L_k^j \hat{v}^k = L_k^j \delta^{ik} \\
  &= \sum_{j=1}^m \sum_{i=1}^n L_i^j _ \hat{w}_j = \sum_{j=1}^m w^j \hat{w}_j = w
\end{align*}    
$$

The output vector $w$ is given by the matrix multiplication

$$
  w^j = \sum_{i=1}^n L_i^j _
$$

### Eigenvectors

Let $L$ be an endomorphic linear transformation $L: V \to V$. A non-zero vector $v \neq 0 \in V$ is an eigenvector of $T$ if

$$
  L(v) = \lambda v
$$

where $\lambda \in \mathbb{F}$ is the eigenvalue corresponding to $v$.

The eigenvalue equation can be rearranged as

$$
  (L - \lambda \cdot \mathrm{Id})v = 0
$$

If $V$ is a finite-dimensional vector space, the determinant of the composed transformation $L - \lambda \cdot \mathrm{Id}$ vanishes

$$
  \mathrm{det}(L - \lambda \cdot \mathrm{Id}) = 0
$$

The determinant gives a polynomial function in $\lambda$, called the characteristic polynomial of $L$.

### Linear map transform

A linear map can be transformed into a new basis through the steps
1. Transform the input vector (contravariant) from the new to the old basis through a forward transform
2. Apply the linear map to the transformed vector
3. Transform the output vector back into the new basis

This can be shown as follows for a linear map $L$

$$
\begin{align*}
  L(\tilde{e}_i) &= L\left( \sum_{j=1}^n A_i^j e_j \right) = \sum_{j=1}^n A_i^j L(e_j) \\
  &= \sum_{j=1}^n A_i^j \sum_{k=1}^n L_j^k e_k \\
  &= \sum_{j=1}^n \sum_{k=1}^n A_i^j t_j^k \sum_{l=1}^n \tilde{A}_k^l \tilde{e}_l \\
  &= \sum_{l=1}^n \sum_{j=1}^n \sum_{k=1}^n \tilde{A}_k^l L_j^k A_i^j   \tilde{e}_l = \sum_{l=1} \tilde{L}_i^l \tilde{e}_l
\end{align*}
$$

The linear map gets transformed as follows

$$
\begin{gather*}
  \tilde{L}_i^l = \sum_{j=1}^n \sum_{k=1}^n \tilde{A}_k^l L_j^k A_i^j = \tilde{A}_k^l L_j^k A_i^j \\
\begin{aligned}
  \tilde{L} &= A^{-1}LA = \tilde{A}LA \\
  A\tilde{L}A^{-1} &= AA^{1}LAA^{-1} = L
\end{aligned}
\end{gather*}
$$

In tensor product notation, this can be derived as follows

$$
\begin{align*}
  L &= L_j^i e_i \otimes \mathbf{\varepsilon}^j \\
  &= L_j^i \left(\tilde{A}_i^k \tilde{e}_k \right) \otimes \left(A_l^j \tilde{\mathbf{\varepsilon}}^l \right) \\
  &= \left( \tilde{A}_i^k L_j^i A_l^j \right) \tilde{e}_k \otimes \tilde{\mathbf{\varepsilon}}^l \\
  &= \tilde{L_j^i} \tilde{e}_i \otimes \tilde{\mathbf{\varepsilon}}^j
\end{align*}
$$


## Inner product (dot product)

$$
  \langle v, w \rangle = v \cdot w = \lVert v \rVert \lVert w \rVert \cos{\theta} = g_{ij}_ v^j
$$

## Norm

The norm of a vector $\mathbf{v} \in V$ for a generalized basis $B = \Set{ e_i }_{i \in I \subset \N}$ is given by

$$
  \lVert v \rVert^2 = \langle v, v \rangle = v^* G v = g_{ij}_ v^j
$$

where $g_{ij}$ is the metric tensor

$$
  g_{ij} = \langle e_i, e_j \rangle
$$

The norm is invariant of coordinate changes

$$
\begin{align*}
  \lVert v \rVert^2 &= \tilde{g}_{ij} \tilde{v}^i \tilde{v}^j \\
  &= \left( A_i^k A_j^l g_{kl} \right) \left( \tilde{A}_m^i v^m \right) \left( \tilde{A}_n^j v^n \right) \\
  &= g_{kl} v^m v^n \left( \tilde{A}_m^i A_i^k  \tilde{A}_n^j A_j^l \right) \\
  &= g_{kl} v^m v^n \left(\delta_m^k \delta_n^l \right) \\
  &= g_{ij} _ v^j 
\end{align*}
$$

## Cross product (wedge product)
$$
\begin{gather*}
  a \times b = \begin{vmatrix} e_1 & e_2 & e_3\\
  a_1 & a_2 & a_3 \\
  b_1 & b_2 & b_3 \end{vmatrix} = \varepsilon^i_{jk}e_i a^j b^k  \\
  \lVert a \times b \rVert = \lVert a \rVert \lVert b \rVert \sin\theta \\
  \lVert a \times b \rVert^2 = \lVert a \rVert^2 \lVert b \rVert^2 - \left( a \cdot b \right)^2 = \lVert a \rVert^2 \lVert b \rVert^2 \left(1 - \cos^2\theta \right)
\end{gather*}
$$

Properties
- Jacobi identity
$$
  a \times \left( b \times c \right) + b \times \left( c \times a \right) + c \times \left( a \times b \right) = 0
$$
- Scalar triple product
$$
  A \cdot \left( B \times C \right) = B \cdot \left( C \times A \right) = C \cdot \left( A \times B \right)
$$

### Triple product

$$
\begin{align*}
  \left[a \times \left( b \times c \right)\right]^i &= \varepsilon^i_{jk} a^j \varepsilon^k_{mn} b^m c^n \\ 
  &= \left(\delta^{im}\delta^{jn} - \delta^{in}\delta^{jm}\right)a^j b^m c^n \\
  &= b^i a^j c^j - c^i a^j b^j \\
  &= \left[b \left(a \cdot c \right) - c\left( a \cdot b \right) \right]^i
\end{align*}
$$

## Outer product (tensor product)

Given an $m$-dimensional vector $u$ and an $n$-dimensional vector $v$, their outer product is an $m \times n$ matrix 

$$
  \left(u \otimes v\right)_{ij} = \left(u v^{\dagger}\right)_{ij} = u_{i} v_{j}^*
$$

The outer product has the following properties $\forall u, \mathbf{v} \in V$, $\psi, \varphi \in V^*$  and $a \in \mathbb{F}$

- $a \left( v \otimes  \right) = \left(a v \right) \otimes \psi = v \otimes \left(a \psi \right)$
- $v \otimes \left( \psi + \varphi \right) = v \otimes \psi + v \otimes \varphi$
- $\left(u + v \right) \otimes \psi = u \otimes \psi + v \otimes \psi$

## Linear form/functional (covector)

A linear form is a linear map from a vector space to its field of scalars, $f : V \to \mathbb{F}$ (1-form).

Linear functionals are represented as row vectors in $\R^n$.

### Bilinear form

A bilinear form is a map $B: V \times V \to \mathbb{F}$ (2-form) that is linear in each argument separately $\forall u, v, w \in V$ and $\lambda \in \mathbb{F}$

- $\lambda B(u, v) = B(\lambda u, v) = B(u, \lambda u)$
- $B(u + v, w) = B(u, w) + B(v, w)$
- $B(u, v + w) = g(u, v) + g(u, w)$

Bilinear forms are formed by linear combinations of covector-covector pairs

$$
  B = B_{ij} \left( e^i \otimes e^j \right)
$$

With this definition a bilinear map can be expressed as follows

$$
\begin{align*}
  s &= B(u, v) \\
  &= B_{ij}e^i e^j \left(u^k e_k, v^l e_l \right) \\
  &= B_{ij}e^i \left(u^k e_k \right) e^j \left( v^l e_l \right) \\
  &= B_{ij} u^k v^l e^i \left(e_k \right) e^j \left(e_l \right) \\
  &= B_{ij} u^k v^l \delta_k^i \delta_l^j \\
  &= B_{ij} u^i v^j
\end{align*}
$$

Bilinear forms are transformed as follows

$$
\begin{align*}
  B &= B_{ij} \left( e^i \otimes e^j \right) = B_{ij} \left(A_k^i \tilde{e}^k \right) \left( A_l^j \tilde{e}^l \right) \\
  &= \left( A_k^i A_l^j B_{ij} \right) \tilde{e}^k  \tilde{e}^l \\
  &= \tilde{B}_{ij}  \tilde{e}^i \tilde{e}^j
\end{align*}
$$

giving the transformation rules

$$
\begin{align*}
  \tilde{B}_{ij} &= A_i^k A_j^l B_{kl} \\
  B_{ij} &= \tilde{A}_i^k \tilde{A}_j^l \tilde{B}_{kl}
\end{align*}
$$

## Dual vector space

Given a vector space $V$ over a field $\mathbb{F}$, the dual space $V^*$ is defined as the set of all linear functionals (covectors) $\varphi : V \to \mathbb{F}$.

The dual space $V^*$ itself becomes a vector space over $\mathbb{F}$ when equipped with the operations of addition and scalar multiplication satisfying $\forall \varphi, \psi \in V^*, x \in V$ and $a \in \mathbb{F}$ 

$$
\begin{gather*}
  (\varphi + \psi)(x) = \varphi(x) + \psi(x) \\
  (a\varphi)(x) = a(\varphi(x))
\end{gather*}
$$

If $V$ is finite-dimensional, then $V^*$ has the same dimension as $V$.

## Basis

A basis $B$ of a vector space $V$ over a field $\mathbb{F}$ is a linearly independent subset of $V$ that spans $V$.

Given a basis $B = \Set{ e_i }_{i \in I \subseteq \N}$, a new basis $\tilde{B} = \Set{\tilde{e}_i}_{i \in I \subseteq \N}$ can be formed by transforming the old basis vectors, and vice versa

$$
\begin{align*}
  \tilde{e}_i &= \sum_{j=1}^n A_i^je_j = A_i^je_j \\
  e_i &= \sum_{j=1}^n \tilde{A}_i^j \tilde{e}_j = \tilde{A}_i^j \tilde{e}_j 
\end{align*}
$$

Basis vectors transform covariantly. The transforms are invertible (bijections) such that the compositions give the identity transform 

$$
\begin{gather*}
  A\tilde{A} = AA^{-1} = I \\
  \sum_{j=1}^n A_{ij}\tilde{A}_{ji} = \sum_{j=1}^n \tilde{A}_{ij}A_{ji} = \delta_{ij} = \begin{cases} 1, \quad i = j \\ 0, \quad i \neq j \end{cases}
\end{gather*}
$$

A vector $v$ can be expressed as a linear combination of basis vectors

$$
\begin{align*}
  v = \sum_{j=1}^n v_j e_j = \sum_{j=1}^n v_j \left( \sum_{i=1}^n \tilde{A}_{ij} \tilde{e}_i \right) = \sum_{i=1}^n \left( \sum_{j=1}^n \tilde{A}_{ij}v_j \right)\tilde{e}_i = \sum_{j=1}^n \tilde{v}_j \tilde{e}_j \\ 
  v = \sum_{j=1}^n \tilde{v}_j \tilde{e}_j = \sum_{j=1}^n \tilde{v}_j \left( \sum_{i=1}^n A_{ij} e_i \right) = \sum_{i=1}^n \left( \sum_{j=1}^n A_{ij} \tilde{v}_j \right)e_i = \sum_{j=1}^n v_j e_j
\end{align*}
$$

Vector components are said to be contravariant as they transform inversely of the basis vectors. To signify this, vector components are denoted with superscript indices. 

$$
\begin{align*}
  v^j &= \sum_{j=1}^n A_{ij} \tilde{v}^j \\
  \tilde{v}^j &= \sum_{j=1}^n \tilde{A}_{ij}v^j
\end{align*}
$$

### Dual basis

Given a basis $B = \Set{ e_i }_{i \in I \subset \N}$ for a finite-dimensional vector space $V$, a basis $B^* = \Set{e^i}_{i \in I \subset \N}$, called the dual basis, can be formed through the bi-orthogonality property

$$
  e^i \left( e_j \right) = \delta_j^i
$$

A new dual basis $\tilde{B}^*$ can be formed by transforming the old dual basis, $\tilde{e}^i = \sum_{i=1}^n t_{ij}e^j$. By the bi-orthogonality property

$$
\begin{align*}
  \tilde{e}^i (\tilde{e}_k) &= \sum_{j=1} t_{ij} e^j \left( \tilde{e}_k\right) \\
  &= \sum_{j=1}^n t_{ij} e^j \left( \sum_{l=1}^n a_{lk} e_l \right) \\
  &= \sum_{j=1}^n \sum_{l=1}^n t_{ij} a_{lk} e^j (e_l) \\
  &= \sum_{j=1}^n \sum_{l=1}^n t_{ij} a_{lk} \delta_j^l \\
  &= \sum_{j=1}^n \sum_{l=1}^n t_{ij} a_{jk} = \delta_k^i \quad \Rightarrow t_{ij} = \tilde{a}_{ij}
\end{align*}
$$

This shows that dual basis vectors transform covariantly, hence the superscripts

$$
\begin{gather*}
  \tilde{e}^i = \sum_{j=1}^n \tilde{a}_{ij} e^j \\
  e^i = \sum_{j=1}^n a_{ij} \tilde{e}^j
\end{gather*}
$$

A covector $\varphi \in V^*$ can be represented as a linear combination of the dual basis vectors

$$
  \varphi = \sum_{i=1}^n \varphi_i e^i = \sum_{i=1}^n \varphi_i \left( a_{ij}e^j \right) = \sum_{j=1}^n \left( \sum_{i=1}^n \varphi_i a_{ij} \right) \tilde{e}^j
$$

Covectors transform covariantly, hence the subscripts.

$$
\begin{gather*}
  \varphi_j = \sum_{i=1}^n \tilde{a}_{ij}\tilde{\varphi}_i \\
  \tilde{\varphi}_j = \sum_{i=1}^n a_{ij}\varphi_i
\end{gather*}
$$

