---
title: 'Linear Algebra'
subject: 'Mathematics'
showToc: true
---

# Matrices

An $m\times n$ matrix $\boldsymbol{A}$ with entries in a field $\mathbb{F}$ is a rectangular array of elements $\boldsymbol{A}_{i,j} = a_{i,j} \in\mathbb{F}$ for $i=1,\dots,m$ and $j=1,\dots,n$ represented as

$$
  \boldsymbol{A} = \begin{bmatrix}
    a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\
    a_{2,1} & a_{2,2} & \cdots & a_{2,n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{m,1} & a_{m,2} & \cdots & a_{m,n}
  \end{bmatrix},\; a_{i,j}\in\mathbb{F}
$$

This notation can be abbreviated by writing the entries as $\boldsymbol{A} = (a_{i,j})_{1\leq i \leq m,\; 1\leq j\leq n}$. The set of $m\times n$ matrices with entries in a field $\mathbb{F}$ is denoted $\mathcal{M}_{m,n}(\mathbb{F}) = \mathbb{F}^{m \times n}$. For $n\times n$ matrices this set is denoted $\mathcal{M}_{n,n}(\mathbb{F}) = \mathbb{F}^{n \times n}$. 

The *main diagonal* of an $m\times n$ matrix $\boldsymbol{A}$ is sequence of entries $(a_{i,i})_{i=1}^{\min\Set{m,n}}$. An $m\times n$ matrix $\boldsymbol{A}$ is called diagonal if its off-diagonal entries are zero, and denoted $\boldsymbol{A} = \mathrm{diag}(a_{i,i})_{i=1}^n$. A square matrix is *upper triangular* if all of its entries below the main diagonal are $0$. Similarly, a square matrix is *lower triangular* if all of its entries above the main diagonal are $0$.

The transpose of $\boldsymbol{A}\in \mathcal{M}_{m,n}(\boldsymbol{F})$ is the matrix $\boldsymbol{A}^T$ defined by $[\boldsymbol{A}^T]_{i,j} = \boldsymbol{A}_{j,i}$. A matrix is symmetric if $\boldsymbol{A} = \boldsymbol{A}^T$ and skew-symmetric if $\boldsymbol{A}^T = -\boldsymbol{A}$. All $n\times n$ diagonal matrices are by definition symmetric.

The conjugate transpose (adjoint) of a complex $m\times n$ matrix $\boldsymbol{A}\in\mathcal{M}_{m,n}(\mathbb{C})$ is the matrix $\boldsymbol{A}^*$ defined by $[\boldsymbol{A}^*]_{i,j} = \overline{\boldsymbol{A}}_{j,i}$.

Suppose that $\boldsymbol{A}\in\mathcal{M}_n (\mathbb{F})$ is an $n\times n$-matrix. For $1\leq k \leq n$, the $k$th leading principal minor of $\boldsymbol{A}$ is the $k\times k$ matrix $H_k = (a_{i,j})_{1\leq i \leq k,\; 1\leq j\leq k}$.

The $n\times n$ identity matrix is defined as $\boldsymbol{I}_n := \mathrm{diag}(1)_{i=1}^n$ with $[\boldsymbol{I}_n]_{i,j} = \delta_{i,j}$ where $\delta_{i,j}$ is the Kronecker delta function

$$
  \delta_{i,j} = \begin{cases} 1,\quad& i=j \\ 0,\quad& i\neq j \end{cases}
$$

<MathBox title='Properties of the transpose' boxType='proposition'>
The transpose operation has the following properties for $A,B\in\mathcal{M}_{m,n}$ and $\lambda\in\mathbb{F}$
1. **(injection):** $(\boldsymbol{A}^T)^T = \boldsymbol{A}$
2. $(\boldsymbol{A} + \boldsymbol{B})^T = \boldsymbol{A}^T + \boldsymbol{B}^T$
3. $(\lambda \boldsymbol{A})^T = \lambda \boldsymbol{A}^T$
4. $(\boldsymbol{A}\boldsymbol{B})^T = \boldsymbol{B}^T \boldsymbol{A}^T$
5. $\det(\boldsymbol{A}^T) = \det(\boldsymbol{A})$
</MathBox>

## Matrix multiplication

If $\boldsymbol{A}$ is an $m\times n$ matrix and $\boldsymbol{B}$ and $p\times q$ matrix, the matrix product $\boldsymbol{A}\boldsymbol{B}$ is defined if $n = p$, resulting in an $m \times q$ matrix given by the dot product of the corresponding row of $A$ and the corresponding column of $B$

$$
  [\boldsymbol{A}\boldsymbol{B}]_{i,j} = \sum_{r=1}^n a_{i,r} b_{r,j}
$$

The matrix product $\boldsymbol{B}\boldsymbol{A}$ is only defined if $m = q$ resulting in an $p \times n$ matrix. If both products are defined, they generally need not be equal, meaning that matrix multiplication is not commutative.

Matrix multiplication has the following properties for matrices $\boldsymbol{A}, \boldsymbol{B}, \boldsymbol{C}$ with appropriate sizes
- $c\boldsymbol{A} = \boldsymbol{A}c$ for a scalar $c$ **(scalar commutativity)**
- $c(\boldsymbol{A}\boldsymbol{B}) = (c\boldsymbol{A})\boldsymbol{B}$ **(left scalar associativity)**
- $(\boldsymbol{A}\boldsymbol{B})c = \boldsymbol{A}(\boldsymbol{B}c)$ **(right scalar associativity)**
- $(\boldsymbol{A}\boldsymbol{B})\boldsymbol{C} = \boldsymbol{A}(\boldsymbol{B}\boldsymbol{C})$ **(associativity)**
- $\boldsymbol{C}(\boldsymbol{A} + \boldsymbol{B}) = \boldsymbol{C}\boldsymbol{A} + \boldsymbol{C}\boldsymbol{B}$ **(left distributivity)**
- $(\boldsymbol{A} + \boldsymbol{B})\boldsymbol{C} = \boldsymbol{A}\boldsymbol{C} + \boldsymbol{B}\boldsymbol{C}$ **(left distributivity)**

## Partitioning and matrix multiplication

Let $\boldsymbol{M}$ be an $m\times n$ matrix. If $B\subseteq \Set{1,\dots,m }$ and $C\subseteq\Set{1,\dots,n}$ then the submatrix $\boldsymbol{M}[B,C]$ is the matrix obtained from $\boldsymbol{M}$ by keeping only the rows index in $B$ and the columns with index in $C$ such that $\boldsymbol{M}[B,C]$ has size $|B|\cdot|\boldsymbol{C}|$.

Suppose that $\boldsymbol{M}\in\mathcal{M}_{m,n}$ and $\boldsymbol{N}\in\mathcal{M}_{n,k}$. If
1. $\mathcal{P} = \Set{B_1,\dots, B_p}$ is a partition of $\Set{1,\dots,m}$
2. $\mathcal{Q} = \Set{C_1,\dots, C_q}$ is a partition of $\Set{1,\dots,n}$
3. $\mathcal{R} = \Set{D_1,\dots, D_r}$ is a partition of $\Set{1,\dots,k}$

we have

$$
  [\boldsymbol{MN}][B_i, D_i] = \sum_{C_h \in\mathcal{Q}} \boldsymbol{M}[B_i, C_h] \boldsymbol{N}[C_h, D_j]
$$

When the partions contain only single-element block, this reduces to the usual formula for matrix multiplication

$$
  [\boldsymbol{MN}]_{i,j} = \sum_{h=1}^m \boldsymbol{M}_{i,h} \boldsymbol{N}_{h,j}
$$

## Block matrices

If $\boldsymbol{B}_{i,j}$ are submatrices of $\boldsymbol{M}\in\mathcal{M}_{m,n}$ with appropriate sizes, then $\boldsymbol{M}$ can be written as the block matrix

$$
  \boldsymbol{M} = \begin{bmatrix} 
    \boldsymbol{B}_{1,1} & \cdots & \boldsymbol{B}_{1,n} \\
    \vdots & \ddots & \vdots \\
    \boldsymbol{B}_{m,1} & \cdots & \boldsymbol{B}_{m,n}
  \end{bmatrix}
$$

A square matrix of the form

$$
  \boldsymbol{M} = \begin{bmatrix} 
    \boldsymbol{B}_1 & \boldsymbol{0} &\cdots & \boldsymbol{0} \\
    \boldsymbol{0} & \ddots & \ddots & \vdots \\
    \vdots & \ddots & \ddots & \boldsymbol{0} \\
    \boldsymbol{0} & \cdots & \boldsymbol{0} & \boldsymbol{B}_n
  \end{bmatrix}
$$

where each $\boldsymbol{B}_i$ is square and $\boldsymbol{0}$ is a zero submatrix, is called a *block diagonal matrix*.

## Elementary row operations

There are three elementary row operations on matrices
- a row within the matrix can be switched with another row (row switching)
- each element in a row can be multiplied by a non-zero constant (row scaling)
- a row can be replaced by the sum of that row and a multiple of another row (row addition)

<MathBox title='Row echelon form' boxType='definition'>
A matrix $\boldsymbol{R}\in\mathcal{M}_{m,n}(\mathbb{F})$ is in *row echelon* form if
1. All zero rows (if there any) appear at the bottom of the matrix.
2. For each nonzero row, the leftmost nonzero entry, called the leading entry or the pivot, is strictly to the right of the leading entry of every row above.
3. Any column that contains a leading entry has zeroes in all other positions.

The matrix $\boldsymbol{R}$ is in *reduced row echelon form* if all the leading entries are equal to $1$.
</MathBox>

<MathBox title='Row equivalence' boxType='proposition'>
Two matrices $\boldsymbol{A}, \boldsymbol{B}\in \mathcal{M}_{m,n}$ are row equivalent, denoted $\boldsymbol{A}\sim \boldsymbol{B}$, if eiter one can be obtained from the other by a series of elementary row operations, i.e. there is an invertible matrix $\boldsymbol{P}$ such that $\boldsymbol{A} = \boldsymbol{PB}$.

A matrix $\boldsymbol{A}$ is row equivalent to one and only one matrix $R$ in reduced row echelon form, i.e. $\boldsymbol{A} = \boldsymbol{E}_1\cdots \boldsymbol{E}_k \boldsymbol{R}$ where $\boldsymbol{E}_i$ are elementary matrices required to reduce $\boldsymbol{A}$ to reduced row echelon form.

A matrix $\boldsymbol{A}$ is invertible if and only if its reduced row echelon form is an identity matrix. Hence a matrix is invertible if and only if it is the product of elementary matrices.
</MathBox>

An $m\times n$ matrix $R$ that is in both reduced row echelon form and reduced column echelon form must have the block form

$$
  \boldsymbol{J}_k = \begin{bmatrix} 
    \boldsymbol{I}_k & \boldsymbol{0}_{k,n-k} \\
    \boldsymbol{0}_{m-k, k} & \boldsymbol{0}_{m-k, n-k}
  \end{bmatrix}
$$

<MathBox title='Similar matrices' boxType='definition'>
Two matrices $\boldsymbol{A}, \boldsymbol{B}\in \mathcal{M}_{m,n}$ are *similar* if there exists an invertible matrix $\boldsymbol{P}$ such that 

$$
  \boldsymbol{A} = \boldsymbol{PBP}^{-1}
$$
</MathBox>

<MathBox title='Congruent matrices' boxType='definition'>
Two matrices $\boldsymbol{A}, \boldsymbol{B}\in\mathcal{M}_{m,n}$ are *congruent* if there exists an invertible matrix $\boldsymbol{P}$ such that 

$$
  \boldsymbol{A} = \boldsymbol{PBP}^T
$$
</MathBox>

<MathBox title='Row-switching transformation' boxType='definition'>
The corresponding elementary matrix for switching rows $i$ and $j$ is obtained by swapping the respective rows of the identity matrix. This in an $n\times n$ matrix $\boldsymbol{P}_{i,j} \in\mathcal{M}_n (\mathbb{F})$ of the form

$$
  \boldsymbol{P}_{i,j} = \begin{bmatrix}
    1 & & & & & & \\
    & \ddots & & & & & \\
    & & 0 & & 1 & & \\
    & & & \ddots & & & \\
    & & 1 & & 0 & & \\
    & & & & & \ddots & \\
    & & & & & & 1
  \end{bmatrix}
$$
</MathBox>

<MathBox title='Row-scaling transformation' boxType='definition'>
The corresponding elementary matrix for scaling the $i$th row by $m \neq 0$ is a diagonal matrix with diagonal entries $1$ except in the $i$th position

$$
  \boldsymbol{D}_i (m) = \begin{bmatrix}
    1 & & & & & & \\
    & \ddots & & & & & \\
    & & 1 & & & & \\
    & & & m & & & \\
    & & & & 1 & & \\
    & & & & & \ddots & \\
    & & & & & & 1
  \end{bmatrix}
$$

The entries of $\boldsymbol{D}_i(m)$ are given by

$$
  [\boldsymbol{D}_i(m)]_{k,l} = \begin{cases} 
    0 \quad& k\neq l \\
    1 \quad& k = l, k\neq i \\
    m \quad& k = l, k = i
  \end{cases}
$$
</MathBox>

<MathBox title='Row addition transformation' boxType='definition'>
The corresponding elementary matrix for adding the $j$th row multiplied by a scalar $\lambda \in\mathbb{F}$ to the $i$th row is the identity matrix with an $\lambda$ in the $(i,j)$th entry. This is an $n\times n$ matrix $\boldsymbol{Z}_{i,j}(\lambda) = \boldsymbol{Z}_{i+\lambda j} \in \mathcal{M}_n (\mathbb{F})$ of the form

$$
  \boldsymbol{Z}_{i+\lambda j} = \begin{bmatrix}
    1 & & & & & & \\
    & \ddots & & & & & \\
    & & 1 & & & & \\
    & & & \ddots & & & \\
    & & \lambda & & 1 & & \\
    & & & & & \ddots & \\
    & & & & & & 1
  \end{bmatrix}
$$

The entries of $\boldsymbol{Z}_{i+\lambda j}$ are given by

$$
  [\boldsymbol{Z}_{i+\lambda j}]_{k,l} = \begin{cases} 
    0 \quad& k\neq l, k\neq i, l\neq j \\
    1 \quad& k = l \\
    \lambda \quad& k = i, l = j
  \end{cases}
$$
</MathBox>

<MathBox title='' boxType='proposition'>
Consider an $m\times n$ matrix $\boldsymbol{A}\mathcal{M}_{m\times n}(\mathbb{F})$, and let $\boldsymbol{M} \in\mathcal{M}_{m\times n}(\mathbb{F})$ be a composition elementary row matrices. Then we have

$$
\begin{align*}
  \ker(\boldsymbol{M}\boldsymbol{A}) =& \ker(\boldsymbol{A}) \\
  \operatorname{ran}(\boldsymbol{M}\boldsymbol{A}) =& \boldsymbol{M}\;\ker(\boldsymbol{A})
\end{align*}
$$
</MathBox>

## Systems of linear equations

A system of $m$ linear equation with $n$ unknowns and coefficients in $\mathbb{F}$ takes the form

$$
\begin{align*}
  a_{11}x_1 + \cdots + a_{1n}x_n =& b_1 \\
  \vdots & \\
  a_{m1}x_1 + \cdots + a_{mn}x_n =& b_m 
\end{align*}
$$

where $x_1,\dots,x_n$ are the unknowns, $a_{11},\dots,a_{mn}$ are the coefficients and $b_1,\dots,b_m$ are the constant terms. The system can be written as the linear combination

$$
  x_1 \begin{bmatrix} a_{11} \\ \vdots \\ a_{m1} \end{bmatrix} + \cdots + x_n \begin{bmatrix} a_{1n} \\ \vdots \\ a_{mn} \end{bmatrix} = \begin{bmatrix} b_1 \\ \vdots \\ b_m \end{bmatrix}
$$

which is equivalent to the matrix equation

$$
  \begin{bmatrix} a_{11} & \cdots & a_{1n} \\ \vdots & \ddots & \vdots \\ a_{m1} & \cdots & a_{mn} \end{bmatrix} \cdot \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix} = \begin{bmatrix} b_1 \\ \vdots \\ b_m \end{bmatrix}
$$

or in short form $\boldsymbol{A}\boldsymbol{x} = \boldsymbol{b}$, where $\boldsymbol{A} = (a_{i,j})_{1\leq i \leq m,\; 1\leq j\leq n} \in\mathcal{M}_{m, n} (\mathbb{F})$, $\boldsymbol{x} = (x_i)_{1\leq i \leq n}\in\mathbb{F}^n$ and $\boldsymbol{b} = (b_i)_{1\leq i \leq m}\in\mathbb{F}^m$.

In its augmented form, the system is written as

$$
\left[\begin{array}{c c c|c}
  a_{11} & \cdots & a_{1n} & b_1 \\ \vdots & \ddots & \vdots & \vdots \\ a_{m1} & \cdots & a_{mn} & b_m
\end{array}\right]
$$

<MathBox title='Free and leading variables' boxType='definition'>
Consider a system of linear equations $\boldsymbol{A}\boldsymbol{x} = \boldsymbol{b}$ where $\boldsymbol{A}\in M_{m,n}(\mathbb{F})$, $\boldsymbol{x}\in\mathbb{F}^n$ and $\boldsymbol{b}\in\mathbb{F}^m$. In its row echelon form, all variables with no pivots in their columns are called *free variables*, while all variables with a pivot in their columns are called *leading variables*. The number of free variables is given by $\dim(\ker(\boldsymbol{A}))$, while the number of leading variables is given by $\dim(\operatorname{ran}(\boldsymbol{A}))$.
</MathBox>

<MathBox title='' boxType='proposition'>
For a system of linear equations $\boldsymbol{A}\boldsymbol{x} = \boldsymbol{b}$ where $\boldsymbol{A}\in M_{m,n}(\mathbb{F})$, $\boldsymbol{x}\in\mathbb{F}^n$ and $\boldsymbol{b}\in\mathbb{F}^m$, the set of solutions $S := \Set{\tilde{\boldsymbol{x}} \in \mathbb{F}^n | \boldsymbol{A} \tilde{\boldsymbol{x}} = \boldsymbol{b}}$ is either an affine subspace $S = \boldsymbol{v}_0 + \ker(\boldsymbol{A})$ for $\boldsymbol{v}_0 \in\boldsymbol{v_0}\in\mathbb{F}^n$, the empty set, $S = \emptyset$.

<details>
<summary>Proof</summary>

Assume that $\boldsymbol{v}_0 \in S$ is a solution, i.e. $\boldsymbol{A}\boldsymbol{v}_0 = \boldsymbol{b}$. Set $\tilde{\boldsymbol{x}} := \boldsymbol{v}_0 + \boldsymbol{x}_0$ for $\boldsymbol{x}_0 \in\mathbb{F}^n$. Then 

$$
\begin{align*}
  \tilde{\boldsymbol{x}}\in S \iff& \boldsymbol{A}\tilde{\boldsymbol{x}} = \boldsymbol{A}(\boldsymbol{v}_0 + \boldsymbol{x}_0) = \boldsymbol{b} \\
  \iff& \underbrace{\boldsymbol{A}\boldsymbol{v}_0}_{=\boldsymbol{b}} + \boldsymbol{A}\boldsymbol{x}_0 = \boldsymbol{b} \\
  \iff& \boldsymbol{A}\boldsymbol{x}_0 = \boldsymbol{0} \\
  \iff& \boldsymbol{x}_0 \in\ker(\boldsymbol{A})
\end{align*}
$$
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
For a system of linear equations $\boldsymbol{A}\boldsymbol{x} = \boldsymbol{b}$ where $\boldsymbol{A}\in M_{m,n}(\mathbb{F})$, $\boldsymbol{x}\in\mathbb{F}^n$ and $\boldsymbol{b}\in\mathbb{F}^m$, the set of solutions $S := \Set{\tilde{\boldsymbol{x}} \in \mathbb{F}^n | \boldsymbol{A} \tilde{\boldsymbol{x}} = \boldsymbol{b}}$ is either an affine subspace $S = \boldsymbol{v}_0 + \ker(\boldsymbol{A})$ for $\boldsymbol{v}_0 \in\boldsymbol{v_0}\in\mathbb{F}^n$, the empty set, $S = \emptyset$.

<details>
<summary>Proof</summary>

Assume that $\boldsymbol{v}_0 \in S$ is a solution, i.e. $\boldsymbol{A}\boldsymbol{v}_0 = \boldsymbol{b}$. Set $\tilde{\boldsymbol{x}} := \boldsymbol{v}_0 + \boldsymbol{x}_0$ for $\boldsymbol{x}_0 \in\mathbb{F}^n$. Then 

$$
\begin{align*}
  \tilde{\boldsymbol{x}}\in S \iff& \boldsymbol{A}\tilde{\boldsymbol{x}} = \boldsymbol{A}(\boldsymbol{v}_0 + \boldsymbol{x}_0) = \boldsymbol{b} \\
  \iff& \underbrace{\boldsymbol{A}\boldsymbol{v}_0}_{=\boldsymbol{b}} + \boldsymbol{A}\boldsymbol{x}_0 = \boldsymbol{b} \\
  \iff& \boldsymbol{A}\boldsymbol{x}_0 = \boldsymbol{0} \\
  \iff& \boldsymbol{x}_0 \in\ker(\boldsymbol{A})
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Solvability of a system linear equation' boxType='proposition'>
For $\boldsymbol{A}\in M_{m,n}(\mathbb{F})$ and $\boldsymbol{b}\in\mathbb{F}^m$, the following claims are equivalent
1. The system $\boldsymbol{A}\boldsymbol{x} = \boldsymbol{b}$ has at least one solution.
2. $\boldsymbol{b}\in\operatorname{ran}(\boldsymbol{A})$
3. $\boldsymbol{b}$ can be written as a linear combination of the columns of $\boldsymbol{A}$
4. All zero rows of the row echelon form a of the system have zero constant terms.

<details>
<summary>Proof</summary>

**(1)** $\iff$ **(2)** is given by the definition of $\operatorname{ran}(\boldsymbol{A})$.
**(2)** $\iff$ **(3)** is given by the column representation of $\operatorname{ran}(\boldsymbol{A})$

$$
  \operatorname{ran}(A) = \Set{[\boldsymbol{a}_1 \cdots \boldsymbol{a_n}]\boldsymbol{x} | \boldsymbol{x}\in\mathbb{F}^n}
$$

**(4)** $\implies$ **(1)** follows from the fact that $\operatorname{rank}(\boldsymbol{A}) = \operatorname{rank}([\boldsymbol{A}|\boldsymbol{b}])$

**(1)** $\implies$ **(4)** can be shown by contraposition. Assume that the row echelon form of $[\boldsymbol{A}|\boldsymbol{b}]$ has a zero row with a non-zero constant term. This implies that the system is not solvable, i.e. there is no solution for $\boldsymbol{A}\boldsymbol{x} = \boldsymbol{b}$.
</details>
</MathBox>

<MathBox title='Uniqueness for solutions of a system linear equation' boxType='proposition'>
For $\boldsymbol{A}\in M_{m,n}(\mathbb{F})$, the following claims are equivalent
1. The system $\boldsymbol{A}\boldsymbol{x} = \boldsymbol{b}$ has at most one solution for every $\boldsymbol{b}\in\mathbb{F}^n$.
2. $\ker(\boldsymbol{A}) = \Set{\boldsymbol{0}}$
3. The row echelon form of $\boldsymbol{A}\boldsymbol{x} = \boldsymbol{b}$ has no free variables.
4. $\operatorname{rank}(\boldsymbol{A}) = n$
5. The linear map $f_{\boldsymbol{A}}:\mathbb{F}^n \to\mathbb{F}^m$ by $\boldsymbol{x}\mapsto \boldsymbol{A}\boldsymbol{x}$ is injective. 

For square matrices, i.e. $\boldsymbol{A}\in M_{n,n}(\mathbb{F})$, it follows that $\ker(\boldsymbol{A}) = \Set{0} \iff \operatorname{ran}(\boldsymbol{A}) = \mathbb{F}^n$, or equivalently that $f_\boldsymbol{A}$ is bijective.
</MathBox>

## Determinants

<MathBox title='Volume measure' boxType='definition'>
The volume measure on $\R^n$ is the $n$-form $\operatorname{vol}_n : \prod_{i=1}^n \R^n \to\R$ satisfying for all $\boldsymbol{u}_1,\dots,\boldsymbol{u}_n \in\R$ 
1. **Linearity:** For $\alpha,\beta\in\mathbb{F}$, $\boldsymbol{v}\in\R^n$ and $j\in\Set{1,\dots,n}$
$$
\begin{align*}
  \operatorname{vol}_n (\boldsymbol{u}_1,\dots,\alpha\boldsymbol{u}_j + \beta\boldsymbol{v},\dots,\boldsymbol{u}_n) =& \alpha\operatorname{vol}_n (\boldsymbol{u}_1,\dots, \boldsymbol{u}_j,\dots,\boldsymbol{u}_n) \\
  &+ \beta\operatorname{vol}_n (\boldsymbol{u}_1,\dots,\boldsymbol{v},\dots,\boldsymbol{u}_n)
\end{align*}
$$
3. **Antisymmetry:** For $i,j\in\Set{1,\dots,n}$ with $i\neq j$
$$
  \operatorname{vol}_n (\boldsymbol{u}_1,\dots,\boldsymbol{u}_i,\dots \boldsymbol{u}_j,\dots,\boldsymbol{u}_n) = -\operatorname{vol}_n (\boldsymbol{u}_1,\dots,\boldsymbol{u}_j,\dots \boldsymbol{u}_i,\dots,\boldsymbol{u}_n)
$$
4. **Normalization:**
$$
  \operatorname{vol}_n (\boldsymbol{e}_1,\dots,\boldsymbol{e}_n) = 1
$$

The measure $\operatorname{vol}_n(\boldsymbol{u}_1,\dots,\boldsymbol{u}_n)$ gives the volume of the parallelepiped spanned by $\boldsymbol{u}_1,\dots,\boldsymbol{u}_n$.
</MathBox>

<MathBox title='Determinant of matrices' boxType='definition'>
The determinant is a function $\det:\mathcal{M}_{n}(\mathbb{F}) \to\mathbb{F}$ with the following properties for $\boldsymbol{A} \in \mathcal{M}_{n}(\mathbb{F})$
1. If $\boldsymbol{A} = \left[\begin{smallmatrix} \shortmid & ~ & \shortmid \\ \boldsymbol{a}_1 & \dots & \boldsymbol{a}_n \\ \shortmid & ~ & \shortmid \end{smallmatrix}\right]$ then $\operatorname{vol}_n (\boldsymbol{a}_1,\dots,\boldsymbol{a_n}) = |\det(\boldsymbol{A})|$ with the geometric interpretation that the column vectors of $\boldsymbol{A}$ span a parallelepiped.
2. $\det(\boldsymbol{A}) = 0$ if and only if $\left[\begin{smallmatrix} \shortmid \\ \boldsymbol{a}_1 \\ \shortmid \end{smallmatrix}\right], \dots, \left[\begin{smallmatrix} \shortmid \\ \boldsymbol{a}_n \\ \shortmid \end{smallmatrix}\right]$ are linearly dependent, or equivalently if $\boldsymbol{A}$ is not invertible.
3. the sign of $\det(\boldsymbol{A})$ gives the orientation of the parallelepiped spanned by $\boldsymbol{A}$. In particular, $\det(\boldsymbol{I}_n) = 1$.
</MathBox>

<MathBox title='Leibniz formula' boxType='proposition'>
The determinant of $\boldsymbol{A} = \left[\begin{smallmatrix} \shortmid & ~ & \shortmid \\ \boldsymbol{a}_1 & \dots & \boldsymbol{a}_n \\ \shortmid & ~ & \shortmid \end{smallmatrix}\right] \in\mathcal{M}_{n}(\mathbb{F})$ is given by the Leibniz formula

$$
  \det(\boldsymbol{A}) = \sum_{\sigma\in S_n} \left( \operatorname{sgn}(\sigma) \prod_{i=1}^n a_{i,\sigma(i)} \right)
$$

where $\sigma$ is a permutation bijection and $S_n$ is the set of all permutations of $\Set{1,\dots,n}$. The signature of a permutation $\sigma$ is given by

$$
  \operatorname{sgn}(\sigma) = \begin{cases} 
    +1,\quad& \sigma \text{ is even} \\
    -1,\quad& \sigma \text{ is odd}
  \end{cases}
$$

In terms of the Levi-Civita symbol $\varepsilon$, the Leibniz formula takes the form

$$
  \det(\boldsymbol{A}) = \sum_{i_1,\dots,i_n} \varepsilon_{i_1,\dots,i_n} \prod_{j=1} a_{i_j, j}
$$

<details>
<summary>Proof</summary>

By definition, $\operatorname{vol}_n (\boldsymbol{a}_1,\dots,\boldsymbol{a}_n) = |\det(\boldsymbol{A})|$. Calculating $\operatorname{vol}_n (\boldsymbol{a}_1,\dots,\boldsymbol{a}_n)$ gives

$$
\begin{align*}
  \operatorname{vol}_n ( \boldsymbol{a}_1,\dots,\boldsymbol{a}_n ) =& \operatorname{vol}_n \left( \begin{bmatrix} a_{11} \\ \vdots \\ a_{n1} \end{bmatrix}, \dots,\begin{bmatrix} a_{1n} \\ \vdots \\ a_{nn} \end{bmatrix} \right) \\
  =& \operatorname{vol}_n \left( \sum_{i_1 = 1}^n a_{i_1, 1} \boldsymbol{e}_{i_1},\dots, \sum_{i_n = 1}^n a_{i_n, n} \boldsymbol{e}_{i_n} \right) \\
  =& \sum_{i_1 = 1}^n \cdots \sum_{i_n = 1}^n a_{i_1, 1}\cdots a_{i_n, n} \operatorname{vol}_n (\boldsymbol{e}_{i_1},\dots,\boldsymbol{e}_{i_n}) \\
  =& \sum_{(i_i,\dots,i_n)\in S_n} a_{i_1, 1}\cdots a_{i_n, n} \operatorname{vol}_n (\boldsymbol{e}_{i_1},\dots,\boldsymbol{e}_{i_n})
\end{align*}
$$

where $S_n$ is the set of all permutations of $\Set{1,\dots,n}$. Due to the antisymmetry and normalization properties of the volume measure we get

$$
  \operatorname{vol}_n (\boldsymbol{e}_{i_1},\dots,\boldsymbol{e}_{i_n}) = \begin{cases} +1, \;& \text{even exchanges of arguments} \\ -1, \;& \text{odd  exhanges of arguments} \end{cases}
$$

This is precisely the definition of $\operatorname{sgn}: S_n \to \Set{-1,1}$ giving the signature of permutations $\sigma = (i_i,\dots,i_n)\in S_n$.
</details>
</MathBox>

<MathBox title='Properties of the determinant' boxType='proposition'>
The determinant has the following properties for $\boldsymbol{A},\boldsymbol{B}\in\mathbb{M}_{n}(\mathbb{F})$
1. $\det(\boldsymbol{AB}) = \det(\boldsymbol{A})\det(\boldsymbol{B})$
2. $\boldsymbol{A}$ is invertible (nonsingular) if and only if $\det(\boldsymbol{A}) \neq 0$, in this case $\det(\boldsymbol{\boldsymbol{A}}^{-1}) = \frac{1}{\det(\boldsymbol{A})}$
3. If $\boldsymbol{A}$ is an upper/lower triangular matrix, then $\det(\boldsymbol{A}) = \prod_{i=1}^n \boldsymbol{A}_{i,j}$
4. $\left|\begin{smallmatrix} \boldsymbol{A} & \boldsymbol{0} \\ \boldsymbol{C} & \boldsymbol{D} \end{smallmatrix}\right| = \det(\boldsymbol{A})\det(\boldsymbol{D}) = \left|\begin{smallmatrix} \boldsymbol{A} & \boldsymbol{B} \\ \boldsymbol{0} & \boldsymbol{D} \end{smallmatrix}\right|$
5. If $\boldsymbol{A}$ is invertible then $\left|\begin{smallmatrix} \boldsymbol{A} & \boldsymbol{B} \\ \boldsymbol{C} & \boldsymbol{D} \end{smallmatrix}\right| = \det(\boldsymbol{A})\det(\boldsymbol{D - CA^{-1}B})$
6. If $\boldsymbol{D}$ is invertible then $\left|\begin{smallmatrix} \boldsymbol{A} & \boldsymbol{B} \\ \boldsymbol{C} & \boldsymbol{D} \end{smallmatrix}\right| = \det(\boldsymbol{D})\det(\boldsymbol{A - BD^{-1}C})$
7. If $\boldsymbol{C}$ and $\boldsymbol{D}$ commute, then $\left|\begin{smallmatrix} \boldsymbol{A} & \boldsymbol{B} \\ \boldsymbol{C} & \boldsymbol{D} \end{smallmatrix}\right| = \det(\boldsymbol{A}\boldsymbol{D} - \boldsymbol{B}\boldsymbol{C})$
8. If $\boldsymbol{M}$ is a block diagonal matrix, i.e. of the form

$$
  \boldsymbol{M} = \begin{bmatrix} 
    \boldsymbol{B}_1 & \boldsymbol{0} &\cdots & \boldsymbol{0} \\
    \boldsymbol{0} & \ddots & \ddots & \vdots \\
    \vdots & \ddots & \ddots & \boldsymbol{0} \\
    \boldsymbol{0} & \cdots & \boldsymbol{0} & \boldsymbol{B}_n
  \end{bmatrix}
$$

then $\det(\boldsymbol{M}) = \prod_{i=1}^n \boldsymbol{B}_i$.
</MathBox>

### Laplace expansion

<MathBox title='Minor and cofactor' boxType='proposition'>
Let $\boldsymbol{A}\in\mathcal{M}_n (\mathbb{F})$, and let $\boldsymbol{A}_{ij}$ be the $(n-1)\times(n-1)$ matrix formed by removin the $i$th row and $j$th column.
- The $(i,j)$ minor of $\boldsymbol{A}$ is $M_{ij} := \det(\boldsymbol{A}_{ij})$.
- The $(i,j)$ cofactor of $\boldsymbol{A}$ is $C_{ij} := (-1)^{i+j} \det(\boldsymbol{A}_{ij})$
</MathBox>

<MathBox title='' boxType='lemma'>
Let $\boldsymbol{A}\in\mathcal{M}_n (\mathbb{F})$ be an $n\times n$ matrix whose $j$th column is the canonical column vector $\boldsymbol{e}_i$. Then $\det(\boldsymbol{A}) = C_{ij}$.
</MathBox>

<MathBox title='Laplace expansion' boxType='theorem'>
Let $\boldsymbol{A} \in \mathcal{M}_{n}(\mathbb{F})$. The *Laplace expansion* of the determinant of $\boldsymbol{A} = a_{ij}$ along the $i$th row is

$$
  \det(\boldsymbol{A}) = \sum_{j=1}^n a_{ij} C_{ij} = \sum_{j=1}^{n} (-1)^{i+j} a_{ij} \det(\boldsymbol{A}_{ij})
$$

Similarly, the Laplace expansion along the $j$th column is

$$
  \det(\boldsymbol{A}) = \sum_{i=1}^n a_{ij} C_{ij}
$$
</MathBox>

### Cramer's rule

<MathBox title='Adjugate matrix' boxType='definition'>
Let $\boldsymbol{A}\in\mathcal{M}_n (\boldsymbol{F})$ be an $n\times n$-matrix. The *adjugate* of $\boldsymbol{A}$ is the transpose of the cofactor matrix $\boldsymbol{C}$ of $\boldsymbol{A}$

$$
  \operatorname{adj}(\boldsymbol{A}) = \boldsymbol{C}^T
$$
</MathBox>

<MathBox title='Properties invertible matrices' boxType='proposition'>
For an $n\times n$-matrix $\boldsymbol{A}\in\mathcal{M}_n (\mathbb{F})$, the following claims are equivalent
1. $\boldsymbol{A}$ is invertible
2. the columns of $\boldsymbol{A}$ are linearly independent
3. the rows of $\boldsymbol{A}$ are linearly independent
4. $\operatorname{rank}(\boldsymbol{A}) = n$
5. $\operatorname{ker}(\boldsymbol{A}) = \Set{\boldsymbol{0}}$
6. $\det(\boldsymbol{A}) \neq 0$
7. $\boldsymbol{A}\boldsymbol{x} = \boldsymbol{b}$ has a unique solution for each $\boldsymbol{b}\in\mathbb{F}^n$
</MathBox>

<MathBox title="Cramer's rule" boxType='theorem'>
Let $\boldsymbol{A}\in\mathcal{M}_n (\mathbb{F})$ be an $n\times n$-matrix and suppose that $\boldsymbol{b}\in\mathbb{F}$. The solution $\boldsymbol{x}\in\mathbb{F}$ to the system of linear equations $\boldsymbol{A}\boldsymbol{x} = \boldsymbol{b}$ with $\boldsymbol{x} = \sum_{j=1}^n x_j \boldsymbol{e}_j$ is 

$$
  x_k = \frac{1}{\det(\boldsymbol{A})} \sum_{i=1}^n b_i C_{ik} = \frac{1}{\det(\boldsymbol{A})}\det\begin{bmatrix}
    \shortmid & \cdots & \shortmid & \shortmid & \shortmid & \cdots & \shortmid \\
    \boldsymbol{a}_1 & \cdots & \boldsymbol{a}_{k-1} & \boldsymbol{b} & \boldsymbol{a}_{k+1} & \cdots & \boldsymbol{a}_n \\
    \shortmid & \cdots & \shortmid & \shortmid & \shortmid & \cdots & \shortmid
  \end{bmatrix}
$$

<details>
<summary>Proof</summary>

If $\boldsymbol{A} = \left[\begin{smallmatrix} \shortmid & \cdots & \shortmid \\ \boldsymbol{a}_1 & \cdots & \boldsymbol{a}_n \\ \shortmid & \cdots & \shortmid \end{smallmatrix}\right]$, the system of linear equations $\boldsymbol{A}\boldsymbol{x} = \boldsymbol{b}$ can be written as $\sum_{j=1}^n x_j\boldsymbol{a}_j = \boldsymbol{b}$. For each $k\in\Set{1,\n}$ define the matrix

$$
  \boldsymbol{A}_k = \begin{bmatrix}
    \shortmid & \cdots & \shortmid & \shortmid & \shortmid & \cdots & \shortmid \\
    \boldsymbol{a}_1 & \cdots & \boldsymbol{a}_{k-1} & \boldsymbol{b} & \boldsymbol{a}_{k+1} & \cdots & \boldsymbol{a}_n \\
    \shortmid & \cdots & \shortmid & \shortmid & \shortmid & \cdots & \shortmid
  \end{bmatrix}
$$

The determinant of $\boldsymbol{A}_k$ is

$$
\begin{align*}
  \det{\boldsymbol{A}_k} =& \det(\boldsymbol{a}_1,\dots,\boldsymbol{a}_{k-1},\boldsymbol{u},\boldsymbol{a}_{k+1},\dots,\boldsymbol{n}) \\
  =& \det\left(\boldsymbol{a}_1,\dots,\boldsymbol{a}_{k-1},\sum_{j=1}^n x_j\boldsymbol{a}_j,\boldsymbol{a}_{k+1},\dots,\boldsymbol{n} \right) \\
  =& \sum_{j=1}^n x_j \det(\boldsymbol{a}_1,\dots,\boldsymbol{a}_{k-1},\boldsymbol{a}_j,\boldsymbol{a}_{k+1},\dots,\boldsymbol{n}) \\
  =& x_k \det(\boldsymbol{a}_1,\dots,\boldsymbol{a}_{k-1},\boldsymbol{a}_k,\boldsymbol{a}_{k+1},\dots,\boldsymbol{n}) \\
  =& x_k \det(\boldsymbol{A})
\end{align*}
$$

Thus, $x_k = \frac{\det(\boldsymbol{A})}{\det(\boldsymbol{A})}$. Using the Laplace expansion for the determinant of $\boldsymbol{A}_k$ we get $\det(\boldsymbol{A}) = \sum_{i=1}^n b_i C_{ik}$. Hence

$$
  x_k = \frac{1}{\det(\boldsymbol{A})} \sum_{i=1}^n b_i C_{ik}
$$
</details>
</MathBox>

<MathBox title='Formula for matrix inverse' boxType='theorem'>
If $\boldsymbol{A}\in\mathcal{M}_n (\mathbb{F})$ is an invertible $n\times n$-matrix, then the $(i,j)$-entry of its inverse $\boldsymbol{A}^{-1}$ is $[\boldsymbol{A}^{-1}]_{ij} = \frac{C_{ji}}{\det(\boldsymbol{A})}$ such that

$$
  \boldsymbol{A}^{-1} = \frac{\operatorname{adj}(\boldsymbol{A})}{\det(\boldsymbol{A})}
$$

<details>
<summary>Proof</summary>

Since $\boldsymbol{A}$ is invertible then the system of linear equations $\boldsymbol{A}\boldsymbol{x} = \boldsymbol{b}$ for any $\boldsymbol{b}\in\mathbb{F}$ has a unique solution given by $\boldsymbol{x} = \boldsymbol{A}^{-1}\boldsymbol{b}$. Thus, for each $k\in\Set{1,\dots,n}$ we have

$$
\begin{align*}
  x_k = \sum_{j=1}^n [\boldsymbol{A}^{-1}]_{kj} u_j = \sum_{j=1}^n \frac{1}{\det(\boldsymbol{A})}C_{jk}u_j
\end{align*}
$$

where the last equality follows from Cramer's rule. Hence

$$
  [\boldsymbol{A}^{-1}]_{ij} = \frac{1}{\det(\boldsymbol{A})}C_{ji}
$$
</details>
</MathBox>

### $2\times 2$-matrices

The determinant of a $2\times 2$-matrix $\boldsymbol{A} = \left[\begin{smallmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{smallmatrix}\right]$ is $\det(\boldsymbol{A}) = a_{11}a_{22} - a_{21}a_{12}$.

<MathBox title='Determinant criterion for two dimensional systems of linear equations' boxType='criterion'>
Let $\boldsymbol{A}\in\mathcal{M}_2 (\mathbb{F})$ be a $2\times 2$-matrix and let $\boldsymbol{b}\in\mathbb{F}^2$. A two-dimensional system of linear equations $\boldsymbol{A}\boldsymbol{x} = \boldsymbol{b}$ has a unique solution if and only if $\det(\boldsymbol{A}) \neq 0$.

<details>
<summary>Proof</summary>

Let $\boldsymbol{A} = \left[\begin{smallmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{smallmatrix}\right]$. Transforming $[\boldsymbol{A}|\boldsymbol{b}]$ into row echelon form we get

$$
\begin{align*}
  &\left[\begin{array}{c c|c}
    a_{11} & a_{12} & b_1 \\
    a_{21} & a_{22} & b_2
  \end{array}\right]~
  \begin{matrix}
    ~ \\
    \scriptsize{\boldsymbol{R}_2 - \frac{a_{21}}{a_{11}}\boldsymbol{R}_1}
  \end{matrix}
  \\
  \sim& \left[\begin{array}{c c|c}
    a_{11} & a_{12} & b_1 \\
    0 & a_{22} - \frac{a_{21}}{a_{11}}a_{12} & b_2 - \frac{a_{21}}{a_{11}}
  \end{array}\right]~
  \begin{matrix}
    ~ \\
    \scriptsize{a_{11}\boldsymbol{R}_2}
  \end{matrix}
  \\
  \sim& \left[\begin{array}{c c|c}
    a_{11} & a_{12} & b_1 \\
    0 & a_{11} a_{22} - a_{21}a_{12} & a_{11}b_2 - a_{21}
  \end{array}\right]
\end{align*}
$$

If $\boldsymbol{A}\boldsymbol{x} = \boldsymbol{b}$ has a unique solution, we require that $a_{11} \neq 0$ and $a_{11} a_{22} - a_{21}a_{12} = \det({\boldsymbol{A}}) \neq 0$.
</details>
</MathBox>

### $3\times 3$-matrices
<MathBox title='Rule of Sarrus' boxType='corollary'>
If $\boldsymbol{A} \in\mathcal{M}_3 (\mathbb{F})$ is a $3\times 3$ matrix, then

$$
\begin{align*}
  \det(\boldsymbol{A}) =& \begin{vmatrix} 
    a_{11} & a_{12} & a_{13} \\
    a_{21} & a_{22} & a_{23} \\
    a_{31} & a_{32} & a_{33} 
  \end{vmatrix} \\
  =& a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32} \\
  & -a_{13}a_{22}a_{31} - a_{11}a_{23}a_{32} - a_{12}a_{21}a_{33}
\end{align*}
$$

<LatexFig width={50} src='/fig/sarrus_rule.svg' alt=''
  caption='Mnemonic for the rule of Sarrus'
>
```latex
\documentclass[tikz]{standalone}
\usepackage{tikz}
\usetikzlibrary{calc,matrix}

\begin{document}
  \begin{tikzpicture}[>=stealth]
    \matrix [%
      matrix of math nodes,
      column sep=1em,
      row sep=1em
    ] (sarrus) {%
      a_{11} & a_{12} & a_{13} & a_{11} & a_{12} \\
      a_{21} & a_{22} & a_{23} & a_{21} & a_{22} \\
      a_{31} & a_{32} & a_{33} & a_{31} & a_{32} \\
    };

    \path ($(sarrus-1-1.north west)-(0.5em,0)$) edge ($(sarrus-3-1.south west)-(0.5em,0)$)
          ($(sarrus-1-3.north east)+(0.5em,0)$) edge ($(sarrus-3-3.south east)+(0.5em,0)$)
          (sarrus-1-1)                          edge            (sarrus-2-2)
          (sarrus-2-2)                          edge[->]        (sarrus-3-3)
          (sarrus-1-2)                          edge            (sarrus-2-3)
          (sarrus-2-3)                          edge[->]        (sarrus-3-4)
          (sarrus-1-3)                          edge            (sarrus-2-4)
          (sarrus-2-4)                          edge[->]        (sarrus-3-5)
          (sarrus-3-1)                          edge[dashed]    (sarrus-2-2)
          (sarrus-2-2)                          edge[->,dashed] (sarrus-1-3)
          (sarrus-3-2)                          edge[dashed]    (sarrus-2-3)
          (sarrus-2-3)                          edge[->,dashed] (sarrus-1-4)
          (sarrus-3-3)                          edge[dashed]    (sarrus-2-4)
          (sarrus-2-4)                          edge[->,dashed] (sarrus-1-5);

    \foreach \c in {1,2,3} {\node[anchor=south] at (sarrus-1-\c.north) {$+$};};
    \foreach \c in {1,2,3} {\node[anchor=north] at (sarrus-3-\c.south) {$-$};};
  \end{tikzpicture}
\end{document}
```
</LatexFig>
</MathBox>

## Trace

<MathBox title='Trace' boxType='definition'>
The trace of an $n\times n$-matrix $\boldsymbol{A}$, denoted $\operatorname{tr}(\boldsymbol{A})$ is the sum of the elements on the main diagonal of $\boldsymbol{A}$

$$
  \operatorname{tr}(\boldsymbol{A}) = \sum_{i_1}^n a_{i,i}
$$
</MathBox>

<MathBox title='Properties of trace' boxType='proposition'>
1. **Linearity:**
    - $\operatorname{tr}(\lambda\boldsymbol{A}) = \lambda\operatorname{tr}(\boldsymbol{A})$
    - $\operatorname{tr}(\boldsymbol{A} + \boldsymbol{B}) = \operatorname{tr}(\boldsymbol{A}) + \operatorname{tr}(\boldsymbol{B})$
2. **Commutativity:** $\operatorname{tr}(\boldsymbol{A}\boldsymbol{B}) = \operatorname{tr}(\boldsymbol{B}\boldsymbol{A})$
3. $\operatorname{tr}(\boldsymbol{A}\boldsymbol{B}\boldsymbol{C}) = \operatorname{tr}(\boldsymbol{C}\boldsymbol{A}\boldsymbol{B}) = \operatorname{tr}(\boldsymbol{B}\boldsymbol{C}\boldsymbol{A})$

<details>
<summary>Proof</summary>

**(3):**

$$
\begin{align*}
  \boldsymbol{A}\boldsymbol{B}_{ii} =& \sum_{j=1}^n a_{ij}b_{ji} \\
  \boldsymbol{B}\boldsymbol{A}_{ii} =& \sum_{j=1}^n b_{ij}a_{ji}
\end{align*}
$$

Due to symmetry, we are free to exchange the indices $i$ and $j$. Thus, $\boldsymbol{A}\boldsymbol{B}_{ii} = \boldsymbol{B}\boldsymbol{A}_{ii}$, showing that $\operatorname{tr}(\boldsymbol{A}\boldsymbol{B}) = \operatorname{tr}(\boldsymbol{B}\boldsymbol{A})$.
</details>
</MathBox>

# Vector space

<MathBox title='Vector space' boxType='definition'>
A vector space is a set $V$ over a field $\mathbb{F}$ equipped with the two closed operations 
- $+: V \times V \to V$ **(vector addition)**
- $\cdot: \mathbb{F} \times V \to V$ **(scalar multiplication)**

and has the following properties

- $V$ is an abelian group under vector addition, i.e. for all $\boldsymbol{u}, \boldsymbol{v}, \boldsymbol{w} \in V$
  - $\boldsymbol{u} + (\boldsymbol{v} + \boldsymbol{w}) = (\boldsymbol{u} + \boldsymbol{v}) + \boldsymbol{w}$ **(associativity)**
  - $\exists \boldsymbol{0} \in V : \boldsymbol{v} + \boldsymbol{0} = \boldsymbol{v}$ **(identity element)**
  - $\forall \boldsymbol{v} \; \exists -\boldsymbol{v} : \boldsymbol{v} + (-\boldsymbol{v}) = 0$ **(additive inverse)**
  - $\boldsymbol{u} + \boldsymbol{v} = \boldsymbol{v} + \boldsymbol{u}$ **(commutativity)**
- Scalar multiplication is compatible, satisfying for all $\alpha, \beta\in \mathbb{F}$
  - $\alpha(\beta \boldsymbol{v}) = (\alpha\beta)\boldsymbol{v}$
  - $1\boldsymbol{v} = \boldsymbol{v}$
- Vector addition and scalar multiplication are related by distributivity
  - $\alpha (\boldsymbol{u} + \boldsymbol{v}) = \alpha \boldsymbol{u} + \alpha \boldsymbol{v}$
  - $(\alpha + \beta)\boldsymbol{v} = \alpha \boldsymbol{v} + \beta \boldsymbol{v}$

The elements of $V$ are called vectors, while the elements of $\mathbb{F}$ are called scalars.
</MathBox>

Note that vector spaces are just special types of modules; a vector space is a module over a field.

<MathBox title='Linear combination' boxType='definition'>
A linear combination of vectors $\Set{ \boldsymbol{v}_i }_{i=1}^k \subseteq V$ for $k\in\N_+$ is a vector of the form

$$
  \sum_{i=1}^k \lambda_i \boldsymbol{v}_i,\; \lambda_i \in\mathbb{F}
$$

If at least one of the scalars $\lambda_i$ is nonzero, the linear combination is nontrivial.
</MathBox>

<MathBox title='Linear subspace' boxType='definition'>
Let $V$ be $\mathbb{F}$-vector space. If a subset $U\subseteq V$ satisfies
- $\boldsymbol{0}\in U$
- $\boldsymbol{u}, \boldsymbol{v}\in U \implies \boldsymbol{u} + \boldsymbol{v} \in U$ (closed under vector addition)
- $\boldsymbol{u}\in U, \alpha\in\mathbb{F} \implies \alpha \boldsymbol{u} \in U$ (closed under scalar multiplication)

then $U$ is also an $\mathbb{F}$-vector space, called a linear subspace of $V$.

The conditions imply that $U$ is closed under linear combinations, i.e. $\alpha \boldsymbol{u} + \beta \boldsymbol{v} \in U$ for all $\alpha,\beta\in\R$ and all $\boldsymbol{u}, \boldsymbol{v} \in U$.
</MathBox>

## Direct sums and products

### External direct sums and products
<MathBox title='External direct sum' boxType='definition'>
Let $V_1,\dots,V_n$ be $\mathbb{F}$-vector spaces. The external direct sum of $V_1,\dots,V_n$ denoted

$$
  V = V_1 \boxplus \cdots \boxplus V_n
$$

is the vector space $V$ whose elements are ordered $n$-tuples

$$
  V = \Set{ (\boldsymbol{v}_i)_{i=1}^n | \boldsymbol{v}_i \in V_i }
$$

and whose vector operations apply componentwise

$$
\begin{gather*}
  (\boldsymbol{u}_i)_{i=1}^n + (\boldsymbol{v}_i)_{i=1}^n = (\boldsymbol{u}_i + \boldsymbol{v}_i)_{i=1}^n,\; \boldsymbol{u}_i, \boldsymbol{v}_i\in V_i \\
  \lambda (\boldsymbol{u}_i)_{i=1}^n = (\lambda \boldsymbol{u}_i)_{i=1}^n,\; \lambda\in\mathbb{F}
\end{gather*}
$$
</MathBox>

<MathBox title='Direct product' boxType='definition'>
Let $\mathcal{F} = \Set{ V_i }_{i\in I}$ be a collection of $\mathbb{F}$-vector spaces for an index set $I$. The direct product of $\mathcal{F}$ is the vector space

$$
  \prod_{i\in I} V_i = \Set{ f: I\to\bigcup_{i\in I} V_i | f(i) \in V_i }
$$

which is subspace of the vector space of all functions from $I$ to $\bigcup_{i\in I} V_i$.
</MathBox>

<MathBox title='Generalized external direct sum' boxType='definition'>
Let $\mathcal{V} = \Set{ V_i }_{i\in I}$ be a collection of $\mathbb{F}$-vector spaces for an index set $I$. The external direct sum of $\mathcal{F}$ is the vector space

$$
  \bigoplus_{i\in I} V = \Set{ f: I\to\bigcup_{i\in I} V_i | f(i) \in V_i, f \text{ has finite support} }
$$

which is subspace of the vector space of all functions from $I$ to $\bigcup_{i\in I} V_i$. The support of $f:I\to\bigcup_{i\in I} V_i$ is the set

$$
  \mathrm{supp}(f) = \Set{i\in I | f(i) \neq 0}
$$

Thus, $f$ has finite support if $f(i) = 0$ for all but a finite number of $i\in I$.
</MathBox>

An important case occurs when $V_i = V$ for all $i\in I$. If $V^I$ denotes the set of all functions from $I$ to $V$ and $(V^I)_0$ denote the set of all functions in $I$ that have finite support then

$$
\begin{align*}
  \prod_{i\in I} V =& V^I \\ 
  \bigoplus_{i\in I} V =& (V^I)_0
\end{align*}
$$

### Internal direct sums

<MathBox title='Internal direct sum' boxType='definition'>
An $\mathbb{F}$-vector space $V$ is the (internal) direct sum of a collection $\mathcal{U} = \Set{ U_i }_{i\in I}$ of subspaces of $V$ if every vector $\boldsymbol{v}\in V$ can be uniquely written (except for order) as a finite sum of vectors from the subspaces in $\mathcal{F}$. That is, if for all $\boldsymbol{v}\in V$

$$
  v = \sum_{i=1}^{n\in I} \boldsymbol{u}_i = \sum_{i=1}^{m\in I} \boldsymbol{w}_j,\; \boldsymbol{u}_i, \boldsymbol{w}_i \in U_i
$$

then $m = n$ and $\boldsymbol{u}_i = \boldsymbol{w}_i$ (after reindexing if necessary) for all $i=1,\dots,n$.

If $V$ is the direct sum of $\mathcal{F}$ we write

$$
  V = \bigoplus_{i\in I} U_i
$$

where each $U_i$ is a direct summand of $V$. If $V = U \oplus W$, then $W$ is called a complement of $U$ in $V$.
</MathBox>

<MathBox title='All subspaces have a complement' boxType='proposition'>
Any subspace $U$ of a vector space $V$ has a complement $W$ for which $V = U \oplus W$.
</MathBox>

<MathBox title='' boxType='proposition'>
Let $I\subseteq\N_+$ be an index set. A vector space $V$ is the direct sum of a collection of subspaces $\mathcal{U} = \Set{ U_i }_{i\in I}$ if and only if
1. $V = \sum_{i\in I} U_i$
2. $U_i \cap \left(\sum_{j\neq i} U_i \right) = \Set{\boldsymbol{0}}$

<details>
<summary>Proof</summary>

Suppose first that $V$ is the direct sum of $\mathcal{U}$. Then $(1)$ clearly holds and if

$$
  \boldsymbol{v} \in U_i \cap \left(\sum_{j\neq i} U_i \right)
$$

then $\boldsymbol{v} = \boldsymbol{u}_i$ for some $\boldsymbol{u}_i \in U_i$ and

$$
  \boldsymbol{v} = \sum_{k=1}^n \boldsymbol{u}_{j_k},\, \boldsymbol{u}_{j_k}\in U_{j_k}
$$

where $j_k \neq 1$ for all $k=1,\dots,n$. Hence, by the uniqueness of direct sum representations, $\boldsymbol{u}_i = \boldsymbol{0}$ and $\boldsymbol{v} = \boldsymbol{0}$.

Conversely, suppose that $(1)$ and $(2)$ hold. We need only verify the uniqueness condition. If

$$
  \boldsymbol{v} = \sum_{i=1}^m \boldsymbol{u}_{j_i}\ = \sum_{i=1}^n \boldsymbol{w}_{k_i},\; \boldsymbol{u}_{j_i}\in U_{j_i}, \boldsymbol{w}_{k_i}\in U_{k_i}
$$

then by including additional zero terms we may assume that the index sets $\Set{j_i}_{i=1}^m$ and $\Set{k_i}_{i=1}^n$ are the same set $\Set{ i_j }_{j=1}^p$, giving

$$
  \sum_{j=1}^p (\boldsymbol{u}_{i_j} - \boldsymbol{w}_{i_j}) = 0
$$

Thus each term $\boldsymbol{u}_{i_j} - \boldsymbol{t}_{i_j}\in S_{i_j}$ is a sum of vectors from subspaces other than $S_{i_j}$, which can happen only if $\boldsymbol{u}_{i_j} - \boldsymbol{t}_{i_j} = 0$. Hence $\boldsymbol{u}_{i_j} = \boldsymbol{t}_{i_j}$ for all $i_j$ and $V$ is a direct sum of $\mathcal{U}$.
</details>
</MathBox>

## Span

<MathBox title='Linear dependence and independence' boxType='definition'>
A set of vectors $S =\Set{ v_i }_{i=1}^k$ for $k\in\N_+$ of an $\mathbb{F}$-vector space $V$ is linearly dependent if there is a non-trivial linear combination for $0\in V$. That is, there is a sequence of scalars $(\lambda_i \in\mathbb{F})_{i=1}^k$ that are not all equal to zero such that

$$
\begin{gather*}
  \sum_{i=1}^k \lambda_i \boldsymbol{v}_i = \boldsymbol{0} \\
  \iff \boldsymbol{v}_j = \sum_{i=1, i\neq j}^k \tilde{\lambda}_i \boldsymbol{v}_i,\; 1 \leq j \leq k, \lambda_j \neq 0, \tilde{\lambda}_i = \frac{-\lambda_i}{\lambda_j}
\end{gather*}  
$$

Equivalently, $S$ is linearly dependent if and only if one of its vectors is a linear combination of the others. The set $S$ is linearly independent if it is not linearly dependent, i.e. 

$$
  \sum_{i=1}^k \lambda_i \boldsymbol{v}_i = \boldsymbol{0} \implies \lambda_i = 0
$$
</MathBox>

<MathBox title='Span' boxType='definition'>
Given a subset $U$ of a $\mathbb{F}$-vector space $V$, the subspace spanned by $U$ is the the smallest set of all linear combinations of vectors in $U$. If $U$ is finite, i.e. for $k\in\N_+$ we can write $U = \Set{ \boldsymbol{u}_i }_{i=1}^k$, the span of $U$ is the set

$$
  \operatorname{span}(U) := \Set{ \boldsymbol{v}\in V | \exists (\lambda_i)_{i=1}^k \in\mathbb{F}^k : \boldsymbol{v} = \sum_{i=1}^k \lambda_i \boldsymbol{u}_i }
$$

In particular $\operatorname{span}(\emptyset) := \Set{\boldsymbol{0}}$. 

The subset $U$ is said to span (generate) $V$ if every $\boldsymbol{v}\in V$ is a linear combination of vectors in $U$, in which case we write $\operatorname{span}(U) = V$.
</MathBox>

## Basis

<MathBox title='Basis' boxType='definition'>
A basis $B$ of an $\mathbb{F}$-vector space $V$ is a linearly independent subset of $V$ spanning $V$. 
</MathBox>

<MathBox title='' boxType='proposition'>
A finite subset $U = \Set{\boldsymbol{u}_i}_{i=1}^n$ of an $\mathbb{F}$-vector space $V$ for $n\in\N$ is a basis for $V$ if and only if

$$
  V = \bigoplus_{i=1}^n \operatorname{span}(\Set{\boldsymbol{u}_i})
$$
</MathBox>

<MathBox title='Basis properties' boxType='proposition'>
If $B$ is a subset of an $\mathbb{F}$-vector space $V$, the following are equivalent
1. $B$ is a basis of $V$ 
2. $S$ is a minimal spanning set, i.e. $B$ spans $V$ while no proper subsets of $B$ does
3. $S$ is a maximal linearly independent set, i.e. $S$ is linearly independent while no proper supersets of $S$ is linearly dependent.

<details>
<summary>Proof</summary>

**$(1)\iff(2)$**
Suppose that $B$ is a basis of $V$, i.e. $B$ is linearly independent with $\operatorname{span}(B) = V$. If $\mathrm{\tilde{B}} = V$ for some $\tilde{B}\subset B$, then any vector in $B\setminus \tilde{B}$ should be a linear combination of the vectors in $\tilde{B}$, contradicting the fact that the vectors in $B$ are linearly independent. Hence $B$ must be a minimal spanning set.

Conversely, if $B$ is a minimal spanning set, then it must be linearly independent. If not some $b\in B$ would be a linear combination of the other vectors in $B$ and so $B\setminus\Set{b}$ would be a proper spanning subset of $B$, which is a contradiction. Hence $B$ must be a basis. 

**$(1)\iff(3)$**
Suppose that $B$ is a bsis of $V$. If $B$ is not maximal, there should be a vector $\boldsymbol{v}\in V\setminus{B}$ for which the set $B\cup\Set{\boldsymbol{v}}$ is linearly independent. However, then $\boldsymbol{v}\notin \operatorname{span}(B)$, contradicting the fact that $B$ is a spanning set. Hence, $B$ is a maximal linearly independent set.

Conversely, if $B$ is a maximal linearly independent set then $\operatorname{span}(B) = V$. If not, we could find a vector $\boldsymbol{v}\in V\setminus{B}$ that is not a linear combination of the vectors in $B$. In this case, $B\cup\Set{\boldsymbol{v}}$ would be a linearly independent proper superset of $B$, which is a contradiction. Hence, $B$ must be a basis.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V$ be a nonzero vector space. Let $I$ be a linearly independent set in $V$ and let $S$ be a spanning set in $V$ containing $I$. Then there is a basis $B$ for $V$ for which $I\subseteq B\subseteq S$. In particular
1. any vector space, except $\Set{\boldsymbol{0}}$ has a basis
2. any linearly independent set in $V$ is contained in a basis
4. any spanning set in $V$ contains a basis

<details>
<summary>Proof</summary>

Consider the collection $\mathcal{A}$ of all linearly independent subsets of $V$ containing $I$ and contained in $S$. Clearly, $\mathcal{A}$ is not empty since $I\in\mathcal{A}$. If $\mathcal{C} = \Set{ I_j}_{j\in J}$ for some index set $J$ is a chain in $\mathcal{A}$ then the union $U = \bigcup_{j\in J} I_j$ is linearly independent and satisfies $I\subseteq U\subseteq S$, i.e. $U\in\mathcal{A}$. Thus, every chain in $\mathcal{A}$ has an upper bound in $\mathcal{A}$ and by Zorn's lemma, $\mathcal{A}$ must contain a maximal element $B$, which is linearly independent.

The set $B$ is a basis for the vector space $\operatorname{span}(S) = V$, for if any $s\in S$ is not a linear combination of the elements of $B$, then $B\cup\Set{s} \subseteq S$ is linearly independent, contradicting the maximality of $B$. Hence $S\subseteq\operatorname{span}(B)$ and so $V = \operatorname{span}(S) \subseteq\operatorname{span}(B)$. 
</details>
</MathBox>

<MathBox title='Steinlitz exchange lemma' boxType='lemma'>
Let $U$ and $W$ be finite subsets of an $\mathbb{F}$-vector space $V$. If $U$ is a set of linearly independent vectors and $W$ spans $V$, then
1. $\dim(U) \leq \dim(W)$
2. There is a set $W' \subseteq W$ with $\dim(W') = \dim(W) - \dim(U)$ such that $U\cup W'$ spans $V$. 

<details>
<summary>Proof</summary>

Suppose that $U = \Set{\boldsymbol{u}_i}_{i=1}^m$ and $W = \Set{\boldsymbol{w}_i}_{i=1}^n$. We must show that $m \leq n$, and that after rearranging the $\boldsymbol{w}_j$ if necessary, the set $\Set{\boldsymbol{u}_1,\dots,\boldsymbol{u}_m,\boldsymbol{w}_{m+1},\dots,\boldsymbol{w}_n}$ spans $V$. This can be proved by induction.

For the base case, suppose that $m$ is zero. In this case, the claim holds because there are no vectors $\boldsymbol{u}_i$, and the set $\Set{\boldsymbol{w}_i}_{i=1}^n$ spans $V$ by hypothesis.

For the inductive step, assume that the proposition is true for $m-1$. By the inductive hypothesis we may reorder the $\boldsymbol{w}_i$ so that $\Set{\boldsymbol{u}_1,\dots,\boldsymbol{u}_{m-1},\boldsymbol{w}_m,\dots,\boldsymbol{w}_n$} spans $V$. Since $\boldsymbol{u}_m \in V$, there exists coefficients $\mu_1,\dots,\mu_n$ such that

$$
  \boldsymbol{u}_m = \sum_{i=1}^{m-1} \mu_i \boldsymbol{u}_i + \sum_{j=m}^n \mu_j \boldsymbol{w}_j
$$

At least one of the $\mu_j$ must be non-zero, since otherwise this equality would contradict the linear independence of $\Set{\boldsymbol{u}_i}_{i=1}^m$. It follows that $m\leq n$. By reordering $\mu_m \boldsymbol{w}_m,\dots,\mu_n \boldsymbol{w}_n$ if necessary, we may assume that $\mu_m$ is nonzero

$$
  \boldsymbol{w}_m = \frac{1}{\mu_m} \left(\boldsymbol{u}_m - \sum_{j=1}^{m-1} \mu_j \boldsymbol{u}_j - \sum_{j=m+1} \mu_j \boldsymbol{w}_j \right)
$$

This shows that $\boldsymbol{w}_m \in\operatorname{span}\Set{\boldsymbol{u}_1,\dots,\boldsymbol{u}_m,\boldsymbol{w}_{m+1},\dots,\boldsymbol{w}_n}$. Since this span contains each of the vectors $\boldsymbol{u}_1,\dots,\boldsymbol{u}_{m-1},\boldsymbol{w}_m,\dots,\boldsymbol{w}_{m+1},\dots,\boldsymbol{w}_n$, by the inductive hypothesis it contains $V$.
</details>
</MathBox>

## Dimension

<MathBox title='' boxType='proposition'>
Let $V$ be a vector space and assume that the vectors $\Set{\boldsymbol{v}_i}_{i=1}^n$ for $n\in\N_+$ are linearly independent and the vectors $\Set{\boldsymbol{s}_i}_{i=1}^m$ for $m\in\N_+$ span $V$. Then $n \leq m$. 

<details>
<summary>Proof</summary>

List the two set of vectors with the spanning set followed by the linearly independent set

$$
  \boldsymbol{s}_1,\dots,\boldsymbol{s}_m;\boldsymbol{v}_1,\dots,\boldsymbol{v}_n
$$

Move the first vector $\boldsymbol{v}_1$ to the front of the list

$$
  \boldsymbol{v}_1, \boldsymbol{s}_1,\dots,\boldsymbol{s}_m;\boldsymbol{v}_2,\dots,\boldsymbol{v}_n
$$

Since $\operatorname{span}\Set{ \boldsymbol{s}_i }_{i=1}^m = V$, it follows that $\boldsymbol{v}_1$ is a linear combination of the $\boldsymbol{s}_i$'s. This implies that we may remove one the $\boldsymbol{s}_i$'s, which by reindexing if necessary can be $\boldsymbol{s}_1$, from the list and still have a spanning set

$$
  \boldsymbol{v}_1, \boldsymbol{s}_2,\dots,\boldsymbol{s}_m;\boldsymbol{v}_2,\dots,\boldsymbol{v}_n
$$

Note that the first set of vectors still spans $V$ and the second set is still linearly independent. Repeat the process, moving $\boldsymbol{v}_2$ from the second list to the first list

$$
  \boldsymbol{v}_1, \boldsymbol{v}_2, \boldsymbol{s}_2,\dots,\boldsymbol{s}_m;\boldsymbol{v}_3,\dots,\boldsymbol{v}_n
$$

As before, the vectors in the first list are linearly independent, since they spanned $V$ before the inclusion of $\boldsymbol{v}_2$. However, since the $\boldsymbol{v}_i$'s are linearly independent, any nontrivial linear combination of the vectors in the first list that equals 0 must involve at least one of the $\boldsymbol{s}_i$'s. Thus, we may remove that vector, which by reindexing if necessary can be $\boldsymbol{s}_2$, and still have a spanning set

$$
  \boldsymbol{v}_1, \boldsymbol{v}_2, \boldsymbol{s}_3,\dots,\boldsymbol{s}_m;\boldsymbol{v}_3,\dots,\boldsymbol{v}_n
$$

If $m < n$, this process will eventually exhaust the $\boldsymbol{s}_i$'s and lead to the list

$$
  \boldsymbol{v}_1,\dots,\boldsymbol{v}_m;\boldsymbol{v}_{m+1},\dots,\boldsymbol{v}_n
$$

where $\operatorname{span}\Set{\boldsymbol{v}_i}_{i=1}^m = V$, which is contradictory since any $\boldsymbol{v}_i$ for $i > m$ is not in the span of $\Set{\boldsymbol{v}_i}_{i=1}^m$. Hence, $n\leq m$. 
</details>
</MathBox>

<MathBox title='All bases have same cardinality' boxType='theorem'>
All bases of an $\mathbb{F}$-vector space $V$ have the same cardinality, called the dimension of $V$, denoted $\dim(V)$.

<details>
<summary>Proof</summary>

For an index set $I\in\N$, let $B = \Set{\boldsymbol{b}_i}_{i\in I}$ be a basis for $V$ and suppose that $C$ is another basis for $V$. Then any vector $\boldsymbol{c}\in C$ can be written as finite linear combination of the vectors in $B$

$$
  \boldsymbol{c} = \sum_{i\in U_C} \lambda_i \boldsymbol{b}_i,\; \lambda_i\in\mathbb{F}\setminus\Set{0}
$$

Because $C$ is basis, we must have $\bigcup_{\boldsymbol{c}\in C} U_{\boldsymbol{c}} = I$. If the vectors in $C$ can be expressed as finite linear combinations of the vectors in a proper subset $B' \subset B$ then $\operatorname{span}(B') = V$, which is contradictory.

Since $U_{\boldsymbol{c}}$ is finite, i.e. $|U_{\boldsymbol{c}}| < \aleph_0$, for all $\boldsymbol{c}\in C$, it follows that 

$$
  |B| = |I| \leq \aleph_0 |C| = |C|
$$

Reversing the roles of $B$ and $C$, we may also conclude that $|C| \leq |B|$ and so $|B| = |C|$ by the Schrder-Bernstein theorem.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
A vector space $V$ is *finite-dimensional* if it is the zero space $\Set{\boldsymbol{0}}$ or if it has a finite basis. Otherwise, $V$ is infinite-dimensional. If $V$ has a basis of cardinality $n\in\N$ we say that $V$ is $n$-dimensional and write $\dim(V) = n$. In particular $\dim(\Set{\boldsymbol{0}}) = 0$.
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V$ be a vector space, then
1. if $B$ is a basis for $V$ and if $B = B_1 \cup B_2$ with $B_1 \cap B_2 = \emptyset$ then

$$
  V = \operatorname{span}(B_1) \oplus\operatorname{span}(B_1)
$$
2. let $V = S\oplus T$. If $B_1$ is a basis for $S$ and $B_2$ is a basis for $T$ then $B_1 \cap B_2 = \emptyset$ and $B = B_1 \cup B_2$ is a basis for $V$. 
</MathBox>

<MathBox title='' boxType='proposition'>
Let $S$ and $T$ be subspaces of a vector space $V$. Then

$$
  \dim(S) + \dim(T) = \dim(S + T) + \dim(S \cap T)
$$

In particular, if $T$ is any complement of $S$ in $V$, i.e. $S\oplus T = V$, then

$$
  \dim(S\oplus T) + \dim(S) + \dim(T) = \dim(V)
$$

<details>
<summary>Proof</summary>

Suppose that $B = \Set{b_i}_{i\in I}$ is a basis for $S\cap T$. Extend this to a basis $A\cup B$ for $S$ where $A = \Set{a_j}_{j\in J}$ is disjoint from $B$. Also, extend $B$ to a basis $B\cup C$ for $T$ where $C = \Set{c_k}_{k\in K}$ is disjoint from $B$. We claim that $A\cup B\cup C$ is a basis for $S+T$. It is clear that $\operatorname{span}(A\cup B\cup C) = S + T$.

To see that $A\cup B\cup C$ is linearly independent, suppose the opposite that

$$
  \sum_{i=1}^n \alpha_i \boldsymbol{v}_i = 0,\;\alpha_i \in\mathbb{F}\setminus\Set{0}, \boldsymbol{v}_i\in A\cup\B\cup C
$$

There must be vectors $\boldsymbol{v}_i$ in this expression from $A$ and $C$ since $A\cup B$ and $B\cup C$ are linearly independent. Isolating the terms involving the vectors from $A$ on one side of the equality shows that there is a nonzero vector $\boldsymbol{x}\in\operatorname{span}(A) \cap \operatorname{span}(B\cup C)$. However, then $\boldsymbol{x}\in S\cap T$ and so $\boldsymbol{x}\in \operatorname{span}(A) \cap \operatorname{span}(B)$, which implies that $\boldsymbol{x} = \boldsymbol{0}$, a contradiction. Hence $A\cup B\cup C$ is linearly independent and a basis for $S + T$, giving

$$
\begin{align*}
  \dim(S) + \dim(T) =& |A\cup B| + |B\cup A| \\
  =& |A| + |B| + |B| + |C| \\
  =& |A| + |B| + |C| + \dim(S\cap T) \\
  =& \dim(S + T) + \dim(S\cap T)
\end{align*}
$$
</details>
</MathBox>

## Coordinates

<MathBox title='Ordered basis' boxType='definition'>
An ordered basis for an $n$-dimensional vector space $V$ is an ordered $n$-tuple $(\boldsymbol{v}_i)_{i=1}^n$ of vectors for which the set $\Set{\boldsymbol{v}_i}_{i=1}^n$ is a basis for $V$.
</MathBox>

<MathBox title='Coordinate map' boxType='definition'>
If $B = \Set{\boldsymbol{b}_i}_{i=1}^n$ is an ordered basis for a vector space $V$ over $\mathbb{F}$ then for each $\boldsymbol{v}\in V$ there is a unique ordered $n$-tuple $(\lambda_i)_{i=1}^n \in \mathbb{F}^n$ such that $\boldsymbol{v} = \sum_{i=1}^n \lambda_i \boldsymbol{b}_i$.

Accordingly we can define the *coordinate map* $\varphi_B: V\to\mathbb{F}^n$ by

$$
  \varphi_B (\boldsymbol{v}) = [\boldsymbol{v}]_B := \begin{bmatrix} \lambda_1 \\ \vdots \\ \lambda_n \end{bmatrix}_B
$$

where the column matrix $[\boldsymbol{v}]_B$ is known as the *coordinate matrix* of $\boldsymbol{v}$ with respect to the ordered basis $B$. The coordinate map is linear, i.e.

$$
\begin{align*}
  \varphi_B (\boldsymbol{v}) =& \varphi_B\left(\sum_{i=1}^n \lambda_i \boldsymbol{b}_i\right) \\
  =& \sum_{i=1}^n \lambda_i \varphi_B (\boldsymbol{b}_i) \\
  =& \sum_{i=1}^n \lambda_i \boldsymbol{e}_i
\end{align*}
$$

where $\boldsymbol{e}_i$ is a canonical unit vector in $\mathbb{F}$.

The coordinate map is an isomorphism (bijection) with inverse $\varphi_B^{-1}:\mathbb{F}^n \to V$ defined by

$$
  (\lambda_i)_{i=1}^n \mapsto \sum_{i=1}^n \lambda_i \boldsymbol{b}_i
$$

The coordinate map is therefore also called the basis isomorphism.
</MathBox>

## Row and column spaces of matrices

Let $\boldsymbol{A}\in\mathcal{M}_{m,n}(\mathbb{F})$ be and $m\times n$ matrix. The rows of $\boldsymbol{A}$ span a subspace of $\mathbb{F}^n$ called the *row space* of $\boldsymbol{A}$, dented $\mathrm{rs}(\boldsymbol{A})$, and the columns of $\boldsymbol{A}$ span a subspace of $\mathbb{F}^m$ called the *column space* of $\boldsymbol{A}$, denoted $\mathrm{cs}(\boldsymbol{A})$. The dimensions of these spaces are called the *row rank* and *column rank*, denoted $\mathrm{rrk}(\boldsymbol{A})$ and $\mathrm{crk}(\boldsymbol{A})$, respectively.

<MathBox title='' boxType='proposition'>
Let $\boldsymbol{A}$ be an $m\times n$ matrix. Then elementary column operations do not affect the row rank of $\boldsymbol{A}$. Similarly, elementary row operations do not affect the column rank of $\boldsymbol{A}$.

<details>
<summary>Proof</summary>

The rowspace of $\boldsymbol{A}$ is 

$$
  \mathrm{rs}(\boldsymbol{A}) = \operatorname{span}(\boldsymbol{e}_i \boldsymbol{A})_{i=1}^n
$$

where $\boldsymbol{e}_i$ are the standard basis vectors in $\mathbb{F}$. Perferming an elementary column operation on $\boldsymbol{A}$ is equivalent to multiplying $\boldsymbol{A}$ on the right by an elementary matrix $\boldsymbol{E}$. Thus, the row space of $\boldsymbol{A}\boldsymbol{E}$ is

$$
  \mathrm{rs}(\boldsymbol{A}) = \operatorname{span}(\boldsymbol{e}_i \boldsymbol{E}\boldsymbol{A})_{i=1}^n
$$

and since $\boldsymbol{E}$ is invertible

$$
  \mathrm{rrk}(\boldsymbol{A}) = \dim(\mathrm{rs}(\boldsymbol{A})) = \dim(\mathrm{rs}(\boldsymbol{A}\boldsymbol{E})) = \mathrm{rrk}(\boldsymbol{A}\boldsymbol{E})
$$

The second statement follows from the first by taking transposes. 
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
If $\boldsymbol{A}\in\mathcal{M}_{m,n}$, then $\mathrm{rrk}(\boldsymbol{A}) = \mathrm{crk}(\boldsymbol{A})$. This number is called the rank of $\boldsymbol{A}$ and is denoted by $\operatorname{rank}(\boldsymbol{A})$.

<details>
<summary>Proof</summary>
 
According to the previous result, $\boldsymbol{A}$ can be transformed into a reduced column echelon form without affecting the row rank. This reduction does not affect the column either. The matrix $\boldsymbol{A}$ can be further transformed into a reduced row echelon form without affecting either rank. The resultiing matrix $\boldsymbol{M}$ has the same row and column ranks ars $\boldsymbol{A}$. However, $\boldsymbol{M}$ is a matrix with $1$'s followed by $0$' on the main diagonals entries and $0$'s elsewhere. Hence

$$
  \mathrm{rrk}(\boldsymbol{A}) = \mathrm{rrk}(\boldsymbol{M}) = \mathrm{crk}(\boldsymbol{M}) = \mathrm{crk}(\boldsymbol{A})
$$
</details>
</MathBox>

# Linear transformations

<MathBox title='Linear transformation' boxType='definition'>
Let $V$ and $W$ be vector spaces over the same field $\mathbb{F}$. A function $\mathrm{T}: V \to W$ is called a *linear transformation* if it preserve vector space operations, i.e.

$$
  \mathrm{T}(\alpha \boldsymbol{v} + \beta \boldsymbol{w}) = \alpha \mathrm{T}(\boldsymbol{v}) + \beta \mathrm{T}(\boldsymbol{w})
$$

for all $\alpha, \beta\in\mathbb{F}$ and $\boldsymbol{v}, \boldsymbol{w}\in V$. A linear transformation $\mathrm{T}: V\to V$ is called a *linear operator* on $V$. The set of all linear transformations from $V$ to $W$ is denoted $\mathcal{L}(V, W) = \hom(V,W)$ and the set of all linear operators on $V$ is denoted $\mathcal{L}(V) = \hom(V,V)$. 

The following terms are used to classify linear transformations and operators
- **homomorphism:** linear transformation
- **endomorphism:** linear operator
- **monomorphism (embedding):** injective linear transformation
- **epimorphism**: surjective linear transformation
- **isomorphism**: bijective linear transformation
- **automorphism**: bijective linear operator
</MathBox>

<MathBox title='Projection' boxType='definition'>
A linear operator $\mathrm{P}\in\mathcal{L}(V)$ is a *projection* if $\mathrm{P}^2 = \mathrm{P}$.
</MathBox>

<MathBox title='Commutator' boxType='definition'>
The *commutator* of $\mathrm{A},\mathrm{B}\in\mathcal{L}(V)$ is $[\mathrm{A},\mathrm{B}] := \mathrm{AB} - \mathrm{BA}$, which is $0$ if and only if $\mathrm{A}$ and $\mathrm{B}$ commute.
</MathBox>

<MathBox title='Sets of homomorphisms form vector spaces' boxType='proposition'>
The set $\mathcal{L}(V, W)$ is a vector space under ordinary addition of funtions and scalar multiplication of functions by elements of $\mathcal{F}$.
</MathBox>

<MathBox title='Sets of endomorphism form vector spaces' boxType='proposition'>
The set $\mathcal{L}(V)$ is a vector space and an algebra. It is an associative, but noncommutative algebra, where multiplication is composition of linear operators. The identity map $\mathrm{I} = \mathrm{id}\in\mathcal{L}(V)$ is the multiplicative identity, satisfying $\mathrm{I}\boldsymbol{v} = \boldsymbol{v}$. The zero map $0\in\mathcal{L}(V)$ is the additive identity.
</MathBox>

<MathBox title='Composition of linear transformations are linear' boxType='proposition'>
If $\mathrm{T}:U\to V$ and $\mathrm{S}\in V\to W$ are linear then so is the composition $\mathrm{S}\circ \mathrm{T}: V\to W$. The composition $\mathrm{S}\circ\mathrm{T}$ is usually written $ST$.

Composition of linear transformations is distributive w.r.t. addition. That is, if $\mathrm{P},\mathrm{T}:X\to U$ and $\mathrm{R}, \mathrm{S}:U\to V$, then

$$
\begin{align*}
  (\mathrm{R}+\mathrm{S})\circ T =& \mathrm{R}\circ \mathrm{T} + \mathrm{S}\circ\mathrm{T} \\
  \mathrm{S}\circ (\mathrm{T}\circ\mathrm{P}) =& \mathrm{S}\circ\mathrm{T} + \mathrm{S}\circ\mathrm{P}
\end{align*}
$$
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V$ and $W$ be vector spaces and let $B = \Set{v_i}_{i\in I}$ be a basis for $V$. Then we can define a linear transformation $\mathrm{T}\in\mathcal{L}(V,W)$ by specfiying the values of $\mathrm{T}(\boldsymbol{v}_i)\in W$ arbitrarily for all $v_i\in B$ and extending the domain of $\mathrm{T}$ to $V$ using linearity, i.e.

$$
  \mathrm{T}(\lambda_i \boldsymbol{v}_i)_{i\in I} = \sum_{i\in I} \lambda_i \mathrm{T}(\boldsymbol{v}_i)
$$

This process uniquely defines a linear transformation. If $\mathrm{T},\mathrm{S}\in\mathcal{L}(V,W)$ satisfy $\mathrm{T}(\boldsymbol{v}_i) = \mathrm{S}(\boldsymbol{v}_i)$ for all $\boldsymbol{v}_i\in B$ then $\mathrm{T} = \mathrm{S}$.

<details>
<summary>Proof</summary>

</details>
</MathBox>

<MathBox title='Kernel and range' boxType='definition'>
A linear transformation $\mathrm{T}\in\mathcal{L}(V, W)$ has the following two principal subspaces:
- the *kernel* (null space) of $\mathrm{T}$ is the set $\ker(\mathrm{T}):= \Set{\boldsymbol{v}\in V | \mathrm{T}(\boldsymbol{v}) = \boldsymbol{0}}$
- the *range* (image) of $\mathrm{T}$ is the set $\operatorname{ran}(\mathrm{T}):= \Set{\mathrm{T}(\boldsymbol{v}) | \boldsymbol{v}\in V}$

The dimension of $\ker(\mathrm{T})$ is called the *nullity* of $\mathrm{T}$ and is denoted $\mathrm{null}(\mathrm{T})$. The dimension of $\operatorname{ran}(\mathrm{T})$ is called the *rank* of $\mathrm{T}$ is denoted $\operatorname{rank}(\mathrm{T})$.
</MathBox>

<MathBox title='' boxType='proposition'>
Let $\mathrm{T}\in\mathcal{L}(V, W)$, then
1. $\mathrm{T}$ is surjective if and only if $\operatorname{ran}(\mathrm{T}) = W$
2. $\mathrm{T}$ is injective if and only if $\ker(\mathrm{T}) = \Set{\boldsymbol{0}}$

<details>
<summary>Proof</summary>

The first statement is merely a restatement of the definition of surjectivity. To show the second statement, note that

$$
  \mathrm{T}(u) = \mathrm{T}(v) \iff \mathrm{T}(u - u) = 0 \iff u - v \in\ker(\mathrm{T})
$$

Thus, if $\ker(\mathrm{T}) = \Set{0}$ then $\mathrm{T}(u) = \mathrm{T}(v) \iff u = v$, showing that $f$ is injective. Conversely, if $\mathrm{T}$ is injective and $u\in\ker(\mathrm{T})$, then $\mathrm{T}(u) = \mathrm{T}(0) \iff u = 0$. hence $\ker(\mathrm{T}) = \Set{\boldsymbol{0}}$.
</details>
</MathBox>

## Isomorphism

<MathBox title='Invertibility and isomorphism' boxType='definition'>
A linear transformation $\mathrm{T}: V\to W$ is *invertible* if it is bijective. A bijective linear transformation $\mathrm{T}:V\to W$ is an isomorphism from $V$ to $W$. The vector space $V$ and $W$ are isomorphic, denoted $V \cong W$, if there is an isomorphism from $V$ to $W$. The inverse of $\mathrm{T}$ is denoted $\mathrm{T}^{-1}$.
</MathBox>

For any ordered basis $B$ of an $\mathbb{F}$-vector space $V$ with $\dim(V) = n$, the coordinate map $\varphi_B: V\to\mathbb{F}^n$ is an isomorphism. Hence, any $n$-dimensional vector space over $\mathbb{F}$ is isomorphic to $\mathbb{F}^n$.

An isomorphism can be characterized as a linear transformation $\mathrm{T}:V\to W$ that maps a basis for $V$ to a basis for $W$.

<MathBox title='Properties of isomorphisms' boxType='prosposition'>
Let $\mathrm{T}\in\mathcal{L}(V, W)$ be an isomorphism. For $S\subseteq V$, then
1. $S$ spans $V$ if and only if $\mathrm{T}(S)$ spans $W$.
2. $S$ is linearly independent in $V$ if and only if $\mathrm{T}(S)$ is linearly independent in $W$.
3. $S$ is a basis for $V$ if and only if $\mathrm{T}(S)$ is a basis for $W$.
</MathBox>

<MathBox title='Isomorphic spaces have same dimension' boxType='proposition'>
Let $V$ and $W$ be vector spaces, then $V\cong W$ if and only if $\dim(V) = \dim(W)$.
</MathBox>

<MathBox title='' boxType='proposition'>
For $n\in\N$, any $n$-dimensional vector space over $\mathbb{F}$ is isomorphic to $\mathbb{F}^n$. If $B$ is a set of cardinality $|B| = \kappa$, then any $\kappa$-dimensional vector space over $\mathbb{F}$ is isomorphic to the vector space $(\mathbb{F}^B)_0$ of all functions from $B$ to $\mathbb{F}$ with finite support.
</MathBox>

<MathBox title='Properties of invertible linear transformations' boxType='proposition'>
If $\boldsymbol{T}:U\to V$ is an invertible linear transformation, then
1. $T^{-1}$ is linear, i.e. $T^{-1} \in\mathcal{L}(U, V)$
2. $\mathrm{T}\mathrm{T}^{-1}$ and $\mathrm{T}^{-1}\mathrm{T}$ is the identity.
3. If $\boldsymbol{S}:V\to W$ is invertible and $\mathrm{S}\mathrm{T}$ is defined, then it is invertible with $(\mathrm{S}\mathrm{T})^{-1} = \mathrm{T}^{-1}\mathrm{S}^{-1}$.

<details>
<summary>Proof</summary>

**(1)**
Let $\mathrm{T}:U\to V$ be a bijective linear transformation. Then $\mathrm{T}^{-1}: V\to U$ is well-defined and since any two vectors $\boldsymbol{v}_1, \boldsymbol{v}_2\in V$ have the form $\boldsymbol{v}_1 = \mathrm{T}(\boldsymbol{u}_1)$ and $\boldsymbol{v}_2 = \mathrm{T}(\boldsymbol{u}_2)$ we have for $\alpha, \beta\in\mathbb{F}$

$$
\begin{align*}
  \mathrm{T}^{-1}(\alpha \boldsymbol{v}_1 + \beta \boldsymbol{v}_2) =& \mathrm{T}^{-1}[\alpha \mathrm{T}(\boldsymbol{u}_1) + \beta \mathrm{T}(\boldsymbol{u}_2)] \\
  =& \mathrm{T}^{-1}[\mathrm{T}(\alpha \boldsymbol{u}_1 + \beta \boldsymbol{u}_2)] \\
  =& \alpha \boldsymbol{u}_1 + \beta \boldsymbol{u}_2 \\
  =& \alpha \mathrm{T}^{-1}(\boldsymbol{v}_1) + \beta \mathrm{T}^{-1}(\boldsymbol{v}_2)
\end{align*}
$$

**(3)**

<LatexFig width={50} src='/fig/invertible_composition.svg' alt=''
  caption='Invertible composition'
>
```latex
\documentclass[tikz]{standalone}
\usepackage{tikz}
\usepackage{amssymb}
\usetikzlibrary{cd}

\begin{document}

\begin{tikzcd}
  U \arrow[r, bend left=50, "\mathrm{T}"] \arrow[rr, bend left=100, "\mathrm{S}\mathrm{T}"] & 
  V \arrow[l, bend left=50, "\mathrm{T}^{-1}"] \arrow[r, bend left=50, "\mathrm{S}"]  &
  W \arrow[l, bend left=50, "\mathrm{S}^{-1}"] \arrow[ll, bend left=100, "(\mathrm{S}\mathrm{T})^{-1} = \mathrm{T}^{-1}\mathrm{S}^{-1}"]
\end{tikzcd}

\end{document}
```
</LatexFig>
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
If $\mathrm{A}\in\mathcal{L}(V)$ is a left inverse of $\mathrm{B}\in\mathcal{L}(V)$, i.e. $\mathrm{A}\mathrm{B} = \mathrm{I}$, then it is also a right inverse, i.e. $\mathrm{B}\mathrm{A} = \mathrm{I}$.
</MathBox>

## Similarity

<MathBox title='Similarity transformation' boxType='proposition'>
Let $V$ be an $\mathbb{F}$-vector space. The invertible elements of $\mathcal{L}(V)$ forms the general linear group, denoted $\mathrm{GL}_n (\mathbb{F})$, where $n = \dim(V)$. Every $\mathrm{S}\in\mathrm{GL}_n (\mathbb{F})$ defines a *similarity transformation* $\phi_\mathrm{S}$ of $\mathcal{L}(V)$, sending $\mathrm{M}\mapsto \mathrm{M_S} := \mathrm{SMS}^{-1}$, for each $\mathrm{M}\in\mathcal{L}(V)$. We say that $\mathrm{M}$ and $\mathrm{M_S}$ are *similar*.
</MathBox>

<MathBox title='Similarity transformations are automorphisms' boxType='proposition'>
Every similarity transformation is an automorphism of $\mathcal{L}(V)$ for an $\mathbb{F}$-vector space $V$, satisfying for $\mathrm{M}\in\mathcal{L}(V)$, $\mathrm{S}\in\mathrm{GL}_n (\mathbb{F})$ and $\lambda\in\mathbb{F}$

1. $(\lambda\mathrm{M})_\mathrm{S} = \lambda\mathrm{M_S}$
2. $(\mathrm{M} + \mathrm{N})_\mathrm{S} = \mathrm{M_S} + \mathrm{N_S}$
3. $(\mathrm{MN})_\mathrm{S} = \mathrm{M_S N_S}$

The set of similarity transformation forms a group under $(\mathrm{M_S})_\mathrm{T} := \mathrm{M_{TS}}$, called the *inner automorphism group* of $\mathrm{GL}_n (\mathbb{F})$.

<details>
<summary>Proof</summary>

1. $\mathrm{S}(\lambda\mathrm{M})S^{-1} = \lambda(\mathrm{SMS}^{-1})$
2. $\mathrm{S}(\mathrm{M} + \mathrm{N})\mathrm{S}^{-1} = \mathrm{SMS}^{-1} + \mathrm{SNS}^{-1}$
3. $\mathrm{S}(\mathrm{MN})\mathrm{S}^{-1} = (\mathrm{SMS}^{-1})(\mathrm{SNS}^{-1})$

Next we show that set of similarity transformation satisfy the group axiom.
- **Identity element:** The identity transformation $\mathrm{I}\in mathrm{GL}_n (K)$ forms a multiplicative $\phi_\mathrm{I}: M \mapsto \mathrm{IMI}^{-1} = M$
- **Inverse element:** The inverse is given by $\psi_{\mathrm{S}^{-1}}: M\mapsto \mathrm{S}^{-1}\mathrm{MS}$.
- **Closure:** For $\phi_\mathrm{S}:\mathrm{M}\mapsto \mathrm{SMS}^{-1}$ and $\phi_\mathrm{T}:\mathrm{M}\mapsto\mathrm{TMT}^{-1}$, then $M\overset{\phi_\mathrm{S}}{\mapsto} \mathrm{SMS}^{-1} \overset{\phi_\mathrm{T}}{\mapsto} (\mathrm{TS})\mathrm{M}(\mathrm{S^{-1}\mathrm{T}^{-1}}) =  (\mathrm{TS})\mathrm{M}(\mathrm{TS})^{-1}$.
</details>
</MathBox>

<MathBox title='Similarity is an equivalence relation' boxType='proposition'>
Similarity is an equivalence relation $\sim$, satisfying
1. **Reflexivity:** $\mathrm{M}\sim \mathrm{M}$
2. **Symmetry:** $\mathrm{L}\sim\mathrm{M} \implies \mathrm{M}\sim\mathrm{L}$
3. **Transitivity:** $\mathrm{L}\sim\mathrm{M} \land \mathrm{M}\sim\mathrm{N} \implies \mathrm{L}\sim\mathrm{N}$
</MathBox>

<MathBox title='' boxType='proposition'>
If either $\mathrm{A}, \mathrm{B}\in\mathcal{L}(V)$ is invertible, then $\mathrm{AB}$ and $\mathrm{BA}$ are similar.
</MathBox>


## Rank-nullity theorem

<MathBox title='' boxType='proposition'>
Let $\mathrm{T}\in\mathcal{L}(V,W)$. Any complement of $\ker(\mathrm{T})$ is isomorphic to $\operatorname{ran}(\mathrm{T})$.

<details>
<summary>Proof</summary>

Let $\mathrm{T}\in\mathcal{L}(V,W)$. Since any subspace of $V$ has a complement, we can write

$$
  V = \ker(\mathrm{T}) \oplus\ker(\mathrm{T})^c
$$

where $\ker(\mathrm{T})^c$ is the complement of $\ker(\mathrm{T})$. The restriction of $\mathrm{T}$ to $\ker(\mathrm{T})^c$, denoted $\mathrm{T}^c:\ker(\mathrm{T})^c \to W$ is injective since

$$
  \ker(\mathrm{T}^c) = \ker(\mathrm{T}) \cap\ker(\mathrm{T})^c = \Set{\boldsymbol{0}}
$$

Also, $\operatorname{ran}(\mathrm{T}^c)\subseteq \operatorname{ran}(\mathrm{T})$. For the reverse inclusion, if $\mathrm{T}\in\operatorname{ran}(f)$ then since $\boldsymbol{v} = \boldsymbol{u} + \boldsymbol{w}$ for $\boldsymbol{u}\in\ker(\mathrm{T})$ and $\boldsymbol{w}\in\ker(\mathrm{T})^c$, we have

$$
  \mathrm{T}(\boldsymbol{v}) = \mathrm{T}(\boldsymbol{u}) + \mathrm{T}(\boldsymbol{w}) = \mathrm{T}(\boldsymbol{w}) = \mathrm{T}^c(\boldsymbol{w}) \in\operatorname{ran}(\mathrm{T}^c)
$$

Thus $\operatorname{ran}(\mathrm{T}^c) = \operatorname{ran}(\mathrm{T})$ and it follows that $\ker(\mathrm{T})^c \cong \operatorname{ran}(\mathrm{T})$.
</details>
</MathBox>

<MathBox title='Rank-nullity theorem' boxType='theorem'>
For any $\mathrm{T}\in\mathcal{L}(V,W)$

$$
\begin{gather*}
  \dim(\ker(\mathrm{T})) + \dim(\operatorname{ran}(\mathrm{T})) = \dim(V) \\
  \operatorname{rank}(\mathrm{T}) + \mathrm{null}(\mathrm{T}) = \dim(V)
\end{gather*}
$$
</MathBox>

<MathBox title='' boxType='corollary'>
Let $\mathrm{T}\in\mathcal{L}(V,W)$, where $\dim(V) = \dim(W) < \infty$. Then $\mathrm{T}$ is injective if and only if it is surjective.
</MathBox>

## Finite-dimensional linear transformations

Any $m\times n$ matrix $\boldsymbol{A}$ over $\mathbb{F}$ defines a linear transformation $\mathrm{T}_{\boldsymbol{A}}:\mathbb{F}^n\to\mathbb{F}^m$ in the form of the multiplication map $\boldsymbol{v}\mapsto \boldsymbol{A}\boldsymbol{v}$. 

<MathBox title='' boxType='lemma'>
1. If $\boldsymbol{A}$ is an $m\times n$ matrix over $\mathbb{F}$, then the multiplication function $\mathrm{T}_\boldsymbol{A}:\mathbb{F}^n \to\mathbb{F}^m$ defined by $\boldsymbol{v} \mapsto \boldsymbol{A}\boldsymbol{v}$ is a linear map, i.e. $\mathrm{T}_\boldsymbol{A} \in\mathcal{L}(\mathbb{F}^n,\mathbb{F}^m)$.
2. If $\mathrm{T}\in\mathcal{L}(\mathbb{F}^n,\mathbb{F}^m)$ then $\mathrm{T} = \mathrm{T}_\boldsymbol{A}$ where for the standard basis $E = \Set{ \boldsymbol{e}_i }_{i=1}^n$

$$
  \boldsymbol{A} = \begin{bmatrix}
    \shortmid & ~ & \shortmid \\
    \mathrm{T}(\boldsymbol{e}_1) & \cdots & \mathrm{T}(\boldsymbol{e}_n) \\
    \shortmid & ~ & \shortmid
  \end{bmatrix} \in\mathcal{M}_{m,n}(\mathbb{F})
$$

is the matrix of $T$.

<details>
<summary>Proof</summary>

**(1)**: For a matrix $\boldsymbol{A}\in\mathcal{M}_{m,n}(\mathbb{F})$, vectors $\boldsymbol{v}, \boldsymbol{w} \in \mathbb{F}^n$ and scalars $\alpha, \beta\in\mathbb{F}$ the associativity and distributivity properties of matrix multiplication gives

$$
  \boldsymbol{A}(\alpha\boldsymbol{v} + \beta\boldsymbol{w}) = \boldsymbol{A}(\alpha\boldsymbol{v}) + \boldsymbol{A}(\beta\boldsymbol{v}) = \alpha \boldsymbol{A}\boldsymbol{v} + \beta\boldsymbol{A}\boldsymbol{w}
$$

showing that $\mathrm{T}_{\boldsymbol{A}} \in\mathcal{L}(\mathbb{F}^n,\mathbb{F}^m)$.

**(2)**: Let $E = \Set{ \boldsymbol{e}_i }_{i=1}^n$ be the standard basis of $\mathbb{F}^n$. If a vector $\boldsymbol{v}\in V$ has coordinates $[\boldsymbol{v}]_E = \left[(\beta_i)_{i=1}^n\right]^T \in\mathbb{F}^n$ then $\boldsymbol{v}$ can be written as the linear combination

$$
  \boldsymbol{v} = \sum_{i=1}^n \beta_i \boldsymbol{e}_i  
$$

By the linearity of $\mathrm{T}$

$$
\begin{align*}
  \mathrm{T}(\boldsymbol{v}) =& \mathrm{T} \left(\sum_{i=1}^n \beta_i \boldsymbol{e}_i \right) = \sum_{i=1}^n \beta_i \mathrm{T}(\boldsymbol{e}_i) \\
  =& \begin{bmatrix} \mathrm{T}(\boldsymbol{e}_1) & \cdots & \mathrm{T}(\boldsymbol{e}_n) \end{bmatrix} [\boldsymbol{v}]_E \\
  =& \boldsymbol{A}[\boldsymbol{v}]_E = \mathrm{T}_\boldsymbol{A} (\boldsymbol{v})
\end{align*}
$$

Hence $\boldsymbol{A} = \begin{bmatrix} \mathrm{T}(\boldsymbol{e}_1) & \cdots & \mathrm{T}(\boldsymbol{e}_n) \end{bmatrix}$.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $\boldsymbol{A}$ be an $m\times n$ matrix over $F$.
1. $\mathrm{T}_\boldsymbol{A}:\mathbb{F}^n \to\mathbb{F}^m$ is injective if and only if $\operatorname{rank}(\boldsymbol{A}) = n$.
2. $\mathrm{T}_\boldsymbol{A}:\mathbb{F}^n \to\mathbb{F}^m$ is surjective if and only if $\operatorname{rank}(\boldsymbol{A}) = m$.
</MathBox>

### Change of basis matrices

<MathBox title='Change of basis operator' boxType='definition'>
Let $B $ and $C$ be ordered bases for an $n$-dimensional vector space $V$. For any $\boldsymbol{v}\in V$, the map $\varphi_{B,C} = \varphi_C \circ \varphi_B^{-1}$ given by $[\boldsymbol{v}]_B \mapsto [\boldsymbol{v}]_C$ is called the change of basis operator.

$$
\begin{CD}
  V @= V \\
  @V{\varphi_B}VV @VV{\varphi_C}V \\
  \mathbb{F} @>>{\varphi_C^{-1} \circ \varphi_D}> \mathbb{F}
\end{CD}
$$
</MathBox>

<MathBox title='' boxType='proposition'>
Let $B = \Set{\boldsymbol{b}_i}_{i=1}^n$ and $C$ be ordered bases for an $n$-dimensional vector space $V$. The change of basis operator $\varphi_{B,C} = \varphi_C \varphi_B^{-1}$ from $B$ to $C$ is an automorphism of $\mathbb{F}^n$ whose standard matrix is

$$
  \boldsymbol{M}_{B,C} = \begin{bmatrix} [\boldsymbol{b}_1]_C & \cdots & [\boldsymbol{b}_n]_C \end{bmatrix}
$$

Hence $[\boldsymbol{v}]_C = \boldsymbol{M}_{B,C}[\boldsymbol{v}]_B$ and $\boldsymbol{M}_{C,B} = \boldsymbol{M}_{B,C}^{-1}$.

<details>
<summary>Proof</summary>

Since $\varphi_{B,C}$ is an operator on $\mathbb{F}^n$ it has the form $\mathrm{T}_\boldsymbol{M}$ where $\boldsymbol{M}\in\mathcal{M}_n$

$$
\begin{align*}
  \boldsymbol{M} =& \begin{bmatrix} \varphi_{B,C}(\boldsymbol{e}_1) & \cdots & \varphi_{B,C}(\boldsymbol{e}_n) \end{bmatrix} \\
  =& \begin{bmatrix} \varphi_C \varphi_B^{-1}([\boldsymbol{b}_1]_B) & \cdots & \varphi_C \varphi_B^{-1}([\boldsymbol{b}_n]_B) \end{bmatrix} \\
  =& \begin{bmatrix} [\boldsymbol{b}_1]_C & \cdots & [\boldsymbol{b}_n]_C \end{bmatrix}
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Functional dependency of bases and transformation matrices' boxType='proposition'>
If given any two of the following
1. an invertible $n\times n$ matrix $\boldsymbol{A}$
2. an ordered basis $B$ for $\mathbb{F}^n$
3. an ordered basis $C$ for $\mathbb{F}^n$

then the third is uniquely determined by the equation

$$
  \boldsymbol{A} = \boldsymbol{M}_{B,C}
$$

<details>
<summary>Proof</summary>

The result is clear if $B$ and $C$ are given or if $\boldsymbol{A}$ and $C$ are given. If $\boldsymbol{A}$ and $B$ are given, then there is a unique $C$ for which $\boldsymbol{A}^{-1} = \boldsymbol{M}_{C,B}$ and so there is a unique $C$ for which $\boldsymbol{A} = \boldsymbol{M}_{B,C}$.
</details>
</MathBox>

### The matrix of a linear transformation

<MathBox title='' boxType='proposition'>
Let $\mathrm{T}:V\to W$ be a linear transformation, where $\dim(V) = n$ and $\dim(W) = m$, and let $B = \Set{\boldsymbol{b}_i}_{i=1}^n$ be an ordered basis for $V$ and $C$ and ordered basis for $W$. Then $\mathrm{T}$ can be represented with respect to $B$ and $C$ as the matrix product $[\mathrm{T}(\boldsymbol{v})]_C = [\mathrm{T}]_{B,C}[\boldsymbol{v}]_B$ where

$$
  [\mathrm{T}]_{B,C} = \begin{bmatrix} [\mathrm{T}(\boldsymbol{b}_1)]_C & \cdots & [\mathrm{T}(\boldsymbol{b}_n)]_C \end{bmatrix}
$$

is called the matrix of $\mathrm{T}$ with respect to $B$ and $C$. If $V = W$ and $B = C$ we denote $[\mathrm{T}]_{B,B}$ by $[\mathrm{T}]_{B}$ and so

$$
  [\mathrm{T}(\boldsymbol{v})]_B = [\mathrm{T}]_B [\boldsymbol{v}]_B
$$

<details>
<summary>Proof</summary>

Let $\mathrm{T}\in\mathcal{L}(V, W)$ and let $B = \Set{b_i}_{i=1}^n$ and $C$ be ordered bases for $V$ and $W$, respectively. Then the map $\theta:[\boldsymbol{v}]_B \mapsto [\mathrm{T}(v)]_C$ is a representation of $\mathrm{T}$ as a linear transformation from $\mathbb{F}^n$ to $\mathbb{F}^m$ in the sense that knowing $\theta$ (along with $B$ and $C$) is equivalent to knowing $\mathrm{T}$.

Since $\theta$ is a linear transformation from $\mathbb{F}^n$ to $\mathbb{F}^m$, it is simply multiplication by an $m \times n$ matrix $\boldsymbol{A}$, i.e. $[\mathrm{T}(v)]_C = \boldsymbol{A}[\boldsymbol{v}]_B$. Since $[\boldsymbol{b}_i]_B = \boldsymbol{e}_i$ we get the columns of $\boldsymbol{A}$

$$
  \boldsymbol{A}[\boldsymbol{v}]_B = \begin{bmatrix} [\mathrm{T}(\boldsymbol{b}_i)]_C & \cdots & [\mathrm{T}(\boldsymbol{b}_n)]_C  \end{bmatrix} [\boldsymbol{v}]_B
$$
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $\mathrm{T}\in\mathcal{L}(V, W)$, and let $V$ and $W$ be vector spaces over $\mathbb{F}$, with ordered bases $B = \Set{\boldsymbol{b}_i}_{i=1}^n$ and $C = \Set{\boldsymbol{c}_i}_{i=1}^m$, respectively.
1. The map $\mathrm{S}:\mathcal{L}(V, W)\to\mathcal{M}_{m,n}(\mathbb{F})$ defined by $\mathrm{S}(\mathrm{T}) = [\mathrm{T}]_{B,C}$ is an isomorphism and so $\mathcal{L}(V,W)\cong\mathcal{M}_{m,n}(\mathbb{F})$.
2. If $\mathrm{R}\in\mathcal{L}(U,V)$ and $\mathrm{T}\in\mathcal{L}(V,W)$ and if $B$, $C$ and $D$ are ordered bases for $U$, $V$ and $W$, respectively, then $[\mathrm{T}\mathrm{R}]_{B,C} = [\mathrm{T}]_{C,D} [\mathrm{R}]_{B,C}$. Thus, the matrix of the product (composition) $\mathrm{T}\mathrm{R}$ is the product of the matrices of $\mathrm{T}$ and $\mathrm{R}$ respectively.

<details>
<summary>Proof</summary>

**(1)**: To see that $S$ is linear, note that for all $i\in\Set{1,\dots,n}$

$$
\begin{align*}
  [\alpha\mathrm{R} + \beta T]_{B,C} [\boldsymbol{b}_i]_B =& [(\alpha\mathrm{R} + \beta T)(\boldsymbol{b}_i)]_C \\
  =& [\alpha\mathrm{R}(\boldsymbol{b}_i) + \beta\mathrm{T}(\boldsymbol{b}_i)]_C \\
  =& \alpha[\mathrm{R}(\boldsymbol{b}_i)]_C + \beta[\mathrm{T}(\boldsymbol{b}_i)]_C \\
  =& \alpha[\mathrm{R}]_{B,C}[\boldsymbol{b}_i]_B + \beta[\mathrm{T}]_{B,C}[\boldsymbol{b}_i]_B \\
  =& (\alpha[\mathrm{R}]_{B,C} + \beta[\mathrm{T}]_{B,C})[\boldsymbol{b}_i]_B
\end{align*}
$$

since $[\boldsymbol{b}_i]_B = \boldsymbol{e}_i$ is a standard basis vector, it follows that

$$
  [\alpha\mathrm{R} + \beta \mathrm{T}]_{B,C} = \alpha[\mathrm{R}]_{B,C} + \beta[\mathrm{T}]_{B,C}
$$

showing that $\mathrm{S}$ is linear. If $\boldsymbol{A}\in\mathcal{M}_{m,n}$, we define $\mathrm{T}$ by the condition $[\mathrm{T}(\boldsymbol{b}_i)]_C = \boldsymbol{A}_i$ giving $\mathrm{S}(\mathrm{T}) = \boldsymbol{A}$ which is surjective. Since $\dim(\mathcal{L}(V,W)) = \dim(\mathcal{M}_{m,n})$, the map $\mathrm{S}$ is an isomorphism.

**(2)**: We have

$$
\begin{align*}
  [\mathrm{T}\mathrm{R}]_{B,D}[\boldsymbol{v}]_B =& [\mathrm{T}(\mathrm{R}(\boldsymbol{v}))]_D = [\mathrm{T}]_{C,D}[\mathrm{R}(\boldsymbol{v})]_{C} \\
  =& [\mathrm{T}]_{C,D}[\mathrm{R}]_{B,C}[\boldsymbol{v}]_B
\end{align*}
$$
</details>
</MathBox>

### Change of bases for linear transforms

$$
\begin{CD}
  \mathbb{F}^n @>{\psi_{C'} \circ\mathrm{T}\circ\phi_{B'}^{-1}}>> \mathbb{F}^m \\
  @A{\phi_{B'}}AA @AA{\psi_{C'}}A \\
  V @>{\mathrm{T}}>> W \\
  @V{\phi_B}VV @VV{\psi_C}V \\
  \mathbb{F}^n @>>{\phi_C \circ\mathrm{T}\circ\phi_B^{-1}}> \mathbb{F}^m
\end{CD}
$$

<MathBox title='Change of bases equivalence' boxType='proposition'>
Let $\mathrm{T}\in\mathcal{L}(V,W)$ and let $(B,C)$ and $(B',C')$ be pairs of ordered bases of $V$ and $W$, respectively, then

$$
  [\mathrm{T}]_{B',C'} = \boldsymbol{M}_{C,C'}[\mathrm{T}]_{B,C}\boldsymbol{M}_{B',B}
$$

in which case $[\mathrm{T}]_{B',C'} \sim [\mathrm{T}]_{B,C}$.

If $\mathrm{T}\in\mathcal{L}(V)$ and $B$ and $C$ are ordered bases for $V$, then the matrix of $T$ reduce to

$$
  [\mathrm{T}]_C = \boldsymbol{M}_{B,C}[\mathrm{T}]_B \boldsymbol{M}_{B,C}^{-1}
$$

in which case $[\mathrm{T}]_C \sim [\mathrm{T}]_B$.

<details>
<summary>Proof</summary>

Multiplication by $[\mathrm{T}]_{B',C'}$ sends $[\boldsymbol{v}]_{B'}$ to $[\mathrm{T}(v)]_{C'}$. This can be reproduced by first switching from $B'$ to $B$, then applying $[\mathrm{T}]_{B,C}$, and finally switching from $C$ to $C'$, i.e.

$$
\begin{align*}
  [\mathrm{T}]_{B',C'} =& \boldsymbol{M}_{C,C'}[\mathrm{T}]_{B,C} \boldsymbol{M}_{B',B} \\
  =& \boldsymbol{M}_{C,C'} [\mathrm{T}]_{B,C} \boldsymbol{M}_{B,B'}^{-1}
\end{align*}
$$
</details>
</MathBox>

### Equivalence of matrices

<MathBox title='Equivalence of matrices' boxType='definition'>
Two matrices $\boldsymbol{A}$ and $\boldsymbol{B}$ are *equivalent* if there exist invertible matrices $\boldsymbol{P}$ and $\boldsymbol{Q}$ for which

$$
  \boldsymbol{B} = \boldsymbol{P}\boldsymbol{A}\boldsymbol{Q}^{-1}
$$
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V$ and $W$ be vector spaces with $\dim(V) = n$ and $\dim(W) = m$. Then two $m\times n$ matrices $\boldsymbol{A}$ and $\boldsymbol{B}$ are equivalent if and only if they represent the same linear transformation $\mathrm{T}\in\mathcal{L}(V,W)$, possibly with respect to different ordered bases. In this case, $\boldsymbol{A}$ and $\boldsymbol{B}$ represent exactly the same set of linear transformation in $\mathcal{L}(V,W)$.

<details>
<summary>Proof</summary>

If $\boldsymbol{A}$ and $\boldsymbol{B}$ represent $\mathrm{T}$, i.e. if $\boldsymbol{A} = [\mathrm{T}]_{B,C}$ and $\boldsymbol{B} = [\mathrm{T}]_{B',C'}$ for ordered bases $B$, $C$, $B'$ and $C'$, then by the change of basis equivalence, $\boldsymbol{A}$ and $\boldsymbol{B}$ are equivalent. 

Conversely, suppose that $\boldsymbol{A}$ and $\boldsymbol{B}$ are eqiuvalent, i.e. $\boldsymbol{B} = \boldsymbol{P}\boldsymbol{A}\boldsymbol{Q}^{-1}$ where $\boldsymbol{P}$ an $\boldsymbol{Q}$ are invertible. Suppose that $\boldsymbol{A}$ represents a linear transformation $\mathrm{T}\in\mathcal{L}(V,W)$ for some ordered basis $B$ and $C$, i.e.

$$
  \boldsymbol{A} = [\mathrm{T}]_{B,C}
$$

By the functional dependency property, there is a unique ordered basis $B'$ for $V$ for which $\boldsymbol{Q} = \boldsymbol{M}_{B,B'}$ and a unique ordered basis $C'$ for $W$ for which $\boldsymbol{P} = \boldsymbol{M}_{C,C'}$. Thus

$$
  \boldsymbol{B} = \boldsymbol{M}_{C,C'}[\mathrm{T}]_{B,C} \boldsymbol{M}_{B', B} = [\mathrm{T}]_{B',C'}
$$

showing that $\boldsymbol{B}$ also represents $\mathrm{T}$. By symmetry, we se that $\boldsymbol{A}$ and $\boldsymbol{B}$ represent the same set of linear transformation.
</details>
</MathBox>

### Similarity of matrices

<MathBox title='Similarity of matrices' boxType='proposition'>
Two matrices are *similar* if there exists an invertible matrix $\boldsymbol{P}$ for which $\boldsymbol{B} = \boldsymbol{P}\boldsymbol{A}\boldsymbol{P}^{-1}$. The equivalence classes associated with similarity are called similarity classes.
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V$ be a vector spaces with $\dim(V) = n$. Then two $n\times n$ matrices $\boldsymbol{A}$ and $\boldsymbol{B}$ are similar if and only if they represent the same linear transformation $\mathrm{T}\in\mathcal{L}(V)$, possibly with respect to different ordered bases. In this case, $\boldsymbol{A}$ and $\boldsymbol{B}$ represent exactly the same set of linear transformation in $\mathcal{L}(V)$.

<details>
<summary>Proof</summary>

If $\boldsymbol{A}$ and $\boldsymbol{B}$ represent $\mathrm{T}\in\mathcal{L}(V)$, i.e. if $\boldsymbol{A} = [\mathrm{T}]_B$ and $\boldsymbol{B} = [\mathrm{T}]_C$ for ordered bases $B$ and $C$, then by the change of bases equivalence $\boldsymbol{A}$ and $\boldsymbol{B}$ are similar.

Conversely, suppose that $\boldsymbol{A}$ and $\boldsymbol{B}$ are similar, i.e. $\boldsymbol{B} = \boldsymbol{P}\boldsymbol{A}\boldsymbol{P}^{-1}$. Suppose that $\boldsymbol{A}$ represents a linear operator $\mathcal{L}(V)$ for some ordered basis $B$, i.e. $\boldsymbol{A} = [\mathrm{T}]_B$. By the functional dependency relation, there is a unique ordered basis $C$ for $V$ for which $\boldsymbol{P} = \boldsymbol{M}_{B,C}$. Thus 

$$
  \boldsymbol{B} = \boldsymbol{M}_{B,C}[\mathrm{T}]_{B}\boldsymbol{M}_{B,C}^{-1} = [\mathrm{T}]_C
$$

Hence, $\boldsymbol{B}$ also represents $\mathrm{T}$. By symmetry, it follows that $\boldsymbol{A}$ and $\boldsymbol{B}$ represent the same set of linear operators.
</details>
</MathBox>

### Similarity of operators

<MathBox title='Similarity of operators' boxType='proposition'>
Two linear operators $\mathrm{T}, \mathrm{S}\in\mathcal{L}(V)$ are *similar* if there exists an automorphism $R\in\mathcal{L}(V)$ for which 

$$
  \mathrm{S} = \mathrm{R}\mathrm{T}\mathrm{R}^{-1}
$$

The equivalence classes associated with similarity are called *similarity classes*.
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V$ be a vector spaces with $\dim(V) = n$. Then two linear operators $\mathrm{T},\mathrm{S}\in\mathcal{L}(V)$ are similar if and only if there is a matrix $\boldsymbol{A}\in\mathcal{M}_n$ that represents both operators with respect to possibly different ordered bases. In this case, $\mathrm{T}$ and $\mathrm{S}$ are represented by exactly the same set of matrices in $\mathcal{M}_n$.

<details>
<summary>Proof</summary>

If $\mathrm{T}$ and $\mathrm{S}$ are represented by $\boldsymbol{A}\in\mathcal{M}_{n}$, i.e. if $[\mathrm{T}]_B = \boldsymbol{A} = [\mathrm{S}]_C$ for ordered bases $B$ and $C$ then

$$
  [\mathrm{S}]_C = [\mathrm{T}]_B = \boldsymbol{M}_{C,B}[\mathrm{T}]_C \boldsymbol{M}_{B_C}
$$

Let $R\in\mathcal{L}(V)$ be the automorphism of $V$ defined by $\mathrm{T}(\boldsymbol{c}_i) = \boldsymbol{b}_i$ where $B = \Set{\boldsymbol{b}_i}_{i=n}$ and $C = \Set{\boldsymbol{c}_i}_{i=1}^n$, then

$$
\begin{align*}
  [\mathrm{R}]_C =& \begin{bmatrix} [\mathrm{R}(\boldsymbol{c}_1)]_C & \cdots & [\mathrm{R}(\boldsymbol{c}_n)]_C \end{bmatrix} \\
  =& \begin{bmatrix} [\boldsymbol{b}_1]_C & \cdots & [\boldsymbol{b}_n]_C \end{bmatrix} \\
  =& \boldsymbol{M}_{B,C}
\end{align*}
$$

and so

$$
  [\mathrm{S}]_C = [\mathrm{R}]_C^{-1}[\mathrm{T}]_C [\mathrm{R}]_C = [\mathrm{R}^{-1}\mathrm{T}\mathrm{R}]_C
$$

from which it follows that $\mathrm{T}$ and $\mathrm{S}$ are similar.

Conversely, suppose that $\mathrm{T}$ and $\mathrm{S}$ are similar, i.e. $\mathrm{S} = \mathrm{R}\mathrm{T}\mathrm{R}^{-1}$. Suppose also that $\mathrm{T}$ is represented by the matrix $\boldsymbol{A}\in\mathcal{M}_n$, i.e. $\boldsymbol{A} = [\mathrm{T}]_B$ for some ordered basis $B$. Then

$$
  [\mathrm{S}]_B = [\mathrm{R}\mathrm{T}\mathrm{R}^{-1}]_B = [\mathrm{R}]_B [\mathrm{T}]_B [\mathrm{R}]_B^{-1}
$$

Setting $\boldsymbol{c}_i = h(\boldsymbol{b}_i)$ then $C = \Set{ \boldsymbol{c}_i }_{i=1}^n$ is an ordered basis for $V$ and

$$
\begin{align*}
  [\mathrm{R}]_B =& \begin{bmatrix} [\mathrm{R}(\boldsymbol{b}_1)]_B & \cdots & [\mathrm{R}(\boldsymbol{b}_n)]_B \end{bmatrix} \\
  =& \begin{bmatrix} [\boldsymbol{c}_1]_B & \cdots & [\boldsymbol{c}_n]_C \end{bmatrix} \\
  =& \boldsymbol{M}_{C,B}
\end{align*}
$$

Thus, $[\mathrm{S}]_B = \boldsymbol{M}_{C,B}[\mathrm{T}]_B \boldsymbol{M}_{C,B}^{-1}$ and it follows that

$$
  \boldsymbol{A} = [\mathrm{T}]_B = \boldsymbol{M}_{B,C}[\mathrm{S}]_B \boldsymbol{M}_{B,C}^{-1} = [\mathrm{S}]_C
$$

and so $A$ also represents $S$. By symmetry, it follows that $\mathrm{T}$ and $\mathrm{S}$ are represented by the same set of matrices.
</details>
</MathBox>

## Invariant subspaces and reducing pairs

The restriction of a linear operator $\mathrm{T}\in\mathcal{L}(V)$ to a subspace $S\subseteq V$ is not necessarily a linear operator on $S$.

<MathBox title='Invatiant subspaces' boxType='definition'>
Let $\mathrm{F}\in\mathcal{L}(V)$. A subspace $S\subseteq V$ is *invariant* under $\mathrm{F}$ if $\mathrm{F}(S) \subseteq S$. That is, $S$ is invariant under $\mathrm{F}$ if the restriction $\mathrm{F}|_S$ is a linear operator on $S$.
</MathBox>

If $V = S \oplus T$ then the fact that $S$ is $\mathrm{F}$-invariant does not imply that the complement $T$ is also $\mathrm{F}$-invariant.

<MathBox title='Reducing pair' boxType='definition'>
Let $\mathrm{F}\in\mathcal{L}(V)$. If $V = S\oplus T$ and if both $S$ and $T$ are $\mathrm{F}$-invariant, we say that the pair $(S,T)$ *reduces* $\mathrm{T}$.
</MathBox>

<MathBox title='Direct sum of linear operators' boxType='definition'>
Let $\mathrm{F}\in\mathcal{L}(V)$. If $(S,T)$ reduces $\mathrm{F}$ we write

$$
  \mathrm{F} = \mathrm{F}|_S \oplus \mathrm{F}|_U
$$

and call $\mathrm{F}$ the *direct sum* of $\mathrm{F}|_S$ and $\mathrm{F}|_T$. Thus, the expression $\mathrm{H} = \mathrm{G} \oplus \mathrm{F}$ means that there exists subspaces $S$ and $T$ of $V$ for which $(S,T)$ reduces $H$ with $\mathrm{G} = \mathrm{R}|_S$ and $\mathrm{F} = \mathrm{R}|_T$.
</MathBox>

# Topological vector space

The standard topology on $\R^n$ is the topology induced by the Euclidean metric on $\R^n$ for which the set of open rectangles

$$
  B = \Set{ \prod_{i=1}^n I_i | I_i \text{ is open an open interval in } \R }
$$

is a basis, i.e. a subset of $\R^n$ is open if and only if it is a union of sets in $B$. The standard topology on $\R^n$ has the properties that the any linear functional $f:\R^n \to\R^n$ together with the addition function

$$
  \mathcal{A}:\R^n\times\R^n \ni (v,w) \mapsto v + w \in\R^n
$$

and the scalar multiplication function

$$
  \mathcal{M}:\R\times\R^n \ni (\lambda,\boldsymbol{v}) \mapsto\lambda \boldsymbol{v} \in\R^n
$$

are continuous. As such $\R^n$ is a topological vector space.

Generally, any real vector space $V$ endowed with a topology $\mathcal{T}$ is called a *topological vector space* if the operations of addition $\mathcal{A}:V\times V\to V$ and scalar multiplication $\mathcal{M}:\R\times V\to V$ are continuous under $\mathcal{T}$.

<MathBox title='Topological vector space' boxType='proposition'>
Let $V$ be a real vector space with $\dim(V) = n$. There is a unique topology $\mathcal{T}$ on $V$, called the *natural topology* for which $V$ is a topological vector space and for which all linear functionals on $V$ are continuous. This topology is determined by the fact that the coordinate map $\varphi: V\to\R^n$ is a homeomorphism.

<details>
<summary>Proof</summary>

Let $V$ be any real vector space with $\dim(V) = n$ and fix and ordered basis $B = \Set{ \boldsymbol{v}_i }_{i=1}^n$ for $V$. Consider the coordinate map

$$
  \varphi = \varphi_B: V\ni \boldsymbol{v} \mapsto [\boldsymbol{v}]_B \in \R^n
$$

and its inverse

$$
  \psi_B = \varphi_B^{-1}:\R^n \ni (\lambda_i)_{i=1}^n \mapsto sum_{i=1}^n \lambda_i \boldsymbol{v}_i
$$

We need to show that there is a unique topology $\mathcal{T}$ on $V$ for which $\varphi_B$ (and thus $\psi_B$) is a homeomorphism, i.e. a bijection that is continuous with a continuous inverse.

**$\psi$ is continuous under $\mathcal{T}$**<br/>
First we show that if $V$ is a topological vector space under a topology $\mathcal{T}$ then $\psi$ is continuous. Since $\psi = \sum_{i=1}^n \psi$ where $\psi_i:\R\to V$ is defined by $\psi_i (\lambda_i)_{i=1}^n = \lambda_i \boldsymbol{v}_i$, it is sufficient to show that these maps continuous, as the sum of continuous maps is continuous.

Let $O$ be an open set in $\mathcal{T}$. Then the inverse scalar multiplication operation

$$
  \mathcal{M}^{-1}(O) = \Set{(\alpha, \boldsymbol{v})\in\R\times V | \alpha}
$$

is an open set in $\R\times V$. We need to show that the set

$$
  \psi_i^{-1}(O) = \Set{(\lambda_i)_{i=1}^n \in\R^n | \lambda_i \boldsymbol{v}_i \in O }
$$

is open in $\R^n$. Let $(\lambda_i)_{i=1}^n \in\psi_i^{-1}(O)$ so that $\lambda_i \boldsymbol{v}_i \in O$. It follows that $(\lambda_i, \boldsymbol{v}_i)\in\mathcal{M}^{-1}(O)$, which is open. Thus, there is an open interval $I\subseteq\mathbb{R}$ and an open set $B\in\mathcal{T}$ of $V$ for which

$$
  (\lambda_i, \boldsymbol{v}_i) \in I \times B \subseteq\mathcal{M}^{-1}(O)
$$

Then the open set $U = \R\times\cdot\R\times I \times\R\times\cdots\times\R$ where $I$ is in the $i$th position, has the property that $\psi_i(U)\subseteq O$. Thus

$$
  (\lambda_i)_{i=1}^n \in U \subseteq \psi_i^{-1}(O)
$$

and so $\psi_i^{-1}(O)$ is open. Hence, $\psi_i$, and therefore $\psi$, is continuous.

**$\varphi$ is continuous under $\mathcal{T}$**<br/>
Next we show that if every linear functional on $V$ is continuous under a topology $\mathcal{T}$ on $V$ then the coordinate map $\varphi$ is continuous. For $\boldsymbol{v}\in V$ let $[\boldsymbol{v}]_{B,i}$ denote the $i$th coordinate of $[\boldsymbol{v}]_B$. The map $\mu:V \to\R$ defined by $\mu(\boldsymbol{v}) = [\boldsymbol{v}]_{B,i}$ is a linear functional and so is continuous by assumption. Thus, for any open interval $I_i \in\R$ the set

$$
  A_i = \Set{ \boldsymbol{\boldsymbol{v}}\in V | [\boldsymbol{v}]_{B,i} \in I_i }
$$

is open. If $I_i$ are open intervals in $\R$ then

$$
  \varphi^{-1}(\prod_{i=1}^n I_i) = \Set{ \boldsymbol{\boldsymbol{v}}\in V | [\boldsymbol{v}]_B \in \prod_{i=1}^n I_i } = \bigcap_{i=1}^n A_i
$$

is open. Hence $\varphi$ is continuous.

**$\mathcal{T}$ is unique topology on $V$**<br/>
If a topology $\mathcal{T}$ has the property that $V$ is a topological vector space and every linear functional is continuous, then $\varphi$ and $\psi = \varphi^{-1}$ are homeomorphisms. This means that $\mathcal{T}$, if it exists, must be unique.

It remains to prove that the topology $\mathcal{T}$ on $V$ that makes $\varphi$ a homeomorphism has the property that $V$ is a topological space under $\mathcal{T}$ and that any linear functional $f$ on $V$ is continuous.

**Addition $\mathcal{A}$ is continuous under $\mathcal{T}$**<br/>
As to addition, the maps $\varphi: V\to\R^n$ and $(\varphi\times\varphi):V\times V\to\R^n \times\R^n$ are homeomorphisms and the map $\mathcal{A}':\R^n\times \R^n \to\R^n$ is continuous and so the map $\mathcal{A}:V\times V\to V$, being equal to $\varphi^{-1}\circ\mathcal{A}'\circ(\varphi\times\varphi)$,, is also continuous.

**Scalar multiplication $\mathcal{M}$ is continuous under $\mathcal{T}$**<br/>
As to scalar multiplication, the maps $\varphi:V\to\R^n$ and $(\iota\times\varphi):\R\times V\to \R\times\R^n$ are homeomorphisms and the map $\mathcal{M}:V\times V\to V$, being equal to $\varphi^{-1}\circ\mathcal{M}'\circ(\iota\times\varphi)$, is also continuous.

**Any linear functional $f:V\to\R$ is continuous**<br/>
Let $f:V\to\R$ be a linear functional. Since $\varphi$ is continuous if and only if $f\circ\varphi^{-1}$ is continuous, we can confine attention to $V=\R^n$. In this case, if $\Set{\boldsymbol{e}_i}_{i=1}^n$ is the standard basis for $\R^n$ and $|f(\boldsymbol{e}_i)|\leq M$, then for any $x=(\lambda_i)\in\R^n$ we have

$$
\begin{align*}
  |f(x)| =& \left| \sum_{i=1}^n \lambda_i f(\boldsymbol{e}_i) \right| \\
  \leq& \sum_{i=1}^n |\lambda_i|\cdot|f(\boldsymbol{e}_i)| \leq M\sum_{i=1}^n |\lambda_i|
\end{align*}
$$

If $|x| < \frac{\varepsilon}{Mn}$ then $|\lambda_i| < \frac{\varepsilon}{Mn}$ and so $|f(x)| < \varepsilon$, which implies that $f$ is continuous.

By the Riesz representation theorem and the Cauchy-Schwarz inequality we have

$$
  \| f(x) \| \leq \| \mathcal{R}_f \| \cdot \| x\|
$$

Hence, $x_n \to 0$ implies $f(x_n)\to 0$ and so by linearity, $x_n \to x$ implies $f(x_n)\to x$ and so $f$ is continuous.
</details>
</MathBox>

# Quotient space

<MathBox title='Quotient space' boxType='definition'>
Let $S$ be a subspace of a $\mathbb{F}$-vector space $V$. The binary relation on $V$ defined by

$$
  \boldsymbol{u} \sim \boldsymbol{v} \iff \boldsymbol{u} - \boldsymbol{v} \in S,\; \boldsymbol{u},\boldsymbol{\boldsymbol{v}}\in V
$$

is an equivalence relation. When $\boldsymbol{u}\sim \boldsymbol{v}$, we say that $\boldsymbol{u}$ and $\boldsymbol{v}$ are *congruent modulo* $S$ written as

$$
  \boldsymbol{u}\sim \boldsymbol{v} \mod S
$$

The equivalence class

$$
  [\boldsymbol{v}]_\sim = \boldsymbol{v} + S = \Set{\boldsymbol{v}+\boldsymbol{s} | \boldsymbol{s}\in S}
$$

is called a *coset* of $S$ in *V* and $\boldsymbol{v}$ is called a *coset representative* for $\boldsymbol{v} + S$. The set of all cosets of $S$ in $V$ is denoted

$$
  V/S = \Set{\boldsymbol{v} + S | \boldsymbol{\boldsymbol{v}}\in V}
$$

and is called the *quotient space* of $V$ modulo $S$. The quotient space is a vector space under the operations
- $\lambda \boldsymbol{u}_1 + S = \lambda \boldsymbol{u}_2 + S$ for $\boldsymbol{u}_1\in V$ and $\lambda\in\mathbb{F}$
- $(\boldsymbol{u}_1 + \boldsymbol{v}_1) + S = (\boldsymbol{u}_2 + v_2) + S$ for $\boldsymbol{u}_1, \boldsymbol{u}_2, \boldsymbol{v}_1, \boldsymbol{v}_2\in V$

The zero vector in $V/S$ is the coset $\boldsymbol{0} + S = S$

<details>
<summary>Details</summary>

The equivalence classes on $\sim$ take the form

$$
\begin{align*}
  [\boldsymbol{v}]_\sim =& \Set{\boldsymbol{u}\in V | \boldsymbol{u}\sim \boldsymbol{v} } \\
  =& \Set{\boldsymbol{u}\in V | \boldsymbol{u}-\boldsymbol{v} \in S } \\
  =& \Set{ \boldsymbol{u}\in V | \boldsymbol{u} = \boldsymbol{v} + \boldsymbol{s},\; \boldsymbol{s}\in S } \\
  =& \Set{ \boldsymbol{v} + \boldsymbol{s} | \boldsymbol{s}\in S } \\
  =& \boldsymbol{v} + S
\end{align*}
$$

If $\boldsymbol{u}_1 \sim \boldsymbol{v}_1$ and $\boldsymbol{u}_2 \sim \boldsymbol{v}_2$, then 

$$
\begin{align*}
  \boldsymbol{u_1} - \boldsymbol{v}_1 \in S, \boldsymbol{u}_2 - \boldsymbol{v}_2 \in \implies& \alpha(\boldsymbol{u}_1 - \boldsymbol{v}_1) + \beta(\boldsymbol{u}_2 - \boldsymbol{v}_2) \in S \\
  \implies& (\alpha\boldsymbol{u}_1 + \beta\boldsymbol{u}_2) - (\alpha\boldsymbol{v}_1 + \beta \boldsymbol{v}_2)\in S \\
  \implies& \alpha\boldsymbol{u}_1 + \beta\boldsymbol{u}_2 \sim \alpha\boldsymbol{v}_1 + \beta \boldsymbol{v}_2
\end{align*}
$$

showing that congruence modulo $S$ is preserved under the vector space operations.
</details>
</MathBox>

## Natural projection

<MathBox title='Natural projection' boxType='proposition'>
The natural (canonical) projection $\pi_S: V\to V/S$ defined by

$$
  \pi_S (\boldsymbol{v}) = \boldsymbol{v} + S
$$

is a surjective linear transformation with $\ker(\pi_S) = S$.

<details>
<summary>Details</summary>

The natural projection $\pi_S$ is clearly surjective.

For $\boldsymbol{u},\boldsymbol{\boldsymbol{v}}\in V$ and $\alpha,\beta\in\mathbb{F}$ we have
$$
\begin{align*}
  \pi_S(\alpha\boldsymbol{u} + \beta\boldsymbol{v}) =& (\alpha\boldsymbol{u} + \beta\boldsymbol{v}) + S \\
  =& \alpha(\boldsymbol{u} + S) \beta(\boldsymbol{v} + S) \\
  =& \alpha\pi_S(\boldsymbol{u}) + \beta\pi_S(\boldsymbol{v})
\end{align*}
$$

showing that $\pi_S$ is linear.

To determine the kernel of $\pi_S$, note that

$$
\begin{align*}
  \boldsymbol{v}\in \ker(\pi_S) \iff& \pi_S(\boldsymbol{v}) = \boldsymbol{0} \\
  \iff& \boldsymbol{v} + S = S \iff \boldsymbol{v}\in S
\end{align*}
$$

Hence $\ker(\pi_S) = S$.
</details>
</MathBox>

<MathBox title='The correspondence theorem' boxType='theorem'>
Let $S$ be a subspace of $V$. Then the function that assigns to each intermediate subspace $S\subseteq T\subseteq V$ the subspace $T/S$ of $V/S$ is an order preserving (with respect to set inclusion) one-to-one correspondence between the set of all subspaces of $V$ containing $S$ and the set of all subspaces of $V/S$.
</MathBox>

## The first isomorphism theorem

<LatexFig width={50} src='/fig/universal_property_linear_transformation.svg' alt=''
  caption='Universal property for linear transformation'
>
```latex
\documentclass[tikz]{standalone}
\usepackage{amssymb}
\usepackage{tikz}
\usetikzlibrary{cd}

\begin{document}

\begin{tikzcd}[
  row sep=large, column sep=huge, 
  every label/.append style={font=\scriptsize}
]
  V \arrow[d, "\mathrm{P}_S" swap] \arrow[r, "\mathrm{T} = \mathrm{T}' \circ \mathrm{P}_S"] & W \\
  V/S \arrow[ru, "\mathrm{T}'" swap] &
\end{tikzcd}

\end{document}
```
</LatexFig>

<MathBox title='Universal property' boxType='theorem'>
Let $S$ be a subspace of $V$ and let $\mathrm{T}\in\mathcal{L}(V,W)$ satisfy $S\subseteq\ker(\mathrm{T})$. Then there is a unique linear transformation $\mathrm{T}':V/S\to W$ with the property that 

$$
  \mathrm{T}' \circ \pi_S = \mathrm{T}
$$

and $\ker(\mathrm{T}') = \ker(\mathrm{T})/S$ and $\operatorname{ran}(\mathrm{T}') = \operatorname{ran}(\mathrm{T})$.

<details>
<summary>Details</summary>

There is no other choice but to define $\mathrm{T}'$ by the condition $\mathrm{T}' = \pi_S = \mathrm{T}$, i.e.

$$
  \mathrm{T}'(\boldsymbol{v} + S) = \mathrm{\mathrm{T}}(\boldsymbol{v})
$$

This function is well-defined if and only if

$$
  \boldsymbol{v} + S = \boldsymbol{u} + S \implies \mathrm{T}'(\boldsymbol{v} + S) = \mathrm{T}'(\boldsymbol{u} + S)
$$

which is equivalent to each of the following statements

$$
\begin{align*}
  \boldsymbol{v} + S = \boldsymbol{u} + S \implies& \mathrm{T}(\boldsymbol{v}) = \mathrm{T}(\boldsymbol{u}) \\
  \boldsymbol{v} - \boldsymbol{u} \in S \implies& \mathrm{T}(\boldsymbol{v} - \boldsymbol{u}) = \boldsymbol{0} \\
  \boldsymbol{x}\in S \implies& \mathrm{T}(\boldsymbol{x}) = \boldsymbol{0} \\
  S \subseteq& \ker(\mathrm{T})
\end{align*}
$$

Thus, $\mathrm{T}':V/S\to W$ is well defined. Also

$$
\begin{align*}
  \operatorname{ran}(\mathrm{T}') =& \Set{ \mathrm{T}'(\boldsymbol{v} + S) | \boldsymbol{\boldsymbol{v}}\in V } \\
  =& \Set{ \mathrm{T}(\boldsymbol{v}) | \boldsymbol{v}\in V } \\
  =& \operatorname{ran}(\mathrm{T})
\end{align*}
$$

and

$$
\begin{align*}
  \ker(\mathrm{T}') =& \Set{\boldsymbol{v} + S | \mathrm{T}'(\boldsymbol{v} + S) = \boldsymbol{0} } \\
  =& \Set{\boldsymbol{v} + S | \mathrm{T}(\boldsymbol{v}) = \boldsymbol{0} } \\
  =& \Set{\boldsymbol{v} + S | \boldsymbol{v}\in\ker(\mathrm{T}) } 
  =& \operatorname{ran}(\mathrm{T})/S
\end{align*}
$$

The uniqueness of $\mathrm{T}'$ is evident.
</details>
</MathBox>

<MathBox title='First isomorphism theorem' boxType='theorem'>
For $\mathrm{T}\in\mathcal{L}(V,W)$, the linear transformation $\mathrm{T}':V/\ker(\mathrm{T}) \to W$ defined by

$$
  \mathrm{T}'(\boldsymbol{v} + \ker(\mathrm{T})) = \mathrm{T}(\boldsymbol{v})
$$

is injective and 

$$
  V/\ker(\mathrm{T}) \cong \operatorname{ran}(\mathrm{T})
$$

<details>
<summary>Details</summary>

The first isomorphism theorem follows from the universal property.
</details>
</MathBox>

According to the first isomorphism theorem, the range of any linear transformation on $V$ is isomorphic to a quotient space of $V$. Conversely, any quotient space $V/S$ of $V$ is the range the natural projection $\pi_S$. Thus, up to isomorpisms, quotient spaces are equivalent to homomorphic images.

If $V = S\oplus T$, then the first isomorphism theorem implies that $T \cong V/S$. This can be written as

$$
  (S\oplus T)/T \cong S/(S\cap T)
$$

<MathBox title='Projection operator' boxType='definition'>
Let $S$ be a subspace of $V$ and let $T$ be a complement of $S$, i.e. $V = S\oplus T$. The linear operator $\mathrm{P}_T:V\to V$ defined by

$$
  \mathrm{P}_T(\boldsymbol{s} + \boldsymbol{t}) = \boldsymbol{t},\; \boldsymbol{s}\in S,\boldsymbol{t}\in T
$$

is called the projection onto $T$ along $S$ and satisfies

$$
\begin{align*}
  \operatorname{ran}(\mathrm{P}_T) =& T \\
  \ker(\mathrm{P}_T) =& \Set{ \boldsymbol{s}+\boldsymbol{t}\in V | \boldsymbol{t} = \boldsymbol{0} } = S
\end{align*}
$$
</MathBox>

## Codimension

<MathBox title='' boxType='proposition'>
Let $S$ be a subspace of $V$. All complements of $S$ in $V$ are isomorphic to $V/S$ and hence to each other.

<details>
<summary>Details</summary>

This follows directly from the first isomorphism theorem.
</details>
</MathBox>

If $A$, $B$ and $C$ are subspaces of the vector space then by the previous proposition

$$
  A\oplus B = A\oplus C \implies B\cong C
$$

However, note that the following cases showcasing that quotients and complements do not always behave nicely with respect to isomorphisms:
1. It is possible that

$$
  A\oplus B = C\oplus D
$$

with $A\cong C$ but $B\not\cong D$. Hence $A\cong C$ does not imply that a complement of $A$ is isomorphic to a complement of $C$.

2. It is possible that $V\cong W$ along with $V = S\oplus B$ and $W = S\oplus D$, yet $B \not\cong$. Hence, $V\cong W$ does not imply that $V/S\not\cong W/S$. However, if $V = W$ then $B \cong D$.

<MathBox title='Codimension' boxType='definition'>
If $S$ is a subspace of $V$, then $\dim(V/S)$ is called the *codimension* of $S$ in $V$ and denoted by $\mathrm{codim}_V(S)$.
</MathBox>

<MathBox title='Relation between dimension and codimension' boxType='proposition'>
Let $S$ be a subspace of $V$. Then

$$
  \dim(V) = \dim(S) + \dim(V/S) = \mathrm{codim}_V(S)
$$

If $V$ is finite-dimensional then

$$
  \mathrm{codim}_V(S) = \dim(V) - \dim(S)
$$
</MathBox>

<MathBox title='Second isomorphism theorem' boxType='theorem'>
Let $S$ and $T$ be subspaces of a vector space $V$. Then

$$
  (S + T)/T \cong S/(S\cap T)
$$

<details>
<summary>Proof</summary>

The function $\mathrm{T}: (S+T) \to S/(S\cap T)$ defined by

$$
  \mathrm{T}(\boldsymbol{s} + \boldsymbol{t}) = \boldsymbol{s} + (S \cap T)
$$

is a well-defined surjective linear transformation with $\ker(\mathrm{T}) = T$. Hence, by the first isomorphism theorem $(S + T)/T \cong S/(S\cap T)$.
</details>
</MathBox>

<MathBox title='Third isomorphism theorem' boxType='theorem'>
Let $V$ be a vector space and suppose that $S\subseteq T\subseteq V$ are subspaces of $V$. Then

$$
  (V/S)/(T/S) \cong V/T
$$

<details>
<summary>Proof</summary>

The function $\mathrm{T}: V/S \to V/S$ defined by

$$
  \mathrm{T}(\boldsymbol{s} + S) = \boldsymbol{s} + T
$$

is a well-defined surjective linear transformation with $\ker(\mathrm{T}) = T/S$. Hence, by the first isomorphism theorem $\mathrm{T}(\boldsymbol{s} + S) = \boldsymbol{s} + T$.
</details>
</MathBox>

<MathBox title='Third isomorphism theorem' boxType='theorem'>
Let $S$ be a subspace of the vector space $V$. Suppose that $V = V_1 \oplus V_2$ and $S = S_2 \oplus S_2$ with $S_i \subseteq V_i$ for $i\in\Set{1,2}$. Then

$$
  (V/S) = (V_1\oplus V_2)/(S_1\oplus S_2) \cong V_1/S_1 \boxplus V_2/S_2
$$

<details>
<summary>Proof</summary>

The function $\mathrm{T}: V\to V_1/S_1 \boxplus V_2/S_2$ defined by

$$
  \mathrm{T}(\boldsymbol{v_1} + \boldsymbol{v_2}) \cong (\boldsymbol{v_1} + S, \boldsymbol{v_2} + S_2)
$$

is a well-defined since $V = V_1 \oplus V_2$ is a direct sum. It is also a surjective linear transformation with $\ker(\mathrm{T}) = S_1 \oplus S_2$. Hence, by the first isomorphism theorem $\mathrm{T}(\boldsymbol{v_1} + \boldsymbol{v_2}) \cong (\boldsymbol{v_1} + S, \boldsymbol{v_2} + S_2)$.
</details>
</MathBox>

# Dual space

## Linear functionals (covectors)

<MathBox title='Linear functional and dual space' boxType='definition'>
Let $V$ be a vector space over $\mathbb{F}$. A linear transformation $f\in\mathcal{L}(V,\mathbb{F})$ taking values in $\mathbb{F}$ is a *linear functional* or a *linear form* on $V$. The vector space of all linear functionals on $V$ is denoted by $V^*$ and is called the *algebraic dual space* of $V$. The elements of $V^*$ are called *dual vectors* or *covectors*.
</MathBox>

<MathBox title='' boxType='proposition'>
1. For any nonzero vector $\boldsymbol{v}\in V\setminus\Set{\boldsymbol{0}}$, there exists a linear functional $f\in V^*$ for which $f(\boldsymbol{v}) \neq 0$.
2. A vector $\boldsymbol{\boldsymbol{v}}\in V$ is zero if and only if $f(\boldsymbol{v}) = 0$ for all $f\in V^*$
3. Let $f\in V^*$. If $f(\boldsymbol{x}) \neq 0$ then $V = \operatorname{span}\Set{\boldsymbol{v}} \oplus \ker(f)$.
4. Two nonzero linear functionals $f,g\in V^*$ have the same kernel if and only if there is a nonzero scalar $\lambda$ such that $f = \lambda g$.

<details>
<summary>Proof</summary>

**(3):** If $\boldsymbol{0}\neq\boldsymbol{v} \in \operatorname{span}\Set{\boldsymbol{x}}\cap\ker(f)$ then $f(\boldsymbol{v}) = 0$ and $\mathrm{v} = \lambda\boldsymbol{x}$ for $0 \neq \lambda a\in\mathbb{F}$ implying $f(\boldsymbol{x}) = 0$, which is false. Thus, $\operatorname{span}\Set{\boldsymbol{x}}\cap\ker(f) = \Set{\boldsymbol{0}}$ and the direct sum $S = \ker(\boldsymbol{x})\oplus\ker(f)$ exists. Also, for any $\boldsymbol{v}\in V$ we have

$$
  \boldsymbol{v} = \frac{f(\boldsymbol{v})}{f(\boldsymbol{x})}\boldsymbol{x} + \left(\boldsymbol{v} - \frac{f\boldsymbol{v}}{f(\boldsymbol{x})}\boldsymbol{x} \right) \in\operatorname{span}\Set{\boldsymbol{x}} + \ker(f)
$$

and so $V = \operatorname{span}\Set{\boldsymbol{x}}\oplus\ker(f)$.

**(4):** If $f = \lambda g$ for $\lambda\neq 0$ then $\ker(f) = \ker(g)$. Conversely, if $K = \ker(f) = \ker(g)$ then for $\boldsymbol{x}\notin K$, then by **(3)**

$$
  V = \operatorname{span}\Set{\boldsymbol{x}}\oplus K
$$

Note that $f|_K = \lambda g|_K$ for any $\lambda$. Thus, if $\lambda = \frac{f(\boldsymbol{x})}{g\boldsymbol{x}}$, it follows that $\lambda g(\boldsymbol{x}) = f(\boldsymbol{x})$ and hence $f = \lambda g$. 
</details>
</MathBox>

## Dual basis

<MathBox title='Dual basis' boxType='proposition'>
Let $V$ be a vector space with basis $B = \Set{\boldsymbol{b}_i }_{i\in I}$ for some index set $I\subseteq\N$.
1. The set $B^* = \Set{b_i}_{i\in I}$ is linearly independent
2. If $V$ is finite-dimensional then $B^*$ is a basis for $V^*$ called the dual basis of $B$

<details>
<summary>Proof</summary>

**(1):** For each $i \in I$ we can define a linear functional $b_i \in V^*$ by the orthogonality condition $b_i (\boldsymbol{b}_j) = \delta_{ij}$ where $\delta_{ij}$ is the Kronecker delta function. Then the set $B^* = \Set{b_i}_{i\in I}$ is linearly independent since applying the equation

$$
  0 = \sum_{j=1}^n \lambda_{i_j} b_{i_j}
$$

to the basis vectors $\boldsymbol{b}_{i_k}$ gives

$$
  0 = \sum_{j=1}^k \lambda{i_j} b_{i_j} (\boldsymbol{b}_{i_k}) = \sum_{j=1}^k \lambda_{i_j}\delta_{i_j, i_k} = \lambda_{i_k}
$$

for all $i_k$.

**(2):** For any $f\in V^*$ we have

$$
  \sum_{i\in I} f(\boldsymbol{b}_i)b_i (\boldsymbol{b}_j) = \sum_{i\in I} f(\boldsymbol{b}_j)\delta_{ij} = f(\boldsymbol{b}_j)
$$

showing that $f = \sum_{i\in I} f(\boldsymbol{b}_i)b_i$ is in the span of $B^*$. Hence $B^*$ is a basis for $V^*$.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
If $\dim(V) < \infty$ then $\dim(V^*) = \dim(V)$.
</MathBox>

## Reflexivity

If $V$ is a vector space, then so is the dual space $V^*$. Hence, we may form the *double dual space* $V^{**}$ which consists of all linear functionals $\gamma: V^* \to\mathbb{F}$. In other words, an element of $V^{**}$ is a linear map that assigns scalar to each linear function on $V$.

<MathBox title='Double dual space' boxType='definition'>
Let $V$ be a vector space over $\mathbb{F}$. The *double dual space* of $V$, denoted $V^{**}$, is the set of all linear functionals $\mathrm{f}: V^* \to\mathbb{F}$. The elements of $V^{**}$ take the form of a linear function $\bar{\boldsymbol{v}}:V^{*}\to\mathbb{F}$ defined as for $\boldsymbol{v}\in V$

$$
  f \mapsto f(\boldsymbol{v})
$$

The function $\bar{\boldsymbol{v}}$ is called *evaluation at* $\boldsymbol{v}$.

<details>
<summary>Details</summary>

For $f,g\in V^*$ and $\alpha,\beta\in\mathbb{F}$ then

$$
\begin{align*}
  \bar{\boldsymbol{v}}(\alpha f + \beta g) =& (\alpha f + \beta g)(\boldsymbol{v}) \\
  =& \alpha f(\boldsymbol{v}) + \beta g(\boldsymbol{v}) \\
  =& \alpha\bar{\boldsymbol{v}}(f) + \beta\bar{\boldsymbol{v}}(g)
\end{align*}
$$

showing that $\bar{\boldsymbol{v}}$ is linear.
</details>
</MathBox>

<MathBox title='Canonical map' boxType='proposition'>
The canonical map $\mathrm{T}:V\to V^{**}$ defined by $\boldsymbol{v}\mapsto \bar{\boldsymbol{v}}$ where $\bar{\boldsymbol{v}}: V^* \to \mathbb{F}$ is the evaluation at $\boldsymbol{v}$, is a monomorphism. If $V$ is finite-dimensional then $\mathrm{T}$ is an isomorphism.

<details>
<summary>Proof</summary>

The function $\mathrm{T}$ is linear since for $\alpha, \beta\in\mathbb{F}$ and $\boldsymbol{u},\boldsymbol{v}\in V$

$$
\begin{align*}
  \overline{\alpha\boldsymbol{u} + \beta\boldsymbol{v}}(f) =& f(\alpha\boldsymbol{u} + \beta\boldsymbol{v}) \\
  =& \alpha f(\boldsymbol{u}) + \beta f(\boldsymbol{v}) \\
  =& (\alpha\bar{\boldsymbol{u}} + \beta\bar{\boldsymbol{v}})(f)
\end{align*}
$$

for all $f\in V^*$. To determine the kernel of $\mathrm{T}$, note that

$$
\begin{align*}
  \mathrm{T}(\boldsymbol{v}) = 0 \implies& \bar{\boldsymbol{v}} = 0 \\
  \implies& \bar{\boldsymbol{v}}(f) = 0,\; \forall f\in V^* \\
  \implies& f(\boldsymbol{v}) = 0,\; \forall f\in V^* \\
  \implies& \boldsymbol{v} = \boldsymbol{0}
\end{align*}
$$

showing that $\ker(\mathrm{T}) = \Set{\boldsymbol{0}}$.

In the finite-dimensional case, since $\dim(V^{**}) = \dim(V^*) = \dim(V)$, it follows that $\mathrm{T}$ is also surjective, hence an isomorphism.
</details>
</MathBox>

The canonical map implies that $V\cong V^{**}$ if $V$ is a finite-dimensional vector space. Since the canonical map is an isomorphism, then $V$ is called *algebraically reflexive*.

## Annihilators

<MathBox title='Annihilator' boxType='definition'>
Let $M$ be a nonempty subset of a vector space $V$. The *annihilator* $M^0$ of $M$ is the set

$$
  M^0 = \Set{ f\in V^* | f(M) } = \Set{ 0 }
$$

where $f(M) = \Set{ f(\boldsymbol{v}) | \boldsymbol{v}\in M }$
</MathBox>

<MathBox title='Properties of annihilators' boxType='proposition'>
1. **(Order reversing)** For any subset $M$ and $N$ of $V$

$$
  M\subseteq N \implies N^0 \subseteq M^0
$$

2. If $\dim(V) < \infty$ then $M^{00} \cong \operatorname{span}(M)$ under the canonical map. In particular, if $S$ is a subspace of $V$, then $S^{00}\cong S$.
3. If $\dim(V) < \infty$ and $S$ and $T$ are subspaces of $V$, then

$$
\begin{align*}
  (S\cap T)^0 =& S^0 + T^0 \\
  (S + T)^0 =& S^0 \cap T^0
\end{align*}
$$
</MathBox>

<MathBox title='Extension by 0' boxType='definition'>
Let $S$ and $T$ be complement subspaces of the vector space $V$ with $V = S\oplus T$. Any linear functional $f\in T^*$ can be extended to a linear functional $\bar{f}$ on $V$ by setting $f(S) = 0$. The linear functional $\bar{f}$ is called the extension by $0$ of $f$. Clearly, $\bar{f}\in S^0$.
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V = S \oplus T$.
1. The extension by zero map is an isomorphism from $T^*$ to $S^0$ and so $T^* \cong S^0$.
2. If $V$ is finite-dimensional then

$$
  \dim(S^0) = \mathrm{codim}_V (S) = \dim(V) - \dim(S)
$$
</MathBox>

<MathBox title='' boxType='proposition'>
A linear functional on the direct sum $V = S\oplus T$ can be written as a direct sum of a linear functional that annihilates $S$ and a linear functional that annihilates $T$, i.e.

$$
  (S\oplus T)^* = S^0 \oplus T^0
$$

<details>
<summary>Proof</summary>

Clearly $S^0 \cap T^0 = \Set{ \boldsymbol{0} }$, since any functional that annihilates both $S$ and $T$ must annihilate $S \oplus T$. Hence, the sum $S^0 + T^0$ is direct. If $f\in V^*$ then we can write

$$
  f = (f \circ \rho_T) + (f + \rho_S) \in S^0 \oplus T^0
$$

Hence $V = S^0 \oplus T^0$
</details>
</MathBox>

## Transpose of linear transformations

<MathBox title='Transpose of linear transformations (algebraic adjoint)' boxType='definition'>
The transpose of a linear transformation $\mathrm{T}\in\mathcal{L}(V,W)$, also called *algebraic adjoint*, is the function $\mathrm{T}^*: W^* \to V^*$ defined by

$$
\begin{gather*}
  T^* (f) = f\circ\mathrm{T} = f\mathrm{T},\; f\in W^* \\
  [T^* (f)](\boldsymbol{v}) = f[\mathrm{T}(\boldsymbol{v})],\; \boldsymbol{v}\in V
\end{gather*}
$$

Note that several notations are commonly used for the transpose of $\mathrm{T}$, including
- $\mathrm{T}^\times$
- $\mathrm{T}^\#$
- $\mathrm{T}'$
- $\mathrm{T}^T$

<LatexFig width={50} src='/fig/algebraic_adjoint.svg' alt=''
  caption='Algebraic adjoint'
>
```latex
\documentclass[tikz]{standalone}
\usepackage{tikz}
\usepackage{amssymb}
\usetikzlibrary{cd}

\begin{document}

\begin{tikzcd}
  V \arrow[r, "\mathrm{T}"] \arrow[rd, swap, "\mathrm{T}^* = f\circ\mathrm{T}"] & 
  W \arrow[d, "f\in W^*"] \\
  & \mathbb{F}
\end{tikzcd}

\end{document}
```
</LatexFig>
</MathBox>

<MathBox title='Properties of the operator adjoint' boxType='proposition'>
Let $V$ be a vector space over $\mathbb{F}$, and $\alpha,\beta \in\mathbb{F}$, then
1. $(\alpha\mathrm{T} + \beta\mathrm{S})^* = \alpha\mathrm{T}^* + \beta\mathrm{S}^*$ for $\mathrm{T},\mathrm{S}\in\mathcal{L}(V,W)$ and $\alpha,\beta\in\mathbb{F}$.
2. $(\mathrm{T}\mathrm{S})^* = \mathrm{S}^* \mathrm{T}^*$ for $\mathrm{S}\in\mathcal{L}(V,W)$ and $\mathrm{T}\in\mathcal{L}(W,U)$.
3. If $\mathrm{T}\in\mathcal{L}(V)$ is invertible, then $(\mathrm{T}^{-1})^* = (\mathrm{T}^*)^{-1}$

<details>
<summary>Proof</summary>

**(1):** For all $f\in V^*$

$$
\begin{align*}
  (\alpha\mathrm{T} + \beta\mathrm{S})^* (f) =& f(\alpha\mathrm{T} + \beta\mathrm{S}) \\
  =& \alpha f(\mathrm{T}) + \beta f(\mathrm{S}) \\
  =& \alpha \mathrm{T}^* (f) + \beta \mathrm{S}^* (f)
\end{align*}
$$

**(2):** For all $f\in U^*$

$$
\begin{align*}
  (\mathrm{T}\mathrm{S})^* (f) =& f(\mathrm{T}\mathrm{S}) = \mathrm{S}^* (f\mathrm{T}) \\
  =& \mathrm{T}(\mathrm{S}^*(f)) = (\mathrm{T}^* \mathrm{S}^*)(f)
\end{align*}
$$

**(3):** Evaluating $\mathrm{T}^* (\mathrm{T}^*)^*$ using property (2) gives

$$
  \mathrm{T}^* (\mathrm{T}^*)^* = (\mathrm{T}^{-1} \mathrm{T})^* = \mathrm{id}^* = \mathrm{id}
$$

Doing the same for $(\mathrm{T}^{-1})^{-1} \mathrm{T}^*$ gives

$$
  (\mathrm{T}^{-1})^{-1} \mathrm{T}^* = (\mathrm{T}\mathrm{T}^{-1})^* = \mathrm{id}^* = \mathrm{id}
$$

Hence, $(\mathrm{T}^{-1})^* = (\mathrm{T}^*)^{-1}$.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V$ and $W$ be finite-dimensional $\mathbb{F}$-vector spaces and let $\mathrm{T}\in\mathcal{L}(V,W)$. If we identify $V^{**}$ with $V$ and $W^{**}$ with $W$ using the canonical maps, then $\mathrm{T}^{**}$ is identified with $\mathrm{T}$.

<details>
<summary>Proof</summary>

For any $\boldsymbol{x}\in V$ let the corresponding element of $V^{**}$ be denoted $\bar{\boldsymbol{x}}:V^* \to\mathbb{F}$ and similarly for $W$, then for all $f\in W^*$

$$
\begin{align*}
  \mathrm{T}^{**}(\bar{\boldsymbol{x}})(f) =& \bar{\boldsymbol{v}}[\mathrm{T}^* (f)] = \bar{\boldsymbol{v}}(f\mathrm{T}) \\
  =& f(\mathrm{T}(\boldsymbol{v})) = \overline{\mathrm{T}(\boldsymbol{v})}(f)
\end{align*}
$$

and so

$$
\begin{align*}
  \mathrm{T}^{**}(\bar{\boldsymbol{v}})(f) =& \overline{\mathrm{T}(\boldsymbol{v})} \in W^{**}
\end{align*}
$$

Using the canonical maps for both $V^{**}$ and $W^{**}$ we have

$$
  \mathrm{T}^{**}(\boldsymbol{v}) = \mathrm{T}(\boldsymbol{v})
$$

for all $\boldsymbol{v}\in V$.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $\mathrm{T}\in\mathcal{L}(V,W)$, then
1. $\ker(\mathrm{T}^*) = \operatorname{ran}(\mathrm{T})^0$
2. $\operatorname{ran}(\mathrm{T}^*) = \ker(\mathrm{T})^0$

<details>
<summary>Proof</summary>

**(1):**

$$
  \ker(\mathrm{T}^*) =& \Set{ f\in W^* | \mathrm{T}^* = 0 } \\
  =& \Set{ f\in W^* | f(\mathrm{T}(V)) = \Set{0} } \\
  =& \Set{ f\in W^* | f(\operatorname{ran}(\mathrm{T})) = \Set{0} } \\
  =& \operatorname{ran}(\mathrm{T})^0
$$

**(2):** If $f = g\mathrm{T} = \mathrm{T}^* g\in\operatorname{ran}(\mathrm{T}^*)$ then $\ker(\mathrm{T})\subseteq \ker(f)$ and so $f\in\ker(\mathrm{T})^0$.

For the reverse inlcusion, let $f\in\ker(\mathrm{T})^0\subseteq V^*$. On $K = \ker(\mathrm{T})$, there is no problem since $f$ and $\mathrm{T}^* g = g\mathrm{T}$ agree on $K$ for any $g\in W^*$. Let $S$ be a complement of $\ker(\mathrm{T})$. Then $\mathrm{T}$ maps a basis $B = \Set{\boldsymbol{b}_i }_{i\in I}$ for $S$ to a linearly independent set $\mathrm{T}(B) = \Set{ \mathrm{T}(\boldsymbol{b}_i) }_{i\in I}\in W$, so we can define $g\in W^*$ any way we want on $\mathrm{T}(B)$. In particular, let $g\in W^*$ be defined by setting

$$
  g(\mathrm{T}(\boldsymbol{b}_i)) = f(\boldsymbol{b}_i),\; i\in I
$$

and extending in any manner to all of $W^*$. Then $f = g\mathrm{T} = \mathrm{T}^* g$ on $B$ and therefore on $S$. Hence $f = \mathrm{T}^* g\in\operatorname{ran}(\mathrm{T}^*)$.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $\mathrm{T}\in\mathcal{L}(V,W)$, where $V$ and $W$ are finite-dimensional vector spaces. Then $\operatorname{rank}(\mathrm{T}) = \operatorname{rank}(\mathrm{T}^*)$.
</MathBox>

<MathBox title='Relation between algebraic adjoint and matrix transposition' boxType='proposition'>
Let $\mathrm{T}\in\mathcal{L}(V,W)$, where $V$ and $W$ are finite-dimensional vector spaces. If $B$ and $C$ are ordered bases for $V$ and $W$, respectively, and $B^*$ and $C^*$ are corresponding dual bases, then

$$
  [\mathrm{T}^*]_{C^*, B^*} = ([\mathrm{T}]_{B,C})^T
$$

<details>
<summary>Proof</summary>

The $(i,j)$th entry of $[\mathrm{T}]_{B,C}$ is

$$
  ([\mathrm{T}]_{B,C})_{i,j} = ([\mathrm{T}(\boldsymbol{b}_j)]_C)_i = c_i[\mathrm{T}(\boldsymbol{b}_j)],\; \boldsymbol{b}_i \in B, c_i \in C^*
$$

and the $(i,j)$th entry of $[\mathrm{T}^*]_{C^*,B^*}$ is

$$
\begin{align*}
  ([\mathrm{T}^*]_{C^*,B^*})_{i,j} =& ([\mathrm{T}^*(c_j)]_{B^*})_i = \boldsymbol{b}_i^{**}[\mathrm{T}^*(c_j)] \\
  =& \mathrm{T}^* (c_j)(\boldsymbol{b}_i) = c_j (\mathrm{T}(\boldsymbol{b}_i))
\end{align*}
$$

Comparing the expression, we see that $[\mathrm{T}^*]_{C^*, B^*} = ([\mathrm{T}]_{B,C})^T$.
</details>
</MathBox>

# Multilinear forms

<MathBox title='Multilinear form' boxType='definition'>
If $V_1,\dots,V_n$ are vector spaces over $\mathcal{F}$, an $n$-linear form is a function $f:V_1 \times\cdots\times V_k \to \mathbb{F}$ that is linear in each coordinate.
</MathBox>

<MathBox title='The set of multilinear forms form vector spaces' boxType='proposition'>
Let $V$ be an $\mathbb{F}$-vector space with $\dim(V) = n$. The set of $k$-linear forms $V^{k\times} \to\mathbb{F}$, denoted $\mathcal{T}^k (V^*)$, is a vector space of dimension $n^k$.
</MathBox>

<MathBox title='Symmetric, skew-symmetric and alternating multilinear forms' boxType='definition'>
Let $f: V^{k\times} \to\mathbb{F}$ be a $k$-linear form. For any permutation $\sigma\in S_k$, define the $k$-linear form $\sigma f$ by

$$
  (\sigma f)(\boldsymbol{v}_1,\dots,\boldsymbol{v}_k) = f(\boldsymbol{v}_{\sigma_1},\dots,\boldsymbol{v}_{\sigma_k})
$$

A $k$-linear form is
1. *symmetric* if $\sigma f = f$ for every permutation $\sigma\in S_k$
2. *skew-symmetric* if $\tau f = -f$ for every transposition $\tau\in S_k$
3. *alternating* if $f(x_1,\dots,x_k) = 0$ whenever $x_i = x_j$ for $i \neq j$
</MathBox>

<MathBox title='Relation between alternating and skew-symmetric forms' boxType='proposition'>
Every alternating form is skew-symmetric.

<details>
<summary>Proof</summary>

Let $f: V^{k\times} \to\mathbb{F}$ be an alternating $k$-form, and choose $\boldsymbol{v}_i = \boldsymbol{v}_j$ for $i \neq j$. Without loss of generality we consider an alternating bilinear form $g:V\times V\to\mathbb{F}$, because all except two variables are fixed. Since $g$ is alternating, we have that

$$
\begin{align*}
  0 =& g(\boldsymbol{v}_i + \boldsymbol{v}_j, \boldsymbol{v}_i + \boldsymbol{v}_j) \\
  =& \underbrace{g(\boldsymbol{v}_i, \boldsymbol{v}_i)}_{=0} + \underbrace{g(\boldsymbol{v}_j, \boldsymbol{v}_j)}_{=0} + g(\boldsymbol{v_i}, \boldsymbol{v}_j) + g(\boldsymbol{v}_j, \boldsymbol{v_i}) \\
  =& g(\boldsymbol{v_i}, \boldsymbol{v}_j) + g(\boldsymbol{v}_j, \boldsymbol{v_i})
\end{align*}
$$

Rearranging gives $g(\boldsymbol{v_i}, \boldsymbol{v}_j) = -g(\boldsymbol{v}_j, \boldsymbol{v_i})$, showing that $g$ is skew-symmetric.
</details>
</MathBox>

<MathBox title='Equivalence of skew-symmetric and alternating forms' boxType='corollary'>
If $1 + 1 \neq 0$ in $\mathbb{F}$, then every skew-symmetric form is alternating. If $1 + 1 = 0 \iff 1 = -1$ in $\mathbb{F}$, then symmetry and skew-symmetry are equivalent. In this case, a $k$-linear form $f$ can be decomposed into a symmetric and a skew-symmetric part, i.e.

$$
  f = \underbrace{\sum_{\sigma \in S_n} \frac{1}{k!}f(\boldsymbol{v}_{\sigma(1)},\dots,\boldsymbol{v}_{\sigma(k)})}_{\text{symmetric}} + \underbrace{\sum_{\sigma \in S_n} \frac{1}{k!}\operatorname{sgn}(\sigma) f(\boldsymbol{v}_{\sigma(1)},\dots,\boldsymbol{v}_{\sigma(k)})}_{\text{skew-symmetric}}
$$

<details>
<summary>Proof</summary>

Assume that $1 + 1 \neq 0$ in $\mathbb{F}$. Let $f: V^{k\times} \to\mathbb{F}$ be a skew-symmetric $k$-form. Without loss of generality we consider a skew-symmetric bilinear form $g:V\times V\to\mathbb{F}$, because all except two variables are fixed. Since $g$ is skew-symmetric

$$
\begin{align*}
  & g(\boldsymbol{v}, \boldsymbol{v}) = -g(\boldsymbol{v}, \boldsymbol{v}) \\
  \iff& 2g(\boldsymbol{v}, \boldsymbol{v}) = 0 \\
  \iff& g(\boldsymbol{v}, \boldsymbol{v}) = 0
\end{align*}
$$

Hence, $g$ is alternating if $2 \neq 0$ in $\mathbb{F}$.
</details>
</MathBox>

<MathBox title='Alternating forms and linear dependence' boxType='proposition'>
If $f: V^{k \times}\to\mathbb{F}$ is an alternating $k$-form and $\boldsymbol{v}_1,\dots,\boldsymbol{v}_k$ is linearly dependent, then $f(\boldsymbol{v}_1,\dots,\boldsymbol{v}_k) = 0$.

<details>
<summary>Proof</summary>

The case $\boldsymbol{v}_1 = \cdots = \boldsymbol{v}_k = \boldsymbol{0}$ is trivial. Assume that $\boldsymbol{v}_j = \sum_{\substack{i=1 \\ i\neq j}}^k \lambda_i \boldsymbol{v}_i$, then

$$
\begin{align*}
  f(\boldsymbol{v}_1,\dots,\boldsymbol{v}_j,\dots,\boldsymbol{v}_k) =& f\left(\boldsymbol{v}_1,\dots, \sum_{\substack{i=1 \\ i\neq j}}^k \lambda_i \boldsymbol{v}_i,\dots,\boldsymbol{v}_k \right) \\
  =& \sum_{\substack{i=1 \\ i\neq j}}^k a_i f(\boldsymbol{v}_1,\dots,\boldsymbol{v}_i,\dots,\boldsymbol{v}_{j-1},\boldsymbol{v}_i,\boldsymbol{j+1},\dots,\boldsymbol{v}_k) \\
  =& 0
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Alternating forms and linear independence' boxType='proposition'>
Let $V$ be an $n$-dimensional $\mathbb{F}$-vector space. If $f: V^{k \times}\to\mathbb{F}$ is a nonzero alternating $n$-linear form and $B = \Set{\boldsymbol{b}_i}_{i=1}^{n}$ is a basis for $V$, then $f(\boldsymbol{b}_1,\dots,\boldsymbol{b}_n) \neq 0$.

<details>
<summary>Proof</summary>

Choose $\boldsymbol{v}_1,\dots,\boldsymbol{v}_n \in V$ and write each $\boldsymbol{v}_i = \sum_{j=1}^n \lambda_{i,j}\boldsymbol{b}_j$, then

$$
\begin{align*}
  f(\boldsymbol{v}_1,\dots,\boldsymbol{v}_n) =& f\left(\sum_{j=1}^n \lambda_{1,j}\boldsymbol{b}_j,\dots,\sum_{j=1}^n \lambda_{n,j}\boldsymbol{b}_j \right) \\
  =& \sum_{\sigma\in S_n} c_\sigma f(\boldsymbol{b}_{\sigma(1)},\dots,\boldsymbol{b}_{\sigma(n)}) \\
  =& f(\boldsymbol{b}_1,\dots,\boldsymbol{b}_n) \sum_{\sigma\in S_n} c_\sigma \operatorname{sgn}(\sigma)
\end{align*}
$$

This show that $f(\boldsymbol{b}_1,\dots,\boldsymbol{b}_n) = 0 \implies f(\boldsymbol{\boldsymbol{v}_1,\dots,\boldsymbol{v}_n}$, contradicting that $f$ is nonzero.
</details>
</MathBox>

<MathBox title='The subspace of alternating forms is 1-dimensional' boxType='corollary'>
Let $V$ be an $n$-dimensional $\mathbb{F}$-vector space. The subspace of alternating linear $n$-forms is $1$-dimensional, that is
1. Any two alternating $n$-linear forms on $V$ are linearly dependent.
2. There is a nonzero alternating $n$-linear form on $V$. 

<details>
<summary>Proof</summary>

**(1):** Let $B = \Set{\boldsymbol{b}_i}_{i=1}^{n}$ is a basis for $V$ and assume that $g(\boldsymbol{e}_1,\dots,\boldsymbol{e}_n) = \lambda f(\boldsymbol{e}_1,\dots,\boldsymbol{e}_n)$. Then we need to show that $g(\boldsymbol{v}_1,\dots,\boldsymbol{v}_n) = \lambda g(\boldsymbol{v}_1,\dots,\boldsymbol{v}_n)$ for any $\boldsymbol{v}_1,\dots,\boldsymbol{v}_n \in V$

$$
\begin{align*}
  g(\boldsymbol{v}_1,\dots,\boldsymbol{v}_n) =& g\left(\sum_{j=1}^n \lambda_{1,j}\boldsymbol{b}_j,\dots,\sum_{j=1}^n \lambda_{n,j}\boldsymbol{b}_j \right) \\
  =& \sum_{\sigma\in S_n} c_\sigma g(\boldsymbol{b}_{\sigma(1)},\dots,\boldsymbol{b}_{\sigma(n)}) \\
  =& g(\boldsymbol{b}_1,\dots,\boldsymbol{b}_n) \sum_{\sigma\in S_n} c_\sigma \operatorname{sgn}(\sigma) \\
  =& lambda f(\boldsymbol{b}_1,\dots,\boldsymbol{b}_n) \sum_{\sigma\in S_n} c_\sigma \operatorname{sgn}(\sigma) \\
  =& \lambda f(\boldsymbol{v}_1,\dots,\boldsymbol{v}_n)
\end{align*}
$$

**(2):** This can be shown with induction on $k \leq n$ for a fixed $n$. The base $k=1$ is trivial since any nonzero $1$-form $V^* \ni f: V\to\mathbb{F}$ is by definition alternating. For the inductive hypothesis, assume that $f(\boldsymbol{v}_1,\dots,\boldsymbol{v}_n)$ is a nonzero alternating $k$-form. We need to construct a nonzero alternating $(k+1)$-form $g(\boldsymbol{v}_1,\dots,\boldsymbol{v}_{k+1})$.

Let $U = \operatorname{span}(\boldsymbol{v}_1,\dots,\boldsymbol{v}_k)$ and $\boldsymbol{v}_{k+1} \notin U$. Choose $\ell\in U^0$ such that $\ell(\boldsymbol{v}_{k+1}) \neq 0$. Define

$$
\begin{align*}
  g(\boldsymbol{v}_1,\dots,\boldsymbol{v}_{k+1}) =& -\underbrace{f(\boldsymbol{v}_1,\dots,\boldsymbol{v}_k)}_{\neq 0}\underbrace{\ell(\boldsymbol{v}_{k+1})}_{\neq 0} \\
  =& -f(\boldsymbol{v}_1,\dots,\boldsymbol{v}_k)\ell(\boldsymbol{v}_{k+1}) \\
  &+ \underbrace{\sum_{i=1} f(\boldsymbol{v}_1,\dots,\boldsymbol{v}_{i-1},\boldsymbol{v}_{k+1},\boldsymbol{v}_{i+1}\dots,\boldsymbol{v}_k)\ell(\boldsymbol{v}_i)}_{=0}
  \neq& 0
\end{align*}
$$

This shows that $g$ is a nonzero linear $(k+1)$-form. It remains to show that $g$ is alternating. In the case $\boldsymbol{v}_i = \boldsymbol{v}_j$ for $i, j < k+1$, the following terms of $g$ do not vanish

$$
\begin{align*}
  g(\boldsymbol{v}_1,\dots,\boldsymbol{v}_{k+1}) =& f(\boldsymbol{v}_1,\dots,\boldsymbol{v}_{i-1},\boldsymbol{v}_{j},\boldsymbol{v}_{i+1}\dots,\boldsymbol{v}_k)\ell(\boldsymbol{v}_i) \\
  &+ f(\boldsymbol{v}_1,\dots,\boldsymbol{v}_{j-1},\boldsymbol{v}_{i},\boldsymbol{v}_{j+1}\dots,\boldsymbol{v}_k)\ell(\boldsymbol{v}_j)
\end{align*}
$$

Note that, $f(\boldsymbol{v}_1,\dots,\boldsymbol{v}_{i-1},\boldsymbol{v}_{j},\boldsymbol{v}_{i+1}\dots,\boldsymbol{v}_k) = -f(\boldsymbol{v}_1,\dots,\boldsymbol{v}_{j-1},\boldsymbol{v}_{i},\boldsymbol{v}_{j+1}\dots,\boldsymbol{v}_k)$, which shows that $g$ is alternating in this case.

In the case $\boldsymbol{v}_i = \boldsymbol{v}_{k+1}$ for $i < k+1$, note that $\ell(\boldsymbol{v}_i) = 0$, since $\boldsymbol{v}_{k+1}\in U$. Thus, $g(\boldsymbol{v}_1,\dots,\boldsymbol{v}_{k+1}) = 0$ in this case.
</details>
</MathBox>

## Determinant of linear operators

Let $\mathrm{T}:V\to V$ be a linear operator on an $\mathbb{F}$-vector space $V$. For an alternating linear $n$-form $f$, define a new alternating $n$-form $\bar{\mathrm{T}}f : V^n \to \mathbb{F}$ by

$$
  (\bar{\mathrm{T}}f)(\boldsymbol{v}_1,\dots,\boldsymbol{v}_n) = f(\mathrm{T}\boldsymbol{v}_1,\dots,\mathrm{T}\boldsymbol{v}_n)
$$

That is, $\mathrm{T}$ induces a map $\bar{\mathrm{T}}$ on the ($1$-dimensional) space of alternating linear $n$-form mapping $f\mapsto \bar{\mathrm{T}}f$. However, since any linear map on a $1$-dimensional space is just scalar multiplication. Thus,

$$
  \bar{\mathrm{T}}: f \mapsto\lambda f,\; \lambda\in\mathbb{F}
$$

The scalar $\lambda$ is the *determinant* of $\mathrm{T}$. The determinant of $\mathrm{T}:\R^n \to \R^n$ is the unique alternating linear $n$-form satisfying $\mathrm{T}(\boldsymbol{e}_1,\dots,\boldsymbol{e}_n) = 1$, where $E = \Set{\boldsymbol{e}_i}_{i=1}^n$ is the canonical basis.

<MathBox title='Universal property of the determinant' boxType='definition'>
Let $V$ be an $n$-dimensional $\mathbb{F}$-vector space. Given a linear operator $\mathrm{T}:V\to V$, there exists a unique scalar $\lambda\in\mathbb{F}$ such that for every alternating linear $n$-form $f$

$$
  f(\mathrm{T}\boldsymbol{v}_1,\dots,\mathrm{T}\boldsymbol{v}_n) = \lambda f(\boldsymbol{v}_1,\dots,\boldsymbol{v},x_n)
$$

$$
\begin{CD}
V^n @>{\mathrm{T}^{n \times}}>> V^n \\
@V{f}VV @VV{f}V \\
\mathbb{F} @>>{\lambda}> \mathbb{F}
\end{CD}
$$
</MathBox>

<MathBox title='Properties of the determinant' boxType='definition'>
Let $\mathrm{T}:V\to V$ be a linear operator on an $n$-dimensional $\mathbb{F}$-vector space $V$. 
1. If $\mathrm{T}\boldsymbol{v} = \alpha\boldsymbol{v}$, then $\det(\mathrm{T}) = \alpha^n$
2. $\det(0) = 0$ and $\det(\mathrm{I}) = 1$
3. For any two linear operators $\mathrm{A},\mathrm{B}:V\to V$, then $\det(\mathrm{AB}) = \det(\mathrm{A})\det(\mathrm{B})$
4. If $\mathrm{A}:V\to V$ is invertible, then $\det(\mathrm{A}^{-1}) = \frac{1}{\det(\mathrm{A})} \neq 0$ 

<details>
<summary>Proof</summary>

**(1):** Calculating $\det(\mathrm{T})$ gives

$$
\begin{align*}
  (\bar{\mathrm{T}}f)(\boldsymbol{v}_1,\dots,\boldsymbol{v}_n) =& f(\mathrm{T}\boldsymbol{v}_1,\dots,\mathrm{T}\mathrm{v}) \\
  =& f(\alpha\boldsymbol{v}_1,\dots,\alpha\boldsymbol{v}_n) \\
  =& \alpha^n f(\boldsymbol{v}_1,\dots,\boldsymbol{v}_n)
\end{align*}
$$

**(3):**

$$
\begin{align*}
  \det(\mathrm{AB})f(\boldsymbol{v}_1,\dots,\boldsymbol{v}_n) =& (\bar{\mathrm{AB}}f)(\boldsymbol{v}_1,\dots,\boldsymbol{v}_n) \\
  =& f(\mathrm{A}(\mathrm{B}\boldsymbol{v}_1),\dots,\mathrm{A}(\mathrm{B}\boldsymbol{v}_n)) \\
  =& (\bar{\mathrm{A}}f)(\mathrm{B}\boldsymbol{v}_1,\dots,\mathrm{B}\boldsymbol{v}_n) \\
  =& \det(\mathrm{A})f(\mathrm{B}\boldsymbol{v}_1,\dots,\mathrm{v}_n) \\
  =& \det(\mathrm{A})(\bar{\mathrm{B}})(\boldsymbol{v}_1,\dots,\mathrm{v}_n) \\
  =& \det(\mathrm{A}\det(\mathrm{B})f(\boldsymbol{v}_1,\dots,\boldsymbol{v}_n)
\end{align*}
$$

**(4):** Noting that $\mathrm{AA}^{-1} = \mathrm{I}$ and $\det(\mathrm{I}) = 1$, we get

$$
\begin{gather*}
  1 = \det(\mathrm{I}) = \det(\mathrm{AA}^{-1}) = \det(\mathrm{A})\det(\mathrm{A}^{-1}) \\
  \iff \det(\mathrm{A}^{-1}) = \frac{1}{\det(\mathrm{A})} 
\end{gather*}
$$
</details>
</MathBox>

# Module

<MathBox title='Module' boxType='definition'>
Let $R$ be a commutative ring with identity $1$, whose elements are called scalars. A *module* is a set $M$ over $R$ equipped with the two closed operations
- $+: M\times M \ni (u,v) \mapsto u + v \in M$ **(addition)**
- $\cdot: R\times M \ni (r, u) \mapsto ru \in M$ **(scalar multiplication)**

and has the following properties
- $M$ is an abelian group under addition
- Scalar multiplication is compatible, satisfying for all $r,s\in R$ and $x\in M$
    - $(rs)x = r(sx)$
    - $1x = x$
- Addition and scalar multiplication are related by distributivity for all $r,s\in R$ and $x, y\in M$
    - $r(x + y) = rx + ry$
    - $(r + s)x = rx + sx$

The ring $R$ is called the *base ring* of $M$.
</MathBox>

<MathBox title='Submodule' boxType='definition'>
A submodule of an $R$-module $M$ is a nonempty subset $N\subseteq M$ that is an $R$-module in its own right by restricting the operations of $M$ to $S$. We write $N \leq M$ to denote the fact that $N$ is a submodule of $M$.
</MathBox>

<MathBox title='' boxType='theorem'>
Let $M$ be an $R$-module. A nonempty subset $N\subseteq M$ is a submodule if and only if it is closed under the taking of linear combinations, i.e.

$$
  r,s \in R, u,v\in N \implies ru + sv \in N
$$
</MathBox>

<MathBox title='' boxType='theorem'>
Let $M$ be an $R$-module. If $S$ and $T$ are submodules of $M$, then $S\cap T$ and $S + T$ are also submodules of $M$.
</MathBox>

When we consider $R$ as an $R$-module rather than as a ring, multiplication is treated as scalar multiplication. This has some important implications. In particular, if $S$ is a submodule of $R$, then it is closed under scalar multiplication, which means that it is closed under multiplication by all elements of the ring $R$. In other words, $S$ is an ideal of the ring $R$. Conversely, if $I$ is an ideal of the ring $R$, then $I$ is also a submodule of the $R$-module $R$. Hence, the submodules of the $R$-module $R$ are precisely the ideals of the ring $R$.

## Spanning sets

<MathBox title='Spanned submodules' boxType='definition'>
The submodule spanned or generated by a subset $N$ of an $R$-module $M$ is the set of all linear combinations of elements of $N$

$$
  \langle N \rangle = \Set{\sum_{i=1}^n r_i v_i | r_i \in R, v_i \in N, n\geq 1}
$$

A subset $N\subseteq M$ spans or generates $M$ if $M = \langle N \rangle$.
</MathBox>

Note that if a nontrivial linear combination of the elements $v_1,\dots,v_n$ in an $R$-module $M$ is $0$, i.e. $\sum_{i=1}^n r_i v_i = 0$, where not all of the coefficients are $0$, then we cannot conclude that one of the elements $v_i$ is a linear combination of the others. After all, this involves dividing by one of the coefficients, which may not be possible in a ring.

<MathBox title='Cyclic submodule' boxType='definition'>
Let $M$ be an $R$-submodule. A submodule of the form

$$
  \langle v \rangle = Rv = \Set{rv | r\in R}
$$

for $v \in M$ is the *cyclic submodule* generated by $v$.
</MathBox>

Any finite-dimensional vector space is the direct sum of cyclic submodules, i.e. one-dimensional subspaces.

<MathBox title='Finitely generated submodule' boxType='definition'>
An $R$-module $M$ is *finitely generated* if it contains a finite set that generates $M$. More specifically, $M$ is $n$-generated if it has a generating set of size $n$.
</MathBox>

A vector space if finitely generated if and only if it has a finite basis, or equivalently, if and only if it is finite-dimensional. This is not alway the case for modules as illustrated in the following example.  

<MathBox title='' boxType='example'>
Let $R$ be the ring $\mathbb{F}[x_1,x_2,\dots]$ of all polynomials in infinitely many variables over a field $\mathbb{F}$. For convenience we denote $x_1,x_2,\dots$ by $X$ and write a polynomial in $R$ in the form $p(X)$. (Each polynomial in $R$, being a finite sum, involves only finitely many variables). Then $R$ is an $R$-module and as such, is finitely generated by the identity element $p(X) = 1$.

Now consider that submodule $S$ of all polynomials with zero constant term. This module is generated by the variables themselves, i.e. $S = \langle x_1,x_2,\dots \rangle$. However, $S$ is not finitely generated. To see this, suppose that $G = \Set{p_1,\dots,p_n}$ is a finite generating set for $S$. Choose a variable $x_k$ that does not appear in any of the polynomials in $G$. Then no linear combination of the polynomials in $G$ can be equal to $x_k$. For if $x_k = \sum_{i=1}^n a_i (X) p_i (X)$, then let $a_i (X) = x_k q_i (X) + r_i (X)$ where $r_i (X)$ does not involve $x_k$. This gives

$$
\begin{align*}
  x_k =& \sum_{i=1}^n [x_k q_i (X) + r_i (X)]p_i (X) \\
  =& x_k \sum_{i=1}^n q_i (X) p_i (X) + \sum_{i=1}^n r_i (X) p_i (X)
\end{align*}
$$

The last sum does not involve $x_k$ and so it must equal $0$. Hence, the first sum must equal $1$, which is not possible since $p_i(X)$ has no constant term.
</MathBox>

## Linear independence

<MathBox title='Linear independence (module)' boxType='definition'>
A subset $N$ of an $R$-module $M$ is *linearly independent* if for any distinct $v_1,\dots,v_n \in N$ and $r_1,\dots,r_n\in R$, we have

$$
  \sum_{i=1} r_i v_i = 0 \implies r_i = 0,\; \forall i
$$

A set $N$ that is not linearly independent is *linearly dependent*.
</MathBox>

In a vector space $V$ over a field $\mathbb{F}$, singleton sets $\Set{v}$, where $\boldsymbol{v} \neq \boldsymbol{0}$ are linearly independent. However, in module this need no be the case.

<MathBox title='Singletons of modules are not necessarily linearly independent' boxType='example'>
The abelian group $\Z_n = \Set{0,1,\dots,n-1}$ is a $\Z$-module, with scalar multiplication defined by $za = (za) \mod n$ for all $z\in\Z$ and $a\in Z_n$. However, since $na = 0$ for all $a \in\Z_n$, no singleton set $\Set{a}$ is linearly independent. Indeed, $\Z_n$ has no linearly independent sets.
</MathBox>

<MathBox title='Torsion element' boxType='definition'>
Let $M$ be an $R$-module. A nonzero element $v\in M$ for which $rv = 0$ for some nonzero $r\in R$ is a *torsion element* of $M$. A module that has no nonzero torsion elements is *torsion free*. If all elements of $M$ are  torsion elements, then $M$ is a *torsion module*. The set of all torsion elements of $M$, together with the zero element, is denoted $M_{\mathrm{tor}}$.
</MathBox>

## Annihilators

<MathBox title='Annihilator' boxType='definition'>
Let $M$ be an $R$-module. The *annihilator* of an element $v\in M$ is

$$
  \operatorname{ann}(v) = \Set{r\in R | rv = 0}
$$

and the annihilator of a submodule $N$ of $M$ is

$$
  \operatorname{ann}(N) = \Set{r\in R | rN = \Set{0}}
$$

where $rN = \Set{rv | v\in N}$. Annihilators are also called *order ideals*.
</MathBox>

It is easy to see that $\operatorname{ann}(v)$ and $\operatorname{ann}(N)$ are ideals of $R$. Clearly, $v\in M$ is a torsion element if and only if $\operatorname{ann}(v) \neq\Set{0}$. Also, if $A$ and $B$ are submodules of $M$, then

$$
  A \leq B \implies \operatorname{ann}(B) \leq \operatorname{ann}(A)
$$

Let $M = \langle u_1,\dots,u_n \rangle$ be a finitely generated module over an integral domain $R$ and assume that each of the generators $u_i$ is torsion, i.e. there is a nonzero $a_i \in\operatorname{ann}(u_i)$ for each $i$. Then the nonzero product $a = \prod_{i=1}^n a_i$ annihilates each generator of $M$ and therefore every element of $M$, i.e. $a\in\operatorname{ann}(M)$. This shows that $\operatorname{ann}(M) \neq\Set{0}$. On the other hand, this may fail if $R$ is not an integral domain.

## Free modules

<MathBox title='Free modue' boxType='definition'>
Let $M$ be an $R$-module. A subset $B \subseteq M$ is a *basis* if $B$ is linearly independent and spans $M$. An $R$-module $M$ is *free* if $M = \Set{0}$ or if $M$ has a basis. If $B$ is a basis for $M$, we say that $M$ is free on $B$.
</MathBox>

<MathBox title='' boxType='theorem'>
Let $M$ be an $R$-module. A subset $B\subseteq M$ is a basis if and only if every nonzero $v\in M$ is an essentially unique linear combination of the elements in $B$.
</MathBox>

<MathBox title='' boxType='theorem'>
Let $B$ be a basis for an $R$-module $M$. Then
1. $B$ is a minimal spanning set
2. $B$ is a maximal linearly independent set
</MathBox>

<MathBox title='A free module can have a submodule that is not free' boxType='example'>
The set $\Z\times\Z$ is a free module over itself, using componentwise scalar multiplication

$$
  (n,m)(a,b) = (na,mb)
$$

with basis $\Set{(1,1)}$. However, the submodule $\Z\times\Set{0}$ is not free since it has no linearly independent elements and hence no basis.
</MathBox>

<MathBox title='' boxType='theorem'>
Let $M$ and $N$ be $R$-modules where $M$ is free with basis $B = \Set{b_i}_{i\in I}$. Then we can define a unique $R$-map $f:M\to N$ by specifying the values of $f(b_i)$, arbitrarily for all $b_i \in B$ and then extending $f$ to $M$ by linearity, i.e.

$$
  f\left(\sum_{i=1} a_i v_i \right) = \sum_{i=1} a_i f(v_i)
$$
</MathBox>

## Homomorphisms

<MathBox title='Homomorphism' boxType='definition'>
Let $M$ and $N$ be $R$-modules. A function $f:M\to N$ is an $R$-homomorphism or $R$-map if it preserves the module operations, i.e. for all $r,s\in R$ and $u,v\in M$

$$
  f(ru + sv) = rf(u) + sf(v)
$$

The set all $R$-homomorphisms from $M$ to $N$ is denoted $\hom_R (M, N)$. The following terms are used to classify $R$-maps
- an $R$-endomorphism is an $R$-homomorphism from $M$ to itself
- an $R$-monomorphism or $R$-embedding is an injective $R$-homomorphism
- an $R$-epimorhism is a surjective $R$-homomorphism
- an $R$-isomorphism is a bijective $R$-homomorphism
</MathBox>

It is east to see that $\hom_R (M, N)$ is itself an $R$-module under addition of functions and scalar multiplication defined by

$$
  (rf)(v) = r(f(v)) = f(rv)
$$

<MathBox title='Kernel and range of homomorphisms' boxType='theorem'>
Let $f:M\to N$ be an $R$-map. The *kernel* and *range* or *image* of $f$ are defined as

$$
\begin{align*}
  \ker(f) =& \Set{v \in M | f(v) = 0}
  \operatorname{ran}(f) =& \Set{f(v) | v \in M}
\end{align*}
$$
</MathBox>

<MathBox title='The kernel and image of homomorphisms are submodules' boxType='theorem'>
If $f\in\hom_R (M, N)$ is an $R$-map, the kernel and range of $f$ are submodules of $M$ and $N$ respectively. Moreover, $f$ is a monomorphism if and only if $\ker(f) = \Set{0}$. 
</MathBox>

If $N$ is a submodule of the $R$-module $M$, the map $j:N\to M$ defined by $j(v) = v$ is evidently an $R$-monomorphism, called the *injection* of $N$ into $M$.

## Quotient modules

<MathBox title='The kernel and image of homomorphisms are submodules' boxType='theorem'>
Let $S$ be a submodule of an $R$-module $M$. The binary relation

$$
  u \equiv v \iff u - v \in S
$$

is an equivalence relation on $M$, whose equivalence classes are the *cosets*

$$
  v + S = \Set{v + s | s\in S}
$$

of $S$ in $M$. The set $M\setminus S$ of all cosets of $S$ in $M$, called the *quotient module* of $M$ *modulo* $S$, is an $R$-module under the well-defined operations

$$
\begin{align*}
  (u + S) + (v + S) =& (u + v) + S \\
  r(u + S) =& ru + S
\end{align*}
$$

The zero element in $M\setminus S$ is the coset $0 + S = S$.
</MathBox>

<MathBox title='The quotient module of a free module is not necessarily free' boxType='example'>
As a module over itself, $\Z$ is free on the set $\Set{1}$. For any $n > 0$, the set $\Z_n = \Set{nz | z\in\Z}$ is a free cyclic submodule of $\Z$, but the quotient $\Z$-module $\Z\setminus\Z_n$ is isomorphic to $\Z_n$ via the map

$$
  f(u + \Z_n) = u \mod n
$$
and since $\Z_n$ is not free as a $\Z$-module, neither is $\Z\setminus\Z_n$.
</MathBox>

<MathBox title='Correspondence theorem' boxType='theorem'>
Let $S$ be a submodule of $M$. Then the function that assigns to each intermediate submodule $S\subseteq T \subseteq M$ the quotient submodule $T\setminus S$ of $M\setminus S$ is an order-preserving (with respect to set inclusion) injective correspondence between submodules of $M$ containing $S$ and all submodules of $M\setminus S$.
</MathBox>

<MathBox title='The first isomorphism theorem' boxType='theorem'>
Let $f:M\to N$ be an $R$-homomorphism. Then the map $f': M\setminus\ker(f) \to N$ defined by $f'(v + \ker(v)) = f(v)$ is an $R$-embedding and so

$$
  M\setminus \ker(f) \simeq\operatorname{ran}(f)
$$
</MathBox>

<MathBox title='The second isomorphism theorem' boxType='theorem'>
Let $M$ be an $R$-module and suppose that $S\subseteq T$ are submodules of $M$. Then

$$
  \frac{S + T}{T} \simeq \frac{S}{S \cap T}
$$
</MathBox>

<MathBox title='The third isomorphism theorem' boxType='theorem'>
Let $M$ be an $R$-module and suppose that $S\subseteq T$ are submodules of $M$. Then

$$
  \frac{M\setminus S}{T \setminus S} \simeq \frac{M}{T}
$$
</MathBox>

# Linear operators

## Eigenvalues and eigenvectors

<MathBox title='Eigenvalue, eigenvector and spectrum' boxType='proposition'>
Let $V$ be a vector space over an algebraically closed field $\mathbb{F}$, which means that every non-constant polynomial has a root in $\mathbb{F}$. 
1. A scalar $\lambda\in\mathbb{F}$ is an *eigenvalue* of an operator $\mathrm{T}\in\mathcal{L}(V)$ if there exists a nonzero vector $\boldsymbol{v}\in V$ for which $\mathrm{T}(\boldsymbol{v}) = \lambda\boldsymbol{v}$. In this case, $\boldsymbol{v}$ is an *eigenvector* of $\mathrm{T}$ associated with $\lambda$.
2. A scalar $\lambda\in\mathbb{F}$ is an eigenvalue for a matrix $\boldsymbol{A}\in\mathcal{M}_{n}(\mathbb{F})$ if there exists a nonzero column vector $\boldsymbol{x}$ for which $\boldsymbol{A}\boldsymbol{x} = \lambda\boldsymbol{x}$. In this case, $\boldsymbol{x}$ is an eigenvector for $\boldsymbol{A}$ associated with $\lambda$.
3. The set of all eigenvectors associated with a given eigenvalue $\lambda$, together with the zero vector, forms a subspace of $V$, called the *eigenspace* of $\lambda$, denoted by $E_\lambda$.
4. The set of all eigenvalues of an operator or matrix is called the *spectrum* of the operator or matrix.
</MathBox>

<MathBox title='Existence of eigenvectors' boxType='proposition'>
Every $n\times n$ matrix $\boldsymbol{A}\in\mathcal{M}_n (\mathbb{F})$ has an eigenvalue.

<details>
<summary>Proof</summary>

Assume that $\boldsymbol{A}$ represent a linear operator $\mathrm{T}_\boldsymbol{A}:V\to V$ and choose any nonzero $\boldsymbol{v}\in V$. Consider the vectors $\Set{\boldsymbol{A}^i \boldsymbol{v}}_{i=0}^n$, where $\boldsymbol{A}^0 = \boldsymbol{I}$. Since this set has $n + 1$ vectors it must be linearly dependent. Thus,

$$
\begin{align*}
  \boldsymbol{0} =& \sum_{i=0}^n \alpha_i \boldsymbol{A}^i \boldsymbol{v} = \left(\sum_{i=0}^n \alpha_i \boldsymbol{A}^i\right)\boldsymbol{v} \\
  c(\boldsymbol{A})
\end{align*}
$$
</details>
</MathBox>


# Inner product space

<MathBox title='Inner product' boxType='definition'>
Let $V$ be an $\mathbb{F}$-vector space. The map $\langle\cdot, \cdot\rangle: V\times V \to\mathbb{F}$ is called an inner product on $V$ if it satisfies for all $\boldsymbol{u},\boldsymbol{v}, \boldsymbol{w}\in V$ and $\alpha, \beta \in\mathbb{F}$

1. **Positive definiteness:** $\langle \boldsymbol{v}, \boldsymbol{v} \rangle \geq 0$ and $\langle \boldsymbol{v}, \boldsymbol{v} \rangle = 0 \implies \boldsymbol{v} = 0$
2. **Linearity in the second argument**: $\langle\boldsymbol{u}, \alpha\boldsymbol{v} + \beta\boldsymbol{w}\rangle = \alpha\langle\boldsymbol{u}, \boldsymbol{v}\rangle + \beta\langle\boldsymbol{u}, \boldsymbol{w}\rangle$
3. **Conjugate symmetry:** $\langle \boldsymbol{v}, \boldsymbol{w}\rangle = \overline{\langle\boldsymbol{w}, \boldsymbol{v}\rangle}$
</MathBox>

<MathBox title='Norm' boxType='definition'>
Let $\langle\cdot, \cdot\rangle: V\times V \to\mathbb{F}$ be an inner product on the $\mathbb{F}$-vector space $V$. The *norm* on $V$ is a map $\lVert\cdot\rVert: V\to[0,\infty)$ defined by

$$
  \lVert \boldsymbol{x} \rVert := \sqrt{\langle \boldsymbol{x}, \boldsymbol{x} \rangle}
$$
</MathBox>

<MathBox title='Cauchy-Schwarz inequality' boxType='proposition'>
Let $\langle\cdot, \cdot\rangle: V\times V \to\mathbb{F}$ be an inner product on the $\mathbb{F}$-vector space $V$. Then for all $\boldsymbol{x},\boldsymbol{y}\in V$ 
1. $|\langle\boldsymbol{y}, \boldsymbol{x}\rangle| \leq \lVert\boldsymbol{x}\rVert\cdot\lVert\boldsymbol{y}\rVert$ 
2. $|\langle\boldsymbol{y},\boldsymbol{x}\rangle| = \lVert\boldsymbol{x}\rVert\cdot\lVert\boldsymbol{y}\rVert$ if and only if $\boldsymbol{x}$ and $\boldsymbol{y}$ are linearly dependent

<details>
<summary>Proof</summary>

In the trivial case $\boldsymbol{x} = \boldsymbol{0}$, we get $\langle \boldsymbol{y}, \underbrace{\boldsymbol{x}}_{0\boldsymbol{v}} \rangle = 0\langle \boldsymbol{y},\boldsymbol{x}\rangle = 0$ and $\lVert\boldsymbol{x}\rVert\cdot\lVert\boldsymbol{y}\rVert$.

In the general case $\boldsymbol{x}\neq\boldsymbol{0}$, we want to show that $\left|\langle \boldsymbol{y}, \lVert\hat{\boldsymbol{x}}\rVert \right|\leq\lVert\boldsymbol{y}\rVert$ where $\frac{\boldsymbol{x}}{\lVert\boldsymbol{x}\rVert}$. For any $\lambda\in\R$, we have that

$$
\begin{align*}
  0 \leq& \langle \boldsymbol{y} - \lambda\hat{\boldsymbol{x}}, \boldsymbol{y} - \lambda\hat{\boldsymbol{x}} \rangle \\
  =& \langle \boldsymbol{y},\boldsymbol{y}\rangle - \lambda \hat{\boldsymbol{x},\boldsymbol{y}} \rangle - \lambda\langle\boldsymbol{y},\hat{\boldsymbol{x}}\rangle + \lambda^2 \langle\hat{\boldsymbol{x}},\hat{\boldsymbol{x}}\rangle \\
  =& \lambda^2 + \lambda \underbrace{(-2\Re(\langle\boldsymbol{x},\hat{\boldsymbol{x}}\rangle))}_{p} + \underbrace{\lVert\boldsymbol{y}\rVert^2}_{q}
\end{align*}
$$

The resulting quadratic polynomial has roots $\lambda_{1,2} = -\frac{p}{2} \pm \sqrt{\left(\frac{p}{2}\right)^2 - q}$. For the inequality to hold, the polynomial must either have a single real root or two imaginary roots. Thus, $\left(\frac{p}{2}\right)^2 - q \leq 0$, implying $|\Re(\langle\boldsymbol{y},\hat{\boldsymbol{x}}\rangle)| \leq \lVert\boldsymbol{y}\rVert$. This proves the Cauchy-Schwartz inequality for $\mathbb{F} = \R$. For $\mathbb{F} = \mathbb{C}$, note that $e^{i\phi}\langle \boldsymbol{y}, \hat{\boldsymbol{x}} \rangle = |\langle\boldsymbol{y},\hat{\boldsymbol{x}}\rangle|$. Thus, we get $|\Re(e^{i\phi}\langle\boldsymbol{y},\hat{\boldsymbol{x}}\rangle)| = |\Re(\langle\boldsymbol{y},e^{i\phi}\hat{\boldsymbol{x}}\rangle)| \leq \lVert\boldsymbol{y}\rVert$.
</details>
</MathBox>

## Orthogonality

<MathBox title='Orthogonal vectors' boxType='definition'>
Let $\langle\cdot, \cdot\rangle: V\times V \to\mathbb{F}$ be an inner product on the $\mathbb{F}$-vector space $V$. Two vectors $\boldsymbol{x}, \boldsymbol{v}\in V$ are *orthogonal*, denoted $\boldsymbol{x}\perp\boldsymbol{y}$ if $\langle\boldsymbol{x},\boldsymbol{y}\rangle = 0$.
</MathBox>

<MathBox title='Orthogonal complement' boxType='definition'>
Let $\langle\cdot, \cdot\rangle: V\times V \to\mathbb{F}$ be an inner product on the $\mathbb{F}$-vector space $V$. For a nonempty subspace $M\subseteq V$, we define the *orthogonal complement*

$$
  M^\perp := \Set{\boldsymbol{x}\in V |\langle\boldsymbol{x},\boldsymbol{m}\rangle = 0 \forall\boldsymbol{m}\in M}
$$

Note that $M \cap M^\perp = \Set{\boldsymbol{0}}$.
</MathBox>

<MathBox title='Orthogonal projection onto a line' boxType='definition'>
Let $\langle\cdot, \cdot\rangle: V\times V \to\mathbb{F}$ be an inner product on the $\mathbb{F}$-vector space $V$. Suppose that $U\subseteq V$ is a 1-dimensional subspace with $U = \operatorname{span}(\boldsymbol{r})$ for $\boldsymbol{r}\neq\boldsymbol{0}$. For $\boldsymbol{x}\in V$ and a decomposition $\boldsymbol{x} = \boldsymbol{p} + \boldsymbol{n}$ with $\boldsymbol{p}\in U$ and $\boldsymbol{n}\in U^\perp$, i.e. $\boldsymbol{n}\perp\boldsymbol{r}$, we call 
- $\boldsymbol{p}$ the *orthogonal projection* of $\boldsymbol{x}$ onto $U$
- $\boldsymbol{n}$ the *normal component* of $\boldsymbol{x}$ with respect to $U$

<details>
<summary>Proof</summary>

To prove the uniqueness of the orthogonal decomposition, assume that $\boldsymbol{x} = \boldsymbol{p} + \boldsymbol{n} = \tilde{\boldsymbol{p}} + \tilde{\boldsymbol{n}}$ for $\boldsymbol{p},\tilde{\boldsymbol{p}}\in U$ and $\boldsymbol{n},\tilde{\boldsymbol{n}}\in U^\perp$. Then

$$
\begin{align*}
  & \boldsymbol{p} + \boldsymbol{n} = \tilde{\boldsymbol{p}} + \tilde{\boldsymbol{n}} \\
  \implies& \underbrace{\boldsymbol{p} - \tilde{\boldsymbol{p}}}_{\in U} = \underbrace{\tilde{\boldsymbol{n}} - \boldsymbol{n}}_{\in U^\perp} \\
  \implies& 0 = \langle \boldsymbol{p} - \tilde{\boldsymbol{p}}, \tilde{\boldsymbol{n}} - \boldsymbol{n} \rangle = \begin{cases} \langle \boldsymbol{p} - \tilde{\boldsymbol{p}}, \boldsymbol{p} - \tilde{\boldsymbol{p}} \rangle \\ \langle \tilde{\boldsymbol{n}} - \boldsymbol{n}, \tilde{\boldsymbol{n}} - \boldsymbol{n} \rangle \end{cases} \\
  \implies& \boldsymbol{p} - \tilde{\boldsymbol{p}} = \boldsymbol{0} = \tilde{\boldsymbol{n}} - \boldsymbol{n} \\
\end{align*}
$$

This shows that $\boldsymbol{p} = \tilde{\boldsymbol{p}}$ and $\boldsymbol{n} = \tilde{\boldsymbol{n}}$.

To prove the existence of the orthogonal projection, assume that $\boldsymbol{p}\in U = \operatorname{span}(\boldsymbol{r})$ such that $\boldsymbol{p} = \lambda\boldsymbol{r}$ for $\mathbb{F}$. Then

$$
\begin{align*}
  &\langle \boldsymbol{r}, \boldsymbol{x} \rangle = \langle \boldsymbol{r}, \lambda\boldsymbol{r} + \boldsymbol{n} \rangle = \lambda\langle\boldsymbol{r},\boldsymbol{r}\rangle + \underbrace{\langle\boldsymbol{r},\boldsymbol{n}\rangle}_{=0} \\
  \implies& \lambda = \frac{\langle\boldsymbol{r},\boldsymbol{x}\rangle}{\langle\boldsymbol{r},\boldsymbol{r}\rangle}
\end{align*}
$$

Hence, $\boldsymbol{p} = \frac{\langle\boldsymbol{r},\boldsymbol{x}\rangle}{\langle\boldsymbol{r},\boldsymbol{r}\rangle}\boldsymbol{r}$ and $\boldsymbol{n} = \boldsymbol{x} - \boldsymbol{p}$.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $\langle\cdot, \cdot\rangle: V\times V \to\mathbb{F}$ be an inner product on the $\mathbb{F}$-vector space $V$. Suppose that $U\subseteq V$ is a $k$-dimensional subspace with basis $B = (\boldsymbol{b}_i)_{i=1}^k$. If $\boldsymbol{y}\in V$, then $\boldsymbol{y}\perp\boldsymbol{u}$ for all $\boldsymbol{u}\in U$ if and only if $\boldsymbol{y}\perp\boldsymbol{b}_i$ for all $i\in\Set{1,\dots,k}$.

<details>
<summary>Proof</summary>

The right implication is trivial, since if $\boldsymbol{y}\perp\boldsymbol{u}$ it follows that all basis vectors $\boldsymbol{b}_i$ are in $U$, and hence $\boldsymbol{y}\perp\boldsymbol{b}_i$ for all $i\in\Set{1,\dots,k}$. 

Conversely, assume that $\langle \boldsymbol{y},\boldsymbol{b}_i \rangle = 0$ for all $i\in\Set{1,\dots,k}$. Then

$$
\begin{align*}
  &\sum_{i=1}^k \lambda_i \langle\boldsymbol{y}, \boldsymbol{b}_i \rangle = 0 \\
  \implies& \left\langle \boldsymbol{y}, \sum_{i=1}^k \lambda_i \boldsymbol{b}_i \right\rangle = 0
\end{align*}
$$

Since $\sum_{i=1}^k \lambda_i \boldsymbol{b}_i \in U$ for all $\boldsymbol{\lambda}_i \in\mathbb{F}$, it follows that $\boldsymbol{y}\perp\boldsymbol{u}$ for all $\boldsymbol{u}\in U$.
</details>
</MathBox>

<MathBox title='Orthogonal projection onto a subspace' boxType='definition'>
Let $\langle\cdot, \cdot\rangle: V\times V \to\mathbb{F}$ be an inner product on the $\mathbb{F}$-vector space $V$. Suppose that $U\subsete V$ is a $k$-dimensional subspace. For $\boldsymbol{x}\in V$ and a decomposition $\boldsymbol{x} = \boldsymbol{p} + \boldsymbol{n}$ with $\boldsymbol{p}\in U$ and $\boldsymbol{n}\in U^\perp$, i.e. $\boldsymbol{n}\perp\boldsymbol{p}$, we call
- $\boldsymbol{p}$ the *orthogonal projection* of $\boldsymbol{x}$ onto $U$
- $\boldsymbol{n}$ the *normal component* of $\boldsymbol{x}$ with respect to $U$
</MathBox>

<MathBox title='' boxType='proposition'>
Let $\langle\cdot, \cdot\rangle: V\times V \to\mathbb{F}$ be an inner product on the $\mathbb{F}$-vector space $V$. Suppose that $U\subsete V$ is a $k$-dimensional subspace with basis $B = (\boldsymbol{b}_i)_{i=1}^k$. The orthogonal projection of $\boldsymbol{x}\in V$ onto $U$ takes the form

$$
  \boldsymbol{p} = \sum_{i=1}^k \lambda_i \boldsymbol{b}_i,\; \lambda_i \in\mathbb{F}
$$

Taking the orthogonal decomposition $\boldsymbol{x} = \boldsymbol{p} + \boldsymbol{n}$ with $\boldsymbol{p}\in U$ and $\boldsymbol{n}\in U^\perp$, we get for each $\boldsymbol{b}_i$ 

$$
  \langle\boldsymbol{b}_i, \boldsymbol{x}\rangle =& \langle \boldsymbol{b}_i,\boldsymbol{p} \rangle + \underbrace{\langle \boldsymbol{b}_i, \boldsymbol{n}\rangle}_{=0} \\
  =& \left\langle \boldsymbol{b}_i, \sum_{j=1}^k \lambda_j \boldsymbol{b}_j \right\rangle \\
  =& \sum_{j=1}^k \lambda_j \langle \boldsymbol{b}_i, \boldsymbol{b}_j \rangle 
$$

This gives a system of $k$ linear equations of the form

$$
\begin{bmatrix}
  \langle\boldsymbol{b}_1,\boldsymbol{b}_1 \rangle & \cdots & \langle\boldsymbol{b}_1,\boldsymbol{b}_k \rangle \\
  \vdots & \ddots & \vdots \\
  \langle\boldsymbol{b}_k,\boldsymbol{b}_1 \rangle & \cdots & \langle\boldsymbol{b}_k,\boldsymbol{b}_k \rangle
\end{bmatrix}\cdot \begin{bmatrix} \lambda_1 \\ \vdots \lambda_k \end{bmatrix} = \begin{bmatrix} \langle\boldsymbol{b}_1,\boldsymbol{x}\rangle \\ \vdots \\ \langle \boldsymbol{b}_k,\boldsymbol{x} \rangle \end{bmatrix}
$$

The coefficient matrix is called a Gramian matrix, denoted $\boldsymbol{G}(B)$. This system of linear equations has a unique solution if $\boldsymbol{G}(B)$ is invertible, or equivalently $\ker(\boldsymbol{G}(B)) = \Set{\boldsymbol{0}}$.

<details>
<summary>Proof</summary>

To show that $\ker(G(B)) = \Set{\boldsymbol{0}}$, choose $\boldsymbol{\beta}\in\ker(G(B))$. From the resulting equation $\boldsymbol{G}(B)\boldsymbol{\beta} = \boldsymbol{0}$ we get for each $j\in\Set{1,\dots,k}$ that $\sum_{i=1}^k \beta_i \langle \boldsymbol{b}_j, \boldsymbol{b}_i\rangle = \left\langle \boldsymbol{b}_j, \sum_{i=1}^k \beta_i \boldsymbol{b}_i \right\rangle = \boldsymbol{0}$. Note that $\boldsymbol{y} = \sum_{i=1}^k \beta_i \boldsymbol \in U$. This shows that $\boldsymbol{y}$ is orthogonal to all $\boldsymbol{b}_j$, implying that $\boldsymbol{y}\in U^\perp$ as well. Since $U\cap U^\perp = \Set{0}$, it follows that $\boldsymbol{y} = \boldsymbol{0}$ and hence $\boldsymbol{\beta} = \boldsymbol{0}$.
</details>
</MathBox>

<MathBox title='Approximation formula' boxType='proposition'>
Let $\langle\cdot, \cdot\rangle: V\times V \to\mathbb{F}$ be an inner product on the $\mathbb{F}$-vector space $V$. Suppose that $U\subsete V$ is a $k$-dimensional subspace. The distance of any $\boldsymbol{x}\in V$ to $U$ is given by

$$
  \operatorname{dist}(\boldsymbol{x}, U) := \inf\Set{\lVert \boldsymbol{x}-\boldsymbol{u} \rVert | \boldsymbol{u}\in U \rVert} = \lVert \boldsymbol{x} - \boldsymbol{x}|_U \rVert
$$

<details>
<summary>Proof</summary>

For all $\boldsymbol{u}\in U$ we have 

$$
  \lVert \boldsymbol{x}-\boldsymbol{u} \rVert^2 =& \lVert \underbrace{(\boldsymbol{x} - \boldsymbol{x}|_U)}_{=\boldsymbol{n}} + \underbrace{\tilde{u}}_{(\boldsymbol{x}|_U - \boldsymbol{u})}_{\tilde{\boldsymbol{u}}\in U} \rVert^2 \\
  =& \langle \boldsymbol{n} + \tilde{\boldsymbol{u}}, \boldsymbol{n} + \tilde{\boldsymbol{u}} \rangle \\
  =& \langle\boldsymbol{n},\boldsymbol{n}\rangle + \underbrace{\langle\boldsymbol{n},\tilde{\boldsymbol{u}}\rangle}_{=\boldsymbol{0}} + \underbrace{\langle\tilde{\boldsymbol{u}},\boldsymbol{n}\rangle}_{=\boldsymbol{0}} + \langle\tilde{\boldsymbol{u}},\tilde{\boldsymbol{u}}\rangle \\
  =& \lVert \boldsymbol{n} \rVert^2 + \underbrace{\lVert\tilde{\boldsymbol{u}}\rVert^2}_{\geq 0} \geq \lVert\boldsymbol{n}\rVert^2
$$

This shows that $\inf\Set{\lVert\boldsymbol{x}-\boldsymbol{u}\rVert | \boldsymbol{u}\in U} \geq \lVert\boldsymbol{n}\rVert$ with equality if and only iff $\tilde{\boldsymbol{u}} = \boldsymbol{0} \iff \boldsymbol{u} = \boldsymbol{x}|_U$.
</details>
</MathBox>

<MathBox title='Orthonormal basis' boxType='proposition'>
Let $\langle\cdot, \cdot\rangle: V\times V \to\mathbb{F}$ be an inner product on the $\mathbb{F}$-vector space $V$. Suppose that $U\subseteq V$ is a $k$-dimensional subspace. A set $B = \Set{\boldsymbol{b}_i}_{i=1}^{m\leq k}$ is called
- *orthogonal system* (OS) if $\langle\boldsymbol{b}_i,\boldsymbol{b}_j\langle = 0$ for all $i\neq j$
- *orthonormal system* (ONS) if $\langle\boldsymbol{b}_i,\boldsymbol{b}_j\langle = \delta_{ij}$
- *orthogonal basis* if it is an OS and a basis of $U$
- *orthonormal basis* if it is an OSN and a basis of $U$
</MathBox>

## Positive definite matrices

<MathBox title='Positive definite matrix' boxType='definition'>
An $n\times n$ matrix $\boldsymbol{A}\in\mathcal{M}_{n}(\mathbb{F})$ is *positive definite* if
1. $\boldsymbol{A}$ is Hermitian or self-adjoint, i.e. $\boldsymbol{A}^* = \boldsymbol{A}$
2. $\langle \boldsymbol{v}, \boldsymbol{A}\boldsymbol{v} \rangle > 0$ for all $\boldsymbol{v}\in \mathbb{F}^n \setminus\Set{\boldsymbol{0}}$

If $\boldsymbol{A}$ is a positive definite matrix, then the map $\langle\cdot, \cdot\rangle: \mathbb{F}\times\mathbb{F}\to\mathbb{F}$ defined by

$$
  \langle\boldsymbol{y}, \boldsymbol{x} \rangle := \langle\boldsymbol{y}, \boldsymbol{A}\boldsymbol{x}\rangle
$$

defines an inner product in $\mathbb{F}^n$.
</MathBox>

<MathBox title="Sylvester's criterion" boxType='criterion'>
A Hermitian matrix is positive definite if and only if all leading principle minors have a positive determinant.

<details>
<summary>Proof</summary>

Suppose that $\boldsymbol{M}$ is an $n\times n$ Hermitian matrix and let $\boldsymbol{M}_k$ for $k=\Set{1,\dots,n}$ be the leading principal minors. First assume that $\boldsymbol{M}$ is positive definite. If $\boldsymbol{x} = \left[\begin{smallmatrix} x_1 & \cdots & x_k & 0 & \cdots & 0 \end{smallmatrix}\right]^* = \left[\begin{smallmatrix} \boldsymbol{x}_k & \boldsymbol{0} \end{smallmatrix}\right]^*$ we note that $0 < \boldsymbol{x}^* \boldsymbol{M}\boldsymbol{x} = \boldsymbol{x}_k^* \boldsymbol{M}_k \boldsymbol{x}_k$. Equivalently, the eigenvalues of $\boldsymbol{M}_k$ are positive, implying that $\det(\boldsymbol{M}_k) > 0$ since the determinant is the product of the eigenvalues.

The inverse implication can be shown inductively. The general form of an $(n+1)\times(n+1)$ Hermitian matrix is $\boldsymbol{M}_{n+1} = \left[\begin{smallmatrix} \boldsymbol{M}_n & \boldsymbol{v} \\ \boldsymbol{v}^* & d \end{smallmatrix}\right]$ where $\boldsymbol{v}\in\mathbb{F}^n$ and $d\in\mathbb{F}$. Suppose that the criterion holds for $\boldsymbol{M}_n$. Assuming that all the leading principle minors of $\boldsymbol{M}$ have positive determinants implies that $\det(\boldsymbol{M}_{n+1}) > 0$, $\det(\boldsymbol{M}) > 0$, and that $\boldsymbol{M}_n$ is positive definite by the inductive hypothesis. Denote $\boldsymbol{x} = \left[\begin{smallmatrix} \boldsymbol{x}_n \\ x_{n+1} \end{smallmatrix}\right]$, then

$$
  \boldsymbol{x}^* M_{n+1} \boldsymbol{x} = \boldsymbol{x}_n^* \boldsymbol{M}_n \boldsymbol{x}_n + x_{n+1}\boldsymbol{x}_n^* \boldsymbol{v} + \bar{x}_{n+1}\boldsymbol{v}^* \boldsymbol{x}_n + d|x_{n+1}|^2
$$

Completing the squares, we get

$$
\begin{align*}
  &(\boldsymbol{x}^* + \boldsymbol{v}^* \boldsymbol{M}_n^{-1} \bar{x}_{n+1})\boldsymbol{M}_n (\boldsymbol{x} + x_{n+1}\boldsymbol{M}_n^{-1}\boldsymbol{v}) \\
  &- |x_{n+1}|^2 \boldsymbol{v}^* \boldsymbol{M}_n^{-1}\boldsymbol{v} + d|x_{n+1}|^2 \\
  =& (\boldsymbol{x} + \boldsymbol{c})^* \boldsymbol{M}_n (\boldsymbol{x} + \boldsymbol{c}) + |x_{n+1}|^2 (d - \boldsymbol{v}^* \boldsymbol{M}_n^{-1}\boldsymbol{v})
\end{align*}
$$

where $\boldsymbol{c} = \boldsymbol{x}_{n+1} M_n^{-1} \boldsymbol{v}$. The first term is positive by the inductive hypothesis. It remains to check the sign of the second terms. Using the block determinant formula $\left|\begin{smallmatrix} \boldsymbol{A} & \boldsymbol{B} \\ \boldsymbol{C} & \boldsymbol{D} \end{smallmatrix}\right| = \det(\boldsymbol{A})\det(\boldsymbol{D} - \boldsymbol{C}\boldsymbol{A}^{-1}\boldsymbol{B})$ on $\boldsymbol{M}_{n+1}$ we obtain

$$
  \det(\boldsymbol{M}_{n+1}) = \det(\boldsymbol{M}_n)\det(d - \boldsymbol{v}^* \boldsymbol{M}_n^{-1}\boldsymbol{v}) > 0
$$

which implies $d - \boldsymbol{v}^* \boldsymbol{M}_n^{-1} \boldsymbol{v} > 0$. Hence, $\boldsymbol{x}^* \boldsymbol{M}\boldsymbol{x} > 0$.
</details>
</MathBox>

<MathBox title='Properties of positive definite matrices' boxType='proposition'>
For a self-adjoint $n\times n$ matrix $\boldsymbol{A}\in\mathcal{M}_{n}(\mathbb{F})$, the following claims are equivalent
1. $\boldsymbol{A}$ is positive definite
2. All eigenvalues of $\boldsymbol{A}$ are positive
3. After Gaussian elimination (without scaling and exchanging rows) only with row operations $\boldsymbol{Z}_{i + \lambda j}$, all pivots in the row echelon are positive.
4. The determinants of leading principal minors of $\boldsymbol{A}$ are positive. **(Sylvester's criterion)**
</MathBox>

# Metric vector space

## Matrices

Every linear map between finite-dimensional vector spaces can be represented by a matrix. This can be shown as follows for a transform $L : V \subseteq \mathbb{F}^n \to W \subseteq \mathbb{F}^m$. For simplicity, it is assumed that the bases $B^V = \Set{\hat{v}_i}_{i \in I \subseteq \N}$ and $B^W = \Set{\hat{w}_i}_{i \in I \subseteq \N}$ are orthonormal

$$
\begin{align*}
  L(v) &= L\left( \sum_{i=1}^n \hat{v}_i \right) = \sum_{i=1}^n L(\hat{v}_i) \\
  &= \sum_{i=1}^n \sum_{j=1}^m \left( \sum_{k=1}^n L_k^j \hat{v}^k \right)\hat{w}_j, \quad \sum_{k=1}^n L_k^j \hat{v}^k = L_k^j \delta^{ik} \\
  &= \sum_{j=1}^m \sum_{i=1}^n L_i^j \hat{w}_j = \sum_{j=1}^m w^j \hat{w}_j = w
\end{align*}    
$$

The output vector $w$ is given by the matrix multiplication

$$
  w^j = \sum_{i=1}^n L_i^j
$$

### Eigenvectors

Let $L$ be an endomorphic linear transformation $L: V \to V$. A non-zero vector $v \neq 0 \in V$ is an eigenvector of $T$ if

$$
  L(v) = \lambda v
$$

where $\lambda \in \mathbb{F}$ is the eigenvalue corresponding to $v$.

The eigenvalue equation can be rearranged as

$$
  (L - \lambda \cdot \mathrm{Id})v = 0
$$

If $V$ is a finite-dimensional vector space, the determinant of the composed transformation $L - \lambda \cdot \mathrm{Id}$ vanishes

$$
  \det(L - \lambda \cdot \mathrm{Id}) = 0
$$

The determinant gives a polynomial function in $\lambda$, called the characteristic polynomial of $L$.

### Linear map transform

A linear map can be transformed into a new basis through the steps
1. Transform the input vector (contravariant) from the new to the old basis through a forward transform
2. Apply the linear map to the transformed vector
3. Transform the output vector back into the new basis

This can be shown as follows for a linear map $L$

$$
\begin{align*}
  L(\tilde{e}_i) &= L\left( \sum_{j=1}^n A_i^j e_j \right) = \sum_{j=1}^n A_i^j L(e_j) \\
  &= \sum_{j=1}^n A_i^j \sum_{k=1}^n L_j^k e_k \\
  &= \sum_{j=1}^n \sum_{k=1}^n A_i^j t_j^k \sum_{l=1}^n \tilde{A}_k^l \tilde{e}_l \\
  &= \sum_{l=1}^n \sum_{j=1}^n \sum_{k=1}^n \tilde{A}_k^l L_j^k A_i^j   \tilde{e}_l = \sum_{l=1} \tilde{L}_i^l \tilde{e}_l
\end{align*}
$$

The linear map gets transformed as follows

$$
\begin{gather*}
  \tilde{L}_i^l = \sum_{j=1}^n \sum_{k=1}^n \tilde{A}_k^l L_j^k A_i^j = \tilde{A}_k^l L_j^k A_i^j \\
\begin{aligned}
  \tilde{L} &= A^{-1}LA = \tilde{A}LA \\
  A\tilde{L}A^{-1} &= AA^{1}LAA^{-1} = L
\end{aligned}
\end{gather*}
$$

In tensor product notation, this can be derived as follows

$$
\begin{align*}
  L &= L_j^i e_i \otimes \boldsymbol{\varepsilon}^j \\
  &= L_j^i \left(\tilde{A}_i^k \tilde{e}_k \right) \otimes \left(A_l^j \tilde{\boldsymbol{\varepsilon}}^l \right) \\
  &= \left( \tilde{A}_i^k L_j^i A_l^j \right) \tilde{e}_k \otimes \tilde{\boldsymbol{\varepsilon}}^l \\
  &= \tilde{L_j^i} \tilde{e}_i \otimes \tilde{\boldsymbol{\varepsilon}}^j
\end{align*}
$$


## Inner product (dot product)

$$
  \langle v, w \rangle = v \cdot w = \| v \| \| w \| \cos{\theta} = g_{ij}_ v^j
$$

## Norm

The norm of a vector $\boldsymbol{v} \in V$ for a generalized basis $B = \Set{ e_i }_{i \in I \subset \N}$ is given by

$$
  \| v \|^2 = \langle v, v \rangle = v^* G v = g_{ij}_ v^j
$$

where $g_{ij}$ is the metric tensor

$$
  g_{ij} = \langle e_i, e_j \rangle
$$

The norm is invariant of coordinate changes

$$
\begin{align*}
  \| v \|^2 &= \tilde{g}_{ij} \tilde{v}^i \tilde{v}^j \\
  &= \left( A_i^k A_j^l g_{kl} \right) \left( \tilde{A}_m^i v^m \right) \left( \tilde{A}_n^j v^n \right) \\
  &= g_{kl} v^m v^n \left( \tilde{A}_m^i A_i^k  \tilde{A}_n^j A_j^l \right) \\
  &= g_{kl} v^m v^n \left(\delta_m^k \delta_n^l \right) \\
  &= g_{ij} _ v^j 
\end{align*}
$$

## Cross product (wedge product)
$$
\begin{gather*}
  a \times b = \begin{vmatrix} e_1 & e_2 & e_3\\
  a_1 & a_2 & a_3 \\
  b_1 & b_2 & b_3 \end{vmatrix} = \varepsilon^i_{jk}e_i a^j b^k  \\
  \| a \times b \| = \| a \| \| b \| \sin\theta \\
  \| a \times b \|^2 = \| a \|^2 \| b \|^2 - \left( a \cdot b \right)^2 = \| a \|^2 \| b \|^2 \left(1 - \cos^2\theta \right)
\end{gather*}
$$

Properties
- Jacobi identity
$$
  a \times \left( b \times c \right) + b \times \left( c \times a \right) + c \times \left( a \times b \right) = 0
$$
- Scalar triple product
$$
  A \cdot \left( B \times C \right) = B \cdot \left( C \times A \right) = C \cdot \left( A \times B \right)
$$

### Triple product

$$
\begin{align*}
  \left[a \times \left( b \times c \right)\right]^i &= \varepsilon^i_{jk} a^j \varepsilon^k_{mn} b^m c^n \\ 
  &= \left(\delta^{im}\delta^{jn} - \delta^{in}\delta^{jm}\right)a^j b^m c^n \\
  &= b^i a^j c^j - c^i a^j b^j \\
  &= \left[b \left(a \cdot c \right) - c\left( a \cdot b \right) \right]^i
\end{align*}
$$

## Outer product (tensor product)

Given an $m$-dimensional vector $u$ and an $n$-dimensional vector $v$, their outer product is an $m \times n$ matrix 

$$
  \left(u \otimes v\right)_{ij} = \left(u v^{\dagger}\right)_{ij} = u_{i} v_{j}^*
$$

The outer product has the following properties $\forall u, \boldsymbol{v} \in V$, $\psi, \varphi \in V^*$  and $a \in \mathbb{F}$

- $a \left( v \otimes  \right) = \left(a v \right) \otimes \psi = v \otimes \left(a \psi \right)$
- $v \otimes \left( \psi + \varphi \right) = v \otimes \psi + v \otimes \varphi$
- $\left(u + v \right) \otimes \psi = u \otimes \psi + v \otimes \psi$

## Linear form/functional (covector)

A linear form is a linear map from a vector space to its field of scalars, $f : V \to \mathbb{F}$ (1-form).

Linear functionals are represented as row vectors in $\R^n$.

### Bilinear form

A bilinear form is a map $B: V \times V \to \mathbb{F}$ (2-form) that is linear in each argument separately $\forall u, v, w \in V$ and $\lambda \in \mathbb{F}$

- $\lambda B(u, v) = B(\lambda u, v) = B(u, \lambda u)$
- $B(u + v, w) = B(u, w) + B(v, w)$
- $B(u, v + w) = g(u, v) + g(u, w)$

Bilinear forms are formed by linear combinations of covector-covector pairs

$$
  B = B_{ij} \left( e^i \otimes e^j \right)
$$

With this definition a bilinear map can be expressed as follows

$$
\begin{align*}
  s &= B(u, v) \\
  &= B_{ij}e^i e^j \left(u^k e_k, v^l e_l \right) \\
  &= B_{ij}e^i \left(u^k e_k \right) e^j \left( v^l e_l \right) \\
  &= B_{ij} u^k v^l e^i \left(e_k \right) e^j \left(e_l \right) \\
  &= B_{ij} u^k v^l \delta_k^i \delta_l^j \\
  &= B_{ij} u^i v^j
\end{align*}
$$

Bilinear forms are transformed as follows

$$
\begin{align*}
  B &= B_{ij} \left( e^i \otimes e^j \right) = B_{ij} \left(A_k^i \tilde{e}^k \right) \left( A_l^j \tilde{e}^l \right) \\
  &= \left( A_k^i A_l^j B_{ij} \right) \tilde{e}^k  \tilde{e}^l \\
  &= \tilde{B}_{ij}  \tilde{e}^i \tilde{e}^j
\end{align*}
$$

giving the transformation rules

$$
\begin{align*}
  \tilde{B}_{ij} &= A_i^k A_j^l B_{kl} \\
  B_{ij} &= \tilde{A}_i^k \tilde{A}_j^l \tilde{B}_{kl}
\end{align*}
$$

## Dual vector space

Given a vector space $V$ over a field $\mathbb{F}$, the dual space $V^*$ is defined as the set of all linear functionals (covectors) $\varphi : V \to \mathbb{F}$.

The dual space $V^*$ itself becomes a vector space over $\mathbb{F}$ when equipped with the operations of addition and scalar multiplication satisfying $\forall \varphi, \psi \in V^*, x \in V$ and $a \in \mathbb{F}$ 

$$
\begin{gather*}
  (\varphi + \psi)(x) = \varphi(x) + \psi(x) \\
  (a\varphi)(x) = a(\varphi(x))
\end{gather*}
$$

If $V$ is finite-dimensional, then $V^*$ has the same dimension as $V$.

## Basis

A basis $B$ of a vector space $V$ over a field $\mathbb{F}$ is a linearly independent subset of $V$ that spans $V$.

Given a basis $B = \Set{ e_i }_{i \in I \subseteq \N}$, a new basis $\tilde{B} = \Set{\tilde{e}_i}_{i \in I \subseteq \N}$ can be formed by transforming the old basis vectors, and vice versa

$$
\begin{align*}
  \tilde{e}_i &= \sum_{j=1}^n A_i^je_j = A_i^je_j \\
  e_i &= \sum_{j=1}^n \tilde{A}_i^j \tilde{e}_j = \tilde{A}_i^j \tilde{e}_j 
\end{align*}
$$

Basis vectors transform covariantly. The transforms are invertible (bijections) such that the compositions give the identity transform 

$$
\begin{gather*}
  A\tilde{A} = AA^{-1} = I \\
  \sum_{j=1}^n A_{ij}\tilde{A}_{ji} = \sum_{j=1}^n \tilde{A}_{ij}A_{ji} = \delta_{ij} = \begin{cases} 1, \quad i = j \\ 0, \quad i \neq j \end{cases}
\end{gather*}
$$

A vector $v$ can be expressed as a linear combination of basis vectors

$$
\begin{align*}
  v = \sum_{j=1}^n v_j e_j = \sum_{j=1}^n v_j \left( \sum_{i=1}^n \tilde{A}_{ij} \tilde{e}_i \right) = \sum_{i=1}^n \left( \sum_{j=1}^n \tilde{A}_{ij}v_j \right)\tilde{e}_i = \sum_{j=1}^n \tilde{v}_j \tilde{e}_j \\ 
  v = \sum_{j=1}^n \tilde{v}_j \tilde{e}_j = \sum_{j=1}^n \tilde{v}_j \left( \sum_{i=1}^n A_{ij} e_i \right) = \sum_{i=1}^n \left( \sum_{j=1}^n A_{ij} \tilde{v}_j \right)e_i = \sum_{j=1}^n v_j e_j
\end{align*}
$$

Vector components are said to be contravariant as they transform inversely of the basis vectors. To signify this, vector components are denoted with superscript indices. 

$$
\begin{align*}
  v^j &= \sum_{j=1}^n A_{ij} \tilde{v}^j \\
  \tilde{v}^j &= \sum_{j=1}^n \tilde{A}_{ij}v^j
\end{align*}
$$

### Dual basis

Given a basis $B = \Set{ e_i }_{i \in I \subset \N}$ for a finite-dimensional vector space $V$, a basis $B^* = \Set{e^i}_{i \in I \subset \N}$, called the dual basis, can be formed through the bi-orthogonality property

$$
  e^i \left( e_j \right) = \delta_j^i
$$

A new dual basis $\tilde{B}^*$ can be formed by transforming the old dual basis, $\tilde{e}^i = \sum_{i=1}^n t_{ij}e^j$. By the bi-orthogonality property

$$
\begin{align*}
  \tilde{e}^i (\tilde{e}_k) &= \sum_{j=1} t_{ij} e^j \left( \tilde{e}_k\right) \\
  &= \sum_{j=1}^n t_{ij} e^j \left( \sum_{l=1}^n a_{lk} e_l \right) \\
  &= \sum_{j=1}^n \sum_{l=1}^n t_{ij} a_{lk} e^j (e_l) \\
  &= \sum_{j=1}^n \sum_{l=1}^n t_{ij} a_{lk} \delta_j^l \\
  &= \sum_{j=1}^n \sum_{l=1}^n t_{ij} a_{jk} = \delta_k^i \quad \Rightarrow t_{ij} = \tilde{a}_{ij}
\end{align*}
$$

This shows that dual basis vectors transform covariantly, hence the superscripts

$$
\begin{gather*}
  \tilde{e}^i = \sum_{j=1}^n \tilde{a}_{ij} e^j \\
  e^i = \sum_{j=1}^n a_{ij} \tilde{e}^j
\end{gather*}
$$

A covector $\varphi \in V^*$ can be represented as a linear combination of the dual basis vectors

$$
  \varphi = \sum_{i=1}^n \varphi_i e^i = \sum_{i=1}^n \varphi_i \left( a_{ij}e^j \right) = \sum_{j=1}^n \left( \sum_{i=1}^n \varphi_i a_{ij} \right) \tilde{e}^j
$$

Covectors transform covariantly, hence the subscripts.

$$
\begin{gather*}
  \varphi_j = \sum_{i=1}^n \tilde{a}_{ij}\tilde{\varphi}_i \\
  \tilde{\varphi}_j = \sum_{i=1}^n a_{ij}\varphi_i
\end{gather*}
$$

