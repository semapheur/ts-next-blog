---
title: 'Linear Algebra'
subject: 'Mathematics'
showToc: true
references:
  - book_feeman_2023
  - book_tsukada_2023
  - book_ashraf_etal_2022
  - book_moon_etal_2022
  - book_johnston_2021
  - book_nair_singh_2018
  - book_saidhouari_2017
  - book_liesen_2015
  - book_petersen_2012
  - book_roman_2008
  - book_smith_1998
---

# Matrices

<MathBox title='Matrix' boxType='definition'>
Let $R$ be a commutative ring with unit and let $n,m\in\N_0$. A matrix of size $m \times n$ over $R$ is an array of the form

$$
  \boldsymbol{A} = [a_{ij}]_{\substack{1\leq i \leq m \\ 1\leq j\leq n}} = \begin{bmatrix}
    a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\
    a_{2,1} & a_{2,2} & \cdots & a_{2,n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{m,1} & a_{m,2} & \cdots & a_{m,n}
  \end{bmatrix}
$$

where
- $a_{ij} \in R$ are the *entries* or *components* of the matrix
- $i = 1,\dots,m$ are the row indices
- $j = 1,\dots,n$ are the column indices

The set of all $m\times n$ matrices over $R$ is denoted by $\mathcal{M}_{m,n} (R) = R^{m \times n}$.

<details>
<summary>Details</summary>

It is common to assume that $1 \neq 0$ in $R$. This excludes the trivial case of the ring that contains only the zero element.
</details>
</MathBox>

In particular, for $m = 0$ or $m = 0$, we obtain empty matrices of size $0\times n$, $m\times 0$ or $0 \times 0$. These matrices matrices by $[\;]$

A matrix $\boldsymbol{A} \in R^{n \times n}$ of size $n\times n$ is called a *square matrix*. The entries $a_{ii} \in R$ for $i=1,\dots,n$ are called the *diagonal entries* of $\boldsymbol{A}$. The *identity matrix* in $R^{n \times n}$ is the matrix $\boldsymbol{I}_n := [\delta_{ij}]$ where

$$
  \delta_{ij} := \begin{cases}
    1,\quad& i = j \\
    0,\quad& i \neq j
  \end{cases}
$$

is the *Kronecker delta*.

The $i$th *row* of $\boldsymbol{A}\in R^{m\times n}$ is $[a_{i1}, a_{i2}, \dots, a_{in}] \in R^{1\times n}$ for $i=1,\dots,m$. The $j$th *column* of $\boldsymbol{A}$ is

$$
  \begin{bmatrix} a_{1j} \\ a_{2j} \\ \vdots \\ a_{mj} \end{bmatrix} \in R^{m\times 1},\; j = 1,\dots,n
$$

We can combine $1\times n$ row matrices $\boldsymbol{a}_i \in R^{1\times n}$ for $i=1,\dots,m$ to the matrix

$$
  \boldsymbol{A} = \begin{bmatrix}
    - \boldsymbol{a}_1 - \\
    - \boldsymbol{a}_2 - \\
    \vdots \\
    - \boldsymbol{a}_m -
  \end{bmatrix} = \begin{bmatrix}
    a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\
    a_{2,1} & a_{2,2} & \cdots & a_{2,n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{m,1} & a_{m,2} & \cdots & a_{m,n}
  \end{bmatrix} \in R^{m\times n}
$$

Likewise can combine $m\times 1$ column matrices $\boldsymbol{a}_j \in R^{m\times 1}$ for $j=1,\dots,n$ to the matrix

$$
  \mathbf{A} = \begin{bmatrix}
    \shortmid & \shortmid & & \shortmid \\
    \boldsymbol{a}_1 & \boldsymbol{a_2} &  \cdots & \boldsymbol{a}_n \\
    \shortmid & \shortmid & & \shortmid
  \end{bmatrix} = \begin{bmatrix}
    a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\
    a_{2,1} & a_{2,2} & \cdots & a_{2,n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{m,1} & a_{m,2} & \cdots & a_{m,n}
  \end{bmatrix} \in R^{m\times n}
$$

## Matrix operations

<MathBox title='Matrix addition' boxType='definition'>
Let $R$ be a ring. Matrix addition is a binary operation $+: R^{m\times n} \times R^{m\times n} \to R^{m\times n}$ defined for matrices of the same size. Given two matrices $\boldsymbol{A} = [a_{ij}]$ and $\boldsymbol{B} = [b_{ij}]$ in $R^{m\times n}$, their sum is defined entrywise by

$$
  \boldsymbol{A} + \boldsymbol{B} := [a_{ij} + b_{ij}]
$$

where the addition on the right-hand side is the ring addition in $R$.
</MathBox>

<MathBox title='Matrix addition' boxType='definition'>
Let $R$ be a ring. Matrix multiplication is a binary operation $\cdot: R^{m\times n} \times R^{n\times p} \to R^{m\times p}$ defined as follows. Given matrices 

$$
\begin{align*}
  \boldsymbol{A} =& [a_{ik}] = \begin{bmatrix} - \boldsymbol{a}_1 - \\ \vdots \\ - \boldsymbol{a}_n - \end{bmatrix} \in R^{m\times n} \\
  \boldsymbol{B} =& [b_{kj}] = \begin{bmatrix} \shortmid & & \shortmid \\ \boldsymbol{b}_i & \cdots & \boldsymbol{b}_n \\ \shortmid & & \shortmid \end{bmatrix} \in R^{n\times p}
\end{align*}
$$

their product is the matrix $\boldsymbol{C} = \boldsymbol{AB} = [c_{ij}] \in R^{m\times p}$, with entries given by

$$
  c_{ij} := \sum_{k=1}^n a_{ik} b_{kj} = \boldsymbol{a}_i \cdot\boldsymbol{b}_j
$$

Each entry $c_{ij} \in R$ is computed by taking the dot product of the $i$th row of $\boldsymbol{A}$ and the $j$th column of $\boldsymbol{B}$, where both the multiplication and addition are performed in $R$. Thus, the product $\boldsymbol{AB}$ is defined only when the number of columns in $\boldsymbol{A}$ equals the number of rows in $\boldsymbol{B}$. The multiplication process can be illustrated schematically as

$$
\begin{array}{cc}
  & \left[\begin{matrix} b_{11} & \cdots \\ \vdots & \\ b_{n1} & \cdots \end{matrix}\right. \begin{bmatrix} b_{1j} \\ \vdots \\ b_{nj} \end{bmatrix} \left.\begin{matrix} \cdots & b_{1p} \\ & \vdots \\ \cdots & b_{np} \end{matrix}\right] \in R^{n \times p} \\
  R^{m\times n} \ni \begin{bmatrix} a_{11} & \cdots & a_{1n} \\ \vdots & & \vdots \\ [a_{i1} & \cdots & a_{in}] \\ \vdots & & \vdots \\ a_{m1} & \cdots & a_{mn} \end{bmatrix} & \begin{bmatrix} \hphantom{b_{n1}} & & \hphantom{\begin{bmatrix} b_{nj} \end{bmatrix}} & \hphantom{\cdots} & \hphantom{b_{nq}} \\ \vphantom{\vdots} & & \downarrow & & \\ & \rightarrow & c_{ij} & & \\ \vphantom{\vdots} & & & & \\ & & & & \end{bmatrix} \in R^{m\times p}
\end{array}
$$

For $\boldsymbol{A}\in R^{n\times n}$ we define $\boldsymbol{A}^0 = \boldsymbol{I}_n$ and

$$
  \boldsymbol{A}^k := \prod_{i=1}^k \boldsymbol{A},\; k\in\N_+
$$

Matrix multiplication is generally not commutative; even when both products $\boldsymbol{AB}$ and $\boldsymbol{BA}$ are defined (which requires $m = p$), it is typically the case that $\boldsymbol{AB} \neq \boldsymbol{BA}$ except under special conditions.
</MathBox>

<MathBox title='Properties of matrix multiplication' boxType='proposition'>
Let $R$ be a ring. For matrices $\boldsymbol{A}, \tilde{\boldsymbol{A}} \in R^{k \times l}$, $\boldsymbol{B}, \tilde{\boldsymbol{B}} \in R^{l \times m}$ and $\boldsymbol{C} \in R^{m\times n}$, matrix multiplication satisfies:
1. **Associativity:** $(\boldsymbol{A}\boldsymbol{B})\boldsymbol{C} = \boldsymbol{A}(\boldsymbol{B}\boldsymbol{C})$
2. **Left distributivity:** \boldsymbol{A}(\boldsymbol{B} + \tilde{\boldsymbol{B}}) = \boldsymbol{A}\boldsymbol{B} + \boldsymbol{A}\tilde{\boldsymbol{B}}
3. **Right distributivity:** $(\boldsymbol{A} + \tilde{\boldsymbol{A}}) \boldsymbol{B} = \boldsymbol{A}\boldsymbol{B} + \tilde{\boldsymbol{A}}\boldsymbol{B}$
4. **Identity commutativity:** $\boldsymbol{I}_l \boldsymbol{A} = \boldsymbol{AI}_k = \boldsymbol{A}$

<details>
<summary>Proof</summary>

**(1):** Let

$$
  (\boldsymbol{AB})\boldsymbol{C} = [d_{ij}],\quad& \boldsymbol{A}(\boldsymbol{BC}) = [\tilde{d}_{ij}]
$$

By definition of matrix multiplication and using the associative and distributive law in $R$, we get

$$
\begin{align*}
  d_{ij} =& \sum_{p=1}^m \left(\sum_{q=1}^k a_{iq} b_{qp} \right) c_{pj} \\
  =& \sum_{p=1}^m \sum_{q=1}^k (a_iq b_{qp}) c_{pj} \\
  =& \sum_{p=1}^m \sum_{q=1}^k a_iq (b_{qp} c_{pj}) \\
  =& \sum_{q=1}^k a_{iq} \left(\sum_{p=1}^m b_{qp} c_{pj} \right) = \tilde{d}_{ij}
\end{align*}
$$

for $1 \leq i \leq n$ and $1 \leq j \leq k$, which implies that $(\boldsymbol{AB})\boldsymbol{C} = \boldsymbol{A}(\boldsymbol{BC})$.
</details>
</MathBox>

<MathBox title='Scalar multiplication of matrices' boxType='definition'>
Let $R$ be a ring. Scalar multiplication of matrices is the binary operation $\cdot : R \times R^{m\times n} \to R^{m\times n}$ defined by

$$
  \lambda \boldsymbol{A} := [\lambda a_{ij}]
$$

where $\lambda \in R$ is a scalar, and $\boldsymbol{A} = [a_{ij}] \in R^{m\times n}$ is an $n \times m$ matrix over $R$. The result is the matrix obtained by multiplying each entry of $\boldsymbol{A}$ by $\lambda$.
</MathBox>

From the definition of scalar multiplication with matrices, it immediately follows that $0\cdot\boldsymbol{A} = \boldsymbol{0}_{m\times n}$ and $1\cdot\boldsymbol{A} = \boldsymbol{A}$ for any matrix $\boldsymbol{A}\in R^{m\times n}$, where $\boldsymbol{0}_{m\times n}$ denotes the $m\times n$ zero matrix over $R$.

<MathBox title='Properties of matrix scalar multiplication' boxType='proposition'>
Let $R$ be a ring. For matrices $\boldsymbol{A}, \boldsymbol{B} \in R^{l\times m}$, $\boldsymbol{C}\in R^{m\times n}$ and scalar $\lamnda, \mu \in R$, scalar multiplication of matrices satifies:
1. $(\lambda\mu) \boldsymbol{A} = \lambda(\mu\boldsymbol{A})$
2. $(\lambda + \mu)\boldsymbol{A} = \lambda\boldsymbol{A} + \mu\boldsymbol{A}$
3. $\lambda(\boldsymbol{A} + \boldsymbol{B}) = \lambda\boldsymbol{A} + \lambda\boldsymbol{B}$
4. $(\lambda\boldsymbol{A})\boldsymbol{C} = \lambda(\boldsymbol{AC}) = \boldsymbol{A}(\lambda\boldsymbol{B})$

- $c\boldsymbol{A} = \boldsymbol{A}c$ for a scalar $c$ **(scalar commutativity)**
- $c(\boldsymbol{A}\boldsymbol{B}) = (c\boldsymbol{A})\boldsymbol{B}$ **(left scalar associativity)**
- $(\boldsymbol{A}\boldsymbol{B})c = \boldsymbol{A}(\boldsymbol{B}c)$ **(right scalar associativity)**
</MathBox>


<MathBox title='Matrix transposition' boxType='definition'>
Let $R$ be a ring. The *transpose* of a matrix is a function $\top: R^{m\times n} \to R^{n\times m}$ defined by

$$
  \boldsymbol{A} = [a_{ij}] \mapsto \boldsymbol{A}^\top = [a_{ji}]
$$

That is, the entry in the $i$th row and $j$th column of $\boldsymbol{A}^\top$ is the entry in the $j$th row and $i$th column of $\boldsymbol{A}$.
</MathBox>

<MathBox title='Matrix symmetry' boxType='definition'>
Let $R$ be a ring and let $\boldsymbol{A} \in R^{n\times n}$ be a square matrix. Then $\boldsymbol{A}$ is
- *symmetric* it is equal to its transpose, i.e. $\boldsymbol{A} = \boldsymbol{A}^\top$
- *skew-symmetric* if it is equal to its negative transpose, i.e. $\boldsymbol{A} = -\boldsymbol{A}^\top$
</MathBox>

<MathBox title='Properties of matrix transposition' boxType='proposition'>
Let $R$ be a ring. For matrices $\boldsymbol{A}, \tilde{\boldsymbol{A}}\in R^{l\times m}$, $\boldsymbol{B}\in R^{m\times n}$ and scalar $\lambda\in R$, the transpose operation satiesfies:
1. **Involution:** $(\boldsymbol{A}^\top)^\top = \boldsymbol{A}$
2. **Additivity:** $(\boldsymbol{A} + \tilde{\boldsymbol{A}})^\top = \boldsymbol{A}^\top + \tilde{\boldsymbol{A}}^\top$
3. **Homogeneity:** $(\lambda \boldsymbol{A})^\top = \lambda \boldsymbol{A}^\top$
4. **Reversal law:** $(\boldsymbol{A}\boldsymbol{B})^\top = \boldsymbol{B}^\top \boldsymbol{A}^\top$
5. $\det(\boldsymbol{A}^\top) = \det(\boldsymbol{A})$

<details>
<summary>Proof</summary>

**(4):** Let $\boldsymbol{AB} = [c_{ij}]$ with $c_{ij} = \sum_{k=1}^m a_{ik} b_{kj}$, $\boldsymbol{A}^\top = [\tilde{a}_{ij}]$, $\boldsymbol{B}^\top = [\tilde{b}_{ij}]$ and $(\boldsymbol{AB})^\top = [\tilde{c}_{ij}]$, then

$$
\begin{align*}
  \tilde{c}_{ji} = c_{ji} =& \sum_{k=1}^m a_{jk} b_{ki} \\
  =& \sum_{k=1}^m \tilde{a}_{kj} \tilde{b}_{ik} \\
  =& \sum_{k=1}^m \tilde{b}_{ik} \tilde{a}_{kj}
\end{align*}
$$

which implies $(\boldsymbol{AB})^\top = \boldsymbol{B}^\top \boldsymbol{A}^\top$.
</details>
</MathBox>


An $m\times n$ matrix $\boldsymbol{A}$ with entries in a field $\mathbb{F}$ is a rectangular array of elements $\boldsymbol{A}_{i,j} = a_{i,j} \in\mathbb{F}$ for $i=1,\dots,m$ and $j=1,\dots,n$ represented as

$$
  \boldsymbol{A} = \begin{bmatrix}
    a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\
    a_{2,1} & a_{2,2} & \cdots & a_{2,n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{m,1} & a_{m,2} & \cdots & a_{m,n}
  \end{bmatrix},\; a_{i,j}\in\mathbb{F}
$$

This notation can be abbreviated by writing the entries as $\boldsymbol{A} = (a_{i,j})_{\substack{1\leq i \leq m \\ 1\leq j\leq n}}$. The set of $m\times n$ matrices with entries in a field $\mathbb{F}$ is denoted $\mathcal{M}_{m,n}(\mathbb{F}) = \mathbb{F}^{m \times n}$. For $n\times n$ matrices this set is denoted $\mathcal{M}_{n,n}(\mathbb{F}) = \mathbb{F}^{n \times n}$. A *column vector* is an $m\times 1$ matrix, while a row vector is a $1\times n$ matrix.

The *main diagonal* of an $m\times n$ matrix $\boldsymbol{A}$ is sequence of entries $(a_{i,i})_{i=1}^{\min\Set{m,n}}$. An $m\times n$ matrix $\boldsymbol{A}$ is called diagonal if its off-diagonal entries are zero, and denoted $\boldsymbol{A} = \mathrm{diag}(a_{i,i})_{i=1}^n$. A square matrix is *upper triangular* if all of its entries below the main diagonal are $0$. Similarly, a square matrix is *lower triangular* if all of its entries above the main diagonal are $0$.

The transpose of $\boldsymbol{A}\in \mathcal{M}_{m,n}(\mathbb{F})$ is the matrix $\boldsymbol{A}^\top$ defined by $[\boldsymbol{A}^\top]_{i,j} = \boldsymbol{A}_{j,i}$. A matrix is symmetric if $\boldsymbol{A} = \boldsymbol{A}^\top$ and skew-symmetric if $\boldsymbol{A}^\top = -\boldsymbol{A}$. All $n\times n$ diagonal matrices are by definition symmetric.

The conjugate transpose (adjoint) of a complex $m\times n$ matrix $\boldsymbol{A}\in\mathcal{M}_{m,n}(\mathbb{C})$ is the matrix $\boldsymbol{A}^*$ defined by $[\boldsymbol{A}^*]_{i,j} = \overline{\boldsymbol{A}}_{j,i}$.

Suppose $\boldsymbol{A}\in\mathcal{M}_n (\mathbb{F})$ is an $n\times n$-matrix. For $1\leq k \leq n$, the $k$th leading principal minor of $\boldsymbol{A}$ is the $k\times k$ matrix $H_k = (a_{i,j})_{1\leq i \leq k,\; 1\leq j\leq k}$.

The $n\times n$ identity matrix is defined as $\boldsymbol{I}_n := \operatorname{diag}(1)_{i=1}^n$ with $[\boldsymbol{I}_n]_{i,j} = \delta_{i,j}$ where $\delta_{i,j}$ is the Kronecker delta function

$$
  \delta_{i,j} = \begin{cases} 1,\quad& i=j \\ 0,\quad& i\neq j \end{cases}
$$

## Matrix multiplication

If $\boldsymbol{A}$ is an $m\times n$ matrix and $\boldsymbol{B}$ and $p\times q$ matrix, the matrix product $\boldsymbol{A}\boldsymbol{B}$ is defined if $n = p$, resulting in an $m \times q$ matrix given by the dot product of the corresponding row of $A$ and the corresponding column of $B$

$$
  [\boldsymbol{A}\boldsymbol{B}]_{i,j} = \sum_{r=1}^n a_{i,r} b_{r,j}
$$

The matrix product $\boldsymbol{B}\boldsymbol{A}$ is only defined if $m = q$ resulting in an $p \times n$ matrix. If both products are defined, they generally need not be equal, meaning that matrix multiplication is not commutative.



## Partitioning and matrix multiplication

Let $\boldsymbol{M}$ be an $m\times n$ matrix. If $B\subseteq \Set{1,\dots,m }$ and $C\subseteq\Set{1,\dots,n}$ then the submatrix $\boldsymbol{M}[B,C]$ is the matrix obtained from $\boldsymbol{M}$ by keeping only the rows index in $B$ and the columns with index in $C$ such that $\boldsymbol{M}[B,C]$ has size $|B|\cdot|\boldsymbol{C}|$.

Suppose $\boldsymbol{M}\in\mathcal{M}_{m,n}$ and $\boldsymbol{N}\in\mathcal{M}_{n,k}$. If
1. $\mathcal{P} = \Set{B_1,\dots, B_p}$ is a partition of $\Set{1,\dots,m}$
2. $\mathcal{Q} = \Set{C_1,\dots, C_q}$ is a partition of $\Set{1,\dots,n}$
3. $\mathcal{R} = \Set{D_1,\dots, D_r}$ is a partition of $\Set{1,\dots,k}$

we have

$$
  [\boldsymbol{MN}][B_i, D_i] = \sum_{C_h \in\mathcal{Q}} \boldsymbol{M}[B_i, C_h] \boldsymbol{N}[C_h, D_j]
$$

When the partions contain only single-element block, this reduces to the usual formula for matrix multiplication

$$
  [\boldsymbol{MN}]_{i,j} = \sum_{h=1}^m \boldsymbol{M}_{i,h} \boldsymbol{N}_{h,j}
$$

## Block matrices

If $\boldsymbol{B}_{i,j}$ are submatrices of $\boldsymbol{M}\in\mathcal{M}_{m,n}$ with appropriate sizes, then $\boldsymbol{M}$ can be written as the block matrix

$$
  \boldsymbol{M} = \begin{bmatrix} 
    \boldsymbol{B}_{1,1} & \cdots & \boldsymbol{B}_{1,n} \\
    \vdots & \ddots & \vdots \\
    \boldsymbol{B}_{m,1} & \cdots & \boldsymbol{B}_{m,n}
  \end{bmatrix}
$$

A square matrix of the form

$$
  \boldsymbol{M} = \begin{bmatrix} 
    \boldsymbol{B}_1 & \boldsymbol{0} &\cdots & \boldsymbol{0} \\
    \boldsymbol{0} & \ddots & \ddots & \vdots \\
    \vdots & \ddots & \ddots & \boldsymbol{0} \\
    \boldsymbol{0} & \cdots & \boldsymbol{0} & \boldsymbol{B}_n
  \end{bmatrix}
$$

where each $\boldsymbol{B}_i$ is square and $\boldsymbol{0}$ is a zero submatrix, is called a *block diagonal matrix*.

<MathBox title='' boxType='proposition'>
Let $m, n\in\N_+$ with $n > m$ and $c\in(0,1]$. For a matrix $\mathbf{A}\in\mathcal{M}_{n,m} (\mathbb{C})$, there is another matrix $\mathbb{B} \in\mathcal{M}_{n, (n-m)} (\mathbb{C})$ such that the block matrix

$$
  \mathbf{V} = \begin{bmatrix} \mathbf{A} & \mathbf{B} \end{bmatrix} \in \mathcal{M}_n (\mathbb{C})
$$

satisfies

$$
\begin{equation}
  \mathbf{V}^* \mathbf{V} = \begin{bmatrix}
    \mathbf{A}^* \mathbf{A} & \mathbf{0}_{m\times (n - m)} \\
    \mathbf{0}_{(n - m) \times m} & c \mathbf{I}_{(n - m)\times (n - m)}
  \end{bmatrix} \tag{\label{equation-1}}
\end{equation}
$$

<details>
<summary>Proof</summary>

Let $\mathbf{a}_1,\dots,\mathbf{a}_m, \mathbf{b}_1,\dots,\mathbf{b}_{n-m} \in\mathbb{C}^n$ be such that

$$
  \mathbf{A} = \begin{bmatrix}
    \shortmid & \cdots & \shortmid \\
    \boldsymbol{a}_1 & \cdots & \boldsymbol{a}_m \\
    \shortmid & \cdots & \shortmid
  \end{bmatrix}, \quad \mathbf{B} = \begin{bmatrix}
    \shortmid & \cdots & \shortmid \\
    \boldsymbol{b}_1 & \cdots & \boldsymbol{b}_{n - m} \\
    \shortmid & \cdots & \shortmid
  \end{bmatrix}
$$

Moreover, set

$$
  \mathbf{V} = \begin{bmatrix} \mathbf{A} & \mathbf{B} \end{bmatrix}
$$

Then we have

$$
  \mathbf{V}^* \mathbf{V} = \begin{bmatrix}
    \overline{\mathbf{a}}_1 \cdots \mathbf{a}_1 & \cdots & \overline{\mathbf{a}}_1 \cdots \mathbf{a}_m & \overline{\mathbf{a}}_1 \cdots \mathbf{b}_1 & \cdots & \overline{\mathbf{a}}_1 \cdots \mathbf{b}_{n-m} \\
    \vdots & & \vdots & \vdots & & \vdots \\
    \overline{\mathbf{a}}_m \cdots \mathbf{a}_1 & \cdots & \overline{\mathbf{a}}_m \cdots \mathbf{a}_m & \overline{\mathbf{a}}_m \cdots \mathbf{b}_1 & \cdots & \overline{\mathbf{a}}_m \cdots \mathbf{b}_{n-m} \\
    \overline{\mathbf{b}}_1 \cdots \mathbf{a}_1 & \cdots & \overline{\mathbf{a}}_1 \cdots \mathbf{a}_m & \overline{\mathbf{b}}_1 \cdots \mathbf{b}_1 & \cdots & \overline{\mathbf{b}}_1 \cdots \mathbf{b}_{n-m} \\
    \vdots & & \vdots & \vdots & & \vdots \\
    \overline{\mathbf{b}}_{n - m} \cdots \mathbf{a}_1 & \cdots & \overline{\mathbf{b}}_{n -m} \cdots \mathbf{a}_m & \overline{\mathbf{b}}_{n - m} \cdots \mathbf{b}_1 & \cdots & \overline{\mathbf{b}}_{n-m} \cdots \mathbf{b}_{n-m}
  \end{bmatrix}
$$

For $\mathbf{V}^* \mathbf{V}$ to satisfy $\eqref{equation-1}$, the $\mathbf{b}_j$ have to satisfy

$$
\begin{align*}
  \overline{\mathbf{a}}_l \cdot \mathbf{b}_k =& 0, \; \forall l\in\set{1,\dots,m},\; k\in{1,\dots,n-m} \tag{\label{equation-2}} \\
  \overline{\mathbf{b}}_j \cdot \mathbf{b}_k =& c\delta_{jk}, \; \forall j,k \in\set{1,\dots,n-m} \tag{\label{equation-3}}
\end{align*}
$$

Each of the $n - m$ vectors $\mathbf{b}_j$ has $n$ components giving us a linear system of $n(n - m)$ unknowns. Equation $\eqref{equation-2}$ gives $m$ equations, where due to symmetry $\eqref{equation-3}$ provides $\frac{(n - m)(n - m + 1)}{2}$ equations. As long as

$$
  \frac{(n - m)(n - m + 1)}{2} + m \leq n(n - m)
$$

or

$$
  m(m + 1) \leq n(n - 1)
$$

we can find the $\mathbf{b}_j$ and this matrix $\mathbf{B}\in\mathcal{M}_{n, n-m} (\mathbb{C})$ delivering the required form for $\mathbf{V}^* \mathbf{V}$. Since by assumption $n > m$ it follows that $n - 1 \geq m$ and $n \geq m + 1$ which guarantees $m(m + 1) \leq n(n - 1)$.
</details>
</MathBox>

## Elementary row operations

There are three elementary row operations on matrices
- a row within the matrix can be switched with another row (row switching)
- each element in a row can be multiplied by a non-zero constant (row scaling)
- a row can be replaced by the sum of that row and a multiple of another row (row addition)

<MathBox title='Row echelon form' boxType='definition'>
A matrix $\boldsymbol{R}\in\mathcal{M}_{m,n}(\mathbb{F})$ is in *row echelon* form if
1. All zero rows (if there any) appear at the bottom of the matrix.
2. For each nonzero row, the leftmost nonzero entry, called the leading entry or the pivot, is strictly to the right of the leading entry of every row above.
3. Any column that contains a leading entry has zeroes in all other positions.

The matrix $\boldsymbol{R}$ is in *reduced row echelon form* if all the leading entries are equal to $1$.
</MathBox>

<MathBox title='Row equivalence' boxType='proposition'>
Two matrices $\boldsymbol{A}, \boldsymbol{B}\in \mathcal{M}_{m,n}$ are row equivalent, denoted $\boldsymbol{A}\sim \boldsymbol{B}$, if eiter one can be obtained from the other by a series of elementary row operations, i.e. there is an invertible matrix $\boldsymbol{P}$ such that $\boldsymbol{A} = \boldsymbol{PB}$.

A matrix $\boldsymbol{A}$ is row equivalent to one and only one matrix $R$ in reduced row echelon form, i.e. $\boldsymbol{A} = \boldsymbol{E}_1\cdots \boldsymbol{E}_k \boldsymbol{R}$ where $\boldsymbol{E}_i$ are elementary matrices required to reduce $\boldsymbol{A}$ to reduced row echelon form.

A matrix $\boldsymbol{A}$ is invertible if and only if its reduced row echelon form is an identity matrix. Hence a matrix is invertible if and only if it is the product of elementary matrices.
</MathBox>

An $m\times n$ matrix $R$ that is in both reduced row echelon form and reduced column echelon form must have the block form

$$
  \boldsymbol{J}_k = \begin{bmatrix} 
    \boldsymbol{I}_k & \boldsymbol{0}_{k,n-k} \\
    \boldsymbol{0}_{m-k, k} & \boldsymbol{0}_{m-k, n-k}
  \end{bmatrix}
$$

<MathBox title='Similar matrices' boxType='definition'>
Two matrices $\boldsymbol{A}, \boldsymbol{B}\in \mathcal{M}_{m,n}$ are *similar* if there exists an invertible matrix $\boldsymbol{P}$ such that 

$$
  \boldsymbol{A} = \boldsymbol{PBP}^{-1}
$$
</MathBox>

<MathBox title='Congruent matrices' boxType='definition'>
Two matrices $\boldsymbol{A}, \boldsymbol{B}\in\mathcal{M}_{m,n}$ are *congruent* if there exists an invertible matrix $\boldsymbol{P}$ such that 

$$
  \boldsymbol{A} = \boldsymbol{PBP}^\top
$$
</MathBox>

<MathBox title='Row-switching transformation' boxType='definition'>
The corresponding elementary matrix for switching rows $i$ and $j$ is obtained by swapping the respective rows of the identity matrix. This in an $n\times n$ matrix $\boldsymbol{P}_{i,j} \in\mathcal{M}_n (\mathbb{F})$ of the form

$$
  \boldsymbol{P}_{i,j} = \begin{bmatrix}
    1 & & & & & & \\
    & \ddots & & & & & \\
    & & 0 & & 1 & & \\
    & & & \ddots & & & \\
    & & 1 & & 0 & & \\
    & & & & & \ddots & \\
    & & & & & & 1
  \end{bmatrix}
$$
</MathBox>

<MathBox title='Row-scaling transformation' boxType='definition'>
The corresponding elementary matrix for scaling the $i$th row by $m \neq 0$ is a diagonal matrix with diagonal entries $1$ except in the $i$th position

$$
  \boldsymbol{D}_i (m) = \begin{bmatrix}
    1 & & & & & & \\
    & \ddots & & & & & \\
    & & 1 & & & & \\
    & & & m & & & \\
    & & & & 1 & & \\
    & & & & & \ddots & \\
    & & & & & & 1
  \end{bmatrix}
$$

The entries of $\boldsymbol{D}_i(m)$ are given by

$$
  [\boldsymbol{D}_i(m)]_{k,l} = \begin{cases} 
    0 \quad& k\neq l \\
    1 \quad& k = l, k\neq i \\
    m \quad& k = l, k = i
  \end{cases}
$$
</MathBox>

<MathBox title='Row addition transformation' boxType='definition'>
The corresponding elementary matrix for adding the $j$th row multiplied by a scalar $\lambda \in\mathbb{F}$ to the $i$th row is the identity matrix with an $\lambda$ in the $(i,j)$th entry. This is an $n\times n$ matrix $\boldsymbol{Z}_{i,j}(\lambda) = \boldsymbol{Z}_{i+\lambda j} \in \mathcal{M}_n (\mathbb{F})$ of the form

$$
  \boldsymbol{Z}_{i+\lambda j} = \begin{bmatrix}
    1 & & & & & & \\
    & \ddots & & & & & \\
    & & 1 & & & & \\
    & & & \ddots & & & \\
    & & \lambda & & 1 & & \\
    & & & & & \ddots & \\
    & & & & & & 1
  \end{bmatrix}
$$

The entries of $\boldsymbol{Z}_{i+\lambda j}$ are given by

$$
  [\boldsymbol{Z}_{i+\lambda j}]_{k,l} = \begin{cases} 
    0 \quad& k\neq l, k\neq i, l\neq j \\
    1 \quad& k = l \\
    \lambda \quad& k = i, l = j
  \end{cases}
$$
</MathBox>

<MathBox title='' boxType='proposition'>
Consider an $m\times n$ matrix $\boldsymbol{A}\mathcal{M}_{m\times n}(\mathbb{F})$, and let $\boldsymbol{M} \in\mathcal{M}_{m\times n}(\mathbb{F})$ be a composition elementary row matrices. Then we have

$$
\begin{align*}
  \ker(\boldsymbol{M}\boldsymbol{A}) =& \ker(\boldsymbol{A}) \\
  \operatorname{ran}(\boldsymbol{M}\boldsymbol{A}) =& \boldsymbol{M}\;\ker(\boldsymbol{A})
\end{align*}
$$
</MathBox>

## Systems of linear equations

A system of $m$ linear equation with $n$ unknowns and coefficients in $\mathbb{F}$ takes the form

$$
\begin{align*}
  a_{11}x_1 + \cdots + a_{1n}x_n =& b_1 \\
  \vdots & \\
  a_{m1}x_1 + \cdots + a_{mn}x_n =& b_m 
\end{align*}
$$

where $x_1,\dots,x_n$ are the unknowns, $a_{11},\dots,a_{mn}$ are the coefficients and $b_1,\dots,b_m$ are the constant terms. The system can be written as the linear combination

$$
  x_1 \begin{bmatrix} a_{11} \\ \vdots \\ a_{m1} \end{bmatrix} + \cdots + x_n \begin{bmatrix} a_{1n} \\ \vdots \\ a_{mn} \end{bmatrix} = \begin{bmatrix} b_1 \\ \vdots \\ b_m \end{bmatrix}
$$

which is equivalent to the matrix equation

$$
  \begin{bmatrix} a_{11} & \cdots & a_{1n} \\ \vdots & \ddots & \vdots \\ a_{m1} & \cdots & a_{mn} \end{bmatrix} \cdot \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix} = \begin{bmatrix} b_1 \\ \vdots \\ b_m \end{bmatrix}
$$

or in short form $\boldsymbol{A}\boldsymbol{x} = \boldsymbol{b}$, where $\boldsymbol{A} = (a_{i,j})_{1\leq i \leq m,\; 1\leq j\leq n} \in\mathcal{M}_{m, n} (\mathbb{F})$, $\boldsymbol{x} = (x_i)_{1\leq i \leq n}\in\mathbb{F}^n$ and $\boldsymbol{b} = (b_i)_{1\leq i \leq m}\in\mathbb{F}^m$.

In its augmented form, the system is written as

$$
\left[\begin{array}{c c c|c}
  a_{11} & \cdots & a_{1n} & b_1 \\ \vdots & \ddots & \vdots & \vdots \\ a_{m1} & \cdots & a_{mn} & b_m
\end{array}\right]
$$

<MathBox title='Free and leading variables' boxType='definition'>
Consider a system of linear equations $\boldsymbol{A}\boldsymbol{x} = \boldsymbol{b}$ where $\boldsymbol{A}\in M_{m,n}(\mathbb{F})$, $\boldsymbol{x}\in\mathbb{F}^n$ and $\boldsymbol{b}\in\mathbb{F}^m$. In its row echelon form, all variables with no pivots in their columns are called *free variables*, while all variables with a pivot in their columns are called *leading variables*. The number of free variables is given by $\dim(\ker(\boldsymbol{A}))$, while the number of leading variables is given by $\dim(\operatorname{ran}(\boldsymbol{A}))$.
</MathBox>

<MathBox title='' boxType='proposition'>
For a system of linear equations $\boldsymbol{A}\boldsymbol{x} = \boldsymbol{b}$ where $\boldsymbol{A}\in M_{m,n}(\mathbb{F})$, $\boldsymbol{x}\in\mathbb{F}^n$ and $\boldsymbol{b}\in\mathbb{F}^m$, the set of solutions $S := \Set{\tilde{\boldsymbol{x}} \in \mathbb{F}^n | \boldsymbol{A} \tilde{\boldsymbol{x}} = \boldsymbol{b}}$ is either an affine subspace $S = \boldsymbol{v}_0 + \ker(\boldsymbol{A})$ for $\boldsymbol{v}_0 \in\boldsymbol{v_0}\in\mathbb{F}^n$, the empty set, $S = \emptyset$.

<details>
<summary>Proof</summary>

Assume $\boldsymbol{v}_0 \in S$ is a solution, i.e. $\boldsymbol{A}\boldsymbol{v}_0 = \boldsymbol{b}$. Set $\tilde{\boldsymbol{x}} := \boldsymbol{v}_0 + \boldsymbol{x}_0$ for $\boldsymbol{x}_0 \in\mathbb{F}^n$. Then 

$$
\begin{align*}
  \tilde{\boldsymbol{x}}\in S \iff& \boldsymbol{A}\tilde{\boldsymbol{x}} = \boldsymbol{A}(\boldsymbol{v}_0 + \boldsymbol{x}_0) = \boldsymbol{b} \\
  \iff& \underbrace{\boldsymbol{A}\boldsymbol{v}_0}_{=\boldsymbol{b}} + \boldsymbol{A}\boldsymbol{x}_0 = \boldsymbol{b} \\
  \iff& \boldsymbol{A}\boldsymbol{x}_0 = \boldsymbol{0} \\
  \iff& \boldsymbol{x}_0 \in\ker(\boldsymbol{A})
\end{align*}
$$
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
For a system of linear equations $\boldsymbol{A}\boldsymbol{x} = \boldsymbol{b}$ where $\boldsymbol{A}\in M_{m,n}(\mathbb{F})$, $\boldsymbol{x}\in\mathbb{F}^n$ and $\boldsymbol{b}\in\mathbb{F}^m$, the set of solutions $S := \Set{\tilde{\boldsymbol{x}} \in \mathbb{F}^n | \boldsymbol{A} \tilde{\boldsymbol{x}} = \boldsymbol{b}}$ is either an affine subspace $S = \boldsymbol{v}_0 + \ker(\boldsymbol{A})$ for $\boldsymbol{v}_0 \in\boldsymbol{v_0}\in\mathbb{F}^n$, the empty set, $S = \emptyset$.

<details>
<summary>Proof</summary>

Assume $\boldsymbol{v}_0 \in S$ is a solution, i.e. $\boldsymbol{A}\boldsymbol{v}_0 = \boldsymbol{b}$. Set $\tilde{\boldsymbol{x}} := \boldsymbol{v}_0 + \boldsymbol{x}_0$ for $\boldsymbol{x}_0 \in\mathbb{F}^n$. Then 

$$
\begin{align*}
  \tilde{\boldsymbol{x}}\in S \iff& \boldsymbol{A}\tilde{\boldsymbol{x}} = \boldsymbol{A}(\boldsymbol{v}_0 + \boldsymbol{x}_0) = \boldsymbol{b} \\
  \iff& \underbrace{\boldsymbol{A}\boldsymbol{v}_0}_{=\boldsymbol{b}} + \boldsymbol{A}\boldsymbol{x}_0 = \boldsymbol{b} \\
  \iff& \boldsymbol{A}\boldsymbol{x}_0 = \boldsymbol{0} \\
  \iff& \boldsymbol{x}_0 \in\ker(\boldsymbol{A})
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Solvability of a system linear equation' boxType='proposition'>
For $\boldsymbol{A}\in M_{m,n}(\mathbb{F})$ and $\boldsymbol{b}\in\mathbb{F}^m$, the following claims are equivalent
1. The system $\boldsymbol{A}\boldsymbol{x} = \boldsymbol{b}$ has at least one solution.
2. $\boldsymbol{b}\in\operatorname{ran}(\boldsymbol{A})$
3. $\boldsymbol{b}$ can be written as a linear combination of the columns of $\boldsymbol{A}$
4. All zero rows of the row echelon form a of the system have zero constant terms.

<details>
<summary>Proof</summary>

**(1)** $\iff$ **(2)** is given by the definition of $\operatorname{ran}(\boldsymbol{A})$.
**(2)** $\iff$ **(3)** is given by the column representation of $\operatorname{ran}(\boldsymbol{A})$

$$
  \operatorname{ran}(A) = \Set{[\boldsymbol{a}_1 \cdots \boldsymbol{a_n}]\boldsymbol{x} | \boldsymbol{x}\in\mathbb{F}^n}
$$

**(4)** $\implies$ **(1)** follows from the fact that $\operatorname{rank}(\boldsymbol{A}) = \operatorname{rank}([\boldsymbol{A}|\boldsymbol{b}])$

**(1)** $\implies$ **(4)** can be shown by contraposition. Assume the row echelon form of $[\boldsymbol{A}|\boldsymbol{b}]$ has a zero row with a non-zero constant term. This implies that the system is not solvable, i.e. there is no solution for $\boldsymbol{A}\boldsymbol{x} = \boldsymbol{b}$.
</details>
</MathBox>

<MathBox title='Uniqueness for solutions of a system linear equation' boxType='proposition'>
For $\boldsymbol{A}\in M_{m,n}(\mathbb{F})$, the following claims are equivalent
1. The system $\boldsymbol{A}\boldsymbol{x} = \boldsymbol{b}$ has at most one solution for every $\boldsymbol{b}\in\mathbb{F}^n$.
2. $\ker(\boldsymbol{A}) = \Set{\boldsymbol{0}}$
3. The row echelon form of $\boldsymbol{A}\boldsymbol{x} = \boldsymbol{b}$ has no free variables.
4. $\operatorname{rank}(\boldsymbol{A}) = n$
5. The linear map $f_{\boldsymbol{A}}:\mathbb{F}^n \to\mathbb{F}^m$ by $\boldsymbol{x}\mapsto \boldsymbol{A}\boldsymbol{x}$ is injective. 

For square matrices, i.e. $\boldsymbol{A}\in M_{n,n}(\mathbb{F})$, it follows that $\ker(\boldsymbol{A}) = \Set{0} \iff \operatorname{ran}(\boldsymbol{A}) = \mathbb{F}^n$, or equivalently that $f_{\boldsymbol{A}}$ is bijective.
</MathBox>

## Determinants

<MathBox title='Volume measure' boxType='definition'>
The volume measure on $\R^n$ is the $n$-form $\operatorname{vol}_n : \prod_{i=1}^n \R^n \to\R$ satisfying for all $\boldsymbol{u}_1,\dots,\boldsymbol{u}_n \in\R$ 
1. **Linearity:** For $\alpha,\beta\in\mathbb{F}$, $\boldsymbol{v}\in\R^n$ and $j\in\Set{1,\dots,n}$
$$
\begin{align*}
  \operatorname{vol}_n (\boldsymbol{u}_1,\dots,\alpha\boldsymbol{u}_j + \beta\boldsymbol{v},\dots,\boldsymbol{u}_n) =& \alpha\operatorname{vol}_n (\boldsymbol{u}_1,\dots, \boldsymbol{u}_j,\dots,\boldsymbol{u}_n) \\
  &+ \beta\operatorname{vol}_n (\boldsymbol{u}_1,\dots,\boldsymbol{v},\dots,\boldsymbol{u}_n)
\end{align*}
$$
3. **Antisymmetry:** For $i,j\in\Set{1,\dots,n}$ with $i\neq j$
$$
  \operatorname{vol}_n (\boldsymbol{u}_1,\dots,\boldsymbol{u}_i,\dots \boldsymbol{u}_j,\dots,\boldsymbol{u}_n) = -\operatorname{vol}_n (\boldsymbol{u}_1,\dots,\boldsymbol{u}_j,\dots \boldsymbol{u}_i,\dots,\boldsymbol{u}_n)
$$
4. **Normalization:**
$$
  \operatorname{vol}_n (\boldsymbol{e}_1,\dots,\boldsymbol{e}_n) = 1
$$

The measure $\operatorname{vol}_n(\boldsymbol{u}_1,\dots,\boldsymbol{u}_n)$ gives the volume of the parallelepiped spanned by $\boldsymbol{u}_1,\dots,\boldsymbol{u}_n$.
</MathBox>

<MathBox title='Determinant of matrices' boxType='definition'>
The determinant is a function $\det:\mathcal{M}_{n}(\mathbb{F}) \to\mathbb{F}$ with the following properties for $\boldsymbol{A} \in \mathcal{M}_{n}(\mathbb{F})$
1. If $\boldsymbol{A} = \left[\begin{smallmatrix} \shortmid & & \shortmid \\ \boldsymbol{a}_1 & \dots & \boldsymbol{a}_n \\ \shortmid & & \shortmid \end{smallmatrix}\right]$ then $\operatorname{vol}_n (\boldsymbol{a}_1,\dots,\boldsymbol{a_n}) = |\det(\boldsymbol{A})|$ with the geometric interpretation that the column vectors of $\boldsymbol{A}$ span a parallelepiped.
2. $\det(\boldsymbol{A}) = 0$ if and only if $\left[\begin{smallmatrix} \shortmid \\ \boldsymbol{a}_1 \\ \shortmid \end{smallmatrix}\right], \dots, \left[\begin{smallmatrix} \shortmid \\ \boldsymbol{a}_n \\ \shortmid \end{smallmatrix}\right]$ are linearly dependent, or equivalently if $\boldsymbol{A}$ is not invertible.
3. the sign of $\det(\boldsymbol{A})$ gives the orientation of the parallelepiped spanned by $\boldsymbol{A}$. In particular, $\det(\boldsymbol{I}_n) = 1$.
</MathBox>

<MathBox title='Leibniz formula' boxType='proposition'>
The determinant of $\boldsymbol{A} = \left[\begin{smallmatrix} \shortmid & & \shortmid \\ \boldsymbol{a}_1 & \dots & \boldsymbol{a}_n \\ \shortmid & & \shortmid \end{smallmatrix}\right] \in\mathcal{M}_{n}(\mathbb{F})$ is given by the Leibniz formula

$$
  \det(\boldsymbol{A}) = \sum_{\sigma\in S_n} \left( \operatorname{sgn}(\sigma) \prod_{i=1}^n a_{i,\sigma(i)} \right)
$$

where $\sigma$ is a permutation bijection and $S_n$ is the set of all permutations of $\Set{1,\dots,n}$. The signature of a permutation $\sigma$ is given by

$$
  \operatorname{sgn}(\sigma) = \begin{cases} 
    +1,\quad& \sigma \text{ is even} \\
    -1,\quad& \sigma \text{ is odd}
  \end{cases}
$$

In terms of the Levi-Civita symbol $\varepsilon$, the Leibniz formula takes the form

$$
  \det(\boldsymbol{A}) = \sum_{i_1,\dots,i_n} \varepsilon_{i_1,\dots,i_n} \prod_{j=1} a_{i_j, j}
$$

<details>
<summary>Proof</summary>

By definition, $\operatorname{vol}_n (\boldsymbol{a}_1,\dots,\boldsymbol{a}_n) = |\det(\boldsymbol{A})|$. Calculating $\operatorname{vol}_n (\boldsymbol{a}_1,\dots,\boldsymbol{a}_n)$ gives

$$
\begin{align*}
  \operatorname{vol}_n ( \boldsymbol{a}_1,\dots,\boldsymbol{a}_n ) =& \operatorname{vol}_n \left( \begin{bmatrix} a_{11} \\ \vdots \\ a_{n1} \end{bmatrix}, \dots,\begin{bmatrix} a_{1n} \\ \vdots \\ a_{nn} \end{bmatrix} \right) \\
  =& \operatorname{vol}_n \left( \sum_{i_1 = 1}^n a_{i_1, 1} \boldsymbol{e}_{i_1},\dots, \sum_{i_n = 1}^n a_{i_n, n} \boldsymbol{e}_{i_n} \right) \\
  =& \sum_{i_1 = 1}^n \cdots \sum_{i_n = 1}^n a_{i_1, 1}\cdots a_{i_n, n} \operatorname{vol}_n (\boldsymbol{e}_{i_1},\dots,\boldsymbol{e}_{i_n}) \\
  =& \sum_{(i_i,\dots,i_n)\in S_n} a_{i_1, 1}\cdots a_{i_n, n} \operatorname{vol}_n (\boldsymbol{e}_{i_1},\dots,\boldsymbol{e}_{i_n})
\end{align*}
$$

where $S_n$ is the set of all permutations of $\Set{1,\dots,n}$. Due to the antisymmetry and normalization properties of the volume measure we get

$$
  \operatorname{vol}_n (\boldsymbol{e}_{i_1},\dots,\boldsymbol{e}_{i_n}) = \begin{cases} +1, \;& \text{even exchanges of arguments} \\ -1, \;& \text{odd  exhanges of arguments} \end{cases}
$$

This is precisely the definition of $\operatorname{sgn}: S_n \to \Set{-1,1}$ giving the signature of permutations $\sigma = (i_i,\dots,i_n)\in S_n$.
</details>
</MathBox>

<MathBox title='Properties of the determinant' boxType='proposition'>
The determinant has the following properties for $\boldsymbol{A},\boldsymbol{B}\in\mathbb{M}_{n}(\mathbb{F})$
1. $\det(\boldsymbol{AB}) = \det(\boldsymbol{A})\det(\boldsymbol{B})$
2. $\boldsymbol{A}$ is invertible (nonsingular) if and only if $\det(\boldsymbol{A}) \neq 0$, in this case $\det(\boldsymbol{\boldsymbol{A}}^{-1}) = \frac{1}{\det(\boldsymbol{A})}$
3. If $\boldsymbol{A}$ is an upper/lower triangular matrix, then $\det(\boldsymbol{A}) = \prod_{i=1}^n \boldsymbol{A}_{i,j}$
4. $\left|\begin{smallmatrix} \boldsymbol{A} & \boldsymbol{0} \\ \boldsymbol{C} & \boldsymbol{D} \end{smallmatrix}\right| = \det(\boldsymbol{A})\det(\boldsymbol{D}) = \left|\begin{smallmatrix} \boldsymbol{A} & \boldsymbol{B} \\ \boldsymbol{0} & \boldsymbol{D} \end{smallmatrix}\right|$
5. If $\boldsymbol{A}$ is invertible then $\left|\begin{smallmatrix} \boldsymbol{A} & \boldsymbol{B} \\ \boldsymbol{C} & \boldsymbol{D} \end{smallmatrix}\right| = \det(\boldsymbol{A})\det(\boldsymbol{D - CA^{-1}B})$
6. If $\boldsymbol{D}$ is invertible then $\left|\begin{smallmatrix} \boldsymbol{A} & \boldsymbol{B} \\ \boldsymbol{C} & \boldsymbol{D} \end{smallmatrix}\right| = \det(\boldsymbol{D})\det(\boldsymbol{A - BD^{-1}C})$
7. If $\boldsymbol{C}$ and $\boldsymbol{D}$ commute, then $\left|\begin{smallmatrix} \boldsymbol{A} & \boldsymbol{B} \\ \boldsymbol{C} & \boldsymbol{D} \end{smallmatrix}\right| = \det(\boldsymbol{A}\boldsymbol{D} - \boldsymbol{B}\boldsymbol{C})$
8. If $\boldsymbol{M}$ is a block diagonal matrix, i.e. of the form

$$
  \boldsymbol{M} = \begin{bmatrix} 
    \boldsymbol{B}_1 & \boldsymbol{0} &\cdots & \boldsymbol{0} \\
    \boldsymbol{0} & \ddots & \ddots & \vdots \\
    \vdots & \ddots & \ddots & \boldsymbol{0} \\
    \boldsymbol{0} & \cdots & \boldsymbol{0} & \boldsymbol{B}_n
  \end{bmatrix}
$$

then $\det(\boldsymbol{M}) = \prod_{i=1}^n \boldsymbol{B}_i$.
</MathBox>

### Laplace expansion

<MathBox title='Minor and cofactor' boxType='proposition'>
Let $\boldsymbol{A}\in\mathcal{M}_n (\mathbb{F})$, and let $\boldsymbol{A}_{ij}$ be the $(n-1)\times(n-1)$ matrix formed by removing the $i$th row and $j$th column.
- The $(i,j)$ minor of $\boldsymbol{A}$ is $M_{ij} := \det(\boldsymbol{A}_{ij})$.
- The $(i,j)$ cofactor of $\boldsymbol{A}$ is $C_{ij} := (-1)^{i+j} \det(\boldsymbol{A}_{ij})$
</MathBox>

<MathBox title='' boxType='lemma'>
Let $\boldsymbol{A}\in\mathcal{M}_n (\mathbb{F})$ be an $n\times n$ matrix whose $j$th column is the canonical column vector $\boldsymbol{e}_i$. Then $\det(\boldsymbol{A}) = C_{ij}$.
</MathBox>

<MathBox title='Laplace expansion' boxType='theorem'>
Let $\boldsymbol{A} \in \mathcal{M}_{n}(\mathbb{F})$. The *Laplace expansion* of the determinant of $\boldsymbol{A} = a_{ij}$ along the $i$th row is

$$
  \det(\boldsymbol{A}) = \sum_{j=1}^n a_{ij} C_{ij} = \sum_{j=1}^{n} (-1)^{i+j} a_{ij} \det(\boldsymbol{A}_{ij})
$$

Similarly, the Laplace expansion along the $j$th column is

$$
  \det(\boldsymbol{A}) = \sum_{i=1}^n a_{ij} C_{ij}
$$
</MathBox>

### Cramer's rule

<MathBox title='Adjugate matrix' boxType='definition'>
Let $\boldsymbol{A}\in\mathcal{M}_n (\boldsymbol{F})$ be an $n\times n$-matrix. The *adjugate* of $\boldsymbol{A}$ is the transpose of the cofactor matrix $\boldsymbol{C}$ of $\boldsymbol{A}$

$$
  \operatorname{adj}(\boldsymbol{A}) = \boldsymbol{C}^\top
$$
</MathBox>

<MathBox title='Properties invertible matrices' boxType='proposition'>
For an $n\times n$-matrix $\boldsymbol{A}\in\mathcal{M}_n (\mathbb{F})$, the following claims are equivalent
1. $\boldsymbol{A}$ is invertible
2. the columns of $\boldsymbol{A}$ are linearly independent
3. the rows of $\boldsymbol{A}$ are linearly independent
4. $\operatorname{rank}(\boldsymbol{A}) = n$
5. $\ker(\boldsymbol{A}) = \Set{\boldsymbol{0}}$
6. $\det(\boldsymbol{A}) \neq 0$
7. $\boldsymbol{A}\boldsymbol{x} = \boldsymbol{b}$ has a unique solution for each $\boldsymbol{b}\in\mathbb{F}^n$
</MathBox>

<MathBox title="Cramer's rule" boxType='theorem'>
Let $\boldsymbol{A}\in\mathcal{M}_n (\mathbb{F})$ be an $n\times n$-matrix and suppose $\boldsymbol{b}\in\mathbb{F}$. The solution $\boldsymbol{x}\in\mathbb{F}$ to the system of linear equations $\boldsymbol{A}\boldsymbol{x} = \boldsymbol{b}$ with $\boldsymbol{x} = \sum_{j=1}^n x_j \boldsymbol{e}_j$ is 

$$
\begin{align*}
  x_k =& \frac{1}{\det(\boldsymbol{A})} \sum_{i=1}^n b_i C_{ik} \\
  =& \frac{1}{\det(\boldsymbol{A})}\det\begin{bmatrix}
    \shortmid & & \shortmid & \shortmid & \shortmid & & \shortmid \\
    \boldsymbol{a}_1 & \cdots & \boldsymbol{a}_{k-1} & \boldsymbol{b} & \boldsymbol{a}_{k+1} & \cdots & \boldsymbol{a}_n \\
    \shortmid & & \shortmid & \shortmid & \shortmid & & \shortmid
  \end{bmatrix}
\end{align*}
$$

<details>
<summary>Proof</summary>

If $\boldsymbol{A} = \left[\begin{smallmatrix} \shortmid & & \shortmid \\ \boldsymbol{a}_1 & \cdots & \boldsymbol{a}_n \\ \shortmid & & \shortmid \end{smallmatrix}\right]$, the system of linear equations $\boldsymbol{A}\boldsymbol{x} = \boldsymbol{b}$ can be written as $\sum_{j=1}^n x_j\boldsymbol{a}_j = \boldsymbol{b}$. For each $k\in\set{1,\dots,n}$ define the matrix

$$
  \boldsymbol{A}_k = \begin{bmatrix}
    \shortmid & & \shortmid & \shortmid & \shortmid & & \shortmid \\
    \boldsymbol{a}_1 & \cdots & \boldsymbol{a}_{k-1} & \boldsymbol{b} & \boldsymbol{a}_{k+1} & \cdots & \boldsymbol{a}_n \\
    \shortmid & & \shortmid & \shortmid & \shortmid & & \shortmid
  \end{bmatrix}
$$

The determinant of $\boldsymbol{A}_k$ is

$$
\begin{align*}
  \det{\boldsymbol{A}_k} =& \det(\boldsymbol{a}_1,\dots,\boldsymbol{a}_{k-1},\boldsymbol{u},\boldsymbol{a}_{k+1},\dots,\boldsymbol{n}) \\
  =& \det\left(\boldsymbol{a}_1,\dots,\boldsymbol{a}_{k-1},\sum_{j=1}^n x_j\boldsymbol{a}_j,\boldsymbol{a}_{k+1},\dots,\boldsymbol{n} \right) \\
  =& \sum_{j=1}^n x_j \det(\boldsymbol{a}_1,\dots,\boldsymbol{a}_{k-1},\boldsymbol{a}_j,\boldsymbol{a}_{k+1},\dots,\boldsymbol{n}) \\
  =& x_k \det(\boldsymbol{a}_1,\dots,\boldsymbol{a}_{k-1},\boldsymbol{a}_k,\boldsymbol{a}_{k+1},\dots,\boldsymbol{n}) \\
  =& x_k \det(\boldsymbol{A})
\end{align*}
$$

Thus, $x_k = \frac{\det(\boldsymbol{A})}{\det(\boldsymbol{A})}$. Using the Laplace expansion for the determinant of $\boldsymbol{A}_k$ we get $\det(\boldsymbol{A}) = \sum_{i=1}^n b_i C_{ik}$. Hence

$$
  x_k = \frac{1}{\det(\boldsymbol{A})} \sum_{i=1}^n b_i C_{ik}
$$
</details>
</MathBox>

<MathBox title='Formula for matrix inverse' boxType='theorem'>
If $\boldsymbol{A}\in\mathcal{M}_n (\mathbb{F})$ is an invertible $n\times n$-matrix, then the $(i,j)$-entry of its inverse $\boldsymbol{A}^{-1}$ is $[\boldsymbol{A}^{-1}]_{ij} = \frac{C_{ji}}{\det(\boldsymbol{A})}$ such that

$$
  \boldsymbol{A}^{-1} = \frac{\operatorname{adj}(\boldsymbol{A})}{\det(\boldsymbol{A})}
$$

<details>
<summary>Proof</summary>

Since $\boldsymbol{A}$ is invertible then the system of linear equations $\boldsymbol{A}\boldsymbol{x} = \boldsymbol{b}$ for any $\boldsymbol{b}\in\mathbb{F}$ has a unique solution given by $\boldsymbol{x} = \boldsymbol{A}^{-1}\boldsymbol{b}$. Thus, for each $k\in\Set{1,\dots,n}$ we have

$$
\begin{align*}
  x_k = \sum_{j=1}^n [\boldsymbol{A}^{-1}]_{kj} u_j = \sum_{j=1}^n \frac{1}{\det(\boldsymbol{A})}C_{jk}u_j
\end{align*}
$$

where the last equality follows from Cramer's rule. Hence

$$
  [\boldsymbol{A}^{-1}]_{ij} = \frac{1}{\det(\boldsymbol{A})}C_{ji}
$$
</details>
</MathBox>

### $2\times 2$-matrices

The determinant of a $2\times 2$-matrix $\boldsymbol{A} = \left[\begin{smallmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{smallmatrix}\right]$ is $\det(\boldsymbol{A}) = a_{11}a_{22} - a_{21}a_{12}$.

<MathBox title='Determinant criterion for two dimensional systems of linear equations' boxType='criterion'>
Let $\boldsymbol{A}\in\mathcal{M}_2 (\mathbb{F})$ be a $2\times 2$-matrix and let $\boldsymbol{b}\in\mathbb{F}^2$. A two-dimensional system of linear equations $\boldsymbol{A}\boldsymbol{x} = \boldsymbol{b}$ has a unique solution if and only if $\det(\boldsymbol{A}) \neq 0$.

<details>
<summary>Proof</summary>

Let $\boldsymbol{A} = \left[\begin{smallmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{smallmatrix}\right]$. Transforming $[\boldsymbol{A}|\boldsymbol{b}]$ into row echelon form we get

$$
\begin{align*}
  &\left[\begin{array}{c c|c}
    a_{11} & a_{12} & b_1 \\
    a_{21} & a_{22} & b_2
  \end{array}\right]
  \begin{matrix}
   \\
    \scriptsize{\boldsymbol{R}_2 - \frac{a_{21}}{a_{11}}\boldsymbol{R}_1}
  \end{matrix}
  \\
  \sim& \left[\begin{array}{c c|c}
    a_{11} & a_{12} & b_1 \\
    0 & a_{22} - \frac{a_{21}}{a_{11}}a_{12} & b_2 - \frac{a_{21}}{a_{11}}
  \end{array}\right]
  \begin{matrix}
   \\
    \scriptsize{a_{11}\boldsymbol{R}_2}
  \end{matrix}
  \\
  \sim& \left[\begin{array}{c c|c}
    a_{11} & a_{12} & b_1 \\
    0 & a_{11} a_{22} - a_{21}a_{12} & a_{11}b_2 - a_{21}
  \end{array}\right]
\end{align*}
$$

If $\boldsymbol{A}\boldsymbol{x} = \boldsymbol{b}$ has a unique solution, we require that $a_{11} \neq 0$ and $a_{11} a_{22} - a_{21}a_{12} = \det({\boldsymbol{A}}) \neq 0$.
</details>
</MathBox>

### $3\times 3$-matrices
<MathBox title='Rule of Sarrus' boxType='corollary'>
If $\boldsymbol{A} \in\mathcal{M}_3 (\mathbb{F})$ is a $3\times 3$ matrix, then

$$
\begin{align*}
  \det(\boldsymbol{A}) =& \begin{vmatrix} 
    a_{11} & a_{12} & a_{13} \\
    a_{21} & a_{22} & a_{23} \\
    a_{31} & a_{32} & a_{33} 
  \end{vmatrix} \\
  =& a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32} \\
  & -a_{13}a_{22}a_{31} - a_{11}a_{23}a_{32} - a_{12}a_{21}a_{33}
\end{align*}
$$

<LatexFigure width={50} src='/fig/sarrus_rule.svg' alt=''
  caption='Mnemonic for the rule of Sarrus'
>
```latex
\documentclass[tikz]{standalone}
\usepackage{tikz}
\usetikzlibrary{calc,matrix}

\begin{document}
  \begin{tikzpicture}[>=stealth]
    \matrix [%
      matrix of math nodes,
      column sep=1em,
      row sep=1em
    ] (sarrus) {%
      a_{11} & a_{12} & a_{13} & a_{11} & a_{12} \\
      a_{21} & a_{22} & a_{23} & a_{21} & a_{22} \\
      a_{31} & a_{32} & a_{33} & a_{31} & a_{32} \\
    };

    \path ($(sarrus-1-1.north west)-(0.5em,0)$) edge ($(sarrus-3-1.south west)-(0.5em,0)$)
          ($(sarrus-1-3.north east)+(0.5em,0)$) edge ($(sarrus-3-3.south east)+(0.5em,0)$)
          (sarrus-1-1)                          edge            (sarrus-2-2)
          (sarrus-2-2)                          edge[->]        (sarrus-3-3)
          (sarrus-1-2)                          edge            (sarrus-2-3)
          (sarrus-2-3)                          edge[->]        (sarrus-3-4)
          (sarrus-1-3)                          edge            (sarrus-2-4)
          (sarrus-2-4)                          edge[->]        (sarrus-3-5)
          (sarrus-3-1)                          edge[dashed]    (sarrus-2-2)
          (sarrus-2-2)                          edge[->,dashed] (sarrus-1-3)
          (sarrus-3-2)                          edge[dashed]    (sarrus-2-3)
          (sarrus-2-3)                          edge[->,dashed] (sarrus-1-4)
          (sarrus-3-3)                          edge[dashed]    (sarrus-2-4)
          (sarrus-2-4)                          edge[->,dashed] (sarrus-1-5);

    \foreach \c in {1,2,3} {\node[anchor=south] at (sarrus-1-\c.north) {$+$};};
    \foreach \c in {1,2,3} {\node[anchor=north] at (sarrus-3-\c.south) {$-$};};
  \end{tikzpicture}
\end{document}
```
</LatexFigure>
</MathBox>

## Trace

<MathBox title='Trace' boxType='definition'>
The trace of an $n\times n$-matrix $\boldsymbol{A}$, denoted $\operatorname{tr}(\boldsymbol{A})$ is the sum of the elements on the main diagonal of $\boldsymbol{A}$

$$
  \operatorname{tr}(\boldsymbol{A}) = \sum_{i_1}^n a_{i,i}
$$
</MathBox>

<MathBox title='Properties of trace' boxType='proposition'>
1. **Linearity:**
    - $\operatorname{tr}(\lambda\boldsymbol{A}) = \lambda\operatorname{tr}(\boldsymbol{A})$
    - $\operatorname{tr}(\boldsymbol{A} + \boldsymbol{B}) = \operatorname{tr}(\boldsymbol{A}) + \operatorname{tr}(\boldsymbol{B})$
2. **Commutativity:** $\operatorname{tr}(\boldsymbol{A}\boldsymbol{B}) = \operatorname{tr}(\boldsymbol{B}\boldsymbol{A})$
3. $\operatorname{tr}(\boldsymbol{A}\boldsymbol{B}\boldsymbol{C}) = \operatorname{tr}(\boldsymbol{C}\boldsymbol{A}\boldsymbol{B}) = \operatorname{tr}(\boldsymbol{B}\boldsymbol{C}\boldsymbol{A})$
4. The trace is invariant under similarity.

<details>
<summary>Proof</summary>

**(3):**

$$
\begin{align*}
  \boldsymbol{A}\boldsymbol{B}_{ii} =& \sum_{j=1}^n a_{ij}b_{ji} \\
  \boldsymbol{B}\boldsymbol{A}_{ii} =& \sum_{j=1}^n b_{ij}a_{ji}
\end{align*}
$$

Due to symmetry, we are free to exchange the indices $i$ and $j$. Thus, $\boldsymbol{A}\boldsymbol{B}_{ii} = \boldsymbol{B}\boldsymbol{A}_{ii}$, showing that $\operatorname{tr}(\boldsymbol{A}\boldsymbol{B}) = \operatorname{tr}(\boldsymbol{B}\boldsymbol{A})$.
</details>
</MathBox>

# Vector space

<MathBox title='Vector space' boxType='definition'>
A vector space is a set $V$ over a field $\mathbb{F}$ equipped with the two closed operations 
- $+: V \times V \to V$ **(vector addition)**
- $\cdot: \mathbb{F} \times V \to V$ **(scalar multiplication)**

and has the following properties

- $V$ is an abelian group under vector addition, i.e. for all $\boldsymbol{u}, \boldsymbol{v}, \boldsymbol{w} \in V$
  - $\boldsymbol{u} + (\boldsymbol{v} + \boldsymbol{w}) = (\boldsymbol{u} + \boldsymbol{v}) + \boldsymbol{w}$ **(associativity)**
  - $\exists \boldsymbol{0} \in V : \boldsymbol{v} + \boldsymbol{0} = \boldsymbol{v}$ **(identity element)**
  - $\forall \boldsymbol{v} \; \exists -\boldsymbol{v} : \boldsymbol{v} + (-\boldsymbol{v}) = 0$ **(additive inverse)**
  - $\boldsymbol{u} + \boldsymbol{v} = \boldsymbol{v} + \boldsymbol{u}$ **(commutativity)**
- Scalar multiplication is compatible, satisfying for all $\alpha, \beta\in \mathbb{F}$
  - $\alpha(\beta \boldsymbol{v}) = (\alpha\beta)\boldsymbol{v}$
  - $1\boldsymbol{v} = \boldsymbol{v}$
- Vector addition and scalar multiplication are related by distributivity
  - $\alpha (\boldsymbol{u} + \boldsymbol{v}) = \alpha \boldsymbol{u} + \alpha \boldsymbol{v}$
  - $(\alpha + \beta)\boldsymbol{v} = \alpha \boldsymbol{v} + \beta \boldsymbol{v}$

The elements of $V$ are called vectors, while the elements of $\mathbb{F}$ are called scalars.
</MathBox>

Note that vector spaces are just special types of modules; a vector space is a module over a field.

<MathBox title='Linear combination' boxType='definition'>
A linear combination of vectors $\Set{ \boldsymbol{v}_i }_{i=1}^k \subseteq V$ for $k\in\N_+$ is a vector of the form

$$
  \sum_{i=1}^k \lambda_i \boldsymbol{v}_i,\; \lambda_i \in\mathbb{F}
$$

If at least one of the scalars $\lambda_i$ is nonzero, the linear combination is nontrivial.
</MathBox>

<MathBox title='Linear subspace' boxType='definition'>
Let $V$ be $\mathbb{F}$-vector space. If a subset $U\subseteq V$ satisfies
- $\boldsymbol{0}\in U$
- $\boldsymbol{u}, \boldsymbol{v}\in U \implies \boldsymbol{u} + \boldsymbol{v} \in U$ (closed under vector addition)
- $\boldsymbol{u}\in U, \alpha\in\mathbb{F} \implies \alpha \boldsymbol{u} \in U$ (closed under scalar multiplication)

then $U$ is also an $\mathbb{F}$-vector space, called a linear subspace of $V$.

The conditions imply that $U$ is closed under linear combinations, i.e. $\alpha \boldsymbol{u} + \beta \boldsymbol{v} \in U$ for all $\alpha,\beta\in\R$ and all $\boldsymbol{u}, \boldsymbol{v} \in U$.
</MathBox>

## Direct sums and products

### External direct sums and products
<MathBox title='External direct sum' boxType='definition'>
Let $V_1,\dots,V_n$ be $\mathbb{F}$-vector spaces. The external direct sum of $V_1,\dots,V_n$ denoted

$$
  V = V_1 \boxplus \cdots \boxplus V_n
$$

is the vector space $V$ whose elements are ordered $n$-tuples

$$
  V = \Set{ (\boldsymbol{v}_i)_{i=1}^n | \boldsymbol{v}_i \in V_i }
$$

and whose vector operations apply componentwise

$$
\begin{gather*}
  (\boldsymbol{u}_i)_{i=1}^n + (\boldsymbol{v}_i)_{i=1}^n = (\boldsymbol{u}_i + \boldsymbol{v}_i)_{i=1}^n,\; \boldsymbol{u}_i, \boldsymbol{v}_i\in V_i \\
  \lambda (\boldsymbol{u}_i)_{i=1}^n = (\lambda \boldsymbol{u}_i)_{i=1}^n,\; \lambda\in\mathbb{F}
\end{gather*}
$$
</MathBox>

<MathBox title='Direct product' boxType='definition'>
Let $\mathcal{F} = \Set{ V_i }_{i\in I}$ be a collection of $\mathbb{F}$-vector spaces for an index set $I$. The direct product of $\mathcal{F}$ is the vector space

$$
  \prod_{i\in I} V_i = \Set{ f: I\to\bigcup_{i\in I} V_i | f(i) \in V_i }
$$

which is subspace of the vector space of all functions from $I$ to $\bigcup_{i\in I} V_i$.
</MathBox>

<MathBox title='Generalized external direct sum' boxType='definition'>
Let $\mathcal{V} = \Set{ V_i }_{i\in I}$ be a collection of $\mathbb{F}$-vector spaces for an index set $I$. The external direct sum of $\mathcal{F}$ is the vector space

$$
  \bigoplus_{i\in I} V = \Set{ f: I\to\bigcup_{i\in I} V_i | f(i) \in V_i, f \text{ has finite support} }
$$

which is subspace of the vector space of all functions from $I$ to $\bigcup_{i\in I} V_i$. The support of $f:I\to\bigcup_{i\in I} V_i$ is the set

$$
  \operatorname{supp}(f) = \Set{i\in I | f(i) \neq 0}
$$

Thus, $f$ has finite support if $f(i) = 0$ for all but a finite number of $i\in I$.
</MathBox>

An important case occurs when $V_i = V$ for all $i\in I$. If $V^I$ denotes the set of all functions from $I$ to $V$ and $(V^I)_0$ denote the set of all functions in $I$ that have finite support then

$$
\begin{align*}
  \prod_{i\in I} V =& V^I \\ 
  \bigoplus_{i\in I} V =& (V^I)_0
\end{align*}
$$

### Internal direct sums

<MathBox title='Internal direct sum' boxType='definition'>
An $\mathbb{F}$-vector space $V$ is the (internal) direct sum of a collection $\mathcal{U} = \Set{ U_i }_{i\in I}$ of subspaces of $V$ if every vector $\boldsymbol{v}\in V$ can be uniquely written (except for order) as a finite sum of vectors from the subspaces in $\mathcal{F}$. That is, if for all $\boldsymbol{v}\in V$

$$
  v = \sum_{i=1}^{n\in I} \boldsymbol{u}_i = \sum_{i=1}^{m\in I} \boldsymbol{w}_j,\; \boldsymbol{u}_i, \boldsymbol{w}_i \in U_i
$$

then $m = n$ and $\boldsymbol{u}_i = \boldsymbol{w}_i$ (after reindexing if necessary) for all $i=1,\dots,n$.

If $V$ is the direct sum of $\mathcal{F}$ we write

$$
  V = \bigoplus_{i\in I} U_i
$$

where each $U_i$ is a direct summand of $V$. If $V = U \oplus W$, then $W$ is called a complement of $U$ in $V$.
</MathBox>

<MathBox title='All subspaces have a complement' boxType='proposition'>
Any subspace $U$ of a vector space $V$ has a complement $W$ for which $V = U \oplus W$.
</MathBox>

<MathBox title='' boxType='proposition'>
Let $I\subseteq\N_+$ be an index set. A vector space $V$ is the direct sum of a collection of subspaces $\mathcal{U} = \Set{ U_i }_{i\in I}$ if and only if
1. $V = \sum_{i\in I} U_i$
2. $U_i \cap \left(\sum_{j\neq i} U_i \right) = \Set{\boldsymbol{0}}$

<details>
<summary>Proof</summary>

Suppose first that $V$ is the direct sum of $\mathcal{U}$. Then $(1)$ clearly holds and if

$$
  \boldsymbol{v} \in U_i \cap \left(\sum_{j\neq i} U_i \right)
$$

then $\boldsymbol{v} = \boldsymbol{u}_i$ for some $\boldsymbol{u}_i \in U_i$ and

$$
  \boldsymbol{v} = \sum_{k=1}^n \boldsymbol{u}_{j_k},\, \boldsymbol{u}_{j_k}\in U_{j_k}
$$

where $j_k \neq 1$ for all $k=1,\dots,n$. Hence, by the uniqueness of direct sum representations, $\boldsymbol{u}_i = \boldsymbol{0}$ and $\boldsymbol{v} = \boldsymbol{0}$.

Conversely, suppose $(1)$ and $(2)$ hold. We need only verify the uniqueness condition. If

$$
  \boldsymbol{v} = \sum_{i=1}^m \boldsymbol{u}_{j_i}\ = \sum_{i=1}^n \boldsymbol{w}_{k_i},\; \boldsymbol{u}_{j_i}\in U_{j_i}, \boldsymbol{w}_{k_i}\in U_{k_i}
$$

then by including additional zero terms we may assume the index sets $\Set{j_i}_{i=1}^m$ and $\Set{k_i}_{i=1}^n$ are the same set $\Set{ i_j }_{j=1}^p$, giving

$$
  \sum_{j=1}^p (\boldsymbol{u}_{i_j} - \boldsymbol{w}_{i_j}) = 0
$$

Thus each term $\boldsymbol{u}_{i_j} - \boldsymbol{t}_{i_j}\in S_{i_j}$ is a sum of vectors from subspaces other than $S_{i_j}$, which can happen only if $\boldsymbol{u}_{i_j} - \boldsymbol{t}_{i_j} = 0$. Hence $\boldsymbol{u}_{i_j} = \boldsymbol{t}_{i_j}$ for all $i_j$ and $V$ is a direct sum of $\mathcal{U}$.
</details>
</MathBox>

## Span

<MathBox title='Linear dependence and independence' boxType='definition'>
A set of vectors $S =\Set{ v_i }_{i=1}^k$ for $k\in\N_+$ of an $\mathbb{F}$-vector space $V$ is linearly dependent if there is a non-trivial linear combination for $0\in V$. That is, there is a sequence of scalars $(\lambda_i \in\mathbb{F})_{i=1}^k$ that are not all equal to zero such that

$$
\begin{gather*}
  \sum_{i=1}^k \lambda_i \boldsymbol{v}_i = \boldsymbol{0} \\
  \iff \boldsymbol{v}_j = \sum_{i=1, i\neq j}^k \tilde{\lambda}_i \boldsymbol{v}_i,\; 1 \leq j \leq k, \lambda_j \neq 0, \tilde{\lambda}_i = \frac{-\lambda_i}{\lambda_j}
\end{gather*}  
$$

Equivalently, $S$ is linearly dependent if and only if one of its vectors is a linear combination of the others. The set $S$ is linearly independent if it is not linearly dependent, i.e. 

$$
  \sum_{i=1}^k \lambda_i \boldsymbol{v}_i = \boldsymbol{0} \implies \lambda_i = 0
$$
</MathBox>

<MathBox title='Span' boxType='definition'>
Given a subset $U$ of a $\mathbb{F}$-vector space $V$, the subspace spanned by $U$ is the the smallest set of all linear combinations of vectors in $U$. If $U$ is finite, i.e. for $k\in\N_+$ we can write $U = \Set{ \boldsymbol{u}_i }_{i=1}^k$, the span of $U$ is the set

$$
  \operatorname{span}(U) := \Set{ \boldsymbol{v}\in V | \exists (\lambda_i)_{i=1}^k \in\mathbb{F}^k : \boldsymbol{v} = \sum_{i=1}^k \lambda_i \boldsymbol{u}_i }
$$

In particular $\operatorname{span}(\emptyset) := \Set{\boldsymbol{0}}$. 

The subset $U$ is said to span (generate) $V$ if every $\boldsymbol{v}\in V$ is a linear combination of vectors in $U$, in which case we write $\operatorname{span}(U) = V$.
</MathBox>

## Basis

<MathBox title='Basis' boxType='definition'>
A basis $B$ of an $\mathbb{F}$-vector space $V$ is a linearly independent subset of $V$ spanning $V$. 
</MathBox>

<MathBox title='' boxType='proposition'>
A finite subset $U = \Set{\boldsymbol{u}_i}_{i=1}^n$ of an $\mathbb{F}$-vector space $V$ for $n\in\N$ is a basis for $V$ if and only if

$$
  V = \bigoplus_{i=1}^n \operatorname{span}(\Set{\boldsymbol{u}_i})
$$
</MathBox>

<MathBox title='Basis properties' boxType='proposition'>
If $B$ is a subset of an $\mathbb{F}$-vector space $V$, the following are equivalent
1. $B$ is a basis of $V$ 
2. $S$ is a minimal spanning set, i.e. $B$ spans $V$ while no proper subsets of $B$ does
3. $S$ is a maximal linearly independent set, i.e. $S$ is linearly independent while no proper supersets of $S$ is linearly dependent.

<details>
<summary>Proof</summary>

**$(1)\iff(2)$**
Suppose $B$ is a basis of $V$, i.e. $B$ is linearly independent with $\operatorname{span}(B) = V$. If $\mathrm{\tilde{B}} = V$ for some $\tilde{B}\subset B$, then any vector in $B\setminus \tilde{B}$ should be a linear combination of the vectors in $\tilde{B}$, contradicting the fact that the vectors in $B$ are linearly independent. Hence $B$ must be a minimal spanning set.

Conversely, if $B$ is a minimal spanning set, then it must be linearly independent. If not some $b\in B$ would be a linear combination of the other vectors in $B$ and so $B\setminus\Set{b}$ would be a proper spanning subset of $B$, which is a contradiction. Hence $B$ must be a basis. 

**$(1)\iff(3)$**
Suppose $B$ is a bsis of $V$. If $B$ is not maximal, there should be a vector $\boldsymbol{v}\in V\setminus{B}$ for which the set $B\cup\Set{\boldsymbol{v}}$ is linearly independent. However, then $\boldsymbol{v}\notin \operatorname{span}(B)$, contradicting the fact that $B$ is a spanning set. Hence, $B$ is a maximal linearly independent set.

Conversely, if $B$ is a maximal linearly independent set then $\operatorname{span}(B) = V$. If not, we could find a vector $\boldsymbol{v}\in V\setminus{B}$ that is not a linear combination of the vectors in $B$. In this case, $B\cup\Set{\boldsymbol{v}}$ would be a linearly independent proper superset of $B$, which is a contradiction. Hence, $B$ must be a basis.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V$ be a nonzero vector space. Let $I$ be a linearly independent set in $V$ and let $S$ be a spanning set in $V$ containing $I$. Then there is a basis $B$ for $V$ for which $I\subseteq B\subseteq S$. In particular
1. any vector space, except $\Set{\boldsymbol{0}}$ has a basis
2. any linearly independent set in $V$ is contained in a basis
4. any spanning set in $V$ contains a basis

<details>
<summary>Proof</summary>

Consider the collection $\mathcal{A}$ of all linearly independent subsets of $V$ containing $I$ and contained in $S$. Clearly, $\mathcal{A}$ is not empty since $I\in\mathcal{A}$. If $\mathcal{C} = \Set{ I_j}_{j\in J}$ for some index set $J$ is a chain in $\mathcal{A}$ then the union $U = \bigcup_{j\in J} I_j$ is linearly independent and satisfies $I\subseteq U\subseteq S$, i.e. $U\in\mathcal{A}$. Thus, every chain in $\mathcal{A}$ has an upper bound in $\mathcal{A}$ and by Zorn's lemma, $\mathcal{A}$ must contain a maximal element $B$, which is linearly independent.

The set $B$ is a basis for the vector space $\operatorname{span}(S) = V$, for if any $s\in S$ is not a linear combination of the elements of $B$, then $B\cup\Set{s} \subseteq S$ is linearly independent, contradicting the maximality of $B$. Hence $S\subseteq\operatorname{span}(B)$ and so $V = \operatorname{span}(S) \subseteq\operatorname{span}(B)$. 
</details>
</MathBox>

<MathBox title='Steinlitz exchange lemma' boxType='lemma'>
Let $U$ and $W$ be finite subsets of an $\mathbb{F}$-vector space $V$. If $U$ is a set of linearly independent vectors and $W$ spans $V$, then
1. $\dim(U) \leq \dim(W)$
2. There is a set $W' \subseteq W$ with $\dim(W') = \dim(W) - \dim(U)$ such that $U\cup W'$ spans $V$. 

<details>
<summary>Proof</summary>

Suppose $U = \Set{\boldsymbol{u}_i}_{i=1}^m$ and $W = \Set{\boldsymbol{w}_i}_{i=1}^n$. We must show that $m \leq n$, and that after rearranging the $\boldsymbol{w}_j$ if necessary, the set $\Set{\boldsymbol{u}_1,\dots,\boldsymbol{u}_m,\boldsymbol{w}_{m+1},\dots,\boldsymbol{w}_n}$ spans $V$. This can be proved by induction.

For the base case, suppose $m$ is zero. In this case, the claim holds because there are no vectors $\boldsymbol{u}_i$, and the set $\Set{\boldsymbol{w}_i}_{i=1}^n$ spans $V$ by hypothesis.

For the inductive step, assume the proposition is true for $m-1$. By the inductive hypothesis we may reorder the $\boldsymbol{w}_i$ so that $\set{\boldsymbol{u}_1,\dots,\boldsymbol{u}_{m-1},\boldsymbol{w}_m,\dots,\boldsymbol{w}_n}$ spans $V$. Since $\boldsymbol{u}_m \in V$, there exists coefficients $\mu_1,\dots,\mu_n$ such that

$$
  \boldsymbol{u}_m = \sum_{i=1}^{m-1} \mu_i \boldsymbol{u}_i + \sum_{j=m}^n \mu_j \boldsymbol{w}_j
$$

At least one of the $\mu_j$ must be non-zero, since otherwise this equality would contradict the linear independence of $\Set{\boldsymbol{u}_i}_{i=1}^m$. It follows that $m\leq n$. By reordering $\mu_m \boldsymbol{w}_m,\dots,\mu_n \boldsymbol{w}_n$ if necessary, we may assume $\mu_m$ is nonzero

$$
  \boldsymbol{w}_m = \frac{1}{\mu_m} \left(\boldsymbol{u}_m - \sum_{j=1}^{m-1} \mu_j \boldsymbol{u}_j - \sum_{j=m+1} \mu_j \boldsymbol{w}_j \right)
$$

This shows that $\boldsymbol{w}_m \in\operatorname{span}\Set{\boldsymbol{u}_1,\dots,\boldsymbol{u}_m,\boldsymbol{w}_{m+1},\dots,\boldsymbol{w}_n}$. Since this span contains each of the vectors $\boldsymbol{u}_1,\dots,\boldsymbol{u}_{m-1},\boldsymbol{w}_m,\dots,\boldsymbol{w}_{m+1},\dots,\boldsymbol{w}_n$, by the inductive hypothesis it contains $V$.
</details>
</MathBox>

## Dimension

<MathBox title='' boxType='proposition'>
Let $V$ be a vector space and assume the vectors $\Set{\boldsymbol{v}_i}_{i=1}^n$ for $n\in\N_+$ are linearly independent and the vectors $\Set{\boldsymbol{s}_i}_{i=1}^m$ for $m\in\N_+$ span $V$. Then $n \leq m$. 

<details>
<summary>Proof</summary>

List the two set of vectors with the spanning set followed by the linearly independent set

$$
  \boldsymbol{s}_1,\dots,\boldsymbol{s}_m;\boldsymbol{v}_1,\dots,\boldsymbol{v}_n
$$

Move the first vector $\boldsymbol{v}_1$ to the front of the list

$$
  \boldsymbol{v}_1, \boldsymbol{s}_1,\dots,\boldsymbol{s}_m;\boldsymbol{v}_2,\dots,\boldsymbol{v}_n
$$

Since $\operatorname{span}\Set{ \boldsymbol{s}_i }_{i=1}^m = V$, it follows that $\boldsymbol{v}_1$ is a linear combination of the $\boldsymbol{s}_i$'s. This implies that we may remove one the $\boldsymbol{s}_i$'s, which by reindexing if necessary can be $\boldsymbol{s}_1$, from the list and still have a spanning set

$$
  \boldsymbol{v}_1, \boldsymbol{s}_2,\dots,\boldsymbol{s}_m;\boldsymbol{v}_2,\dots,\boldsymbol{v}_n
$$

Note that the first set of vectors still spans $V$ and the second set is still linearly independent. Repeat the process, moving $\boldsymbol{v}_2$ from the second list to the first list

$$
  \boldsymbol{v}_1, \boldsymbol{v}_2, \boldsymbol{s}_2,\dots,\boldsymbol{s}_m;\boldsymbol{v}_3,\dots,\boldsymbol{v}_n
$$

As before, the vectors in the first list are linearly independent, since they spanned $V$ before the inclusion of $\boldsymbol{v}_2$. However, since the $\boldsymbol{v}_i$'s are linearly independent, any nontrivial linear combination of the vectors in the first list that equals 0 must involve at least one of the $\boldsymbol{s}_i$'s. Thus, we may remove that vector, which by reindexing if necessary can be $\boldsymbol{s}_2$, and still have a spanning set

$$
  \boldsymbol{v}_1, \boldsymbol{v}_2, \boldsymbol{s}_3,\dots,\boldsymbol{s}_m;\boldsymbol{v}_3,\dots,\boldsymbol{v}_n
$$

If $m < n$, this process will eventually exhaust the $\boldsymbol{s}_i$'s and lead to the list

$$
  \boldsymbol{v}_1,\dots,\boldsymbol{v}_m;\boldsymbol{v}_{m+1},\dots,\boldsymbol{v}_n
$$

where $\operatorname{span}\Set{\boldsymbol{v}_i}_{i=1}^m = V$, which is contradictory since any $\boldsymbol{v}_i$ for $i > m$ is not in the span of $\Set{\boldsymbol{v}_i}_{i=1}^m$. Hence, $n\leq m$. 
</details>
</MathBox>

<MathBox title='All bases have same cardinality' boxType='theorem'>
All bases of an $\mathbb{F}$-vector space $V$ have the same cardinality, called the dimension of $V$, denoted $\dim(V)$.

<details>
<summary>Proof</summary>

For an index set $I\in\N$, let $B = \Set{\boldsymbol{b}_i}_{i\in I}$ be a basis for $V$ and suppose $C$ is another basis for $V$. Then any vector $\boldsymbol{c}\in C$ can be written as finite linear combination of the vectors in $B$

$$
  \boldsymbol{c} = \sum_{i\in U_C} \lambda_i \boldsymbol{b}_i,\; \lambda_i\in\mathbb{F}\setminus\Set{0}
$$

Because $C$ is basis, we must have $\bigcup_{\boldsymbol{c}\in C} U_{\boldsymbol{c}} = I$. If the vectors in $C$ can be expressed as finite linear combinations of the vectors in a proper subset $B' \subset B$ then $\operatorname{span}(B') = V$, which is contradictory.

Since $U_{\boldsymbol{c}}$ is finite, i.e. $|U_{\boldsymbol{c}}| < \aleph_0$, for all $\boldsymbol{c}\in C$, it follows that 

$$
  |B| = |I| \leq \aleph_0 |C| = |C|
$$

Reversing the roles of $B$ and $C$, we may also conclude that $|C| \leq |B|$ and so $|B| = |C|$ by the Schrder-Bernstein theorem.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
A vector space $V$ is *finite-dimensional* if it is the zero space $\Set{\boldsymbol{0}}$ or if it has a finite basis. Otherwise, $V$ is infinite-dimensional. If $V$ has a basis of cardinality $n\in\N$ we say that $V$ is $n$-dimensional and write $\dim(V) = n$. In particular $\dim(\Set{\boldsymbol{0}}) = 0$.
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V$ be a vector space, then
1. if $B$ is a basis for $V$ and if $B = B_1 \cup B_2$ with $B_1 \cap B_2 = \emptyset$ then

$$
  V = \operatorname{span}(B_1) \oplus\operatorname{span}(B_1)
$$
2. let $V = S\oplus T$. If $B_1$ is a basis for $S$ and $B_2$ is a basis for $T$ then $B_1 \cap B_2 = \emptyset$ and $B = B_1 \cup B_2$ is a basis for $V$. 
</MathBox>

<MathBox title='' boxType='proposition'>
Let $S$ and $T$ be subspaces of a vector space $V$. Then

$$
  \dim(S) + \dim(T) = \dim(S + T) + \dim(S \cap T)
$$

In particular, if $T$ is any complement of $S$ in $V$, i.e. $S\oplus T = V$, then

$$
  \dim(S\oplus T) + \dim(S) + \dim(T) = \dim(V)
$$

<details>
<summary>Proof</summary>

Suppose $B = \Set{b_i}_{i\in I}$ is a basis for $S\cap T$. Extend this to a basis $A\cup B$ for $S$ where $A = \Set{a_j}_{j\in J}$ is disjoint from $B$. Also, extend $B$ to a basis $B\cup C$ for $T$ where $C = \Set{c_k}_{k\in K}$ is disjoint from $B$. We claim that $A\cup B\cup C$ is a basis for $S+T$. It is clear that $\operatorname{span}(A\cup B\cup C) = S + T$.

To see that $A\cup B\cup C$ is linearly independent, suppose the opposite that

$$
  \sum_{i=1}^n \alpha_i \boldsymbol{v}_i = 0,\;\alpha_i \in\mathbb{F}\setminus\Set{0}, \boldsymbol{v}_i\in A\cup\B\cup C
$$

There must be vectors $\boldsymbol{v}_i$ in this expression from $A$ and $C$ since $A\cup B$ and $B\cup C$ are linearly independent. Isolating the terms involving the vectors from $A$ on one side of the equality shows that there is a nonzero vector $\boldsymbol{x}\in\operatorname{span}(A) \cap \operatorname{span}(B\cup C)$. However, then $\boldsymbol{x}\in S\cap T$ and so $\boldsymbol{x}\in \operatorname{span}(A) \cap \operatorname{span}(B)$, which implies that $\boldsymbol{x} = \boldsymbol{0}$, a contradiction. Hence $A\cup B\cup C$ is linearly independent and a basis for $S + T$, giving

$$
\begin{align*}
  \dim(S) + \dim(T) =& |A\cup B| + |B\cup A| \\
  =& |A| + |B| + |B| + |C| \\
  =& |A| + |B| + |C| + \dim(S\cap T) \\
  =& \dim(S + T) + \dim(S\cap T)
\end{align*}
$$
</details>
</MathBox>

## Coordinates

<MathBox title='Ordered basis' boxType='definition'>
An ordered basis for an $n$-dimensional vector space $V$ is an ordered $n$-tuple $(\boldsymbol{v}_i)_{i=1}^n$ of vectors for which the set $\Set{\boldsymbol{v}_i}_{i=1}^n$ is a basis for $V$.
</MathBox>

<MathBox title='Coordinate map' boxType='definition'>
If $B = \Set{\boldsymbol{b}_i}_{i=1}^n$ is an ordered basis for a vector space $V$ over $\mathbb{F}$ then for each $\boldsymbol{v}\in V$ there is a unique ordered $n$-tuple $(\lambda_i)_{i=1}^n \in \mathbb{F}^n$ such that $\boldsymbol{v} = \sum_{i=1}^n \lambda_i \boldsymbol{b}_i$.

Accordingly we can define the *coordinate map* $\varphi_B: V\to\mathbb{F}^n$ by

$$
  \varphi_B (\boldsymbol{v}) = [\boldsymbol{v}]_B := \begin{bmatrix} \lambda_1 \\ \vdots \\ \lambda_n \end{bmatrix}_B
$$

where the column matrix $[\boldsymbol{v}]_B$ is known as the *coordinate matrix* of $\boldsymbol{v}$ with respect to the ordered basis $B$. The coordinate map is linear, i.e.

$$
\begin{align*}
  \varphi_B (\boldsymbol{v}) =& \varphi_B\left(\sum_{i=1}^n \lambda_i \boldsymbol{b}_i\right) \\
  =& \sum_{i=1}^n \lambda_i \varphi_B (\boldsymbol{b}_i) \\
  =& \sum_{i=1}^n \lambda_i \boldsymbol{e}_i
\end{align*}
$$

where $\boldsymbol{e}_i$ is a canonical unit vector in $\mathbb{F}$.

The coordinate map is an isomorphism (bijection) with inverse $\varphi_B^{-1}:\mathbb{F}^n \to V$ defined by

$$
  (\lambda_i)_{i=1}^n \mapsto \sum_{i=1}^n \lambda_i \boldsymbol{b}_i
$$

The coordinate map is therefore also called the basis isomorphism.
</MathBox>

## Row and column spaces of matrices

Let $\boldsymbol{A}\in\mathcal{M}_{m,n}(\mathbb{F})$ be and $m\times n$ matrix. The rows of $\boldsymbol{A}$ span a subspace of $\mathbb{F}^n$ called the *row space* of $\boldsymbol{A}$, dented $\mathrm{rs}(\boldsymbol{A})$, and the columns of $\boldsymbol{A}$ span a subspace of $\mathbb{F}^m$ called the *column space* of $\boldsymbol{A}$, denoted $\mathrm{cs}(\boldsymbol{A})$. The dimensions of these spaces are called the *row rank* and *column rank*, denoted $\mathrm{rrk}(\boldsymbol{A})$ and $\mathrm{crk}(\boldsymbol{A})$, respectively.

<MathBox title='' boxType='proposition'>
Let $\boldsymbol{A}$ be an $m\times n$ matrix. Then elementary column operations do not affect the row rank of $\boldsymbol{A}$. Similarly, elementary row operations do not affect the column rank of $\boldsymbol{A}$.

<details>
<summary>Proof</summary>

The rowspace of $\boldsymbol{A}$ is 

$$
  \mathrm{rs}(\boldsymbol{A}) = \operatorname{span}(\boldsymbol{e}_i \boldsymbol{A})_{i=1}^n
$$

where $\boldsymbol{e}_i$ are the standard basis vectors in $\mathbb{F}$. Perferming an elementary column operation on $\boldsymbol{A}$ is equivalent to multiplying $\boldsymbol{A}$ on the right by an elementary matrix $\boldsymbol{E}$. Thus, the row space of $\boldsymbol{A}\boldsymbol{E}$ is

$$
  \mathrm{rs}(\boldsymbol{A}) = \operatorname{span}(\boldsymbol{e}_i \boldsymbol{E}\boldsymbol{A})_{i=1}^n
$$

and since $\boldsymbol{E}$ is invertible

$$
  \mathrm{rrk}(\boldsymbol{A}) = \dim(\mathrm{rs}(\boldsymbol{A})) = \dim(\mathrm{rs}(\boldsymbol{A}\boldsymbol{E})) = \mathrm{rrk}(\boldsymbol{A}\boldsymbol{E})
$$

The second statement follows from the first by taking transposes. 
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
If $\boldsymbol{A}\in\mathcal{M}_{m,n}$, then $\mathrm{rrk}(\boldsymbol{A}) = \mathrm{crk}(\boldsymbol{A})$. This number is called the rank of $\boldsymbol{A}$ and is denoted by $\operatorname{rank}(\boldsymbol{A})$.

<details>
<summary>Proof</summary>

According to the previous result, $\boldsymbol{A}$ can be transformed into a reduced column echelon form without affecting the row rank. This reduction does not affect the column either. The matrix $\boldsymbol{A}$ can be further transformed into a reduced row echelon form without affecting either rank. The resultiing matrix $\boldsymbol{M}$ has the same row and column ranks ars $\boldsymbol{A}$. However, $\boldsymbol{M}$ is a matrix with $1$'s followed by $0$' on the main diagonals entries and $0$'s elsewhere. Hence

$$
  \mathrm{rrk}(\boldsymbol{A}) = \mathrm{rrk}(\boldsymbol{M}) = \mathrm{crk}(\boldsymbol{M}) = \mathrm{crk}(\boldsymbol{A})
$$
</details>
</MathBox>

# Linear transformations

<MathBox title='Linear transformation' boxType='definition'>
Let $V$ and $W$ be vector spaces over the same field $\mathbb{F}$. A function $\mathrm{T}: V \to W$ is called a *linear transformation* if it preserve vector space operations, i.e.

$$
  \mathrm{T}(\alpha \boldsymbol{v} + \beta \boldsymbol{w}) = \alpha \mathrm{T}(\boldsymbol{v}) + \beta \mathrm{T}(\boldsymbol{w})
$$

for all $\alpha, \beta\in\mathbb{F}$ and $\boldsymbol{v}, \boldsymbol{w}\in V$. A linear transformation $\mathrm{T}: V\to V$ is called a *linear operator* on $V$. The set of all linear transformations from $V$ to $W$ is denoted $\mathcal{L}(V, W) = \hom(V,W)$ and the set of all linear operators on $V$ is denoted $\mathcal{L}(V) = \hom(V,V)$. 

The following terms are used to classify linear transformations and operators
- **homomorphism:** linear transformation
- **endomorphism:** linear operator
- **monomorphism (embedding):** injective linear transformation
- **epimorphism**: surjective linear transformation
- **isomorphism**: bijective linear transformation
- **automorphism**: bijective linear operator
</MathBox>

<MathBox title='Projection' boxType='definition'>
A linear operator $\mathrm{P}\in\mathcal{L}(V)$ is a *projection* if $\mathrm{P}^2 = \mathrm{P}$.
</MathBox>

<MathBox title='Commutator' boxType='definition'>
The *commutator* of $\mathrm{A},\mathrm{B}\in\mathcal{L}(V)$ is $[\mathrm{A},\mathrm{B}] := \mathrm{AB} - \mathrm{BA}$, which is $0$ if and only if $\mathrm{A}$ and $\mathrm{B}$ commute.
</MathBox>

<MathBox title='Sets of homomorphisms form vector spaces' boxType='proposition'>
The set $\mathcal{L}(V, W)$ is a vector space under ordinary addition of funtions and scalar multiplication of functions by elements of $\mathcal{F}$.
</MathBox>

<MathBox title='Sets of endomorphism form vector spaces' boxType='proposition'>
The set $\mathcal{L}(V)$ is a vector space and an algebra. It is an associative, but noncommutative algebra, where multiplication is composition of linear operators. The identity map $\mathrm{I} = \operatorname{id}\in\mathcal{L}(V)$ is the multiplicative identity, satisfying $\mathrm{I}\boldsymbol{v} = \boldsymbol{v}$. The zero map $0\in\mathcal{L}(V)$ is the additive identity.
</MathBox>

<MathBox title='Composition of linear transformations are linear' boxType='proposition'>
If $\mathrm{T}:U\to V$ and $\mathrm{S}\in V\to W$ are linear then so is the composition $\mathrm{S}\circ \mathrm{T}: V\to W$. The composition $\mathrm{S}\circ\mathrm{T}$ is usually written $ST$.

Composition of linear transformations is distributive w.r.t. addition. That is, if $\mathrm{P},\mathrm{T}:X\to U$ and $\mathrm{R}, \mathrm{S}:U\to V$, then

$$
\begin{align*}
  (\mathrm{R}+\mathrm{S})\circ T =& \mathrm{R}\circ \mathrm{T} + \mathrm{S}\circ\mathrm{T} \\
  \mathrm{S}\circ (\mathrm{T}\circ\mathrm{P}) =& \mathrm{S}\circ\mathrm{T} + \mathrm{S}\circ\mathrm{P}
\end{align*}
$$
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V$ and $W$ be vector spaces and let $B = \Set{v_i}_{i\in I}$ be a basis for $V$. Then we can define a linear transformation $\mathrm{T}\in\mathcal{L}(V,W)$ by specfiying the values of $\mathrm{T}(\boldsymbol{v}_i)\in W$ arbitrarily for all $v_i\in B$ and extending the domain of $\mathrm{T}$ to $V$ using linearity, i.e.

$$
  \mathrm{T}(\lambda_i \boldsymbol{v}_i)_{i\in I} = \sum_{i\in I} \lambda_i \mathrm{T}(\boldsymbol{v}_i)
$$

This process uniquely defines a linear transformation. If $\mathrm{T},\mathrm{S}\in\mathcal{L}(V,W)$ satisfy $\mathrm{T}(\boldsymbol{v}_i) = \mathrm{S}(\boldsymbol{v}_i)$ for all $\boldsymbol{v}_i\in B$ then $\mathrm{T} = \mathrm{S}$.

<details>
<summary>Proof</summary>

</details>
</MathBox>

<MathBox title='Kernel and range' boxType='definition'>
A linear transformation $\mathrm{T}\in\mathcal{L}(V, W)$ has the following two principal subspaces:
- the *kernel* (null space) of $\mathrm{T}$ is the set $\ker(\mathrm{T}):= \Set{\boldsymbol{v}\in V | \mathrm{T}(\boldsymbol{v}) = \boldsymbol{0}}$
- the *range* (image) of $\mathrm{T}$ is the set $\operatorname{ran}(\mathrm{T}):= \Set{\mathrm{T}(\boldsymbol{v}) | \boldsymbol{v}\in V}$

The dimension of $\ker(\mathrm{T})$ is called the *nullity* of $\mathrm{T}$ and is denoted $\mathrm{null}(\mathrm{T})$. The dimension of $\operatorname{ran}(\mathrm{T})$ is called the *rank* of $\mathrm{T}$ is denoted $\operatorname{rank}(\mathrm{T})$.
</MathBox>

<MathBox title='' boxType='proposition'>
Let $\mathrm{T}\in\mathcal{L}(V, W)$, then
1. $\mathrm{T}$ is surjective if and only if $\operatorname{ran}(\mathrm{T}) = W$
2. $\mathrm{T}$ is injective if and only if $\ker(\mathrm{T}) = \Set{\boldsymbol{0}}$

<details>
<summary>Proof</summary>

The first statement is merely a restatement of the definition of surjectivity. To show the second statement, note that

$$
  \mathrm{T}(u) = \mathrm{T}(v) \iff \mathrm{T}(u - u) = 0 \iff u - v \in\ker(\mathrm{T})
$$

Thus, if $\ker(\mathrm{T}) = \Set{0}$ then $\mathrm{T}(u) = \mathrm{T}(v) \iff u = v$, showing that $f$ is injective. Conversely, if $\mathrm{T}$ is injective and $u\in\ker(\mathrm{T})$, then $\mathrm{T}(u) = \mathrm{T}(0) \iff u = 0$. hence $\ker(\mathrm{T}) = \Set{\boldsymbol{0}}$.
</details>
</MathBox>

## Isomorphism

<MathBox title='Invertibility and isomorphism' boxType='definition'>
A linear transformation $\mathrm{T}: V\to W$ is *invertible* if it is bijective. A bijective linear transformation $\mathrm{T}:V\to W$ is an isomorphism from $V$ to $W$. The vector space $V$ and $W$ are isomorphic, denoted $V \cong W$, if there is an isomorphism from $V$ to $W$. The inverse of $\mathrm{T}$ is denoted $\mathrm{T}^{-1}$.
</MathBox>

For any ordered basis $B$ of an $\mathbb{F}$-vector space $V$ with $\dim(V) = n$, the coordinate map $\varphi_B: V\to\mathbb{F}^n$ is an isomorphism. Hence, any $n$-dimensional vector space over $\mathbb{F}$ is isomorphic to $\mathbb{F}^n$.

An isomorphism can be characterized as a linear transformation $\mathrm{T}:V\to W$ that maps a basis for $V$ to a basis for $W$.

<MathBox title='Properties of isomorphisms' boxType='prosposition'>
Let $\mathrm{T}\in\mathcal{L}(V, W)$ be an isomorphism. For $S\subseteq V$, then
1. $S$ spans $V$ if and only if $\mathrm{T}(S)$ spans $W$.
2. $S$ is linearly independent in $V$ if and only if $\mathrm{T}(S)$ is linearly independent in $W$.
3. $S$ is a basis for $V$ if and only if $\mathrm{T}(S)$ is a basis for $W$.
</MathBox>

<MathBox title='Isomorphic spaces have same dimension' boxType='proposition'>
Let $V$ and $W$ be vector spaces, then $V\cong W$ if and only if $\dim(V) = \dim(W)$.
</MathBox>

<MathBox title='' boxType='proposition'>
For $n\in\N$, any $n$-dimensional vector space over $\mathbb{F}$ is isomorphic to $\mathbb{F}^n$. If $B$ is a set of cardinality $|B| = \kappa$, then any $\kappa$-dimensional vector space over $\mathbb{F}$ is isomorphic to the vector space $(\mathbb{F}^B)_0$ of all functions from $B$ to $\mathbb{F}$ with finite support.
</MathBox>

<MathBox title='Properties of invertible linear transformations' boxType='proposition'>
If $\boldsymbol{T}:U\to V$ is an invertible linear transformation, then
1. $T^{-1}$ is linear, i.e. $T^{-1} \in\mathcal{L}(U, V)$
2. $\mathrm{T}\mathrm{T}^{-1}$ and $\mathrm{T}^{-1}\mathrm{T}$ is the identity.
3. If $\boldsymbol{S}:V\to W$ is invertible and $\mathrm{S}\mathrm{T}$ is defined, then it is invertible with $(\mathrm{S}\mathrm{T})^{-1} = \mathrm{T}^{-1}\mathrm{S}^{-1}$.

<details>
<summary>Proof</summary>

**(1)**
Let $\mathrm{T}:U\to V$ be a bijective linear transformation. Then $\mathrm{T}^{-1}: V\to U$ is well-defined and since any two vectors $\boldsymbol{v}_1, \boldsymbol{v}_2\in V$ have the form $\boldsymbol{v}_1 = \mathrm{T}(\boldsymbol{u}_1)$ and $\boldsymbol{v}_2 = \mathrm{T}(\boldsymbol{u}_2)$ we have for $\alpha, \beta\in\mathbb{F}$

$$
\begin{align*}
  \mathrm{T}^{-1}(\alpha \boldsymbol{v}_1 + \beta \boldsymbol{v}_2) =& \mathrm{T}^{-1}[\alpha \mathrm{T}(\boldsymbol{u}_1) + \beta \mathrm{T}(\boldsymbol{u}_2)] \\
  =& \mathrm{T}^{-1}[\mathrm{T}(\alpha \boldsymbol{u}_1 + \beta \boldsymbol{u}_2)] \\
  =& \alpha \boldsymbol{u}_1 + \beta \boldsymbol{u}_2 \\
  =& \alpha \mathrm{T}^{-1}(\boldsymbol{v}_1) + \beta \mathrm{T}^{-1}(\boldsymbol{v}_2)
\end{align*}
$$

**(3)**

<LatexFigure width={50} src='/fig/invertible_composition.svg' alt=''
  caption='Invertible composition'
>
```latex
\documentclass[tikz]{standalone}
\usepackage{tikz}
\usepackage{amssymb}
\usetikzlibrary{cd}

\begin{document}

\begin{tikzcd}
  U \arrow[r, bend left=50, "\mathrm{T}"] \arrow[rr, bend left=100, "\mathrm{S}\mathrm{T}"] & 
  V \arrow[l, bend left=50, "\mathrm{T}^{-1}"] \arrow[r, bend left=50, "\mathrm{S}"]  &
  W \arrow[l, bend left=50, "\mathrm{S}^{-1}"] \arrow[ll, bend left=100, "(\mathrm{S}\mathrm{T})^{-1} = \mathrm{T}^{-1}\mathrm{S}^{-1}"]
\end{tikzcd}

\end{document}
```
</LatexFigure>
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
If $\mathrm{A}\in\mathcal{L}(V)$ is a left inverse of $\mathrm{B}\in\mathcal{L}(V)$, i.e. $\mathrm{A}\mathrm{B} = \mathrm{I}$, then it is also a right inverse, i.e. $\mathrm{B}\mathrm{A} = \mathrm{I}$.
</MathBox>

## Similarity

<MathBox title='Similarity transformation' boxType='proposition'>
Let $V$ be an $\mathbb{F}$-vector space. The invertible elements of $\mathcal{L}(V)$ forms the general linear group, denoted $\mathrm{GL}_n (\mathbb{F})$, where $n = \dim(V)$. Every $\mathrm{S}\in\mathrm{GL}_n (\mathbb{F})$ defines a *similarity transformation* $\phi_\mathrm{S}$ of $\mathcal{L}(V)$, sending $\mathrm{M}\mapsto \mathrm{M_S} := \mathrm{SMS}^{-1}$, for each $\mathrm{M}\in\mathcal{L}(V)$. We say that $\mathrm{M}$ and $\mathrm{M_S}$ are *similar*.
</MathBox>

<MathBox title='Similarity transformations are automorphisms' boxType='proposition'>
Every similarity transformation is an automorphism of $\mathcal{L}(V)$ for an $\mathbb{F}$-vector space $V$, satisfying for $\mathrm{M}\in\mathcal{L}(V)$, $\mathrm{S}\in\mathrm{GL}_n (\mathbb{F})$ and $\lambda\in\mathbb{F}$

1. $(\lambda\mathrm{M})_\mathrm{S} = \lambda\mathrm{M_S}$
2. $(\mathrm{M} + \mathrm{N})_\mathrm{S} = \mathrm{M_S} + \mathrm{N_S}$
3. $(\mathrm{MN})_\mathrm{S} = \mathrm{M_S N_S}$

The set of similarity transformation forms a group under $(\mathrm{M_S})_\mathrm{T} := \mathrm{M_{TS}}$, called the *inner automorphism group* of $\mathrm{GL}_n (\mathbb{F})$.

<details>
<summary>Proof</summary>

1. $\mathrm{S}(\lambda\mathrm{M})S^{-1} = \lambda(\mathrm{SMS}^{-1})$
2. $\mathrm{S}(\mathrm{M} + \mathrm{N})\mathrm{S}^{-1} = \mathrm{SMS}^{-1} + \mathrm{SNS}^{-1}$
3. $\mathrm{S}(\mathrm{MN})\mathrm{S}^{-1} = (\mathrm{SMS}^{-1})(\mathrm{SNS}^{-1})$

Next we show that set of similarity transformation satisfy the group axiom.
- **Identity element:** The identity transformation $\mathrm{I}\in mathrm{GL}_n (K)$ forms a multiplicative $\phi_\mathrm{I}: M \mapsto \mathrm{IMI}^{-1} = M$
- **Inverse element:** The inverse is given by $\psi_{\mathrm{S}^{-1}}: M\mapsto \mathrm{S}^{-1}\mathrm{MS}$.
- **Closure:** For $\phi_\mathrm{S}:\mathrm{M}\mapsto \mathrm{SMS}^{-1}$ and $\phi_\mathrm{T}:\mathrm{M}\mapsto\mathrm{TMT}^{-1}$, then $M\overset{\phi_\mathrm{S}}{\mapsto} \mathrm{SMS}^{-1} \overset{\phi_\mathrm{T}}{\mapsto} (\mathrm{TS})\mathrm{M}(\mathrm{S^{-1}\mathrm{T}^{-1}}) =  (\mathrm{TS})\mathrm{M}(\mathrm{TS})^{-1}$.
</details>
</MathBox>

<MathBox title='Similarity is an equivalence relation' boxType='proposition'>
Similarity is an equivalence relation $\sim$, satisfying
1. **Reflexivity:** $\mathrm{M}\sim \mathrm{M}$
2. **Symmetry:** $\mathrm{L}\sim\mathrm{M} \implies \mathrm{M}\sim\mathrm{L}$
3. **Transitivity:** $\mathrm{L}\sim\mathrm{M} \land \mathrm{M}\sim\mathrm{N} \implies \mathrm{L}\sim\mathrm{N}$
</MathBox>

<MathBox title='' boxType='proposition'>
If either $\mathrm{A}, \mathrm{B}\in\mathcal{L}(V)$ is invertible, then $\mathrm{AB}$ and $\mathrm{BA}$ are similar.
</MathBox>


## Rank-nullity theorem

<MathBox title='' boxType='proposition'>
Let $\mathrm{T}\in\mathcal{L}(V,W)$. Any complement of $\ker(\mathrm{T})$ is isomorphic to $\operatorname{ran}(\mathrm{T})$.

<details>
<summary>Proof</summary>

Let $\mathrm{T}\in\mathcal{L}(V,W)$. Since any subspace of $V$ has a complement, we can write

$$
  V = \ker(\mathrm{T}) \oplus\ker(\mathrm{T})^c
$$

where $\ker(\mathrm{T})^c$ is the complement of $\ker(\mathrm{T})$. The restriction of $\mathrm{T}$ to $\ker(\mathrm{T})^c$, denoted $\mathrm{T}^c:\ker(\mathrm{T})^c \to W$ is injective since

$$
  \ker(\mathrm{T}^c) = \ker(\mathrm{T}) \cap\ker(\mathrm{T})^c = \Set{\boldsymbol{0}}
$$

Also, $\operatorname{ran}(\mathrm{T}^c)\subseteq \operatorname{ran}(\mathrm{T})$. For the reverse inclusion, if $\mathrm{T}\in\operatorname{ran}(f)$ then since $\boldsymbol{v} = \boldsymbol{u} + \boldsymbol{w}$ for $\boldsymbol{u}\in\ker(\mathrm{T})$ and $\boldsymbol{w}\in\ker(\mathrm{T})^c$, we have

$$
  \mathrm{T}(\boldsymbol{v}) = \mathrm{T}(\boldsymbol{u}) + \mathrm{T}(\boldsymbol{w}) = \mathrm{T}(\boldsymbol{w}) = \mathrm{T}^c(\boldsymbol{w}) \in\operatorname{ran}(\mathrm{T}^c)
$$

Thus $\operatorname{ran}(\mathrm{T}^c) = \operatorname{ran}(\mathrm{T})$ and it follows that $\ker(\mathrm{T})^c \cong \operatorname{ran}(\mathrm{T})$.
</details>
</MathBox>

<MathBox title='Rank-nullity theorem' boxType='theorem'>
For any $\mathrm{T}\in\mathcal{L}(V,W)$

$$
\begin{gather*}
  \dim(\ker(\mathrm{T})) + \dim(\operatorname{ran}(\mathrm{T})) = \dim(V) \\
  \operatorname{rank}(\mathrm{T}) + \mathrm{null}(\mathrm{T}) = \dim(V)
\end{gather*}
$$
</MathBox>

<MathBox title='' boxType='corollary'>
Let $\mathrm{T}\in\mathcal{L}(V,W)$, where $\dim(V) = \dim(W) < \infty$. Then $\mathrm{T}$ is injective if and only if it is surjective.
</MathBox>

## Finite-dimensional linear transformations

Any $m\times n$ matrix $\boldsymbol{A}$ over $\mathbb{F}$ defines a linear transformation $\mathrm{T}_{\boldsymbol{A}}:\mathbb{F}^n\to\mathbb{F}^m$ in the form of the multiplication map $\boldsymbol{v}\mapsto \boldsymbol{A}\boldsymbol{v}$. 

<MathBox title='' boxType='lemma'>
1. If $\boldsymbol{A}$ is an $m\times n$ matrix over $\mathbb{F}$, then the multiplication function $\mathrm{T}_{\boldsymbol{A}}:\mathbb{F}^n \to\mathbb{F}^m$ defined by $\boldsymbol{v} \mapsto \boldsymbol{A}\boldsymbol{v}$ is a linear map, i.e. $\mathrm{T}_{\boldsymbol{A}} \in\mathcal{L}(\mathbb{F}^n,\mathbb{F}^m)$.
2. If $\mathrm{T}\in\mathcal{L}(\mathbb{F}^n,\mathbb{F}^m)$ then $\mathrm{T} = \mathrm{T}_{\boldsymbol{A}}$ where for the standard basis $E = \Set{ \boldsymbol{e}_i }_{i=1}^n$

$$
  \boldsymbol{A} = \begin{bmatrix}
    \shortmid & & \shortmid \\
    \mathrm{T}(\boldsymbol{e}_1) & \cdots & \mathrm{T}(\boldsymbol{e}_n) \\
    \shortmid & & \shortmid
  \end{bmatrix} \in\mathcal{M}_{m,n}(\mathbb{F})
$$

is the matrix of $T$.

<details>
<summary>Proof</summary>

**(1)**: For a matrix $\boldsymbol{A}\in\mathcal{M}_{m,n}(\mathbb{F})$, vectors $\boldsymbol{v}, \boldsymbol{w} \in \mathbb{F}^n$ and scalars $\alpha, \beta\in\mathbb{F}$ the associativity and distributivity properties of matrix multiplication gives

$$
  \boldsymbol{A}(\alpha\boldsymbol{v} + \beta\boldsymbol{w}) = \boldsymbol{A}(\alpha\boldsymbol{v}) + \boldsymbol{A}(\beta\boldsymbol{v}) = \alpha \boldsymbol{A}\boldsymbol{v} + \beta\boldsymbol{A}\boldsymbol{w}
$$

showing that $\mathrm{T}_{\boldsymbol{A}} \in\mathcal{L}(\mathbb{F}^n,\mathbb{F}^m)$.

**(2)**: Let $E = \Set{ \boldsymbol{e}_i }_{i=1}^n$ be the standard basis of $\mathbb{F}^n$. If a vector $\boldsymbol{v}\in V$ has coordinates $[\boldsymbol{v}]_E = \left[(\beta_i)_{i=1}^n\right]^\top \in\mathbb{F}^n$ then $\boldsymbol{v}$ can be written as the linear combination

$$
  \boldsymbol{v} = \sum_{i=1}^n \beta_i \boldsymbol{e}_i  
$$

By the linearity of $\mathrm{T}$

$$
\begin{align*}
  \mathrm{T}(\boldsymbol{v}) =& \mathrm{T} \left(\sum_{i=1}^n \beta_i \boldsymbol{e}_i \right) = \sum_{i=1}^n \beta_i \mathrm{T}(\boldsymbol{e}_i) \\
  =& \begin{bmatrix} \mathrm{T}(\boldsymbol{e}_1) & \cdots & \mathrm{T}(\boldsymbol{e}_n) \end{bmatrix} [\boldsymbol{v}]_E \\
  =& \boldsymbol{A}[\boldsymbol{v}]_E = \mathrm{T}_{\boldsymbol{A}} (\boldsymbol{v})
\end{align*}
$$

Hence $\boldsymbol{A} = \begin{bmatrix} \mathrm{T}(\boldsymbol{e}_1) & \cdots & \mathrm{T}(\boldsymbol{e}_n) \end{bmatrix}$.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $\boldsymbol{A}$ be an $m\times n$ matrix over $F$.
1. $\mathrm{T}_{\boldsymbol{A}}:\mathbb{F}^n \to\mathbb{F}^m$ is injective if and only if $\operatorname{rank}(\boldsymbol{A}) = n$.
2. $\mathrm{T}_{\boldsymbol{A}}:\mathbb{F}^n \to\mathbb{F}^m$ is surjective if and only if $\operatorname{rank}(\boldsymbol{A}) = m$.
</MathBox>

### Change of basis matrices

<MathBox title='Change of basis operator' boxType='definition'>
Let $B $ and $C$ be ordered bases for an $n$-dimensional vector space $V$. For any $\boldsymbol{v}\in V$, the map $\varphi_{B,C} = \varphi_C \circ \varphi_B^{-1}$ given by $[\boldsymbol{v}]_B \mapsto [\boldsymbol{v}]_C$ is called the change of basis operator.

$$
\begin{CD}
  V @= V \\
  @V{\varphi_B}VV @VV{\varphi_C}V \\
  \mathbb{F} @>>{\varphi_C^{-1} \circ \varphi_D}> \mathbb{F}
\end{CD}
$$
</MathBox>

<MathBox title='' boxType='proposition'>
Let $B = \Set{\boldsymbol{b}_i}_{i=1}^n$ and $C$ be ordered bases for an $n$-dimensional vector space $V$. The change of basis operator $\varphi_{B,C} = \varphi_C \varphi_B^{-1}$ from $B$ to $C$ is an automorphism of $\mathbb{F}^n$ whose standard matrix is

$$
  \boldsymbol{M}_{B,C} = \begin{bmatrix} [\boldsymbol{b}_1]_C & \cdots & [\boldsymbol{b}_n]_C \end{bmatrix}
$$

Hence $[\boldsymbol{v}]_C = \boldsymbol{M}_{B,C}[\boldsymbol{v}]_B$ and $\boldsymbol{M}_{C,B} = \boldsymbol{M}_{B,C}^{-1}$.

<details>
<summary>Proof</summary>

Since $\varphi_{B,C}$ is an operator on $\mathbb{F}^n$ it has the form $\mathrm{T}_{\boldsymbol{M}}$ where $\boldsymbol{M}\in\mathcal{M}_n$

$$
\begin{align*}
  \boldsymbol{M} =& \begin{bmatrix} \varphi_{B,C}(\boldsymbol{e}_1) & \cdots & \varphi_{B,C}(\boldsymbol{e}_n) \end{bmatrix} \\
  =& \begin{bmatrix} \varphi_C \varphi_B^{-1}([\boldsymbol{b}_1]_B) & \cdots & \varphi_C \varphi_B^{-1}([\boldsymbol{b}_n]_B) \end{bmatrix} \\
  =& \begin{bmatrix} [\boldsymbol{b}_1]_C & \cdots & [\boldsymbol{b}_n]_C \end{bmatrix}
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Functional dependency of bases and transformation matrices' boxType='proposition'>
If given any two of the following
1. an invertible $n\times n$ matrix $\boldsymbol{A}$
2. an ordered basis $B$ for $\mathbb{F}^n$
3. an ordered basis $C$ for $\mathbb{F}^n$

then the third is uniquely determined by the equation

$$
  \boldsymbol{A} = \boldsymbol{M}_{B,C}
$$

<details>
<summary>Proof</summary>

The result is clear if $B$ and $C$ are given or if $\boldsymbol{A}$ and $C$ are given. If $\boldsymbol{A}$ and $B$ are given, then there is a unique $C$ for which $\boldsymbol{A}^{-1} = \boldsymbol{M}_{C,B}$ and so there is a unique $C$ for which $\boldsymbol{A} = \boldsymbol{M}_{B,C}$.
</details>
</MathBox>

### The matrix of a linear transformation

<MathBox title='' boxType='proposition'>
Let $\mathrm{T}:V\to W$ be a linear transformation, where $\dim(V) = n$ and $\dim(W) = m$, and let $B = \Set{\boldsymbol{b}_i}_{i=1}^n$ be an ordered basis for $V$ and $C$ and ordered basis for $W$. Then $\mathrm{T}$ can be represented with respect to $B$ and $C$ as the matrix product $[\mathrm{T}(\boldsymbol{v})]_C = [\mathrm{T}]_{B,C}[\boldsymbol{v}]_B$ where

$$
  [\mathrm{T}]_{B,C} = \begin{bmatrix} [\mathrm{T}(\boldsymbol{b}_1)]_C & \cdots & [\mathrm{T}(\boldsymbol{b}_n)]_C \end{bmatrix}
$$

is called the matrix of $\mathrm{T}$ with respect to $B$ and $C$. If $V = W$ and $B = C$ we denote $[\mathrm{T}]_{B,B}$ by $[\mathrm{T}]_{B}$ and so

$$
  [\mathrm{T}(\boldsymbol{v})]_B = [\mathrm{T}]_B [\boldsymbol{v}]_B
$$

<details>
<summary>Proof</summary>

Let $\mathrm{T}\in\mathcal{L}(V, W)$ and let $B = \Set{b_i}_{i=1}^n$ and $C$ be ordered bases for $V$ and $W$, respectively. Then the map $\theta:[\boldsymbol{v}]_B \mapsto [\mathrm{T}(v)]_C$ is a representation of $\mathrm{T}$ as a linear transformation from $\mathbb{F}^n$ to $\mathbb{F}^m$ in the sense that knowing $\theta$ (along with $B$ and $C$) is equivalent to knowing $\mathrm{T}$.

Since $\theta$ is a linear transformation from $\mathbb{F}^n$ to $\mathbb{F}^m$, it is simply multiplication by an $m \times n$ matrix $\boldsymbol{A}$, i.e. $[\mathrm{T}(v)]_C = \boldsymbol{A}[\boldsymbol{v}]_B$. Since $[\boldsymbol{b}_i]_B = \boldsymbol{e}_i$ we get the columns of $\boldsymbol{A}$

$$
  \boldsymbol{A}[\boldsymbol{v}]_B = \begin{bmatrix} [\mathrm{T}(\boldsymbol{b}_i)]_C & \cdots & [\mathrm{T}(\boldsymbol{b}_n)]_C  \end{bmatrix} [\boldsymbol{v}]_B
$$
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $\mathrm{T}\in\mathcal{L}(V, W)$, and let $V$ and $W$ be vector spaces over $\mathbb{F}$, with ordered bases $B = \Set{\boldsymbol{b}_i}_{i=1}^n$ and $C = \Set{\boldsymbol{c}_i}_{i=1}^m$, respectively.
1. The map $\mathrm{S}:\mathcal{L}(V, W)\to\mathcal{M}_{m,n}(\mathbb{F})$ defined by $\mathrm{S}(\mathrm{T}) = [\mathrm{T}]_{B,C}$ is an isomorphism and so $\mathcal{L}(V,W)\cong\mathcal{M}_{m,n}(\mathbb{F})$.
2. If $\mathrm{R}\in\mathcal{L}(U,V)$ and $\mathrm{T}\in\mathcal{L}(V,W)$ and if $B$, $C$ and $D$ are ordered bases for $U$, $V$ and $W$, respectively, then $[\mathrm{T}\mathrm{R}]_{B,C} = [\mathrm{T}]_{C,D} [\mathrm{R}]_{B,C}$. Thus, the matrix of the product (composition) $\mathrm{T}\mathrm{R}$ is the product of the matrices of $\mathrm{T}$ and $\mathrm{R}$ respectively.

<details>
<summary>Proof</summary>

**(1)**: To see that $S$ is linear, note that for all $i\in\Set{1,\dots,n}$

$$
\begin{align*}
  [\alpha\mathrm{R} + \beta T]_{B,C} [\boldsymbol{b}_i]_B =& [(\alpha\mathrm{R} + \beta T)(\boldsymbol{b}_i)]_C \\
  =& [\alpha\mathrm{R}(\boldsymbol{b}_i) + \beta\mathrm{T}(\boldsymbol{b}_i)]_C \\
  =& \alpha[\mathrm{R}(\boldsymbol{b}_i)]_C + \beta[\mathrm{T}(\boldsymbol{b}_i)]_C \\
  =& \alpha[\mathrm{R}]_{B,C}[\boldsymbol{b}_i]_B + \beta[\mathrm{T}]_{B,C}[\boldsymbol{b}_i]_B \\
  =& (\alpha[\mathrm{R}]_{B,C} + \beta[\mathrm{T}]_{B,C})[\boldsymbol{b}_i]_B
\end{align*}
$$

since $[\boldsymbol{b}_i]_B = \boldsymbol{e}_i$ is a standard basis vector, it follows that

$$
  [\alpha\mathrm{R} + \beta \mathrm{T}]_{B,C} = \alpha[\mathrm{R}]_{B,C} + \beta[\mathrm{T}]_{B,C}
$$

showing that $\mathrm{S}$ is linear. If $\boldsymbol{A}\in\mathcal{M}_{m,n}$, we define $\mathrm{T}$ by the condition $[\mathrm{T}(\boldsymbol{b}_i)]_C = \boldsymbol{A}_i$ giving $\mathrm{S}(\mathrm{T}) = \boldsymbol{A}$ which is surjective. Since $\dim(\mathcal{L}(V,W)) = \dim(\mathcal{M}_{m,n})$, the map $\mathrm{S}$ is an isomorphism.

**(2)**: We have

$$
\begin{align*}
  [\mathrm{T}\mathrm{R}]_{B,D}[\boldsymbol{v}]_B =& [\mathrm{T}(\mathrm{R}(\boldsymbol{v}))]_D = [\mathrm{T}]_{C,D}[\mathrm{R}(\boldsymbol{v})]_{C} \\
  =& [\mathrm{T}]_{C,D}[\mathrm{R}]_{B,C}[\boldsymbol{v}]_B
\end{align*}
$$
</details>
</MathBox>

### Change of bases for linear transforms

$$
\begin{CD}
  \mathbb{F}^n @>{\psi_{C'} \circ\mathrm{T}\circ\phi_{B'}^{-1}}>> \mathbb{F}^m \\
  @A{\phi_{B'}}AA @AA{\psi_{C'}}A \\
  V @>{\mathrm{T}}>> W \\
  @V{\phi_B}VV @VV{\psi_C}V \\
  \mathbb{F}^n @>>{\phi_C \circ\mathrm{T}\circ\phi_B^{-1}}> \mathbb{F}^m
\end{CD}
$$

<MathBox title='Change of bases equivalence' boxType='proposition'>
Let $\mathrm{T}\in\mathcal{L}(V,W)$ and let $(B,C)$ and $(B',C')$ be pairs of ordered bases of $V$ and $W$, respectively, then

$$
  [\mathrm{T}]_{B',C'} = \boldsymbol{M}_{C,C'}[\mathrm{T}]_{B,C}\boldsymbol{M}_{B',B}
$$

in which case $[\mathrm{T}]_{B',C'} \sim [\mathrm{T}]_{B,C}$.

If $\mathrm{T}\in\mathcal{L}(V)$ and $B$ and $C$ are ordered bases for $V$, then the matrix of $T$ reduce to

$$
  [\mathrm{T}]_C = \boldsymbol{M}_{B,C}[\mathrm{T}]_B \boldsymbol{M}_{B,C}^{-1}
$$

in which case $[\mathrm{T}]_C \sim [\mathrm{T}]_B$.

<details>
<summary>Proof</summary>

Multiplication by $[\mathrm{T}]_{B',C'}$ sends $[\boldsymbol{v}]_{B'}$ to $[\mathrm{T}(v)]_{C'}$. This can be reproduced by first switching from $B'$ to $B$, then applying $[\mathrm{T}]_{B,C}$, and finally switching from $C$ to $C'$, i.e.

$$
\begin{align*}
  [\mathrm{T}]_{B',C'} =& \boldsymbol{M}_{C,C'}[\mathrm{T}]_{B,C} \boldsymbol{M}_{B',B} \\
  =& \boldsymbol{M}_{C,C'} [\mathrm{T}]_{B,C} \boldsymbol{M}_{B,B'}^{-1}
\end{align*}
$$
</details>
</MathBox>

### Equivalence of matrices

<MathBox title='Equivalence of matrices' boxType='definition'>
Two matrices $\boldsymbol{A}$ and $\boldsymbol{B}$ are *equivalent* if there exist invertible matrices $\boldsymbol{P}$ and $\boldsymbol{Q}$ for which

$$
  \boldsymbol{B} = \boldsymbol{P}\boldsymbol{A}\boldsymbol{Q}^{-1}
$$
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V$ and $W$ be vector spaces with $\dim(V) = n$ and $\dim(W) = m$. Then two $m\times n$ matrices $\boldsymbol{A}$ and $\boldsymbol{B}$ are equivalent if and only if they represent the same linear transformation $\mathrm{T}\in\mathcal{L}(V,W)$, possibly with respect to different ordered bases. In this case, $\boldsymbol{A}$ and $\boldsymbol{B}$ represent exactly the same set of linear transformation in $\mathcal{L}(V,W)$.

<details>
<summary>Proof</summary>

If $\boldsymbol{A}$ and $\boldsymbol{B}$ represent $\mathrm{T}$, i.e. if $\boldsymbol{A} = [\mathrm{T}]_{B,C}$ and $\boldsymbol{B} = [\mathrm{T}]_{B',C'}$ for ordered bases $B$, $C$, $B'$ and $C'$, then by the change of basis equivalence, $\boldsymbol{A}$ and $\boldsymbol{B}$ are equivalent. 

Conversely, suppose $\boldsymbol{A}$ and $\boldsymbol{B}$ are eqiuvalent, i.e. $\boldsymbol{B} = \boldsymbol{P}\boldsymbol{A}\boldsymbol{Q}^{-1}$ where $\boldsymbol{P}$ an $\boldsymbol{Q}$ are invertible. Suppose $\boldsymbol{A}$ represents a linear transformation $\mathrm{T}\in\mathcal{L}(V,W)$ for some ordered basis $B$ and $C$, i.e.

$$
  \boldsymbol{A} = [\mathrm{T}]_{B,C}
$$

By the functional dependency property, there is a unique ordered basis $B'$ for $V$ for which $\boldsymbol{Q} = \boldsymbol{M}_{B,B'}$ and a unique ordered basis $C'$ for $W$ for which $\boldsymbol{P} = \boldsymbol{M}_{C,C'}$. Thus

$$
  \boldsymbol{B} = \boldsymbol{M}_{C,C'}[\mathrm{T}]_{B,C} \boldsymbol{M}_{B', B} = [\mathrm{T}]_{B',C'}
$$

showing that $\boldsymbol{B}$ also represents $\mathrm{T}$. By symmetry, we se that $\boldsymbol{A}$ and $\boldsymbol{B}$ represent the same set of linear transformation.
</details>
</MathBox>

### Similarity of matrices

<MathBox title='Similarity of matrices' boxType='proposition'>
Two matrices are *similar* if there exists an invertible matrix $\boldsymbol{P}$ for which $\boldsymbol{B} = \boldsymbol{P}\boldsymbol{A}\boldsymbol{P}^{-1}$. The equivalence classes associated with similarity are called similarity classes.
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V$ be a vector spaces with $\dim(V) = n$. Then two $n\times n$ matrices $\boldsymbol{A}$ and $\boldsymbol{B}$ are similar if and only if they represent the same linear transformation $\mathrm{T}\in\mathcal{L}(V)$, possibly with respect to different ordered bases. In this case, $\boldsymbol{A}$ and $\boldsymbol{B}$ represent exactly the same set of linear transformation in $\mathcal{L}(V)$.

<details>
<summary>Proof</summary>

If $\boldsymbol{A}$ and $\boldsymbol{B}$ represent $\mathrm{T}\in\mathcal{L}(V)$, i.e. if $\boldsymbol{A} = [\mathrm{T}]_B$ and $\boldsymbol{B} = [\mathrm{T}]_C$ for ordered bases $B$ and $C$, then by the change of bases equivalence $\boldsymbol{A}$ and $\boldsymbol{B}$ are similar.

Conversely, suppose $\boldsymbol{A}$ and $\boldsymbol{B}$ are similar, i.e. $\boldsymbol{B} = \boldsymbol{P}\boldsymbol{A}\boldsymbol{P}^{-1}$. Suppose $\boldsymbol{A}$ represents a linear operator $\mathcal{L}(V)$ for some ordered basis $B$, i.e. $\boldsymbol{A} = [\mathrm{T}]_B$. By the functional dependency relation, there is a unique ordered basis $C$ for $V$ for which $\boldsymbol{P} = \boldsymbol{M}_{B,C}$. Thus 

$$
  \boldsymbol{B} = \boldsymbol{M}_{B,C}[\mathrm{T}]_{B}\boldsymbol{M}_{B,C}^{-1} = [\mathrm{T}]_C
$$

Hence, $\boldsymbol{B}$ also represents $\mathrm{T}$. By symmetry, it follows that $\boldsymbol{A}$ and $\boldsymbol{B}$ represent the same set of linear operators.
</details>
</MathBox>

### Similarity of operators

<MathBox title='Similarity of operators' boxType='proposition'>
Two linear operators $\mathrm{T}, \mathrm{S}\in\mathcal{L}(V)$ are *similar* if there exists an automorphism $R\in\mathcal{L}(V)$ for which 

$$
  \mathrm{S} = \mathrm{R}\mathrm{T}\mathrm{R}^{-1}
$$

The equivalence classes associated with similarity are called *similarity classes*.
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V$ be a vector spaces with $\dim(V) = n$. Then two linear operators $\mathrm{T},\mathrm{S}\in\mathcal{L}(V)$ are similar if and only if there is a matrix $\boldsymbol{A}\in\mathcal{M}_n$ that represents both operators with respect to possibly different ordered bases. In this case, $\mathrm{T}$ and $\mathrm{S}$ are represented by exactly the same set of matrices in $\mathcal{M}_n$.

<details>
<summary>Proof</summary>

If $\mathrm{T}$ and $\mathrm{S}$ are represented by $\boldsymbol{A}\in\mathcal{M}_{n}$, i.e. if $[\mathrm{T}]_B = \boldsymbol{A} = [\mathrm{S}]_C$ for ordered bases $B$ and $C$ then

$$
  [\mathrm{S}]_C = [\mathrm{T}]_B = \boldsymbol{M}_{C,B}[\mathrm{T}]_C \boldsymbol{M}_{B_C}
$$

Let $R\in\mathcal{L}(V)$ be the automorphism of $V$ defined by $\mathrm{T}(\boldsymbol{c}_i) = \boldsymbol{b}_i$ where $B = \Set{\boldsymbol{b}_i}_{i=n}$ and $C = \Set{\boldsymbol{c}_i}_{i=1}^n$, then

$$
\begin{align*}
  [\mathrm{R}]_C =& \begin{bmatrix} [\mathrm{R}(\boldsymbol{c}_1)]_C & \cdots & [\mathrm{R}(\boldsymbol{c}_n)]_C \end{bmatrix} \\
  =& \begin{bmatrix} [\boldsymbol{b}_1]_C & \cdots & [\boldsymbol{b}_n]_C \end{bmatrix} \\
  =& \boldsymbol{M}_{B,C}
\end{align*}
$$

and so

$$
  [\mathrm{S}]_C = [\mathrm{R}]_C^{-1}[\mathrm{T}]_C [\mathrm{R}]_C = [\mathrm{R}^{-1}\mathrm{T}\mathrm{R}]_C
$$

from which it follows that $\mathrm{T}$ and $\mathrm{S}$ are similar.

Conversely, suppose $\mathrm{T}$ and $\mathrm{S}$ are similar, i.e. $\mathrm{S} = \mathrm{R}\mathrm{T}\mathrm{R}^{-1}$. Suppose also that $\mathrm{T}$ is represented by the matrix $\boldsymbol{A}\in\mathcal{M}_n$, i.e. $\boldsymbol{A} = [\mathrm{T}]_B$ for some ordered basis $B$. Then

$$
  [\mathrm{S}]_B = [\mathrm{R}\mathrm{T}\mathrm{R}^{-1}]_B = [\mathrm{R}]_B [\mathrm{T}]_B [\mathrm{R}]_B^{-1}
$$

Setting $\boldsymbol{c}_i = h(\boldsymbol{b}_i)$ then $C = \Set{ \boldsymbol{c}_i }_{i=1}^n$ is an ordered basis for $V$ and

$$
\begin{align*}
  [\mathrm{R}]_B =& \begin{bmatrix} [\mathrm{R}(\boldsymbol{b}_1)]_B & \cdots & [\mathrm{R}(\boldsymbol{b}_n)]_B \end{bmatrix} \\
  =& \begin{bmatrix} [\boldsymbol{c}_1]_B & \cdots & [\boldsymbol{c}_n]_C \end{bmatrix} \\
  =& \boldsymbol{M}_{C,B}
\end{align*}
$$

Thus, $[\mathrm{S}]_B = \boldsymbol{M}_{C,B}[\mathrm{T}]_B \boldsymbol{M}_{C,B}^{-1}$ and it follows that

$$
  \boldsymbol{A} = [\mathrm{T}]_B = \boldsymbol{M}_{B,C}[\mathrm{S}]_B \boldsymbol{M}_{B,C}^{-1} = [\mathrm{S}]_C
$$

and so $A$ also represents $S$. By symmetry, it follows that $\mathrm{T}$ and $\mathrm{S}$ are represented by the same set of matrices.
</details>
</MathBox>

## Invariant subspaces and reducing pairs

The restriction of a linear operator $\mathrm{T}\in\mathcal{L}(V)$ to a subspace $S\subseteq V$ is not necessarily a linear operator on $S$.

<MathBox title='Invatiant subspaces' boxType='definition'>
Let $\mathrm{F}\in\mathcal{L}(V)$. A subspace $S\subseteq V$ is *invariant* under $\mathrm{F}$ if $\mathrm{F}(S) \subseteq S$. That is, $S$ is invariant under $\mathrm{F}$ if the restriction $\mathrm{F}|_S$ is a linear operator on $S$.
</MathBox>

If $V = S \oplus T$ then the fact that $S$ is $\mathrm{F}$-invariant does not imply that the complement $T$ is also $\mathrm{F}$-invariant.

<MathBox title='Reducing pair' boxType='definition'>
Let $\mathrm{F}\in\mathcal{L}(V)$. If $V = S\oplus T$ and if both $S$ and $T$ are $\mathrm{F}$-invariant, we say that the pair $(S,T)$ *reduces* $\mathrm{T}$.
</MathBox>

<MathBox title='Direct sum of linear operators' boxType='definition'>
Let $\mathrm{F}\in\mathcal{L}(V)$. If $(S,T)$ reduces $\mathrm{F}$ we write

$$
  \mathrm{F} = \mathrm{F}|_S \oplus \mathrm{F}|_U
$$

and call $\mathrm{F}$ the *direct sum* of $\mathrm{F}|_S$ and $\mathrm{F}|_T$. Thus, the expression $\mathrm{H} = \mathrm{G} \oplus \mathrm{F}$ means that there exists subspaces $S$ and $T$ of $V$ for which $(S,T)$ reduces $H$ with $\mathrm{G} = \mathrm{R}|_S$ and $\mathrm{F} = \mathrm{R}|_T$.
</MathBox>

# Topological vector space

The standard topology on $\R^n$ is the topology induced by the Euclidean metric on $\R^n$ for which the set of open rectangles

$$
  B = \Set{ \prod_{i=1}^n I_i | I_i \text{ is open an open interval in } \R }
$$

is a basis, i.e. a subset of $\R^n$ is open if and only if it is a union of sets in $B$. The standard topology on $\R^n$ has the properties that the any linear functional $f:\R^n \to\R^n$ together with the addition function

$$
  \mathcal{A}:\R^n\times\R^n \ni (v,w) \mapsto v + w \in\R^n
$$

and the scalar multiplication function

$$
  \mathcal{M}:\R\times\R^n \ni (\lambda,\boldsymbol{v}) \mapsto\lambda \boldsymbol{v} \in\R^n
$$

are continuous. As such $\R^n$ is a topological vector space.

Generally, any real vector space $V$ endowed with a topology $\mathcal{T}$ is called a *topological vector space* if the operations of addition $\mathcal{A}:V\times V\to V$ and scalar multiplication $\mathcal{M}:\R\times V\to V$ are continuous under $\mathcal{T}$.

<MathBox title='Topological vector space' boxType='proposition'>
Let $V$ be a real vector space with $\dim(V) = n$. There is a unique topology $\mathcal{T}$ on $V$, called the *natural topology* for which $V$ is a topological vector space and for which all linear functionals on $V$ are continuous. This topology is determined by the fact that the coordinate map $\varphi: V\to\R^n$ is a homeomorphism.

<details>
<summary>Proof</summary>

Let $V$ be any real vector space with $\dim(V) = n$ and fix and ordered basis $B = \Set{ \boldsymbol{v}_i }_{i=1}^n$ for $V$. Consider the coordinate map

$$
  \varphi = \varphi_B: V\ni \boldsymbol{v} \mapsto [\boldsymbol{v}]_B \in \R^n
$$

and its inverse

$$
  \psi_B = \varphi_B^{-1}:\R^n \ni (\lambda_i)_{i=1}^n \mapsto sum_{i=1}^n \lambda_i \boldsymbol{v}_i
$$

We need to show that there is a unique topology $\mathcal{T}$ on $V$ for which $\varphi_B$ (and thus $\psi_B$) is a homeomorphism, i.e. a bijection that is continuous with a continuous inverse.

**$\psi$ is continuous under $\mathcal{T}$**<br/>
First we show that if $V$ is a topological vector space under a topology $\mathcal{T}$ then $\psi$ is continuous. Since $\psi = \sum_{i=1}^n \psi$ where $\psi_i:\R\to V$ is defined by $\psi_i (\lambda_i)_{i=1}^n = \lambda_i \boldsymbol{v}_i$, it is sufficient to show that these maps continuous, as the sum of continuous maps is continuous.

Let $O$ be an open set in $\mathcal{T}$. Then the inverse scalar multiplication operation

$$
  \mathcal{M}^{-1}(O) = \Set{(\alpha, \boldsymbol{v})\in\R\times V | \alpha}
$$

is an open set in $\R\times V$. We need to show that the set

$$
  \psi_i^{-1}(O) = \Set{(\lambda_i)_{i=1}^n \in\R^n | \lambda_i \boldsymbol{v}_i \in O }
$$

is open in $\R^n$. Let $(\lambda_i)_{i=1}^n \in\psi_i^{-1}(O)$ so that $\lambda_i \boldsymbol{v}_i \in O$. It follows that $(\lambda_i, \boldsymbol{v}_i)\in\mathcal{M}^{-1}(O)$, which is open. Thus, there is an open interval $I\subseteq\mathbb{R}$ and an open set $B\in\mathcal{T}$ of $V$ for which

$$
  (\lambda_i, \boldsymbol{v}_i) \in I \times B \subseteq\mathcal{M}^{-1}(O)
$$

Then the open set $U = \R\times\cdot\R\times I \times\R\times\cdots\times\R$ where $I$ is in the $i$th position, has the property that $\psi_i(U)\subseteq O$. Thus

$$
  (\lambda_i)_{i=1}^n \in U \subseteq \psi_i^{-1}(O)
$$

and so $\psi_i^{-1}(O)$ is open. Hence, $\psi_i$, and therefore $\psi$, is continuous.

**$\varphi$ is continuous under $\mathcal{T}$**<br/>
Next we show that if every linear functional on $V$ is continuous under a topology $\mathcal{T}$ on $V$ then the coordinate map $\varphi$ is continuous. For $\boldsymbol{v}\in V$ let $[\boldsymbol{v}]_{B,i}$ denote the $i$th coordinate of $[\boldsymbol{v}]_B$. The map $\mu:V \to\R$ defined by $\mu(\boldsymbol{v}) = [\boldsymbol{v}]_{B,i}$ is a linear functional and so is continuous by assumption. Thus, for any open interval $I_i \in\R$ the set

$$
  A_i = \Set{ \boldsymbol{\boldsymbol{v}}\in V | [\boldsymbol{v}]_{B,i} \in I_i }
$$

is open. If $I_i$ are open intervals in $\R$ then

$$
  \varphi^{-1}(\prod_{i=1}^n I_i) = \Set{ \boldsymbol{\boldsymbol{v}}\in V | [\boldsymbol{v}]_B \in \prod_{i=1}^n I_i } = \bigcap_{i=1}^n A_i
$$

is open. Hence $\varphi$ is continuous.

**$\mathcal{T}$ is unique topology on $V$**<br/>
If a topology $\mathcal{T}$ has the property that $V$ is a topological vector space and every linear functional is continuous, then $\varphi$ and $\psi = \varphi^{-1}$ are homeomorphisms. This means that $\mathcal{T}$, if it exists, must be unique.

It remains to prove that the topology $\mathcal{T}$ on $V$ that makes $\varphi$ a homeomorphism has the property that $V$ is a topological space under $\mathcal{T}$ and that any linear functional $f$ on $V$ is continuous.

**Addition $\mathcal{A}$ is continuous under $\mathcal{T}$**<br/>
As to addition, the maps $\varphi: V\to\R^n$ and $(\varphi\times\varphi):V\times V\to\R^n \times\R^n$ are homeomorphisms and the map $\mathcal{A}':\R^n\times \R^n \to\R^n$ is continuous and so the map $\mathcal{A}:V\times V\to V$, being equal to $\varphi^{-1}\circ\mathcal{A}'\circ(\varphi\times\varphi)$,, is also continuous.

**Scalar multiplication $\mathcal{M}$ is continuous under $\mathcal{T}$**<br/>
As to scalar multiplication, the maps $\varphi:V\to\R^n$ and $(\iota\times\varphi):\R\times V\to \R\times\R^n$ are homeomorphisms and the map $\mathcal{M}:V\times V\to V$, being equal to $\varphi^{-1}\circ\mathcal{M}'\circ(\iota\times\varphi)$, is also continuous.

**Any linear functional $f:V\to\R$ is continuous**<br/>
Let $f:V\to\R$ be a linear functional. Since $\varphi$ is continuous if and only if $f\circ\varphi^{-1}$ is continuous, we can confine attention to $V=\R^n$. In this case, if $\Set{\boldsymbol{e}_i}_{i=1}^n$ is the standard basis for $\R^n$ and $|f(\boldsymbol{e}_i)|\leq M$, then for any $x=(\lambda_i)\in\R^n$ we have

$$
\begin{align*}
  |f(x)| =& \left| \sum_{i=1}^n \lambda_i f(\boldsymbol{e}_i) \right| \\
  \leq& \sum_{i=1}^n |\lambda_i|\cdot|f(\boldsymbol{e}_i)| \leq M\sum_{i=1}^n |\lambda_i|
\end{align*}
$$

If $|x| < \frac{\varepsilon}{Mn}$ then $|\lambda_i| < \frac{\varepsilon}{Mn}$ and so $|f(x)| < \varepsilon$, which implies that $f$ is continuous.

By the Riesz representation theorem and the Cauchy-Schwarz inequality we have

$$
  \norm{ f(x)} \leq \norm{ \mathcal{R}_f} \cdot \| x\|
$$

Hence, $x_n \to 0$ implies $f(x_n)\to 0$ and so by linearity, $x_n \to x$ implies $f(x_n)\to x$ and so $f$ is continuous.
</details>
</MathBox>

# Quotient space

<MathBox title='Quotient space' boxType='definition'>
Let $S$ be a subspace of a $\mathbb{F}$-vector space $V$. The binary relation on $V$ defined by

$$
  \boldsymbol{u} \sim \boldsymbol{v} \iff \boldsymbol{u} - \boldsymbol{v} \in S,\; \boldsymbol{u},\boldsymbol{\boldsymbol{v}}\in V
$$

is an equivalence relation. When $\boldsymbol{u}\sim \boldsymbol{v}$, we say that $\boldsymbol{u}$ and $\boldsymbol{v}$ are *congruent modulo* $S$ written as

$$
  \boldsymbol{u}\sim \boldsymbol{v} \mod S
$$

The equivalence class

$$
  [\boldsymbol{v}]_\sim = \boldsymbol{v} + S = \Set{\boldsymbol{v}+\boldsymbol{s} | \boldsymbol{s}\in S}
$$

is called a *coset* of $S$ in $V$ and $\boldsymbol{v}$ is called a *coset representative* for $\boldsymbol{v} + S$. The set of all cosets of $S$ in $V$ is denoted

$$
  V/S = \Set{\boldsymbol{v} + S | \boldsymbol{\boldsymbol{v}}\in V}
$$

and is called the *quotient space* of $V$ modulo $S$. The quotient space is a vector space under the operations
- $\lambda \boldsymbol{u}_1 + S = \lambda \boldsymbol{u}_2 + S$ for $\boldsymbol{u}_1\in V$ and $\lambda\in\mathbb{F}$
- $(\boldsymbol{u}_1 + \boldsymbol{v}_1) + S = (\boldsymbol{u}_2 + v_2) + S$ for $\boldsymbol{u}_1, \boldsymbol{u}_2, \boldsymbol{v}_1, \boldsymbol{v}_2\in V$

The zero vector in $V/S$ is the coset $\boldsymbol{0} + S = S$

<details>
<summary>Details</summary>

The equivalence classes on $\sim$ take the form

$$
\begin{align*}
  [\boldsymbol{v}]_\sim =& \Set{\boldsymbol{u}\in V | \boldsymbol{u}\sim \boldsymbol{v} } \\
  =& \Set{\boldsymbol{u}\in V | \boldsymbol{u}-\boldsymbol{v} \in S } \\
  =& \Set{ \boldsymbol{u}\in V | \boldsymbol{u} = \boldsymbol{v} + \boldsymbol{s},\; \boldsymbol{s}\in S } \\
  =& \Set{ \boldsymbol{v} + \boldsymbol{s} | \boldsymbol{s}\in S } \\
  =& \boldsymbol{v} + S
\end{align*}
$$

If $\boldsymbol{u}_1 \sim \boldsymbol{v}_1$ and $\boldsymbol{u}_2 \sim \boldsymbol{v}_2$, then 

$$
\begin{align*}
  \boldsymbol{u_1} - \boldsymbol{v}_1 \in S, \boldsymbol{u}_2 - \boldsymbol{v}_2 \in \implies& \alpha(\boldsymbol{u}_1 - \boldsymbol{v}_1) + \beta(\boldsymbol{u}_2 - \boldsymbol{v}_2) \in S \\
  \implies& (\alpha\boldsymbol{u}_1 + \beta\boldsymbol{u}_2) - (\alpha\boldsymbol{v}_1 + \beta \boldsymbol{v}_2)\in S \\
  \implies& \alpha\boldsymbol{u}_1 + \beta\boldsymbol{u}_2 \sim \alpha\boldsymbol{v}_1 + \beta \boldsymbol{v}_2
\end{align*}
$$

showing that congruence modulo $S$ is preserved under the vector space operations.
</details>
</MathBox>

## Natural projection

<MathBox title='Natural projection' boxType='proposition'>
The natural (canonical) projection $\pi_S: V\to V/S$ defined by

$$
  \pi_S (\boldsymbol{v}) = \boldsymbol{v} + S
$$

is a surjective linear transformation with $\ker(\pi_S) = S$.

<details>
<summary>Details</summary>

The natural projection $\pi_S$ is clearly surjective.

For $\boldsymbol{u},\boldsymbol{\boldsymbol{v}}\in V$ and $\alpha,\beta\in\mathbb{F}$ we have
$$
\begin{align*}
  \pi_S(\alpha\boldsymbol{u} + \beta\boldsymbol{v}) =& (\alpha\boldsymbol{u} + \beta\boldsymbol{v}) + S \\
  =& \alpha(\boldsymbol{u} + S) \beta(\boldsymbol{v} + S) \\
  =& \alpha\pi_S(\boldsymbol{u}) + \beta\pi_S(\boldsymbol{v})
\end{align*}
$$

showing that $\pi_S$ is linear.

To determine the kernel of $\pi_S$, note that

$$
\begin{align*}
  \boldsymbol{v}\in \ker(\pi_S) \iff& \pi_S(\boldsymbol{v}) = \boldsymbol{0} \\
  \iff& \boldsymbol{v} + S = S \iff \boldsymbol{v}\in S
\end{align*}
$$

Hence $\ker(\pi_S) = S$.
</details>
</MathBox>

<MathBox title='Correspondence theorem for vector spaces' boxType='theorem'>
Let $S$ be a subspace of a vector space $V$. Then the function that assigns to each intermediate subspace $S\subseteq T\subseteq V$ the subspace $T/S$ of $V/S$ is an order preserving (with respect to set inclusion) one-to-one correspondence between the set of all subspaces of $V$ containing $S$ and the set of all subspaces of $V/S$.
</MathBox>

## The first isomorphism theorem

<LatexFigure width={50} src='/fig/universal_property_linear_transformation.svg' alt=''
  caption='Universal property for linear transformation'
>
```latex
\documentclass[tikz]{standalone}
\usepackage{amssymb}
\usepackage{tikz}
\usetikzlibrary{cd}

\begin{document}

\begin{tikzcd}[
  row sep=large, column sep=huge, 
  every label/.append style={font=\scriptsize}
]
  V \arrow[d, "\mathrm{P}_S" swap] \arrow[r, "\mathrm{T} = \mathrm{T}' \circ \mathrm{P}_S"] & W \\
  V/S \arrow[ru, "\mathrm{T}'" swap] &
\end{tikzcd}

\end{document}
```
</LatexFigure>

<MathBox title='Universal property for linear transformations' boxType='theorem'>
Let $S$ be a subspace of $V$ and let $\mathrm{T}\in\mathcal{L}(V,W)$ satisfy $S\subseteq\ker(\mathrm{T})$. Then there is a unique linear transformation $\mathrm{T}':V/S\to W$ with the property that 

$$
  \mathrm{T}' \circ \pi_S = \mathrm{T}
$$

and $\ker(\mathrm{T}') = \ker(\mathrm{T})/S$ and $\operatorname{ran}(\mathrm{T}') = \operatorname{ran}(\mathrm{T})$.

<details>
<summary>Details</summary>

There is no other choice but to define $\mathrm{T}'$ by the condition $\mathrm{T}' = \pi_S = \mathrm{T}$, i.e.

$$
  \mathrm{T}'(\boldsymbol{v} + S) = \mathrm{\mathrm{T}}(\boldsymbol{v})
$$

This function is well-defined if and only if

$$
  \boldsymbol{v} + S = \boldsymbol{u} + S \implies \mathrm{T}'(\boldsymbol{v} + S) = \mathrm{T}'(\boldsymbol{u} + S)
$$

which is equivalent to each of the following statements

$$
\begin{align*}
  \boldsymbol{v} + S = \boldsymbol{u} + S \implies& \mathrm{T}(\boldsymbol{v}) = \mathrm{T}(\boldsymbol{u}) \\
  \boldsymbol{v} - \boldsymbol{u} \in S \implies& \mathrm{T}(\boldsymbol{v} - \boldsymbol{u}) = \boldsymbol{0} \\
  \boldsymbol{x}\in S \implies& \mathrm{T}(\boldsymbol{x}) = \boldsymbol{0} \\
  S \subseteq& \ker(\mathrm{T})
\end{align*}
$$

Thus, $\mathrm{T}':V/S\to W$ is well defined. Also

$$
\begin{align*}
  \operatorname{ran}(\mathrm{T}') =& \Set{ \mathrm{T}'(\boldsymbol{v} + S) | \boldsymbol{\boldsymbol{v}}\in V } \\
  =& \Set{ \mathrm{T}(\boldsymbol{v}) | \boldsymbol{v}\in V } \\
  =& \operatorname{ran}(\mathrm{T})
\end{align*}
$$

and

$$
\begin{align*}
  \ker(\mathrm{T}') =& \Set{\boldsymbol{v} + S | \mathrm{T}'(\boldsymbol{v} + S) = \boldsymbol{0} } \\
  =& \Set{\boldsymbol{v} + S | \mathrm{T}(\boldsymbol{v}) = \boldsymbol{0} } \\
  =& \Set{\boldsymbol{v} + S | \boldsymbol{v}\in\ker(\mathrm{T}) } 
  =& \operatorname{ran}(\mathrm{T})/S
\end{align*}
$$

The uniqueness of $\mathrm{T}'$ is evident.
</details>
</MathBox>

<MathBox title='First isomorphism theorem for vector spaces' boxType='theorem'>
For $\mathrm{T}\in\mathcal{L}(V,W)$, the linear transformation $\mathrm{T}':V/\ker(\mathrm{T}) \to W$ defined by

$$
  \mathrm{T}'(\boldsymbol{v} + \ker(\mathrm{T})) = \mathrm{T}(\boldsymbol{v})
$$

is injective and 

$$
  V/\ker(\mathrm{T}) \cong \operatorname{ran}(\mathrm{T})
$$

<details>
<summary>Details</summary>

The first isomorphism theorem follows from the universal property.
</details>
</MathBox>

According to the first isomorphism theorem, the range of any linear transformation on $V$ is isomorphic to a quotient space of $V$. Conversely, any quotient space $V/S$ of $V$ is the range the natural projection $\pi_S$. Thus, up to isomorpisms, quotient spaces are equivalent to homomorphic images.

If $V = S\oplus T$, then the first isomorphism theorem implies that $T \cong V/S$. This can be written as

$$
  (S\oplus T)/T \cong S/(S\cap T)
$$

<MathBox title='Projection operator' boxType='definition'>
Let $S$ be a subspace of $V$ and let $T$ be a complement of $S$, i.e. $V = S\oplus T$. The linear operator $\mathrm{P}_T:V\to V$ defined by

$$
  \mathrm{P}_T(\boldsymbol{s} + \boldsymbol{t}) = \boldsymbol{t},\; \boldsymbol{s}\in S,\boldsymbol{t}\in T
$$

is called the projection onto $T$ along $S$ and satisfies

$$
\begin{align*}
  \operatorname{ran}(\mathrm{P}_T) =& T \\
  \ker(\mathrm{P}_T) =& \Set{ \boldsymbol{s}+\boldsymbol{t}\in V | \boldsymbol{t} = \boldsymbol{0} } = S
\end{align*}
$$
</MathBox>

## Codimension

<MathBox title='' boxType='proposition'>
Let $S$ be a subspace of $V$. All complements of $S$ in $V$ are isomorphic to $V/S$ and hence to each other.

<details>
<summary>Details</summary>

This follows directly from the first isomorphism theorem.
</details>
</MathBox>

If $A$, $B$ and $C$ are subspaces of the vector space then by the previous proposition

$$
  A\oplus B = A\oplus C \implies B\cong C
$$

However, note that the following cases showcasing that quotients and complements do not always behave nicely with respect to isomorphisms:
1. It is possible that

$$
  A\oplus B = C\oplus D
$$

with $A\cong C$ but $B\not\cong D$. Hence $A\cong C$ does not imply that a complement of $A$ is isomorphic to a complement of $C$.

2. It is possible that $V\cong W$ along with $V = S\oplus B$ and $W = S\oplus D$, yet $B \not\cong$. Hence, $V\cong W$ does not imply that $V/S\not\cong W/S$. However, if $V = W$ then $B \cong D$.

<MathBox title='Codimension' boxType='definition'>
If $S$ is a subspace of $V$, then $\dim(V/S)$ is called the *codimension* of $S$ in $V$ and denoted by $\mathrm{codim}_V(S)$.
</MathBox>

<MathBox title='Relation between dimension and codimension' boxType='proposition'>
Let $S$ be a subspace of $V$. Then

$$
  \dim(V) = \dim(S) + \dim(V/S) = \mathrm{codim}_V(S)
$$

If $V$ is finite-dimensional then

$$
  \mathrm{codim}_V(S) = \dim(V) - \dim(S)
$$
</MathBox>

<MathBox title='Second isomorphism theorem for vector spaces' boxType='theorem'>
Let $S$ and $T$ be subspaces of a vector space $V$. Then

$$
  (S + T)/T \cong S/(S\cap T)
$$

<details>
<summary>Proof</summary>

The function $\mathrm{T}: (S+T) \to S/(S\cap T)$ defined by

$$
  \mathrm{T}(\boldsymbol{s} + \boldsymbol{t}) = \boldsymbol{s} + (S \cap T)
$$

is a well-defined surjective linear transformation with $\ker(\mathrm{T}) = T$. Hence, by the first isomorphism theorem $(S + T)/T \cong S/(S\cap T)$.
</details>
</MathBox>

<MathBox title='Third isomorphism theorem' boxType='theorem'>
Let $V$ be a vector space and suppose $S\subseteq T\subseteq V$ are subspaces of $V$. Then

$$
  (V/S)/(T/S) \cong V/T
$$

<details>
<summary>Proof</summary>

The function $\mathrm{T}: V/S \to V/S$ defined by

$$
  \mathrm{T}(\boldsymbol{s} + S) = \boldsymbol{s} + T
$$

is a well-defined surjective linear transformation with $\ker(\mathrm{T}) = T/S$. Hence, by the first isomorphism theorem $\mathrm{T}(\boldsymbol{s} + S) = \boldsymbol{s} + T$.
</details>
</MathBox>

<MathBox title='Third isomorphism theorem for vector spaces' boxType='theorem'>
Let $S$ be a subspace of the vector space $V$. Suppose $V = V_1 \oplus V_2$ and $S = S_2 \oplus S_2$ with $S_i \subseteq V_i$ for $i\in\Set{1,2}$. Then

$$
  (V/S) = (V_1\oplus V_2)/(S_1\oplus S_2) \cong V_1/S_1 \boxplus V_2/S_2
$$

<details>
<summary>Proof</summary>

The function $\mathrm{T}: V\to V_1/S_1 \boxplus V_2/S_2$ defined by

$$
  \mathrm{T}(\boldsymbol{v_1} + \boldsymbol{v_2}) \cong (\boldsymbol{v_1} + S, \boldsymbol{v_2} + S_2)
$$

is a well-defined since $V = V_1 \oplus V_2$ is a direct sum. It is also a surjective linear transformation with $\ker(\mathrm{T}) = S_1 \oplus S_2$. Hence, by the first isomorphism theorem $\mathrm{T}(\boldsymbol{v_1} + \boldsymbol{v_2}) \cong (\boldsymbol{v_1} + S, \boldsymbol{v_2} + S_2)$.
</details>
</MathBox>

# Dual space

## Linear functionals (covectors)

<MathBox title='Linear functional and dual space' boxType='definition'>
Let $V$ be a vector space over $\mathbb{F}$. A linear transformation $f\in\mathcal{L}(V,\mathbb{F})$ taking values in $\mathbb{F}$ is a *linear functional* or a *linear form* on $V$. The vector space of all linear functionals on $V$ is denoted by $V^*$ and is called the *algebraic dual space* of $V$. The elements of $V^*$ are called *dual vectors* or *covectors*.
</MathBox>

<MathBox title='' boxType='proposition'>
1. For any nonzero vector $\boldsymbol{v}\in V\setminus\Set{\boldsymbol{0}}$, there exists a linear functional $f\in V^*$ for which $f(\boldsymbol{v}) \neq 0$.
2. A vector $\boldsymbol{\boldsymbol{v}}\in V$ is zero if and only if $f(\boldsymbol{v}) = 0$ for all $f\in V^*$
3. Let $f\in V^*$. If $f(\boldsymbol{x}) \neq 0$ then $V = \operatorname{span}\Set{\boldsymbol{v}} \oplus \ker(f)$.
4. Two nonzero linear functionals $f,g\in V^*$ have the same kernel if and only if there is a nonzero scalar $\lambda$ such that $f = \lambda g$.

<details>
<summary>Proof</summary>

**(3):** If $\boldsymbol{0}\neq\boldsymbol{v} \in \operatorname{span}\Set{\boldsymbol{x}}\cap\ker(f)$ then $f(\boldsymbol{v}) = 0$ and $\mathrm{v} = \lambda\boldsymbol{x}$ for $0 \neq \lambda a\in\mathbb{F}$ implying $f(\boldsymbol{x}) = 0$, which is false. Thus, $\operatorname{span}\Set{\boldsymbol{x}}\cap\ker(f) = \Set{\boldsymbol{0}}$ and the direct sum $S = \ker(\boldsymbol{x})\oplus\ker(f)$ exists. Also, for any $\boldsymbol{v}\in V$ we have

$$
  \boldsymbol{v} = \frac{f(\boldsymbol{v})}{f(\boldsymbol{x})}\boldsymbol{x} + \left(\boldsymbol{v} - \frac{f\boldsymbol{v}}{f(\boldsymbol{x})}\boldsymbol{x} \right) \in\operatorname{span}\Set{\boldsymbol{x}} + \ker(f)
$$

and so $V = \operatorname{span}\Set{\boldsymbol{x}}\oplus\ker(f)$.

**(4):** If $f = \lambda g$ for $\lambda\neq 0$ then $\ker(f) = \ker(g)$. Conversely, if $K = \ker(f) = \ker(g)$ then for $\boldsymbol{x}\notin K$, then by **(3)**

$$
  V = \operatorname{span}\Set{\boldsymbol{x}}\oplus K
$$

Note that $f|_K = \lambda g|_K$ for any $\lambda$. Thus, if $\lambda = \frac{f(\boldsymbol{x})}{g\boldsymbol{x}}$, it follows that $\lambda g(\boldsymbol{x}) = f(\boldsymbol{x})$ and hence $f = \lambda g$. 
</details>
</MathBox>

## Dual basis

<MathBox title='Dual basis' boxType='proposition'>
Let $V$ be a vector space with basis $B = \Set{\boldsymbol{b}_i }_{i\in I}$ for some index set $I\subseteq\N$.
1. The set $B^* = \Set{b_i}_{i\in I}$ is linearly independent
2. If $V$ is finite-dimensional then $B^*$ is a basis for $V^*$ called the dual basis of $B$

<details>
<summary>Proof</summary>

**(1):** For each $i \in I$ we can define a linear functional $b_i \in V^*$ by the orthogonality condition $b_i (\boldsymbol{b}_j) = \delta_{ij}$ where $\delta_{ij}$ is the Kronecker delta function. Then the set $B^* = \Set{b_i}_{i\in I}$ is linearly independent since applying the equation

$$
  0 = \sum_{j=1}^n \lambda_{i_j} b_{i_j}
$$

to the basis vectors $\boldsymbol{b}_{i_k}$ gives

$$
  0 = \sum_{j=1}^k \lambda{i_j} b_{i_j} (\boldsymbol{b}_{i_k}) = \sum_{j=1}^k \lambda_{i_j}\delta_{i_j, i_k} = \lambda_{i_k}
$$

for all $i_k$.

**(2):** For any $f\in V^*$ we have

$$
  \sum_{i\in I} f(\boldsymbol{b}_i)b_i (\boldsymbol{b}_j) = \sum_{i\in I} f(\boldsymbol{b}_j)\delta_{ij} = f(\boldsymbol{b}_j)
$$

showing that $f = \sum_{i\in I} f(\boldsymbol{b}_i)b_i$ is in the span of $B^*$. Hence $B^*$ is a basis for $V^*$.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
If $\dim(V) < \infty$ then $\dim(V^*) = \dim(V)$.
</MathBox>

## Reflexivity

If $V$ is a vector space, then so is the dual space $V^*$. Hence, we may form the *double dual space* $V^{**}$ which consists of all linear functionals $\gamma: V^* \to\mathbb{F}$. In other words, an element of $V^{**}$ is a linear map that assigns scalar to each linear function on $V$.

<MathBox title='Double dual space' boxType='definition'>
Let $V$ be a vector space over $\mathbb{F}$. The *double dual space* of $V$, denoted $V^{**}$, is the set of all linear functionals $\mathrm{f}: V^* \to\mathbb{F}$. The elements of $V^{**}$ take the form of a linear function $\bar{\boldsymbol{v}}:V^{*}\to\mathbb{F}$ defined as for $\boldsymbol{v}\in V$

$$
  f \mapsto f(\boldsymbol{v})
$$

The function $\bar{\boldsymbol{v}}$ is called *evaluation at* $\boldsymbol{v}$.

<details>
<summary>Details</summary>

For $f,g\in V^*$ and $\alpha,\beta\in\mathbb{F}$ then

$$
\begin{align*}
  \bar{\boldsymbol{v}}(\alpha f + \beta g) =& (\alpha f + \beta g)(\boldsymbol{v}) \\
  =& \alpha f(\boldsymbol{v}) + \beta g(\boldsymbol{v}) \\
  =& \alpha\bar{\boldsymbol{v}}(f) + \beta\bar{\boldsymbol{v}}(g)
\end{align*}
$$

showing that $\bar{\boldsymbol{v}}$ is linear.
</details>
</MathBox>

<MathBox title='Canonical map' boxType='proposition'>
The canonical map $\mathrm{T}:V\to V^{**}$ defined by $\boldsymbol{v}\mapsto \bar{\boldsymbol{v}}$ where $\bar{\boldsymbol{v}}: V^* \to \mathbb{F}$ is the evaluation at $\boldsymbol{v}$, is a monomorphism. If $V$ is finite-dimensional then $\mathrm{T}$ is an isomorphism.

<details>
<summary>Proof</summary>

The function $\mathrm{T}$ is linear since for $\alpha, \beta\in\mathbb{F}$ and $\boldsymbol{u},\boldsymbol{v}\in V$

$$
\begin{align*}
  \overline{\alpha\boldsymbol{u} + \beta\boldsymbol{v}}(f) =& f(\alpha\boldsymbol{u} + \beta\boldsymbol{v}) \\
  =& \alpha f(\boldsymbol{u}) + \beta f(\boldsymbol{v}) \\
  =& (\alpha\bar{\boldsymbol{u}} + \beta\bar{\boldsymbol{v}})(f)
\end{align*}
$$

for all $f\in V^*$. To determine the kernel of $\mathrm{T}$, note that

$$
\begin{align*}
  \mathrm{T}(\boldsymbol{v}) = 0 \implies& \bar{\boldsymbol{v}} = 0 \\
  \implies& \bar{\boldsymbol{v}}(f) = 0,\; \forall f\in V^* \\
  \implies& f(\boldsymbol{v}) = 0,\; \forall f\in V^* \\
  \implies& \boldsymbol{v} = \boldsymbol{0}
\end{align*}
$$

showing that $\ker(\mathrm{T}) = \Set{\boldsymbol{0}}$.

In the finite-dimensional case, since $\dim(V^{**}) = \dim(V^*) = \dim(V)$, it follows that $\mathrm{T}$ is also surjective, hence an isomorphism.
</details>
</MathBox>

The canonical map implies that $V\cong V^{**}$ if $V$ is a finite-dimensional vector space. Since the canonical map is an isomorphism, then $V$ is called *algebraically reflexive*.

## Annihilators

<MathBox title='Annihilator' boxType='definition'>
Let $M$ be a nonempty subset of a vector space $V$. The *annihilator* $M^0$ of $M$ is the set

$$
  M^0 = \Set{ f\in V^* | f(M) } = \Set{ 0 }
$$

where $f(M) = \Set{ f(\boldsymbol{v}) | \boldsymbol{v}\in M }$
</MathBox>

<MathBox title='Properties of annihilators' boxType='proposition'>
1. **(Order reversing)** For any subset $M$ and $N$ of $V$

$$
  M\subseteq N \implies N^0 \subseteq M^0
$$

2. If $\dim(V) < \infty$ then $M^{00} \cong \operatorname{span}(M)$ under the canonical map. In particular, if $S$ is a subspace of $V$, then $S^{00}\cong S$.
3. If $\dim(V) < \infty$ and $S$ and $T$ are subspaces of $V$, then

$$
\begin{align*}
  (S\cap T)^0 =& S^0 + T^0 \\
  (S + T)^0 =& S^0 \cap T^0
\end{align*}
$$
</MathBox>

<MathBox title='Extension by 0' boxType='definition'>
Let $S$ and $T$ be complement subspaces of the vector space $V$ with $V = S\oplus T$. Any linear functional $f\in T^*$ can be extended to a linear functional $\bar{f}$ on $V$ by setting $f(S) = 0$. The linear functional $\bar{f}$ is called the extension by $0$ of $f$. Clearly, $\bar{f}\in S^0$.
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V = S \oplus T$.
1. The extension by zero map is an isomorphism from $T^*$ to $S^0$ and so $T^* \cong S^0$.
2. If $V$ is finite-dimensional then

$$
  \dim(S^0) = \mathrm{codim}_V (S) = \dim(V) - \dim(S)
$$
</MathBox>

<MathBox title='' boxType='proposition'>
A linear functional on the direct sum $V = S\oplus T$ can be written as a direct sum of a linear functional that annihilates $S$ and a linear functional that annihilates $T$, i.e.

$$
  (S\oplus T)^* = S^0 \oplus T^0
$$

<details>
<summary>Proof</summary>

Clearly $S^0 \cap T^0 = \Set{ \boldsymbol{0} }$, since any functional that annihilates both $S$ and $T$ must annihilate $S \oplus T$. Hence, the sum $S^0 + T^0$ is direct. If $f\in V^*$ then we can write

$$
  f = (f \circ \rho_T) + (f + \rho_S) \in S^0 \oplus T^0
$$

Hence $V = S^0 \oplus T^0$
</details>
</MathBox>

## Transpose of linear transformations

<MathBox title='Transpose of linear transformations (algebraic adjoint)' boxType='definition'>
The transpose of a linear transformation $\mathrm{T}\in\mathcal{L}(V,W)$, also called *algebraic adjoint*, is the function $\mathrm{T}^*: W^* \to V^*$ defined by

$$
\begin{gather*}
  T^* (f) = f\circ\mathrm{T} = f\mathrm{T},\; f\in W^* \\
  [T^* (f)](\boldsymbol{v}) = f[\mathrm{T}(\boldsymbol{v})],\; \boldsymbol{v}\in V
\end{gather*}
$$

Note that several notations are commonly used for the transpose of $\mathrm{T}$, including
- $\mathrm{T}^\times$
- $\mathrm{T}^\#$
- $\mathrm{T}'$
- $\mathrm{T}^\top$

<LatexFigure width={50} src='/fig/algebraic_adjoint.svg' alt=''
  caption='Algebraic adjoint'
>
```latex
\documentclass[tikz]{standalone}
\usepackage{tikz}
\usepackage{amssymb}
\usetikzlibrary{cd}

\begin{document}

\begin{tikzcd}
  V \arrow[r, "\mathrm{T}"] \arrow[rd, swap, "\mathrm{T}^* = f\circ\mathrm{T}"] & 
  W \arrow[d, "f\in W^*"] \\
  & \mathbb{F}
\end{tikzcd}

\end{document}
```
</LatexFigure>
</MathBox>

<MathBox title='Properties of the operator adjoint' boxType='proposition'>
Let $V$ be a vector space over $\mathbb{F}$, and $\alpha,\beta \in\mathbb{F}$, then
1. $(\alpha\mathrm{T} + \beta\mathrm{S})^* = \alpha\mathrm{T}^* + \beta\mathrm{S}^*$ for $\mathrm{T},\mathrm{S}\in\mathcal{L}(V,W)$ and $\alpha,\beta\in\mathbb{F}$.
2. $(\mathrm{T}\mathrm{S})^* = \mathrm{S}^* \mathrm{T}^*$ for $\mathrm{S}\in\mathcal{L}(V,W)$ and $\mathrm{T}\in\mathcal{L}(W,U)$.
3. If $\mathrm{T}\in\mathcal{L}(V)$ is invertible, then $(\mathrm{T}^{-1})^* = (\mathrm{T}^*)^{-1}$

<details>
<summary>Proof</summary>

**(1):** For all $f\in V^*$

$$
\begin{align*}
  (\alpha\mathrm{T} + \beta\mathrm{S})^* (f) =& f(\alpha\mathrm{T} + \beta\mathrm{S}) \\
  =& \alpha f(\mathrm{T}) + \beta f(\mathrm{S}) \\
  =& \alpha \mathrm{T}^* (f) + \beta \mathrm{S}^* (f)
\end{align*}
$$

**(2):** For all $f\in U^*$

$$
\begin{align*}
  (\mathrm{T}\mathrm{S})^* (f) =& f(\mathrm{T}\mathrm{S}) = \mathrm{S}^* (f\mathrm{T}) \\
  =& \mathrm{T}(\mathrm{S}^*(f)) = (\mathrm{T}^* \mathrm{S}^*)(f)
\end{align*}
$$

**(3):** Evaluating $\mathrm{T}^* (\mathrm{T}^*)^*$ using property (2) gives

$$
  \mathrm{T}^* (\mathrm{T}^*)^* = (\mathrm{T}^{-1} \mathrm{T})^* = \operatorname{id}^* = \operatorname{id}
$$

Doing the same for $(\mathrm{T}^{-1})^{-1} \mathrm{T}^*$ gives

$$
  (\mathrm{T}^{-1})^{-1} \mathrm{T}^* = (\mathrm{T}\mathrm{T}^{-1})^* = \operatorname{id}^* = \operatorname{id}
$$

Hence, $(\mathrm{T}^{-1})^* = (\mathrm{T}^*)^{-1}$.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V$ and $W$ be finite-dimensional $\mathbb{F}$-vector spaces and let $\mathrm{T}\in\mathcal{L}(V,W)$. If we identify $V^{**}$ with $V$ and $W^{**}$ with $W$ using the canonical maps, then $\mathrm{T}^{**}$ is identified with $\mathrm{T}$.

<details>
<summary>Proof</summary>

For any $\boldsymbol{x}\in V$ let the corresponding element of $V^{**}$ be denoted $\bar{\boldsymbol{x}}:V^* \to\mathbb{F}$ and similarly for $W$, then for all $f\in W^*$

$$
\begin{align*}
  \mathrm{T}^{**}(\bar{\boldsymbol{x}})(f) =& \bar{\boldsymbol{v}}[\mathrm{T}^* (f)] = \bar{\boldsymbol{v}}(f\mathrm{T}) \\
  =& f(\mathrm{T}(\boldsymbol{v})) = \overline{\mathrm{T}(\boldsymbol{v})}(f)
\end{align*}
$$

and so

$$
\begin{align*}
  \mathrm{T}^{**}(\bar{\boldsymbol{v}})(f) =& \overline{\mathrm{T}(\boldsymbol{v})} \in W^{**}
\end{align*}
$$

Using the canonical maps for both $V^{**}$ and $W^{**}$ we have

$$
  \mathrm{T}^{**}(\boldsymbol{v}) = \mathrm{T}(\boldsymbol{v})
$$

for all $\boldsymbol{v}\in V$.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $\mathrm{T}\in\mathcal{L}(V,W)$, then
1. $\ker(\mathrm{T}^*) = \operatorname{ran}(\mathrm{T})^0$
2. $\operatorname{ran}(\mathrm{T}^*) = \ker(\mathrm{T})^0$

<details>
<summary>Proof</summary>

**(1):**

$$
\begin{align*}
  \ker(\mathrm{T}^*) =& \set{ f\in W^* | \mathrm{T}^* = 0 } \\
  =& \Set{ f\in W^* | f(\mathrm{T}(V)) = \set{0} } \\
  =& \Set{ f\in W^* | f(\operatorname{ran}(\mathrm{T})) = \set{0} } \\
  =& \operatorname{ran}(\mathrm{T})^0
\end{align*}
$$

**(2):** If $f = g\mathrm{T} = \mathrm{T}^* g\in\operatorname{ran}(\mathrm{T}^*)$ then $\ker(\mathrm{T})\subseteq \ker(f)$ and so $f\in\ker(\mathrm{T})^0$.

For the reverse inclusion, let $f\in\ker(\mathrm{T})^0\subseteq V^*$. On $K = \ker(\mathrm{T})$, there is no problem since $f$ and $\mathrm{T}^* g = g\mathrm{T}$ agree on $K$ for any $g\in W^*$. Let $S$ be a complement of $\ker(\mathrm{T})$. Then $\mathrm{T}$ maps a basis $B = \Set{\boldsymbol{b}_i }_{i\in I}$ for $S$ to a linearly independent set $\mathrm{T}(B) = \Set{ \mathrm{T}(\boldsymbol{b}_i) }_{i\in I}\in W$, so we can define $g\in W^*$ any way we want on $\mathrm{T}(B)$. In particular, let $g\in W^*$ be defined by setting

$$
  g(\mathrm{T}(\boldsymbol{b}_i)) = f(\boldsymbol{b}_i),\; i\in I
$$

and extending in any manner to all of $W^*$. Then $f = g\mathrm{T} = \mathrm{T}^* g$ on $B$ and therefore on $S$. Hence $f = \mathrm{T}^* g\in\operatorname{ran}(\mathrm{T}^*)$.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $\mathrm{T}\in\mathcal{L}(V,W)$, where $V$ and $W$ are finite-dimensional vector spaces. Then $\operatorname{rank}(\mathrm{T}) = \operatorname{rank}(\mathrm{T}^*)$.
</MathBox>

<MathBox title='Relation between algebraic adjoint and matrix transposition' boxType='proposition'>
Let $\mathrm{T}\in\mathcal{L}(V,W)$, where $V$ and $W$ are finite-dimensional vector spaces. If $B$ and $C$ are ordered bases for $V$ and $W$, respectively, and $B^*$ and $C^*$ are corresponding dual bases, then

$$
  [\mathrm{T}^*]_{C^*, B^*} = ([\mathrm{T}]_{B,C})^\top
$$

<details>
<summary>Proof</summary>

The $(i,j)$th entry of $[\mathrm{T}]_{B,C}$ is

$$
  ([\mathrm{T}]_{B,C})_{i,j} = ([\mathrm{T}(\boldsymbol{b}_j)]_C)_i = c_i[\mathrm{T}(\boldsymbol{b}_j)],\; \boldsymbol{b}_i \in B, c_i \in C^*
$$

and the $(i,j)$th entry of $[\mathrm{T}^*]_{C^*,B^*}$ is

$$
\begin{align*}
  ([\mathrm{T}^*]_{C^*,B^*})_{i,j} =& ([\mathrm{T}^*(c_j)]_{B^*})_i = \boldsymbol{b}_i^{**}[\mathrm{T}^*(c_j)] \\
  =& \mathrm{T}^* (c_j)(\boldsymbol{b}_i) = c_j (\mathrm{T}(\boldsymbol{b}_i))
\end{align*}
$$

Comparing the expression, we see that $[\mathrm{T}^*]_{C^*, B^*} = ([\mathrm{T}]_{B,C})^\top$.
</details>
</MathBox>

# Multilinear forms

<MathBox title='Multilinear form' boxType='definition'>
If $V_1,\dots,V_n$ are vector spaces over $\mathcal{F}$, an $n$-linear form is a function $f:V_1 \times\cdots\times V_k \to \mathbb{F}$ that is linear in each coordinate.
</MathBox>

<MathBox title='The set of multilinear forms form vector spaces' boxType='proposition'>
Let $V$ be an $\mathbb{F}$-vector space with $\dim(V) = n$. The set of $k$-linear forms $V^{k\times} \to\mathbb{F}$, denoted $\mathcal{T}^k (V^*)$, is a vector space of dimension $n^k$.
</MathBox>

<MathBox title='Symmetric, skew-symmetric and alternating multilinear forms' boxType='definition'>
Let $f: V^{k\times} \to\mathbb{F}$ be a $k$-linear form. For any permutation $\sigma\in S_k$, define the $k$-linear form $\sigma f$ by

$$
  (\sigma f)(\boldsymbol{v}_1,\dots,\boldsymbol{v}_k) = f(\boldsymbol{v}_{\sigma_1},\dots,\boldsymbol{v}_{\sigma_k})
$$

A $k$-linear form is
1. *symmetric* if $\sigma f = f$ for every permutation $\sigma\in S_k$
2. *skew-symmetric* if $\tau f = -f$ for every transposition $\tau\in S_k$
3. *alternating* if $f(x_1,\dots,x_k) = 0$ whenever $x_i = x_j$ for $i \neq j$
</MathBox>

<MathBox title='Relation between alternating and skew-symmetric forms' boxType='proposition'>
Every alternating form is skew-symmetric.

<details>
<summary>Proof</summary>

Let $f: V^{k\times} \to\mathbb{F}$ be an alternating $k$-form, and choose $\boldsymbol{v}_i = \boldsymbol{v}_j$ for $i \neq j$. Without loss of generality we consider an alternating bilinear form $g:V\times V\to\mathbb{F}$, because all except two variables are fixed. Since $g$ is alternating, we have that

$$
\begin{align*}
  0 =& g(\boldsymbol{v}_i + \boldsymbol{v}_j, \boldsymbol{v}_i + \boldsymbol{v}_j) \\
  =& \underbrace{g(\boldsymbol{v}_i, \boldsymbol{v}_i)}_{=0} + \underbrace{g(\boldsymbol{v}_j, \boldsymbol{v}_j)}_{=0} + g(\boldsymbol{v_i}, \boldsymbol{v}_j) + g(\boldsymbol{v}_j, \boldsymbol{v_i}) \\
  =& g(\boldsymbol{v_i}, \boldsymbol{v}_j) + g(\boldsymbol{v}_j, \boldsymbol{v_i})
\end{align*}
$$

Rearranging gives $g(\boldsymbol{v_i}, \boldsymbol{v}_j) = -g(\boldsymbol{v}_j, \boldsymbol{v_i})$, showing that $g$ is skew-symmetric.
</details>
</MathBox>

<MathBox title='Equivalence of skew-symmetric and alternating forms' boxType='corollary'>
If $1 + 1 \neq 0$ in $\mathbb{F}$, then every skew-symmetric form is alternating. If $1 + 1 = 0 \iff 1 = -1$ in $\mathbb{F}$, then symmetry and skew-symmetry are equivalent. In this case, a $k$-linear form $f$ can be decomposed into a symmetric and a skew-symmetric part, i.e.

$$
  f = \underbrace{\sum_{\sigma \in S_n} \frac{1}{k!}f(\boldsymbol{v}_{\sigma(1)},\dots,\boldsymbol{v}_{\sigma(k)})}_{\text{symmetric}} + \underbrace{\sum_{\sigma \in S_n} \frac{1}{k!}\operatorname{sgn}(\sigma) f(\boldsymbol{v}_{\sigma(1)},\dots,\boldsymbol{v}_{\sigma(k)})}_{\text{skew-symmetric}}
$$

<details>
<summary>Proof</summary>

Assume $1 + 1 \neq 0$ in $\mathbb{F}$. Let $f: V^{k\times} \to\mathbb{F}$ be a skew-symmetric $k$-form. Without loss of generality we consider a skew-symmetric bilinear form $g:V\times V\to\mathbb{F}$, because all except two variables are fixed. Since $g$ is skew-symmetric

$$
\begin{align*}
  & g(\boldsymbol{v}, \boldsymbol{v}) = -g(\boldsymbol{v}, \boldsymbol{v}) \\
  \iff& 2g(\boldsymbol{v}, \boldsymbol{v}) = 0 \\
  \iff& g(\boldsymbol{v}, \boldsymbol{v}) = 0
\end{align*}
$$

Hence, $g$ is alternating if $2 \neq 0$ in $\mathbb{F}$.
</details>
</MathBox>

<MathBox title='Alternating forms and linear dependence' boxType='proposition'>
If $f: V^{k \times}\to\mathbb{F}$ is an alternating $k$-form and $\boldsymbol{v}_1,\dots,\boldsymbol{v}_k$ is linearly dependent, then $f(\boldsymbol{v}_1,\dots,\boldsymbol{v}_k) = 0$.

<details>
<summary>Proof</summary>

The case $\boldsymbol{v}_1 = \cdots = \boldsymbol{v}_k = \boldsymbol{0}$ is trivial. Assume $\boldsymbol{v}_j = \sum_{\substack{i=1 \\ i\neq j}}^k \lambda_i \boldsymbol{v}_i$, then

$$
\begin{align*}
  f(\boldsymbol{v}_1,\dots,\boldsymbol{v}_j,\dots,\boldsymbol{v}_k) =& f\left(\boldsymbol{v}_1,\dots, \sum_{\substack{i=1 \\ i\neq j}}^k \lambda_i \boldsymbol{v}_i,\dots,\boldsymbol{v}_k \right) \\
  =& \sum_{\substack{i=1 \\ i\neq j}}^k a_i f(\boldsymbol{v}_1,\dots,\boldsymbol{v}_i,\dots,\boldsymbol{v}_{j-1},\boldsymbol{v}_i,\boldsymbol{j+1},\dots,\boldsymbol{v}_k) \\
  =& 0
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Alternating forms and linear independence' boxType='proposition'>
Let $V$ be an $n$-dimensional $\mathbb{F}$-vector space. If $f: V^{k \times}\to\mathbb{F}$ is a nonzero alternating $n$-linear form and $B = \Set{\boldsymbol{b}_i}_{i=1}^{n}$ is a basis for $V$, then $f(\boldsymbol{b}_1,\dots,\boldsymbol{b}_n) \neq 0$.

<details>
<summary>Proof</summary>

Choose $\boldsymbol{v}_1,\dots,\boldsymbol{v}_n \in V$ and write each $\boldsymbol{v}_i = \sum_{j=1}^n \lambda_{i,j}\boldsymbol{b}_j$, then

$$
\begin{align*}
  f(\boldsymbol{v}_1,\dots,\boldsymbol{v}_n) =& f\left(\sum_{j=1}^n \lambda_{1,j}\boldsymbol{b}_j,\dots,\sum_{j=1}^n \lambda_{n,j}\boldsymbol{b}_j \right) \\
  =& \sum_{\sigma\in S_n} c_\sigma f(\boldsymbol{b}_{\sigma(1)},\dots,\boldsymbol{b}_{\sigma(n)}) \\
  =& f(\boldsymbol{b}_1,\dots,\boldsymbol{b}_n) \sum_{\sigma\in S_n} c_\sigma \operatorname{sgn}(\sigma)
\end{align*}
$$

This show that $f(\boldsymbol{b}_1,\dots,\boldsymbol{b}_n) = 0 \implies f(\boldsymbol{\boldsymbol{v}_1,\dots,\boldsymbol{v}_n}$, contradicting that $f$ is nonzero.
</details>
</MathBox>

<MathBox title='The subspace of alternating forms is 1-dimensional' boxType='corollary'>
Let $V$ be an $n$-dimensional $\mathbb{F}$-vector space. The subspace of alternating linear $n$-forms is $1$-dimensional, that is
1. Any two alternating $n$-linear forms on $V$ are linearly dependent.
2. There is a nonzero alternating $n$-linear form on $V$. 

<details>
<summary>Proof</summary>

**(1):** Let $B = \Set{\boldsymbol{b}_i}_{i=1}^{n}$ is a basis for $V$ and assume $g(\boldsymbol{e}_1,\dots,\boldsymbol{e}_n) = \lambda f(\boldsymbol{e}_1,\dots,\boldsymbol{e}_n)$. Then we need to show that $g(\boldsymbol{v}_1,\dots,\boldsymbol{v}_n) = \lambda g(\boldsymbol{v}_1,\dots,\boldsymbol{v}_n)$ for any $\boldsymbol{v}_1,\dots,\boldsymbol{v}_n \in V$

$$
\begin{align*}
  g(\boldsymbol{v}_1,\dots,\boldsymbol{v}_n) =& g\left(\sum_{j=1}^n \lambda_{1,j}\boldsymbol{b}_j,\dots,\sum_{j=1}^n \lambda_{n,j}\boldsymbol{b}_j \right) \\
  =& \sum_{\sigma\in S_n} c_\sigma g(\boldsymbol{b}_{\sigma(1)},\dots,\boldsymbol{b}_{\sigma(n)}) \\
  =& g(\boldsymbol{b}_1,\dots,\boldsymbol{b}_n) \sum_{\sigma\in S_n} c_\sigma \operatorname{sgn}(\sigma) \\
  =& lambda f(\boldsymbol{b}_1,\dots,\boldsymbol{b}_n) \sum_{\sigma\in S_n} c_\sigma \operatorname{sgn}(\sigma) \\
  =& \lambda f(\boldsymbol{v}_1,\dots,\boldsymbol{v}_n)
\end{align*}
$$

**(2):** This can be shown with induction on $k \leq n$ for a fixed $n$. The base $k=1$ is trivial since any nonzero $1$-form $V^* \ni f: V\to\mathbb{F}$ is by definition alternating. For the inductive hypothesis, assume $f(\boldsymbol{v}_1,\dots,\boldsymbol{v}_n)$ is a nonzero alternating $k$-form. We need to construct a nonzero alternating $(k+1)$-form $g(\boldsymbol{v}_1,\dots,\boldsymbol{v}_{k+1})$.

Let $U = \operatorname{span}(\boldsymbol{v}_1,\dots,\boldsymbol{v}_k)$ and $\boldsymbol{v}_{k+1} \notin U$. Choose $\ell\in U^0$ such that $\ell(\boldsymbol{v}_{k+1}) \neq 0$. Define

$$
\begin{align*}
  g(\boldsymbol{v}_1,\dots,\boldsymbol{v}_{k+1}) =& -\underbrace{f(\boldsymbol{v}_1,\dots,\boldsymbol{v}_k)}_{\neq 0}\underbrace{\ell(\boldsymbol{v}_{k+1})}_{\neq 0} \\
  =& -f(\boldsymbol{v}_1,\dots,\boldsymbol{v}_k)\ell(\boldsymbol{v}_{k+1}) \\
  &+ \underbrace{\sum_{i=1} f(\boldsymbol{v}_1,\dots,\boldsymbol{v}_{i-1},\boldsymbol{v}_{k+1},\boldsymbol{v}_{i+1}\dots,\boldsymbol{v}_k)\ell(\boldsymbol{v}_i)}_{=0}
  \neq& 0
\end{align*}
$$

This shows that $g$ is a nonzero linear $(k+1)$-form. It remains to show that $g$ is alternating. In the case $\boldsymbol{v}_i = \boldsymbol{v}_j$ for $i, j < k+1$, the following terms of $g$ do not vanish

$$
\begin{align*}
  g(\boldsymbol{v}_1,\dots,\boldsymbol{v}_{k+1}) =& f(\boldsymbol{v}_1,\dots,\boldsymbol{v}_{i-1},\boldsymbol{v}_{j},\boldsymbol{v}_{i+1}\dots,\boldsymbol{v}_k)\ell(\boldsymbol{v}_i) \\
  &+ f(\boldsymbol{v}_1,\dots,\boldsymbol{v}_{j-1},\boldsymbol{v}_{i},\boldsymbol{v}_{j+1}\dots,\boldsymbol{v}_k)\ell(\boldsymbol{v}_j)
\end{align*}
$$

Note that, $f(\boldsymbol{v}_1,\dots,\boldsymbol{v}_{i-1},\boldsymbol{v}_{j},\boldsymbol{v}_{i+1}\dots,\boldsymbol{v}_k) = -f(\boldsymbol{v}_1,\dots,\boldsymbol{v}_{j-1},\boldsymbol{v}_{i},\boldsymbol{v}_{j+1}\dots,\boldsymbol{v}_k)$, which shows that $g$ is alternating in this case.

In the case $\boldsymbol{v}_i = \boldsymbol{v}_{k+1}$ for $i < k+1$, note that $\ell(\boldsymbol{v}_i) = 0$, since $\boldsymbol{v}_{k+1}\in U$. Thus, $g(\boldsymbol{v}_1,\dots,\boldsymbol{v}_{k+1}) = 0$ in this case.
</details>
</MathBox>

## Determinant of linear operators

Let $\mathrm{T}:V\to V$ be a linear operator on an $\mathbb{F}$-vector space $V$. For an alternating linear $n$-form $f$, define a new alternating $n$-form $\bar{\mathrm{T}}f : V^n \to \mathbb{F}$ by

$$
  (\bar{\mathrm{T}}f)(\boldsymbol{v}_1,\dots,\boldsymbol{v}_n) = f(\mathrm{T}\boldsymbol{v}_1,\dots,\mathrm{T}\boldsymbol{v}_n)
$$

That is, $\mathrm{T}$ induces a map $\bar{\mathrm{T}}$ on the ($1$-dimensional) space of alternating linear $n$-form mapping $f\mapsto \bar{\mathrm{T}}f$. However, since any linear map on a $1$-dimensional space is just scalar multiplication. Thus,

$$
  \bar{\mathrm{T}}: f \mapsto\lambda f,\; \lambda\in\mathbb{F}
$$

The scalar $\lambda$ is the *determinant* of $\mathrm{T}$. The determinant of $\mathrm{T}:\R^n \to \R^n$ is the unique alternating linear $n$-form satisfying $\mathrm{T}(\boldsymbol{e}_1,\dots,\boldsymbol{e}_n) = 1$, where $E = \Set{\boldsymbol{e}_i}_{i=1}^n$ is the canonical basis.

<MathBox title='Universal property of the determinant' boxType='definition'>
Let $V$ be an $n$-dimensional $\mathbb{F}$-vector space. Given a linear operator $\mathrm{T}:V\to V$, there exists a unique scalar $\lambda\in\mathbb{F}$ such that for every alternating linear $n$-form $f$

$$
  f(\mathrm{T}\boldsymbol{v}_1,\dots,\mathrm{T}\boldsymbol{v}_n) = \lambda f(\boldsymbol{v}_1,\dots,\boldsymbol{v},\boldsymbol{x}_n)
$$

$$
\begin{CD}
V^n @>{\mathrm{T}^n}>> V^n \\
@V{f}VV @VV{f}V \\
\mathbb{F} @>>{\lambda}> \mathbb{F}
\end{CD}
$$
</MathBox>

<MathBox title='Properties of the determinant' boxType='definition'>
Let $\mathrm{T}:V\to V$ be a linear operator on an $n$-dimensional $\mathbb{F}$-vector space $V$. 
1. If $\mathrm{T}\boldsymbol{v} = \alpha\boldsymbol{v}$, then $\det(\mathrm{T}) = \alpha^n$
2. $\det(0) = 0$ and $\det(\mathrm{I}) = 1$
3. For any two linear operators $\mathrm{A},\mathrm{B}:V\to V$, then $\det(\mathrm{AB}) = \det(\mathrm{A})\det(\mathrm{B})$
4. If $\mathrm{A}:V\to V$ is invertible, then $\det(\mathrm{A}^{-1}) = \frac{1}{\det(\mathrm{A})} \neq 0$ 

<details>
<summary>Proof</summary>

**(1):** Calculating $\det(\mathrm{T})$ gives

$$
\begin{align*}
  (\bar{\mathrm{T}}f)(\boldsymbol{v}_1,\dots,\boldsymbol{v}_n) =& f(\mathrm{T}\boldsymbol{v}_1,\dots,\mathrm{T}\mathrm{v}) \\
  =& f(\alpha\boldsymbol{v}_1,\dots,\alpha\boldsymbol{v}_n) \\
  =& \alpha^n f(\boldsymbol{v}_1,\dots,\boldsymbol{v}_n)
\end{align*}
$$

**(3):**

$$
\begin{align*}
  \det(\mathrm{AB})f(\boldsymbol{v}_1,\dots,\boldsymbol{v}_n) =& (\bar{\mathrm{AB}}f)(\boldsymbol{v}_1,\dots,\boldsymbol{v}_n) \\
  =& f(\mathrm{A}(\mathrm{B}\boldsymbol{v}_1),\dots,\mathrm{A}(\mathrm{B}\boldsymbol{v}_n)) \\
  =& (\bar{\mathrm{A}}f)(\mathrm{B}\boldsymbol{v}_1,\dots,\mathrm{B}\boldsymbol{v}_n) \\
  =& \det(\mathrm{A})f(\mathrm{B}\boldsymbol{v}_1,\dots,\mathrm{v}_n) \\
  =& \det(\mathrm{A})(\bar{\mathrm{B}})(\boldsymbol{v}_1,\dots,\mathrm{v}_n) \\
  =& \det(\mathrm{A}\det(\mathrm{B})f(\boldsymbol{v}_1,\dots,\boldsymbol{v}_n)
\end{align*}
$$

**(4):** Noting that $\mathrm{AA}^{-1} = \mathrm{I}$ and $\det(\mathrm{I}) = 1$, we get

$$
\begin{gather*}
  1 = \det(\mathrm{I}) = \det(\mathrm{AA}^{-1}) = \det(\mathrm{A})\det(\mathrm{A}^{-1}) \\
  \iff \det(\mathrm{A}^{-1}) = \frac{1}{\det(\mathrm{A})} 
\end{gather*}
$$
</details>
</MathBox>

# Module

<MathBox title='Module' boxType='definition'>
Let $R$ be a commutative ring with identity $1$, whose elements are called scalars. A *module* is a set $M$ over $R$ equipped with the two closed operations
- $+: M\times M \ni (\boldsymbol{u},\boldsymbol{v}) \mapsto \boldsymbol{u} + \boldsymbol{v} \in M$ **(addition)**
- $\cdot: R\times M \ni (\rho, \boldsymbol{u}) \mapsto \rho\boldsymbol{u} \in M$ **(scalar multiplication)**

and has the following properties
- $M$ is an abelian group under addition
- Scalar multiplication is compatible, satisfying for all $\alpha,\beta\in R$ and $\boldsymbol{x}\in M$
    - $(\alpha\beta)\boldsymbol{x} = \alpha(\beta\boldsymbol{x})$
    - $1\boldsymbol{x} = \boldsymbol{x}$
- Addition and scalar multiplication are related by distributivity for all $r,s\in R$ and $x, y\in M$
    - $\alpha(\boldsymbol{x} + \boldsymbol{y}) = \alpha\boldsymbol{x} + \alpha\boldsymbol{y}$
    - $(\alpha + \beta)\boldsymbol{x} = \alpha\boldsymbol{x} + \beta\boldsymbol{x}$

The ring $R$ is called the *base ring* of $M$.
</MathBox>

<MathBox title='Submodule' boxType='definition'>
A submodule of an $R$-module $M$ is a nonempty subset $N\subseteq M$ that is an $R$-module in its own right by restricting the operations of $M$ to $S$. We write $N \leq M$ to denote the fact that $N$ is a submodule of $M$.
</MathBox>

<MathBox title='' boxType='theorem'>
Let $M$ be an $R$-module. A nonempty subset $N\subseteq M$ is a submodule if and only if it is closed under the taking of linear combinations, i.e.

$$
  \alpha,\beta \in R, \boldsymbol{u},\boldsymbol{v}\in N \implies \alpha\boldsymbol{u} + \beta\boldsymbol{v} \in N
$$
</MathBox>

<MathBox title='' boxType='theorem'>
Let $M$ be an $R$-module. If $S$ and $T$ are submodules of $M$, then $S\cap T$ and $S + T$ are also submodules of $M$.
</MathBox>

When we consider $R$ as an $R$-module rather than as a ring, multiplication is treated as scalar multiplication. This has some important implications. In particular, if $S$ is a submodule of $R$, then it is closed under scalar multiplication, which means that it is closed under multiplication by all elements of the ring $R$. In other words, $S$ is an ideal of the ring $R$. Conversely, if $I$ is an ideal of the ring $R$, then $I$ is also a submodule of the $R$-module $R$. Hence, the submodules of the $R$-module $R$ are precisely the ideals of the ring $R$.

## Spanning sets

<MathBox title='Spanned submodules' boxType='definition'>
The submodule spanned or generated by a subset $N$ of an $R$-module $M$ is the set of all linear combinations of elements of $N$

$$
  \langle N \rangle = \Set{\sum_{i=1}^n \alpha_i \boldsymbol{v}_i | \alpha_i \in R, \boldsymbol{v}_i \in N, n\geq 1}
$$

A subset $N\subseteq M$ spans or generates $M$ if $M = \langle N \rangle$.
</MathBox>

Note that if a nontrivial linear combination of the elements $\boldsymbol{v}_1,\dots,\boldsymbol{v}_n$ in an $R$-module $M$ is $0$, i.e. $\sum_{i=1}^n = \rho_i \boldsymbol{v}_i = 0$, where not all of the coefficients are $0$, then we cannot conclude that one of the elements $\boldsymbol{v}_i$ is a linear combination of the others. After all, this involves dividing by one of the coefficients, which may not be possible in a ring.

<MathBox title='Cyclic submodule' boxType='definition'>
Let $M$ be an $R$-submodule. A submodule of the form

$$
  \langle \boldsymbol{v} \rangle = R\boldsymbol{v} = \Set{\rho\boldsymbol{v} | \rho\in R}
$$

for $\boldsymbol{v}\in M$ is the *cyclic submodule* generated by $\boldsymbol{v}$.
</MathBox>

Any finite-dimensional vector space is the direct sum of cyclic submodules, i.e. one-dimensional subspaces.

<MathBox title='Finitely generated submodule' boxType='definition'>
An $R$-module $M$ is *finitely generated* if it contains a finite set that generates $M$. More specifically, $M$ is $n$-generated if it has a generating set of size $n$.
</MathBox>

A vector space if finitely generated if and only if it has a finite basis, or equivalently, if and only if it is finite-dimensional. This is not alway the case for modules as illustrated in the following example.  

<MathBox title='' boxType='example'>
Let $R$ be the ring $\mathbb{F}[x_1,x_2,\dots]$ of all polynomials in infinitely many variables over a field $\mathbb{F}$. For convenience we denote $x_1,x_2,\dots$ by $X$ and write a polynomial in $R$ in the form $p(X)$. (Each polynomial in $R$, being a finite sum, involves only finitely many variables). Then $R$ is an $R$-module and as such, is finitely generated by the identity element $p(X) = 1$.

Now consider that submodule $S$ of all polynomials with zero constant term. This module is generated by the variables themselves, i.e. $S = \langle x_1,x_2,\dots \rangle$. However, $S$ is not finitely generated. To see this, suppose $G = \Set{p_1,\dots,p_n}$ is a finite generating set for $S$. Choose a variable $x_k$ that does not appear in any of the polynomials in $G$. Then no linear combination of the polynomials in $G$ can be equal to $x_k$. For if $x_k = \sum_{i=1}^n a_i (X) p_i (X)$, then let $a_i (X) = x_k q_i (X) + r_i (X)$ where $r_i (X)$ does not involve $x_k$. This gives

$$
\begin{align*}
  x_k =& \sum_{i=1}^n [x_k q_i (X) + r_i (X)]p_i (X) \\
  =& x_k \sum_{i=1}^n q_i (X) p_i (X) + \sum_{i=1}^n r_i (X) p_i (X)
\end{align*}
$$

The last sum does not involve $x_k$ and so it must equal $0$. Hence, the first sum must equal $1$, which is not possible since $p_i(X)$ has no constant term.
</MathBox>

## Linear independence

<MathBox title='Linear independence for modules' boxType='definition'>
A subset $N$ of an $R$-module $M$ is *linearly independent* if for any distinct $\boldsymbol{v}_1,\dots,\boldsymbol{v}_n \in N$ and $\rho_1,\dots,\rho_n\in R$, we have

$$
  \sum_{i=1} \rho_i \boldsymbol{v}_i = 0 \implies \rho_i = 0,\; \forall i
$$

A set $N$ that is not linearly independent is *linearly dependent*.
</MathBox>

In a vector space $V$ over a field $\mathbb{F}$, singleton sets $\Set{v}$, where $\boldsymbol{v} \neq\boldsymbol{0}$ are linearly independent. However, in module this need no be the case.

<MathBox title='Singletons of modules are not necessarily linearly independent' boxType='example'>
The abelian group $\Z_n = \Set{0,1,\dots,n-1}$ is a $\Z$-module, with scalar multiplication defined by $za = (za) \mod n$ for all $z\in\Z$ and $a\in Z_n$. However, since $na = 0$ for all $a \in\Z_n$, no singleton set $\Set{a}$ is linearly independent. Indeed, $\Z_n$ has no linearly independent sets.
</MathBox>

<MathBox title='Torsion element' boxType='definition'>
Let $M$ be an $R$-module. A nonzero element $\boldsymbol{v}\in M$ for which $\rho\boldsymbol{v} = 0$ for some nonzero $\rho\in R$ is a *torsion element* of $M$. A module that has no nonzero torsion elements is *torsion free*. If all elements of $M$ are  torsion elements, then $M$ is a *torsion module*. The set of all torsion elements of $M$, together with the zero element, is denoted $M_{\mathrm{tor}}$.
</MathBox>

## Annihilators

<MathBox title='Annihilator' boxType='definition'>
Let $M$ be an $R$-module. The *annihilator* of an element $\boldsymbol{v}\in M$ is

$$
  \operatorname{ann}(\boldsymbol{v}) = \Set{\rho\in R | \rho\boldsymbol{v} = 0}
$$

and the annihilator of a submodule $N$ of $M$ is

$$
  \operatorname{ann}(N) = \Set{\rho\in R | \rho N = \Set{0}}
$$

where $\rho N = \Set{\rho\boldsymbol{v} | \boldsymbol{v}\in N}$. Annihilators are also called *order ideals*.
</MathBox>

It is easy to see that $\operatorname{ann}(\boldsymbol{v})$ and $\operatorname{ann}(N)$ are ideals of $R$. Clearly, $\boldsymbol{v}\in M$ is a torsion element if and only if $\operatorname{ann}(\boldsymbol{v}) \neq\Set{0}$. Also, if $A$ and $B$ are submodules of $M$, then

$$
  A \leq B \implies\operatorname{ann}(B) \leq\operatorname{ann}(A)
$$

Let $M = \langle \boldsymbol{u}_1,\dots,\boldsymbol{u}_n \rangle$ be a finitely generated module over an integral domain $R$ and assume each of the generators $\boldsymbol{u}_i$ is torsion, i.e. there is a nonzero $\alpha_i \in\operatorname{ann}(\boldsymbol{u}_i)$ for each $i$. Then the nonzero product $\alpha = \prod_{i=1}^n \alpha_i$ annihilates each generator of $M$ and therefore every element of $M$, i.e. $\alpha\in\operatorname{ann}(M)$. This shows that $\operatorname{ann}(M) \neq\Set{0}$. On the other hand, this may fail if $R$ is not an integral domain.

## Homomorphisms

<MathBox title='Homomorphism' boxType='definition'>
Let $M$ and $N$ be $R$-modules. A function $f:M\to N$ is an $R$-homomorphism or $R$-map if it preserves the module operations, i.e. for all $\rho,\sigma\in R$ and $\boldsymbol{u},\boldsymbol{v}\in M$

$$
  f(\rho\boldsymbol{u} + \sigma\boldsymbol{v}) = \rho f(\boldsymbol{u}) + \sigma f(\boldsymbol{v})
$$

The set all $R$-homomorphisms from $M$ to $N$ is denoted $\hom_R (M, N)$. The following terms are used to classify $R$-maps
- an $R$-endomorphism is an $R$-homomorphism from $M$ to itself
- an $R$-monomorphism or $R$-embedding is an injective $R$-homomorphism
- an $R$-epimorhism is a surjective $R$-homomorphism
- an $R$-isomorphism is a bijective $R$-homomorphism
</MathBox>

It is east to see that $\hom_R (M, N)$ is itself an $R$-module under addition of functions and scalar multiplication defined by

$$
  (\rho f)(\boldsymbol{v}) = \rho(f(\boldsymbol{v})) = f(\rho\boldsymbol{v})
$$

<MathBox title='Kernel and range of homomorphisms' boxType='definition'>
Let $f:M\to N$ be an $R$-map. The *kernel* and *range* or *image* of $f$ are defined as

$$
\begin{align*}
  \ker(f) =& \Set{\boldsymbol{v} \in M | f(\boldsymbol{v}) = 0} \\
  \operatorname{ran}(f) =& \Set{f(\boldsymbol{v}) | \boldsymbol{v} \in M}
\end{align*}
$$
</MathBox>

<MathBox title='The kernel and image of homomorphisms are submodules' boxType='theorem'>
If $f\in\hom_R (M, N)$ is an $R$-map, the kernel and range of $f$ are submodules of $M$ and $N$ respectively. Moreover, $f$ is a monomorphism if and only if $\ker(f) = \Set{0}$. 
</MathBox>

If $N$ is a submodule of the $R$-module $M$, the map $j:N\to M$ defined by $j(\boldsymbol{v}) = \boldsymbol{v}$ is evidently an $R$-monomorphism, called the *injection* of $N$ into $M$.

## Direct sums and direct summands

<MathBox title='External direct sum of modules' boxType='definition'>
The *external direct sum* of $R$-modules $M_1,\dots,M_n$ denoted

$$
  M = M_1 \boxplus\cdots \boxplus M_n
$$

is the $R$-module whose elements are ordered $n$-tuples

$$
  M = \Set{(\boldsymbol{v}_i)_{i=1}^n | \boldsymbol{v}_i \in M_i, i=1,\dots,n}
$$

with componentwise operations

$$
\begin{align*}
  (\boldsymbol{u}_1,\dots,\boldsymbol{u}_n) + (\boldsymbol{v}_1,\dots,\boldsymbol{v}_n) =& (\boldsymbol{u}_1 + \boldsymbol{v}_1, \dots, \boldsymbol{u}_n + \boldsymbol{v}_n) \\
  \rho(\boldsymbol{v}_1,\dots,\boldsymbol{v}_n) =& (\rho\boldsymbol{v}_1,\dots,\rho\boldsymbol{v}_n)
\end{align*}
$$

for $\rho\in R$.
</MathBox>

<MathBox title='Internal direct sum of modules' boxType='definition'>
An $R$-module $M$ is the *(internal) direct sum* of a family $\mathcal{F} = \Set{S_i \subseteq M}_{i \in I}$ of submodules of $M$ denoted

$$
  M = \bigoplus_{i\in I} S_i
$$

satisfying the following
1. **Join of the family:** $M$ is the sum (join) of the family $\mathbb{F}$, i.e. $M = \sum_{i\in I} S_i$
2. **Independence of the family:** For each $i\in I$, then $S_i \cap \left(\sum_{j\neq i} S_j \right) = \Set{0}$

In this case, each $S_i$ is called a *direct summand* of $M$. If $M = S \oplus T$, then $S$ is *complemented* by $T$ and $T$ is a *complement* of $S$ in $M$.
</MathBox>

<MathBox title='' boxType='theorem'>
If $\mathcal{F} = \Set{S_i}_{i\in I}$ is a family of distinct submodules of an $R$-module $M$, the following are equivalent
1. **Independence of the family:** For each $i\in I$, then $S_i \cap \left(\sum_{j\neq i} S_j \right) = \Set{0}$
2. **Uniqueness of zero element:** The zero element $0$ cannot be written as a sum of nonzero elements from distinct submodules in $\mathcal{F}$
3. **Uniqueness of expression:** Every nonzero $\boldsymbol{v}\in M$ has a unique, except for order of terms, expression as a sum $\boldsymbol{v} = \sum_{i=1}^n \boldsymbol{s}_n$ of nonzero elements from distinct submodules in $\mathcal{F}$.
</MathBox>

<MathBox title='' boxType='theorem'>
Let $S$ be a complemented submodule of $M$. All complements of $S$ are isomorphic to $M\setminus S$ and hence to each other.

<details>
<summary>Proof</summary>

For any complement $T$ of $S$, the first isomorphism theorem applied to the projection $\rho_{T,S}:M\to T$ gives $T \cong M\setminus S$.
</details>
</MathBox>

### Direct summands and extensions of isomorphisms

<MathBox title='' boxType='theorem'>
Let $M$ and $M_1$ be $R$-modules and let $N\leq M$.
1. If $M = N \oplus H$, then any $R$-epimorphism $f: N\to M_1$ has a unique extension $\bar{f}:M\to M_1$ to an epimorphism with
$$
  \ker(\bar{f}) = \ker(f) \oplus H
$$
2. Let $f:N\cong M$ be an $R$-isomorphism. Then the correspondence $H\mapsto \bar{f}$, where $\ker(\bar{f}) = H$ is a bijection from complements of $N$ onto the extensions of $f$. Thus, and isomorphism $f:N\cong M_1$ has an extension to $M$ if and only iff $N$ is complemented.

<details>
<summary>Proof</summary>

**(1):** If $M = N\oplus H$, then a module epimorphism $f:N\to M_1$ can be extended to an epimorphism $\bar{f}:M\to M_1$ simply be sending the elements of $H$ to zero, i.e. $\bar{f}(\boldsymbol{n}+\boldsymbol{h}) = f(\boldsymbol{n})$. This is easily seen to be an $R$-map with $\ker(\bar{f}) = \ker(f) \oplus H$. Moreover, if $g$ is another extension of $f$ with the same kernel as $\bar{f}$, then $\tau$ and $\bar{f}$ agree on $H$ as well as on $N$, thus $\tau = \bar{f}$. Thus, there is a unique extension of $f$ with kernel $\ker(f)\oplus H$.

**(2):** Suppose that $f: N\cong M_1$ is an isomorphism. If $N$ is complemented, i.e. if $G = N \oplus H$, then there is a unique extension $\bar{f}$ of $f$ for which $\ker(\bar{f}) = H$. Thus, the correspondence $H\mapsto\bar{f}$, where $\ker(f) = H$, from complements of $N$ to extensions of $f$ is an injection. To see that this correspondence is a bijection, if $\bar{f}:M\to M_1$ is an extension of $f$, then $M = N\oplus\ker(\bar{f})$. To see this, we have

$$
  N \cap\ker(\bar{f}) = \ker(f) = \Set{0}
$$

and if $\boldsymbol{a}\in M$, then there is a $\boldsymbol{b}\in N$, for which $f \boldsymbol{b} = \bar{f}\boldsymbol{a}$ and so

$$
  \bar{f}(\boldsymbol{a} - \boldsymbol{b}) = \bar{f}\boldsymbol{a} - f \boldsymbol{b} = 0
$$

Thus,

$$
  \boldsymbol{a} = \boldsymbol{b} + (\boldsymbol{a} - \boldsymbol{b}) \in N + \ker(\bar{f})
$$

which shows that $\ker(\bar{f})$ is a complement of $N$.
</details>
</MathBox>

<MathBox title='Retract' boxType='definition'>
Let $N$ be a submodule of an $R$-module $M$. When the identity map $\operatorname{id}: N\cong N$ has an extension to $f:M\to N$, the submodule $N$ is called a *retract* of $M$ and $f$ is called the *retraction map*.
</MathBox>

<MathBox title='' boxType='corollary'>
A submodule $N \leq M$ is a retract of $M$ if and only if $N$ has a complement in $M$.
</MathBox>

### Direct summands and one-sided invertibility

<MathBox title='Invertibility of module homomorphisms' boxType='definition'>
Let $f: A\to B$ be a module homomorphism.
1. A *left inverse* of $f$ is a module homomorphism $f_L: B\to A$ for which $f_L \circ f = \operatorname{id}$
2. A *right inverse* of $f$ is a module homomorphism $f_R: B\to A$ for which $f\circ f_R = \operatorname{id}$

Left and right inverses are called *one-sided inverses*. An ordinary inverse is called a *two-sided inverse*.
</MathBox>

<MathBox title='' boxType='theorem'>
Let $M$ and $M_1$ be $R$-modules and let $f :M\to M_1$ be an $R$-map.
1. Let $f:M\hookrightarrow M_1$ be injective. The map 
$$
  H \mapsto \text{extension of } (f|^{\operatorname{ran}(f)})^{-1} \text{ with kernel } H
$$
is a bijection from the complements $H$ of $\operatorname{ran}(f)$ onto the left inverses of $f$. Thus, there is exactly one left inverse of $f$ for each complement of $\operatorname{ran}(f)$ and that complement is the kernel of the left inverse.
2. Let $f:M\to M_1$ be surjective. The map $H\mapsto (f|_H)^{-1}: M\to M_1$ is a bijection from the complements of $H$ of $\ker(f)$ to the right inverses of $f$. Thus, there is exactly one right inverse of $f$ for each complement of $\ker(f)$ and that complement is the range of the right inverse. Thus,
$$
  M = \ker(f)\oplus H \cong\ker(f)\boxplus\operatorname{ran}(f)
$$

<details>
<summary>Proof</summary>

**(1):** A left-invertible homomorphism $f$ must be injective, since

$$
  f a = f b \implies f_L \circ f a = f_L \circ f b \implies a = b
$$

Also, a right-invertible homomorphism $f: A\to B$ must be surjective, since if $b\in B$, then

$$
  b = f(f_R(b)) \in \operatorname{ran}(f)
$$

For set of functions, converses of these statements hold: $f$ is left-invertible if and only if it is injective, and $f$ is right-invertible if and only if it is surjective. However, this is not the case for $R$-maps.

Let $f:M \to M_1$ be an injective $R$-map. The map $f|^{\operatorname{ran}(f)}: M\cong\operatorname{ran}(f)$ obtained from $f$ by restricting its range to $\operatorname{ran}(f)$ is an isomorphism and the left inverses $f_L$ of $f$ are precisely the extensions of $(f|^{\operatorname{ran}(f)})^{-1}:\ker(f)\cong M$ to $M_1$. By the previous theorem, the correspondence

$$
  H \mapsto \text{extension of } (f|^{\operatorname{ran}(f)})^{-1} \text{ with kernel } H
$$

is a bijection from the complements $H$ of $\operatorname{ran}(f)$ onto the left inverses of $f$.

**(2):** Let $f:M\to M_1$ be a surjective $R$-map. If $\ker(f)$ is complemented, i.e. if $M = \ker(f)\oplus H$, then $f|_H :H\cong M$ is an isomorphism. Thus, a map $\tau:M_1 \to M$ is a right inverse of $f$ if and only if $\tau$ is a range-extension of $(f|_H)^{-1}:M_1 \to M$, the only difference being in the ranges of the two functions. Thus, $(f|_H)^{-1}:M_1 \to M$ is the only right inverse of $f$ with range $H$. It follows that the correspondence

$$
  H \mapsto (f|_H)^{-1}: M_1 \to M
$$

is an injection from the complements $H$ of $\ker(f)$ to the right inverses of $f$. Moreover, this map is a bijection, since if $f_R:M_1\to M$ is a right-inverse of $f$, then $f_R: M_1 \cong\operatorname{ran}(f_R)$ and $f$ is an extension of $f_R^{-1} :\operatorname{ran}(f_R) \cong M_1$, which implies that

$$
  M = \operatorname{ran}(f_R)\oplus\ker(f)
$$

</details>
</MathBox>

## Free modules

<MathBox title='Free module' boxType='definition'>
Let $M$ be an $R$-module. A subset $B \subseteq M$ is a *basis* if $B$ is linearly independent and spans $M$. An $R$-module $M$ is *free* if $M = \Set{0}$ or if $M$ has a basis. If $B$ is a basis for $M$, we say that $M$ is free on $B$.
</MathBox>

<MathBox title='' boxType='theorem'>
Let $M$ be an $R$-module. A subset $B\subseteq M$ is a basis if and only if every nonzero $v\in M$ is an essentially unique linear combination of the elements in $B$.
</MathBox>

<MathBox title='' boxType='theorem'>
Let $B$ be a basis for an $R$-module $M$. Then
1. $B$ is a minimal spanning set
2. $B$ is a maximal linearly independent set
</MathBox>

<MathBox title='Free module can have nonfree submodules' boxType='example'>
The set $\Z\times\Z$ is a free module over itself, using componentwise scalar multiplication

$$
  (n,m)(a,b) = (na,mb)
$$

with basis $\Set{(1,1)}$. However, the submodule $\Z\times\Set{0}$ is not free since it has no linearly independent elements and hence no basis.
</MathBox>

<MathBox title='' boxType='theorem'>
Let $M$ and $N$ be $R$-modules where $M$ is free with basis $B = \Set{\mathbb{b}_i}_{i\in I}$. Then we can define a unique $R$-map $f:M\to N$ by specifying the values of $f(\mathbb{b}_i)$, arbitrarily for all $\mathbb{b}_i \in B$ and then extending $f$ to $M$ by linearity, i.e.

$$
  f\left(\sum_{i=1} \alpha_i \boldsymbol{v}_i \right) = \sum_{i=1} \alpha_i f(\boldsymbol{v}_i)
$$
</MathBox>

### Rank of free modules
<MathBox title='' boxType='theorem'>
Let $M$ be a free module over a commutative ring $R$ with identity, then
1. Any two bases of $M$ have the same cardinality
2. The cardinality of a spanning set is greater than or equal to that of a basis.

<details>
<summary>Proof</summary>

The idea of the proof is to find a vector space $V$ with the property that, for any basis for $M$, there is a basis of the same cardinality for $V$. This allows us to apply the corresponding resul for vector spaces.

Let $\mathcal{I}$ be a maximal ideal of $R$. Then $R\setminus\mathcal{I}$ is a field. Note that $M$ is not a vector space over $R\setminus\mathcal{I}$. Scalar multiplication using the field $R\setminus\mathcal{I}$,

$$
  (\rho + \mathcal{I})\boldsymbol{v} = \rho\boldsymbol{v}
$$

is not well-defined, since this would require that $\mathcal{I}M = \Set{0}$. On the other hand, we can fix this issue by factoring out the submodule

$$
  \mathcal{I}M = \Set{\sum_{i=1} \alpha_i \boldsymbol{v}_i | \alpha_i \in\mathcal{I}, \boldsymbol{v}_i \in M}
$$

Indeed, $M\setminus \mathcal{I}M$ is a vector space over $R\setminus\mathcal{I}$, with scalar multiplication defined by

$$
  (\rho + \mathcal{I})(\boldsymbol{u} + \mathcal{I}M) = \rho\boldsymbol{u} + \mathcal{I}M
$$

To see that this is well-defined, we must show that the conditions

$$
\begin{align*}
  \rho + \mathcal{I} =& \rho' + \mathcal{I} \\
  \boldsymbol{u} + \mathcal{I}M =& \boldsymbol{u}' + \mathcal{I}M
\end{align*}
$$

imply

$$
  \rho\boldsymbol{u} + \mathcal{I}M = \rho'\boldsymbol{u}' + \mathcal{I}M
$$

But this follows from the fact that

$$
  \rho\boldsymbol{u} - \rho'\boldsymbol{u}' = \rho(\boldsymbol{u} - \boldsymbol{u}') + (\rho - \rho')\boldsymbol{u}' \in\mathcal{I}M 
$$

Hence, scalar multiplication is well-defined. It is easy to show that $M\setminus\mathcal{I}M$ is a vector space over $R\setminus\mathcal{I}$.

Consider a set $B = \Set{\boldsymbol{b}_i}_{i\in I}\subseteq M$ and the corresponding set

$$
  B + \mathcal{I}M = \Set{\boldsymbol{b}_i + \mathcal{I}M | i\in I} \subseteq M\setminus\mathcal{I}M
$$

If $B$ spans $M$ over $R$, then $B + \mathcal{I}M$ spans $M\setminus\mathcal{I}M$ over $R\setminus\mathcal{I}$. To see this, note that any $\boldsymbol{v}\in M$ has the form $\boldsymbol{v} = \sum \rho_i \boldsymbol{b}_i$ for $\rho_i \in R$ and so

$$
\begin{align*}
  \boldsymbol{v} + \mathcal{I}M =& \left(\sum_j \rho_{i_j} \boldsymbol{b}_{i_j} \right) + \mathcal{I}M \\
  =& \sum_j \rho_{i_j} (\boldsymbol{b}_{i_j} + \mathcal{I}M) \\
  =& \sum_j (\rho_{i_j} + \mathcal{I})(\boldsymbol{b}_{i_j} + \mathcal{I}M)
\end{align*}
$$

showing that $B + \mathcal{I}M$ spans $M\setminus\mathcal{I}M$.

Suppose that $B = \Set{\boldsymbol{b}_i}_{i\in I}$ is a basis for $M$ over $R$. We will show that $B + \mathcal{I}M$ is a basis for $M\setminus\mathcal{I}M$ over $R\setminus\mathcal{I}$. We have seen that $B + \mathcal{I}M$ spans $M\setminus\mathcal{I}M$. Also, if

$$
  \sum_j (\rho_{i_j} + \mathcal{I})(\boldsymbol{b}_{i_j} + \mathcal{I}M) = \mathcal{I}M
$$

then $\sum_j \rho_{i_j}\boldsymbol{b}_{i_j} \in\mathcal{I}M$ and so

$$
  \sum_j \rho_{i_j}\boldsymbol{b}_{i_j} = \sum_k \alpha_{i_k} \boldsymbol{b}_{i_k}
$$

where $\alpha_{i_k}\in\mathcal{I}$. From the linear independence of $B$ we deduce that $\rho_{i_j} \in\mathcal{I}$ for all $j$ and so $\rho_{i_j} + \mathcal{I} = \mathcal{I}$. Hence $B + \mathcal{I}M$ is linearly independent and therefore a basis.

To see that $|B| = |B + \mathcal{I}M|$, note that if $\boldsymbol{b}_i + \mathcal{I}M = \boldsymbol{b}_k + \mathcal{I}M$, then

$$
  \boldsymbol{b}_i - \boldsymbol{b}_k = \sum_j \alpha_{i_j} \boldsymbol{b}_{i_j}
$$

where $\alpha_{i_j}\in\mathcal{I}$. If $\boldsymbol{b}_i \neq \boldsymbol{b}_k$, then the coefficients of $\boldsymbol{b}_i$ on the right must be equal to $1$ and so $1\in\mathcal{I}$, which is not possible since $\mathcal{I}$ is a maximal ideal. Thus, $\boldsymbol{b}_i = \boldsymbol{b}_k$.

Hence, if $B$ is a basis for $M$ over $R$, then

$$
  |B| = |B + \mathcal{I}M| = \dim_{R\setminus\mathcal{I}}(M\setminus\mathcal{I}M)
$$

and so all bases for $M$ over $R$ have the same cardinality, which proves part **(1)**.

Finally, if $B$ spans $M$ over $R$, then $B + \mathcal{I}M$ spans $M\setminus\mathcal{I}M$ and so

$$
  \dim_{R\setminus\mathcal{I}} (M\setminus\mathcal{I}M) \leq |B + \mathcal{I}M| \leq |B|
$$

Thus, $B$ has cardinality at least as great as that of any basis for $M$ over $R$.
</details>
</MathBox>

<MathBox title='Rank of free modules' boxType='definition'>
Let $R$ be a commutative ring with identity. The *rank* of a nonzero free $R$-module $M$, denoted $\operatorname{rank}(M)$, is the cardinality of any basis for $M$. The rank of the trivial module $\Set{0}$ is $0$.
</MathBox>

<MathBox title='' boxType='example'>
Let $V$ be a vector space over $\mathbb{F}$ with a countably infinite basis $B = \Set{\boldsymbol{b}_i}_{i\in I}$. Let $\mathcal{L}(V)$ be the ring of linear operators on $V$. Note that $\mathcal{L}(V)$ is not commutative, since composition of functions is not commutative.

The ring $\mathcal{L}(V)$ is an $\mathcal{L}(V)$-module and as such, the identity map $\operatorname{id}$ forms a basis for $\mathcal{L}(V)$. However, we can also construct a basis for $\mathcal{L}(V)$ of any finite size $n$. 

We begin by partitioning $B$ into $n$ blocks. For each $s = 0,\dots,n-1$, let

$$
  B_s = \Set{\boldsymbol{b}_i | i \equiv s\mod n}
$$

Define elements $f_s \in\mathbb{L}(V)$ by $f_s (\boldsymbol{b}_{kn+t}) = \delta_{ts} \boldsymbol{b}_k$ where $0\leq t < n$ and $\delta_{ts}$ is the Kronecker delta function. These functions are surjective and have disjoint support. It follows that $C_n = \Set{f_i}_{i=0}^{n-1}$ is linearly independent: if $0 = \sum{i=0}^{n-1} g_i f_i$ for $g_i \in\mathcal{L}(V)$, then applying this to $\boldsymbol{b}_{kn+t}$ gives

$$
  0 = g_t f_t (\boldsymbol{b}_{kn+1}) = g_t (\boldsymbol{b}_k),\; \forall k
$$

Thus, $g_t = 0$. Also $C_n$ spans $\mathcal{L}(V)$, for if $h \in\mathcal{L}(V)$, we define $g_s \in\mathcal{L}(V)$ by $g_s (\boldsymbol{b}_k) = h(\boldsymbol{b}_{kn+1})$ to get

$$
  \left(\sum_{i=0}^{n-1} g_i f_i \right)(b_{kn+1}) = g_t f_t (b_{kn+t}) = g_t (b_k) = h(b_{kn+1})
$$

and so $h = (\sum_{i=0}^{n-1} g_i f_i$. Hence, $C_n$ is a basis for $\mathcal{L}(V)$ of size $n$.
</MathBox>

<MathBox title='' boxType='theorem'>
Let $A$ be any set and let $R$ be a commutative ring with identity. The set $(R^A)_0$ of all functions from $A$ to $R$ that have finite support is a free $R$-module of rank $|A|$ with basis $B = \Set{\delta_a}$, where

$$
  \delta_a (x) = \begin{cases}
    1 \quad x = b \\
    0 \quad x\neq b
  \end{cases}
$$

This basis called the *standard basis* for $(R^A)_0$.
</MathBox>

<MathBox title='' boxType='theorem'>
Let $M$ be an $R$-module. If $B$ is a basis for $M$, then $M$ is isomorphic to $(R^B)_0$.

<details>
<summary>Proof</summary>

Consider the map $f: M\to (R^B)_0$ defined by setting $f b = \delta_b$, where

$$
  \delta_a (x) = \begin{cases}
    1 \quad x = b \\
    0 \quad x\neq b
  \end{cases}
$$

and extending $f$ to $M$ by linearity. Since $f$ maps a basis for $M$ to a basis $\Set{\delta_b}$ for $(R^B)_0$, it follows that $f$ is an isomorphism from $M$ to $(R^B)_0$.
</details>
</MathBox>

<MathBox title='' boxType='theorem'>
Two free $R$-modules (over a commutative ring) are isomorphic if and only if the have the same rank.

<details>
<summary>Proof</summary>

If $M \cong N$, then any isomorphism $f$ from $M$ to $N$ maps a basis for $M$ to a basis for $N$. Since $f$ is a bijection, we have $\operatorname{rank}(M) = \operatorname{rank}(N)$. Conversely, suppose that $\operatorname{rank}(M) = \operatorname{rank}(N)$. Let $B$ be a basis for $M$ and let $C$ be a basis for $N$. Since $|B| = |C|$, there is a bijective map $f:B\to C$. This map can be extended by linearity to an isomorphism of $M$ onto $N$ and so $M\cong N$.
</details>
</MathBox>

<MathBox title='' boxType='theorem'>
Let $R$ be an integral domain and let $M$ be a free $R$-module. Then all linearly independent sets have cardinality at most $\operatorname{rank}(M)$.

<details>
<summary>Proof</summary>

Since $M\cong (R^\kappa)_0$ we need only prove the result for $(R^\kappa)_0$. Let $Q$ be the field of quotients of $R$. Then $(Q^\kappa)_0$ is a vector space. If

$$
  B = \Set{\boldsymbol{v}_i}_{i\in I}\subseteq (R^\kappa)_0 \subseteq (Q^\kappa)_0
$$

is linearly independent over $Q$ as a subset of $(Q^\kappa)_0$, then $B$ is clearly linearly independent over $R$ as a subset of $(R^\kappa)_0$. Conversely, suppose that $B$ is linearly independent over $R$ and

$$
  \sum_{j=1}^k \frac{\rho_j}{\sigma_j} \boldsymbol{v}_{i_j} = 0
$$

where $\sigma_j \neq 0$ for all $j$ and $\rho_j \neq 0$ for some $j$. Multiplying by $\sigma = \prod_{i=1} \sigma_i \neq 0$ produces a nontrivial linear dependency over $R$

$$
  \sum_{j=1}^k \frac{\sigma}{\sigma_j} \rho_j \boldsymbol{v}_{i_j} = 0
$$

which implies that $\rho_i = 0$ for all $i$. Thus $B$ is linearly dependent over $R$ if and only if it is linearly dependent over $Q$. However, in the vector space $(Q^\kappa)_0$, all sets of cardinality greater than $\kappa$ are linearly dependent over $Q$ and hence all subsets of $(R^\kappa)_0$ of cardinality greater than $\kappa$ are linearly dependent over $R$.
</details>
</MathBox>

### Free modules and epimorphisms

<MathBox title='' boxType='theorem'>
Let $R$ be a commutative ring with identity.
1. If $f: M\to F$ is an $R$-epimorphism and $F$ is free, then $\ker(f)$ is complemented and
$$
  M = \ker(f)\oplus N \cong \ker(f)\boxplus F
$$
where $N\cong F$.
2. If $S$ is a submodule of $M$ and if $M\setminus S$ is free, then $S$ is complemented and $M \cong S\boxplus M\setminus S$. If $M$, $S$ and $M\setminus S$ are free, then
$$
  \operatorname{rank}(M) = \operatorname{rank}(S) + \operatorname{rank}(M\setminus S)
$$
and if the ranks are all finite, then
$$
  \operatorname{rank}(M\setminus S) = \operatorname{rank}(M) - \operatorname{rank}(S)
$$

<details>
<summary>Proof</summary>

If $f: M\to F$ is a module epimorphism where $F$ is free on $B$, then it is easy to define a right inverse for $f$, since we can define an $R$-map $f_R: F\to M$ by specifying its values arbitrarily on $B$ and extending by linearity. Thus, we take $f_R (\mathbf{b})$ to be any member of $f^{-1}(\mathbf{b})$. Then by a previous theorem, $\ker(f)$ is a direct summand of $M$ and

$$
  M \cong \ker(f)\boxplus F
$$

This applies to the canonical projection $\pi:M\to M\setminus S$ provided that the quotient $M\setminus S$ is free.
</details>
</MathBox>

## Quotient modules

<MathBox title='The kernel and image of homomorphisms are submodules' boxType='theorem'>
Let $S$ be a submodule of an $R$-module $M$. The binary relation

$$
  \boldsymbol{u} \equiv \boldsymbol{v} \iff \boldsymbol{u} - \boldsymbol{v} \in S
$$

is an equivalence relation on $M$, whose equivalence classes are the *cosets*

$$
  \boldsymbol{v} + S = \Set{\boldsymbol{v} + \boldsymbol{s} | \boldsymbol{s}\in S}
$$

of $S$ in $M$. The set $M\setminus S$ of all cosets of $S$ in $M$, called the *quotient module* of $M$ *modulo* $S$, is an $R$-module under the well-defined operations

$$
\begin{align*}
  (\boldsymbol{u} + S) + (\boldsymbol{v} + S) =& (\boldsymbol{u} + \boldsymbol{v}) + S \\
  \rho(\boldsymbol{u} + S) =& \rho\boldsymbol{u} + S
\end{align*}
$$

The zero element in $M\setminus S$ is the coset $0 + S = S$.
</MathBox>

<MathBox title='Quotient modules of a free module are not necessarily free' boxType='example'>
As a module over itself, $\Z$ is free on the set $\Set{1}$. For any $n > 0$, the set $\Z_n = \Set{nz | z\in\Z}$ is a free cyclic submodule of $\Z$, but the quotient $\Z$-module $\Z\setminus\Z_n$ is isomorphic to $\Z_n$ via the map

$$
  f(u + \Z_n) = u \mod n
$$
and since $\Z_n$ is not free as a $\Z$-module, neither is $\Z\setminus\Z_n$.
</MathBox>

<MathBox title='Correspondence theorem for modules' boxType='theorem'>
Let $S$ be a submodule of $M$. Then the function that assigns to each intermediate submodule $S\subseteq T \subseteq M$ the quotient submodule $T\setminus S$ of $M\setminus S$ is an order-preserving (with respect to set inclusion) injective correspondence between submodules of $M$ containing $S$ and all submodules of $M\setminus S$.
</MathBox>

<MathBox title='First isomorphism theorem for modules' boxType='theorem'>
Let $f:M\to N$ be an $R$-homomorphism. Then the map $f': M\setminus\ker(f) \to N$ defined by $f'(v + \ker(v)) = f(v)$ is an $R$-embedding and so

$$
  M\setminus \ker(f) \cong\operatorname{ran}(f)
$$
</MathBox>

<MathBox title='Second isomorphism theorem for modules' boxType='theorem'>
Let $M$ be an $R$-module and suppose $S\subseteq T$ are submodules of $M$. Then

$$
  \frac{S + T}{T} \cong \frac{S}{S \cap T}
$$
</MathBox>

<MathBox title='Third isomorphism theorem for modules' boxType='theorem'>
Let $M$ be an $R$-module and suppose $S\subseteq T$ are submodules of $M$. Then

$$
  \frac{M\setminus S}{T \setminus S} \cong \frac{M}{T}
$$
</MathBox>


## Noetherian modules

<MathBox title='Noetherian module' boxType='definition'>
An $R$-module $M$ satisfies the *ascending chain condition* on submodules if every ascending sequence of submodules

$$
  S_1 \subseteq S_2 \subseteq \cdots
$$

of $M$ stabilizes, i.e. there exists an index $k$ for which

$$
  S_{k} = S_{k+1} = \cdots
$$

Modules satisfying the ascending chain condition on submodules are also called *Noetherian modules*.
</MathBox>

<MathBox title='' boxType='theorem'>
1. An $R$-module $M$ is Noetherian if and only if every submodule of $M$ is finitely generated.
2. In particular, a ring $R$ is Noetherian if and only if every ideal of $R$ is finitely generated.

<details>
<summary>Proof</summary>

Suppose that all submodules of $M$ are finitely generated and that $M$ contains an infinite increasing sequence

$$
  S_1 \subseteq S_2 \subseteq \cdots
$$

of submodules. Then the union $S = \bigcup_j S_j$ is easily seen to be a submodule of $M$. Hence, $S$ is finitely generated, say $S = \langle \boldsymbol{u}_1,\dots, u_n \rangle$. Since $\boldsymbol{u}_i \in S$, there exists an index $k_i$, such that $\boldsymbol{u}_i \in S_{k_i}$. Thus, if $k = \max\Set{k_i}_{i=1}^n$, we have $\Set{\boldsymbol{u}_i}_{i=1}^n \subseteq S_k$ and so

$$
  S = \langle \boldsymbol{u}_1,\dots,\boldsymbol{u}_n \rangle \subseteq S_k \subseteq S_{k+1} \subseteq\cdots\subseteq S
$$

which shows that the chain is eventually stable.

Conversely, suppose that $M$ satisfies the ascending chain condition on submodules and let $S$ be a submodule of $M$. Pick $\boldsymbol{u}_1 \in S$ and consider the submodule $S_1 = \langle \boldsymbol{u}_1 \rangle \subseteq S$ generated by $\boldsymbol{u}_1$. If $S_1 = S$, then $S$ is finitely generated. If $S_1 \neq S$, then there is a $\boldsymbol{u}_2 \in S - S_1$. Let $S_2 = \langle \boldsymbol{u}_1, \boldsymbol{u}_2 \rangle$. If $S_2 = S$, then $S$ is finitely generated. If $S_2 \neq S$, then pick $\boldsymbol{u}_3 \in S - S_2$ and consider the submodule $S_3 = \langle \boldsymbol{u}_1,\boldsymbol{u}_2,\boldsymbol{u}_3\rangle$. Continuing this way, we get an ascending chain of submodules

$$
  \langle \boldsymbol{u}_1 \rangle \subseteq \langle \boldsymbol{u}_1,\boldsymbol{u}_2 \rangle \subseteq\cdots\subseteq S
$$

If none of these submodules were equal to $S$, we would have an infinite ascending chain of submodules, each properly contained in the next, which contradicts the fact that $M$ satisfies the ascending chain condition on submodules. Hence, $S = \langle \boldsymbol{u}_1,\dots,\boldsymbol{u}_n \rangle$ for some $n$ and so $S$ is finitely generated.
</details>
</MathBox>

<MathBox title='' boxType='theorem'>
Let $R$ be a commutative ring with identity.
1. $R$ is a Noetherian if and only if every finitely generated $R$-module is Noetherian.
2. Let $R$ be a principal ideal domain. If an $R$-module $M$ is $n$-generated, then any submodule of $M$ is also $n$-generated.

<details>
<summary>Proof</summary>

**(1):** If $M$ is a Noetherian $R$-module, then evidently $R$ is Noetherian. Conversely, assume that $R$ is Noetherian and let $M = \langle \boldsymbol{u}_1,\dots,\boldsymbol{u}_n \rangle$ be a finitely generated $R$-module. Consider the epimorphism $f:R^n \to M$ defined by

$$
  f(\rho_1,\dots,\rho_n) = \sum_{i=1}^n \rho_i \boldsymbol{u}_i
$$

Let $S$ be a submodule of $M$. Then $f^{-1}(S) = \Set{\boldsymbol{u} \in R^n | f \boldsymbol{u} \in S}$ is a submodule of $R^n$ and $f(f^{-1}S) = S$. If every submodule of $R^n$ is finitely generated, then $f^{-1}(S)$ is finitely generated and $f^{-1}(S) = \langle \boldsymbol{v}_1,\dots,\boldsymbol{v}_k \rangle$. Then $S$ is finitely generated by $\Set{f \boldsymbol{v}_i}_{i=1}^k$. Thus, it is sufficient to prove the theorem for $R^n$, which we do by induction on $n$.

In the base case $n = 1$, we can extract from $S$ something that is isomorphic to an ideal of $R$, and so will be finitely generated. In particular, let $S_1$ be the "last coordinates" in $S$, specifically, let

$$
  S_1 = \Set{0,\dots,0,\alpha_n} | (\alpha_i)_{i=1}^n \in S,\; \alpha_i \in R
$$

The set $S_1$ is isomorphic to an ideal of $R$ and is therefore finitely generated, say $S_1 = \langle G_1 \rangle$, where $G_1 = \Set{\boldsymbol{g}_i}_{i=1}^k$ is a finite subset of $S_1$.

Also let

$$
  S_2 = \Set{\boldsymbol{v} \in S | \boldsymbol{v} = (\alpha_1,\dots,\alpha_{n-1},0), \alpha_1,\dots,\alpha_{n-1}\in R}
$$

be the set of all elements of $S$ that have last coordinate equal to $0$. Note that $S_2$ is a submodule of $R^n$ and is isomorphic to a submodule of $R^{n-1}$. Hence, the inductive hypothesis implies that $S_2$ is finitely generated, say $S_2 = \langle G_2 \rangle$ where $G_2$ is a finite subset of $S$.

By definition of $S_1$, each $\boldsymbol{g}_i \in G_1$ has the form $\boldsymbol{g}_i = (0,\dots,0,g_{i,n})$ for $g_{i,n}\in R$ where there is a $\bar{\boldsymbol{b}}_i \in S$ of the form

$$
  \bar{\boldsymbol{g}}_i = (g_{i,1},\dots,g_{i,n-1},g_{i,n})
$$

Let $\bar{G} = \set{\bar{\boldsymbol{g}}_i}_{i=1}^k$. We claim that $S$ is generated by the finite set $\bar{G}_1 \cup G_2$. To see this, let $\boldsymbol{v} = (\alpha_i)_{i=1}^n \in S$. Then $(0,\dots,0,\alpha_n)\in S_1$ and so

$$
  (0,\dots,0,\alpha_n) = \sum_{i=1}^k \rho_i \boldsymbol{g}_i,\; \rho_i \in R
$$

Consider the sum

$$
  w = \sum_{i=1}^k \rho_i \bar{\boldsymbol{g}}_i \in \braket{\bar{G}_1}
$$

The last coordinate of this sum is $\sum_{i=1}^k \rho_i \boldsymbol{g}_{i,n} = a_n$ and so the difference $\boldsymbol{v} - \boldsymbol{w}$ has last coordinate $0$ and is thus in $S_2 = \braket{G_2}$. Hence

$$
  \boldsymbol{v} = (\boldsymbol{v} - \boldsymbol{w}) + \boldsymbol{w} \in \braket{\bar{G}_1} + \braket{G_2} = \braket{\bar{G}_1 \cup G_2}
$$
</details>
</MathBox>

<MathBox title='Hilbert basis theorem' boxType='theorem'>
If a ring $R$ is Noetherian, then so is the polynomial ring $R[x]$.

<details>
<summary>Proof</summary>

We want to show that any ideal $\mathcal{I}$ in $R[x]$ is finitely generated. Let $L$ denote the set of all leading coefficients of polynomials in $\mathcal{I}$, together with the $0$ element of $R$. Then $L$ is an ideal of $R$.

To see this, note that if $\alpha\in L$ is the leading coefficient of $f(x)\in\mathcal{I}$ and if $r\in R$, then either $r\alpha = 0$ or else $r\alpha$ is the leading coefficient of $rf(x) \in\mathcal{I}$. In either case, $r\alpha\in L$. Similarly, suppose that $\beta\in L$ is the leading coefficient of $g(x) \in\mathcal{I}$. We may assume that $\deg(f(x)) = i$ and $\deg(g(x)) = j$ with $i \leq j$. Then $h(x) = x^{j-i} f(x)$ is in $\mathcal{I}$, has leading coefficient $\alpha$ and has the same degree as $g(x)$. Hence, either $\alpha - \beta$ is $0$ or $\alpha - \beta$ is the leading coefficient of $h(x) - g(x) \in\mathcal{I}$. In either case $\alpha - \beta \in L$.

Since $L$ is an ideal of the Noetherian ring $R$, it must be finitely generated, say $L = \langle a_1,\dots,a_m \rangle$. Since $a_i \in L$, there exists polynomials $f_i (x) \in\mathcal{I}$ with leading coefficient $a_i$. By multiplying each $f_i (x)$ by a suitable power of $x$, we may assume that $\deg(f_i(x)) = d = \max\Set{\deg(f_i(x))}$ for all $i = 1,\dots,m$.

For $k = 0,\dots,d-1$ let $L_k$ be the set of all leading coefficients of polynomials in $\mathcal{I}$ of degree $k$, together with the $0$ element of $R$. A similar argument shows that $L_k$ is an ideal of $R$ and so $L_k$ is also finitely generated. Hence, we can find polynomials $P_k = \Set{p_{k,1}(x),\dots,p_{k,n_k}()}$ in $\mathcal{I}$ whose leading coefficients constitute a generating set for $L_k$.

Consider now the the finite set

$$
  P = \left( \bigcup_{k=0}^{d-1} P_k \right) \cup \Set{f_i (x)}_{i=1}^m
$$

If $\mathcal{J}$ is the ideal generated by $P$, then $\mathcal{J}\subseteq\mathcal{I}$. An induction argument can be used to show that $\mathcal{J} = \mathcal{I}$. If $g(x) \in\mathcal{I}$ has degree $0$, then it is a linear combination of the elements of $P_0$ (which are constants) and is thus in $\mathcal{J}$. Assume that any polynomial in $\mathcal{I}$ of degree less than $k$ is in $\mathcal{J}$ and let $g(x) \in\mathcal{I}$ have degree $k$.

If $k < d$, then some linear combination $h(x)$ over $R$ of the polynomials in $P_k$ has the same leading coefficient as $g(x)$ and if $k \geq d$, then some linear combination $h(x)$ of the polynomials

$$
  \Set{x^{k-d} f_i(x)}_{i=1}^m \subseteq \mathcal{J}
$$

has the same leading coefficients as $g(x)$. In either case, there is a polynomial $h(x)\in\mathcal{J}$ that has the same leading coefficients as $g(x)$. Since $g(x) - h(x) \in\mathcal{I}$ has degree strictly smaller than that of $g(x)$ the induction hypothesis implies that $g(x) - h(x)\in\mathcal{J}$ and so

$$
  g(x) = [g(x) - h(x)] + h(x) \in\mathcal{J}
$$

This completes the induction and shows that $\mathcal{I} = \mathcal{J}$ is finitely generated.
</details>
</MathBox>

## Differences between modules and vector spaces

The following is a list of some of the properties of modules over commutative rings with identity that emphasize the differences between modules and vector spaces:
- A submodule of a module need not have a complement
- A submodule of a finitely generated module need no be finitely generated
- There exist modules with no linearly independent elements and hence with no basis
- A minimal spanning set or maximal linearly independent set is not necessarily a module basis
- There exist free modules with submodules that are not free
- There exist free modules with linearly independent sets that are not contained in a basis and spanning sets that do not contain a basis

Recall that a module over a noncommutative ring may have bases of different sizes. However, all bases for a free module over a commutative ring with identity have the same size.

# Modules over a principal ideal domain

## Annhilators and orders

When $R$ is a principal ideal domain, all annihilators are generated by a single element.

<MathBox title='Order' boxType='definition'>
Let $R$ be a principal ideal domain and let $M$ be an $R$-module.
1. If $N$ is a submodule of $M$, then any generator of $\operatorname{ann}(N)$ is called an *order* of $N$.
2. An *order* of an element $\mathbf{v}\in M$ is an order of the submodule $\langle \mathbf{v} \rangle$.
</MathBox>

<MathBox title='Order' boxType='theorem'>
Let $R$ be a principal ideal domain and let $M$ be an $R$-module.
1. If $\alpha$ is an order of $N\leq M$, then the orders of $N$ are precisely the associates of $\alpha$. We denote any order of $N$ by $o(N)$.
2. If $M = A \oplus B$, then $o(M) = \operatorname{lcm}(o(A),o(B))$, i.e. the orders of $M$ are precisely the least common multiples of the orders of $A$ and $B$.

<details>
<summary>Proof</summary>

**(2):** Suppose that

$$
\begin{align*}
  o(M) =& \delta \\
  o(A) =& \alpha \\
  o(B) =& \beta \\
  \lambda =& \operatorname{lcm}(\alpha, \beta)
\end{align*}
$$

Then $\delta A = \Set{0}$ and $\beta B = \Set{0}$ imply that $\alpha | \delta$ and $\beta | \delta$ and so $\lambda | \delta$. On the other hand, $\lambda$ annihilates both $A$ and $B$ and therefore also $M = A \oplus B$. Hence, $\delta | \lambda$ and so $\lambda\sim\delta$ is an order of $M$.
</details>
</MathBox>

## Cyclic modules

<MathBox title='' boxType='theorem'>
Let $R$ be a principal ideal domain.
1. If $\langle\mathbf{v}\rangle$ is a cyclic $R$-module with annihilator $\langle\alpha\rangle$, then the multiplication map $f:R\to\langle\mathbf{v}\rangle$ defined by $f\rho = \rho\mathbf{v}$ is an $R$-epimorphism with $\ker(f) = \langle\alpha\rangle$. Hence, the induced map $\bar{g}: R\setminus\langle\alpha\rangle \to \langle\mathbf{v}\rangle$ defined by $\bar{g}(\rho + \langle\alpha\rangle) = \rho\mathbf{v}$ is an isomorphism. In other words, cyclic $R$-modules are isomorphic to quotient modules of the base ring $R$.
2. Any submodule of a cyclic $R$-module is cyclic.
3. If $\langle\mathbf{v}\rangle$ is a cyclic submodule of $M$ of order $\alpha$, then for $\beta\in R$,
$$
  o(\langle\beta\mathbf{v}\rangle) = \frac{\alpha}{\gcd(\beta,\alpha)}
$$
Also
$$
\begin{align*}
  &\langle\beta\mathbf{v}\rangle = \langle\mathbf{v}\rangle \iff& (o(\mathbf{v}),\beta) = 1 \\
  \iff& o(\beta\mathbf{v}) = o(\mathbf{v})
\end{align*}
$$

<details>
<summary>Proof</summary>

**(2):** Let $S\leq\langle\mathbf{v}\rangle$, then $\mathcal{I} = \Set{\rho\in R | \rho\mathbf{v}\in S}$ is an ideal of $R$ and so $\mathcal{I} = \langle\sigma\rangle$ for some $\sigma\in R$. Thus,

$$
  S = \mathcal{I}\mathbf{v} = R\sigma\mathbf{v} = \langle\sigma\mathbf{v}\rangle
$$

**(3):** We have $\rho(\beta\mathbf{v}) = 0$ if and only iff $\rho(\beta)\mathbf{v} = 0$, i.e. if and only if $\alpha|\rho\beta$, which is equivalent to

$$
  \gamma := \left.\frac{\alpha}{\gcd(\alpha,\beta)}\right| r
$$

Thus, $\rho\in\operatorname{ann}(\beta\mathbf{v})$ if and only if $\rho\in\langle\gamma\rangle$ and so $\operatorname{ann}(\beta\mathbf{v}) = \langle\gamma\rangle$. For the second statements, if $(alpha,\beta) = 1$ then there exist $a,b \in R$ for which $a\alpha + b\beta = 1$ and so

$$
  \mathbf{v} = (a\alpha + b\beta)\mathbf{v} = b\beta\mathbf{v} \in\langle\beta\mathbf{v}\rangle \subseteq \langle\mathbf{v}\rangle
$$

and so $\langle\beta\mathbf{v}\rangle = \langle\mathbf{v}\rangle$. Indeed, if $\langle\beta\mathbf{v}\rangle = \langle\mathbf{v}\rangle$, then $o(\beta\mathbf{v}) = \alpha$. Finally, if $o(\beta\mathbf{v}) = \alpha$, then

$$
  \alpha = o(\beta\mathbf{v}) = \frac{\alpha}{\gcd(\alpha,\beta)}
$$

and so $(\alpha,\beta) = 1$.
</details>
</MathBox>

### Decomposition of cyclic modules

<MathBox title='Composition and decomposition of cyclic modules' boxType='theorem'>
Let $M$ be an $R$-module.
1. **Composing cyclic modules:** If $u_1,\dots,u_n \in M$ have relatively prime orders, then $o\left(\sum_{i=1}^n \mathbf{u}_i \right) = \prod_{i=1}^n o(\mathbf{u}_i)$ and $\bigoplus_{i=1}^n \langle\mathbf{u}_i \rangle = \left\langle \sum_{i=1}^n \mathbf{u}_i \right\rangle$. Consequently, if $M = \sum_{i=1}^n A_i$ where the submodules $A_i$ have relatively prime orders, then the sum is direct.
2. **Decomposing cyclic modules:** If $o(\mathbf{v}) = \prod_{i=1}^n \alpha_i$ where the $\alpha_i$ are pairwise relatively prime, then $\mathbf{v}$ has the form $\mathbf{v} = \sum_{i=1}^n \mathbf{u}_i$ where $o(\mathbf{u}_i) = \alpha_i$ and so
$$
  \langle\mathbf{v}\rangle = \left\langle \sum_{i=1}^n \mathbf{u}_i \right\rangle = \bigoplus_{i=1}^n \langle\mathbf{u}_i \rangle
$$

<details>
<summary>Proof</summary>

**(1):** Let $\alpha_k = o(\mathbf{u}_k)$, $\mu := \prod_{i=1}^n \alpha_i$ and $\mathbf{v} := \sum_{i=1}^n \mathbf{u}_i$. Since $\mu$ annihilates $\mathbf{v}$, the order of $\mathbf{v}$ divides $\mu$. If $o(\mathbf{v})$ is a proper divisor of $\mu$, then for some index $k$, there is a prime $p|\alpha_k$ for which $\frac{\mu}{p}$ annihilates $\mathbf{v}$. However, $\frac{\mu}{p}$ annihilates each $\mathbf{u}_i$ for $i \neq k$. Thus

$$
  0 = \frac{\mu}{p}\mathbf{v} = \frac{\mu}{p}\mathbf{u}_k = \frac{\alpha_k}{p}\left(\frac{\mu}{p}\right)\mathbf{u}_k
$$

Since $o(\mathbf{u}_k)$ and $\frac{\mu}{\alpha_k}$ are relatively prime, the order of $\frac{\mu}{\alpha_k}\mathbf{u}_k$ is equal to $o(\mathbf{u}_i) = \alpha_k$, which contradicts the equation above. Hence, $o(\mathbf{v}) = \mu$.

It is clear that $\left\langle \sum_{i=1}^n \mathbf{u}_i \right\rangle \subseteq bigoplus_{i=1}^n \langle\mathbf{u}_i \rangle$. For the reverse inclusion, since $\alpha_1$ and $\frac{\mu}{\alpha_1}$ are relatively prime, there exist $r,s \in R$ for which

$$
  r\alpha_1 + s\frac{\mu}{\alpha_1} = 1
$$

Hence

$$
\begin{align*}
  \mathbf{u}_1 =& \left( r\alpha_1 + s\frac{\mu}{\alpha_1} \right)\mathbf{u}_1 = s\frac{\mu}{\alpha_1} \\
  =& s \frac{\mu}{\alpha_1} \sum_{i=1}^n \mathbf{u}_n \in \left\langle\sum_{i=1}^n \mathbf{u}_i \right\rangle
\end{align*}
$$

Similarly, $\mathbf{u}_k \in\left\langle\sum_{i=1}^n \mathbf{u}_i \right\rangle$ for all $k$ and so we get the reverse inclusion. Finally, to see that the sum above is direct, note that if $\sum_{i=1}^n \mathbf{v}_i = 0$, where $\mathbf{v}_i \in A_i$, then each $\mathbf{v}_i$ must be $0$, for otherwise the order of the sum on the left would be different from $1$.

**(2):** The scalars $\beta_k = \frac{\mu}{\alpha_k}$ are relatively prime and so there exist $a_i \in R$ for which $\sum_{i=1}^n a_i \beta_i = 1$. Hence,

$$
  \mathbf{v} = \left(\sum_{i=1}^n a_i \beta_i \right)\mathbf{v} = \sum_{i=1}^n a_i \beta_i \mathbf{v}
$$

Since $o(\beta_k \mathbf{v}) = \frac{\mu}{\gcd(\mu,\beta_k)} = \alpha_k$ and since $a_k$ and $\alpha_k$ are relatively prime, we have $o(a_k \beta_k \mathbf{v}) = \alpha_k$. The second statement follows from **(1)**.
</details>
</MathBox>

## Free modules of a prinicipal ideal domain

<MathBox title='Submodules of free modules are free' boxType='proposition'>
Let $M$ be a free module over a principal ideal domain $R$. Then any submodule $S$ of $M$ is also free and $\operatorname{rank}(S) \leq\operatorname{rank}(M)$.

<details>
<summary>Proof</summary>

We will give the proof first for modules of finite rank and then generalize to modules of arbitrary rank. Since $M\cong R^n$ where $n = \operatorname{rank}(M)$ is finite, we may assume that $M = R^n$. For each $1\leq k \leq n$, let

$$
  \mathcal{I} = \Set{\rho\in R | (\alpha_1,\dots,\alpha_{k-1},\rho,0,\dots,0)\in S,\; \alpha_1,\dots,\alpha_{k-1}\in R}
$$

Then it is easy to see that $\mathcal{I}_k$ is an ideal of $R$ and so $\mathcal{I}_k = \langle \rho_k \rangle$ for some $\rho_k \in R$. Let

$$
  \mathbb{u}_k = (\alpha_1,\dots,\alpha_{k-1},\rho_k,0,\dots,0)\in S
$$

We claim that $B = \Set{\mathbf{u}_k | \rho_k \neq 0}_{k=1}^n$ is a basis for $S$. As to linear independence, suppose that $B = \Set{\mathbf{u}_{i_j}}_{j=1}^m$ and that $\sum_{\ell=1}^s \alpha_{j_\ell}\mathbf{u}_{j_\ell}$. Comparing the $j_s$th coordinates gives $\alpha_{j_s}\rho_{j_s} = 0$ and since $\rho_{j_s} \neq 0$, if follows that $\alpha_{j_s} = 0$. In a similar way, all coefficients are $0$ and so $B$ is linearly independent.

To see that $B$ spans $S$, we partition the elements $\mathbf{x}\in S$ according to the largest coordinate index $i(\mathbf{x})$ with nonzero entry and induct on $i(\mathbf{x})$. If $i(\mathbf{x}) = 0$, then $\mathbf{x} = 0$, which is in the span of $B$. Suppose that all $\mathbf{x}\in S$ with $i(\mathbf{x}) < k$ are in the span of $B$ and let $i(\mathbf{x}) = k$, i.e.

$$
  \mathbf{x} = (\alpha_1,\dots,\alpha_k,0,\dots,0),\; \alpha_k \neq 0
$$

Then $\alpha_k \in\mathcal{I}_k$ and so $\rho_k \neq 0$ and $\alpha_k = c\rho_k$ for some $c\in R$. Thus, $i(\mathbf{x} - c\mathbf{u}_k) < k$ and so $\mathbf{y} = \mathbf{x} - c\mathbf{u}_k \in \langle B \rangle$ and therefore $\mathbf{x}\in\langle B \rangle$. Hence, $B$ is a basis for $S$.

The proof can be generalized to modules of arbitrary rank. In this case, we may assume that $M = (R^\kappa)_0$ is the $R$-module of functions with finite support from $\kappa$ to $R$, where $\kappa$ is a cardinal number. We use that fact that $\kappa$ is a well-ordered  set, i.e. $\kappa$ is a totally ordered set in which any nonempty subset has a smallest element. If $\alpha \in \kappa$, the closed interval $[0,\alpha]$ is

$$
  [0,\alpha] = \Set{x \in\kappa | 0 \leq x \leq \alpha}
$$

Let $S\leq M$. For each $0 < \alpha \leq\kappa$, let

$$
  M_\alpha = \Set{f\in S | \operatorname{supp}(f) \subseteq [0,\alpha]}
$$

Then the set $\mathcal{I}_\alpha = \Set{f(\alpha) | f\in M_\alpha}$ is an ideal of $R$ and so $\mathcal{I}_\alpha = \langle \mathbf{f}_\alpha (\alpha) \rangle$ for some $\mathbf{f}_\alpha \in S$. We show that

$$
  B = \Set{f_\alpha | 0 < \alpha \leq\kappa, \mathbf{f}_\alpha (\alpha) \neq 0}
$$

is a basis for $S$. First, suppose that $\sum_{i=1}^n \rho_i f_{\alpha_i} = 0$ where $\alpha_i < \alpha_j$ for $i < j$. Applying this to $\alpha_n$ gives $\rho_n f_{\alpha_n} (\alpha_n) = 0$, and since $R$ is an integral domain, $\rho_n = 0$. Similarly $\rho_i = 0$ for all $i$ and so $B$ is linearly independent.

To show that $B$ spans $B$, since any $\mathbf{f}\in S$ has finite support, there is a largest index $\alpha_\mathbf{f} = i(\mathbf{f})$ for which $\mathbf{f}(\alpha_\mathbf{f}) \neq 0$. If $\langle B \rangle < S$, then since $\kappa$ is well-ordered, we may choose a $\mathbf{g}\in S\setminus \langle B \rangle$ for which $\alpha = \alpha_\mathbf{g} = i(\mathbf{g})$ is as small as possible. Then $\mathbf{g}\in M$. Moreover, since $0 \neq \mathbf{g}(\alpha)\in\mathcal{I}_\alpha$, it follows that $\mathbf{f}_\alpha (\alpha) \neq 0$ and $\mathbf{g}(\alpha) = c\mathbf{f}_\alpha (\alpha)$ for some $c\in R$. Then $\operatorname{supp}(\mathbf{g} - c\mathbf{f}_\alpha) \subseteq [0, \alpha]$ and

$$
  (\mathbf{g} - c\mathbf{f}_\alpha)(\alpha) = \mathbf{g}(\alpha) - c\mathbf{f}_\alpha(\alpha) = 0
$$

and so $i(\mathbf{g} - c\mathbf{f}_\alpha) < \alpha$, which implies that $\mathbf{g} - c\mathbf{f}_\alpha \in \langle B \rangle$. But then

$$
  \mathbf{g} = (\mathbf{g} - c\mathbf{f}_\alpha) + c\mathbf{f}_\alpha \in \langle B \rangle
$$

which is a contradiction. Hence, $B$ is a basis for $S$.
</details>
</MathBox>

<MathBox title='' boxType='theorem'>
Let $M$ be a free $R$-module of finite rank $n$, where $R$ is a principal ideal domain. Let $S = \Set{\mathbf{s}_i}_{i=1}^n$ be a spanning set for $M$. Then $S$ is a bsis for $M$.

<details>
<summary>Proof</summary>

Let $B = Set{\mathbf{b}_i}_{i=1}^n$ be a basis for $M$ and define the map $f: M\to M$ by $f\mathbf{b} = \mathbf{s}_i$ and extending to a surjective $R$-homomorphism. Since $M$ is free, a previous theorem implies that

$$
  M \cong \ker(f) \boxplus \operatorname{ran}(f) = \ker(f) \boxplus M
$$

Since $\ker(f)$ is a submodule of the free module and since $R$ is a principal ideal domain, we know that $\ker(f)$ is free of rank at most $n$. It follows that

$$
  \operatorname{rank}(M) = \operatorname{rank}(\ker(f)) + \operatorname{rank}(M)
$$

and so $\operatorname{rank}(\ker(f)) = 0$, i.e. $\ker(f) = \Set{0}$, which implies that $f$ is an $R$-isomorphism and so $S$ is a basis.
</details>
</MathBox>

<MathBox title='' boxType='theorem'>
Let $M$ be a free $R$-module of rank $n$, where $R$ is a principal ideal domain. Let $N$ be a submodule of $M$ that is free of rank $k\leq n$. Then there is a basis $B$ for $M$ that contains a subset $S=\Set{\mathbf{v}_i}_{i=1}^k$ for which $\Set{\rho_i \mathbf{v}_i}_{i=1}^k$ is a basis for $N$ for some nonzero elements $\rho_i \in R$.
</MathBox>

## Torsion-free and free modules

<MathBox title='' boxType='proposition'>
A finitely generated module over a principal ideal domain is free if and only if it is torsion-free.

<details>
<summary>Proof</summary>

It is straightforward to see that a finitely generated module $M$ over a principal ideal domain that is free, it is also torsion-free. Conversely, suppose $M$ is torsion free and let $G = \Set{\mathbf{v}_i}_{i=1}^n$ be a generating set for $M$. In the case $n = 1$, we have $G = \Set{\mathbf{v}}$. Then $G$ is a basis for $M$ since singleton are linearly independent in a torsion-free module. Hence $M$ is free.

Suppose that $G = \Set{\mathbf{u}, \mathbf{v}}$ is a generating set with $\mathbf{u},\mathbf{v}\neq 0$. If $G$ is linearly independent, we are done. If not, then there exist nonzero $\alpha, \beta \in R$ for which $\alpha\mathbf{u} = \beta\mathbf{v}$. It follows that $\beta M = \beta\langle\mathbf{u},\mathbf{v}\rangle \subseteq \langle\mathbf{u}\rangle$ and so $\beta M$ is a submodule of a free module and is therefore free by the previous proposition. However, the map $f:M\to\beta M$ defined by $f\mathbf{v} = \beta\mathbf{v}$ is an isomorphism because $M$ is torsion-free. Hence, $M$ is also free.

For the general case, write

$$
  G = \Set{\mathbf{u}_1,\dots,\mathbf{u}_k,\mathbf{v}_1,\dots,\mathbf{v}_{n-k}}
$$

where $S = \Set{\mathbf{u}_i}_{i=1}^k$ is a maximal linearly independent subset of $G$. Note that $S$ is nonempty because singleton sets are linearly independent.

For each $\mathbf{v}_i$, the set $\Set{\mathbf{u}_i,\dots,\mathbf{u}_k,\mathbf{v}_i}$ is linearly dependent and so there exist $\alpha_i \in R$ and $\rho_1,\dots,\rho_k \in R$ for which

$$
  \alpha_i \mathbf{v}_i + \sum_{j=1}^k \rho_j \mathbf{u}_j = 0
$$

If $\alpha = \prod_{i=1}^{n-k} \alpha_i$, then

$$
  \alpha M = \alpha\langle \mathbf{u}_1,\dots,\mathbf{u}_k,\mathbf{v}_1,\dots,\mathbf{v}_{n-k}\rangle \subseteq \langle\mathbf{u}_1,\dots,\mathbf{u}_k \rangle
$$

Since the latter is a free module, so is $\alpha M$, and therefore so is $M$.
</details>
</MathBox>

## The primary cyclic decomposition theorem

<MathBox title='' boxType='proposition'>
Any finitely generated module $M$ over a principal ideal domain $R$ is the direct sum of a finitely generated free $R$-module and a finitely generated torsion $R$ module

$$
  M = M_\mathrm{free} \oplus M_\mathrm{tor}
$$

The torsion part $M_\mathrm{tor}$ is unique, since it must be the set all torsion elements of $M$, whereas the free part $M_\mathrm{free}$ is unique only up to isomorphism, i.e. the rank of the free part is unique.

<details>
<summary>Proof</summary>

It is easy to see that the set $M_\mathrm{tor}$ of all torsion elements is a submodule of $M$ and the quotient $M\setminus M_\mathrm{tor}$ is torsion-free. Moreover, since $M$ is finitely generated, so is $M\setminus M_\mathrm{tor}$. Hence, the previous theorem implies that $M\setminus M_\mathrm{tor}$ is free. Hence, by a previous theorem

$$
  M = M_\mathrm{tor} \oplus F
$$

where $F \cong M\setminus M_\mathrm{tor}$ is free.

As to the uniqueness of the torsion part, suppose that $M = T \oplus G$ where $T$ is torsion and $G$ is free. Then $T \subseteq M_\mathrm{tor}$. However, if $\mathbf{v} = \mathbf{t} + \mathbf{g} \in M_\mathrm{tor}$ for $\mathbf{t}\in T$ and $\mathbf{g}\in G$, then $\mathbf{g} = \mathbf{v} - \mathbf{t}\in M_\mathrm{tor}$ and so $\mathbf{g} = 0$ and $\mathbf{v}\in T$. Thus, $T = M_\mathrm{tor}$.

For the free part, since $M = M_\mathrm{tor} \oplus F = M_\mathrm{tor} \oplus G$, the submodules $F$ and $G$ are both complements of $M_\mathrm{tor}$ and hence are isomorphic. 
</details>
</MathBox>

Note that if $\Set{\mathbf{w}_i}_{i=1}^m$ is a basis for $M_\mathrm{free}$ we can write $M = \left(\bigoplus_{i=1}^m \langle \mathbf{w}_i \rangle\right) M_\mathrm{tor}$, where each cyclic submodule $\langle \mathbf{w}_i \rangle$ has zero annihilator. This is a partial decomposition of $M$ into a direct sum of cyclic submodules.

### Primary decomposition

<MathBox title='Primary module' boxType='definition'>
Let $p$ be a prime in $R$. A $p$-*primary* module is a module whose order is a power of $p$.
</MathBox>

<MathBox title='Primary decomposition theorem' boxType='theorem'>
Let $M$ be a torsion module over a principal ideal domain $R$, with order

$$
  \mu = \prod_{i=1}^n p_i^{e_i}
$$

where the $p_i$ are distinct nonassociate primes in $R$.
1. $M$ is the direct sum $M = \bigoplus_{i=1}^n M_{p_i}$ where
$$
  M_{p_i} = \frac{\mu}{p_i^{e_i}} M = \Set{\mathbf{v}\in M | p_i^{e_i}\mathbf{v} = 0}
$$
is a primary submodule of order $p_i^{e_i}$. This decomposition of $M$ into primary submodules is called the *primary decomposition* of $M$.
2. The primary decomposition of $M$ is unique up to order of the summands, i.e. if $M = \bigoplus_{i=1}^m N_{q_i}$, where $N_{q_i}$ is primary of order $q_i^{f_i}$ and $q_1,\dots,q_m$ are distinct nonassociate primes, then $m = n$ and $N_{q_i} = M_{p_i}$ (after reindexing if necessary). Hence $f_i = e_i$ and $q_i \sim p_i$ for $i = 1,\dots,n$.
3. Two $R$-modules $M$ and $N$ are isomorphic if and only if the summands in their primary decomposition are pairwise isomorphic, i.e. if $M = \bigoplus_{i=1}^n M_{p_i}$ and $N = \bigoplus_{i=1}^m N_{q_i}$ are primary decompositions, then $m = n$ and $M_{p_i} \cong N_{q_i}$ (after reindexing if necessary) for $i=1,\dots,n$.

<details>
<summary>Proof</summary>

Let us write $\mu_i = \frac{\mu}{p_i^{e_i}}$ and show first that

$$
  M_{p_i} = \mu_i M = \Set{\mu_i \mathbf{v} | \mathbf{v}\in M}
$$

Since $p_i^{e_i} (\mu_i M) = \mu M = \Set{0}$, we have $\mu_i M \subseteq M_{p_i}$. On the other hand, since $\mu_i$ and $p_i^{e_i}$ are relatively prime, there exists $a, b\in R$ for which

$$
  a\mu_i + bp_i^{e_i} = 1
$$

and so if $\mathbf{x}\in M_{p_i}$ then

$$
  \mathbf{x} = (a\mu_i + bp_i^{e_i})\mathbf{x} = a\mu_i \mathbf{x} \in \mu_i M
$$

Hence $M_{p_i} = \mu_i M$.

**(1):** Since $\gcd(\mu_1,\dots,\mu_n) = 1$, there exist scalars $a_i \in R$ for which $\sum_{i=1}^n a_i \mu_i = 1$, and so for any $\mathbf{x}\in M$

$$
  \mathbf{x} = \left(\sum_{i=1}^n a_i \mu_i\right)\mathbf{x} \in \sum_{i=1}^n \mu_i M
$$

Moreover, since the $o(\mu_i M) | p_i^{e_i}$ and the $p_i^{e_i}$ are pairwise relatively prime, it follows that the sum of the submodules $\mu_i M$ is direct, i.e.

$$
  M = \bigoplus_{i=1}^n \mu_i M = \bigoplus_{i=1}^n M_{p_i}
$$

As to the annihilators, it is clear that $\langle p_i^{e_i} \subseteq \operatorname{ann}(\mu_i M)$. For the reverse inclusion, if $r\in\operatorname{ann}(\mu_iM)$, then $r\mu_i \in\operatorname{ann}(M)$ and so $p_i^{e_i} \mu_i | r\mu_i$, i.e. $p_i^{e_i} | r$ and so $r\in \langle p_i^{e_i} \rangle$. Hence $\operatorname{ann}(\mu_i M) = \langle p_i^{e_i} \rangle$.

**(2):** As to uniqueness, we claim that $q = \prod_{i=1}^m q_i^{f_i}$ is an order of $M$. It is clear that $q$ annihilates $M$ and so $\mu|q$. On the other hand, $N_{q_i}$ contains an element $u_i$ of order $q_i^{f_i}$, and so the sum $\mathbf{v} = \sum_{i=1}^m \mathbf{u}_i$ has order $q$, which implies that $q | \mu$. Hence, $q$ and $\mu$ are associates.

Unique factorization in $R$ implies that $m = n$ and, after a suitable reindexing, that $f_i = e_i$ and $q_i$ and $p_i$ are associates. Hence $N_{q_i}$ is primary of order $p_i^{e_i}$. For convenience, we can write $N_{q_i}$ as $N_{p_i}$. Hence

$$
  N_{p_i} \subseteq \Set{\mathbf{v} \in M | p_i^{e_i} \mathbf{v} = 0} = M_{p_i}
$$

However, if $\bigoplus_{i=1}^n N_{p_i} = \bigoplus_{i=1}^n M_{p_i}$ and $N_{p_i} \subseteq M_{p_i}$, we must have $N_{p_i} = M_{p_i}$ for all $i$.

**(3):** If $m = n$ and $g_i: M_{p_i} \cong N_{q_i}$, then the map $g: M\to N$ defined by $h\left(\sum_{i=1}^n \mathbf{a}_i \right) = \sum_{i=1}^n g_i (\mathbf{a}_i)$ is an isomorphism and so $M \cong N$. Conversely, suppose that $g: M\cong N$. Then $M$ and $N$ have the same annihilators and therefore the same order

$$
  \mu = \prod_{i=1}^n p_i^{e_i}
$$

Hence, **(1)** and **(2)** imply that $m = n$ and after a suitable reindexing, $q_i = p_i$. Moreover, since

$$
  \mathbf{a}\in M_{p_i} \iff \mu_i \mathbf{a} = 0 \iff g(\mu_i\mathbf{a}) = 0 \iff \mu_i g\mathbf{a} = 0 \iff g\mathbf{a} \in N_{p_i}
$$

it follows that $g: M_{p_i}\cong N_{p_i}$.
</details>
</MathBox>

### Cyclic decomposition of primary modules

<MathBox title='' boxType='lemma'>
Let $M$ be a module over a principal ideal domain $R$ and let $p\in R$ be a prime.
1. If $pM = \Set{0}$, then $M$ is a vector space over the field $R\setminus\langle p \rangle$ with scalar multiplication defined by $(\rho + \langle p\rangle)\mathbf{v} = \rho\mathbf{v}$ for all $\mathbf{v}\in M$ and $\rho\in R$.
2. For any submodules $S$ of $M$, the set $S^{(p)} = \Set{\mathbf{v}\in S | p\mathbf{v} = 0}$ is also a submodule of $M$ and if $M = S\oplus T$, then $M^{(p)} = S^{(p)} \oplus T^{(p)}$.

<details>
<summary>Proof</summary>

**(1):** Since $p$ is a prime, the ideal $\langle p \rangle$ is maximal and so $R\setminus \langle p \rangle$ is a field.

**(2):** Since $S^{(p)} \subseteq S$ and $T^{(p)} \subseteq T$ we see that $S^{(p)} \cap T^{(p)} = \Set{0}$. Also, if $\mathbf{v}\in M^{(p)}$, then $p\mathbf{v} = 0$. However, $\mathbf{v} = \mathbf{s} + \mathbf{t}$ for some $\mathbf{s}\in S$ and $\mathbf{t}\in T$ and so $0 = p\mathbf{v} = p\mathbf{s} + p\mathbf{t}$. Since $p\mathbf{s}\in S$ and $p\mathbf{t}\in T$, we deduce that $p\mathbf{s} = p\mathbf{t} = 0$, such that $\mathbf{v}\in S^{(p)} \oplus T^{(p)}$. Thus, $M^{(p)} \subseteq S^{(p)} \oplus T^{(p)}$. The reverse inclusion is manifest.
</details>
</MathBox>

<MathBox title='Cyclic decomposition theorem of a primary module' boxType='theorem'>
Let $M$ be a primary finitely generated torsion module over a principal ideal domain $R$, with order $p^e$.
1. $M$ is a direct sum $M = \bigoplus_{i=1}^n \langle \mathbf{v}_i \rangle$ of cyclic submodules with annihilators $\operatorname{ann}(\langle \mathbf{v}_i \rangle) = \langle p^{e_i} \rangle$, which can be arranged in ascending order
$$
  \operatorname{ann}(\langle \mathbf{v}_1 \rangle) \subseteq\cdots\subseteq \operatorname{ann}(\langle \mathbf{v}_n \rangle)
$$
or equivalently, $e = e_1 \geq\cdots\geq e_n$
2. If $M$ is also the direct sum $M = \bigoplus_{i=1}^n \langle \mathbf{u}_i \rangle$ of cyclic submodules with annihilators $\operatorname{ann}(\langle\mathbf{u}_i\rangle) = \langle q^{f_i} \rangle$, arranged in ascending order
$$
  \operatorname{ann}(\langle \mathbf{u}_1 \rangle) \subseteq\cdots\subseteq \operatorname{ann}(\langle \mathbf{u}_n \rangle)
$$
or equivalently $f_1 \geq\cdots\geq f_m$. Then the two chains of annihilators are identical, i.e. $m = n$ and
$$
  \operatorname{ann}(\langle \mathbf{u}_i \rangle) = \operatorname{ann}(\langle \mathbf{v}_i \rangle),\;\forall i
$$
Thus, $p\sim q$ and $f_i = e_i$ for all $i$.
3. Two $p$-primary $R$-modules $M = \bigoplus_{i=1}^n \langle \mathbf{v}_i \rangle$ and $N = \bigoplus_{i=1}^n \langle \mathbf{u}_i \rangle$ are isomorphic if and only if they have the same annihilator chains, i.e. if and only if $m = n$ and, after a possible reindexing 
$$
  \operatorname{ann}(\langle \mathbf{u}_i \rangle) = \operatorname{ann}(\langle \mathbf{v}_i \rangle),\;\forall i
$$

<details>
<summary>Proof</summary>

**(1):** Let $\mathbf{v}_1 \in M$ have order equal to the order of $M$, i.e. $\operatorname{ann}(\mathbf{v}_1) = \operatorname{ann}(M) = \langle p^e \rangle$. Such an element must exist since $o(\mathbf{v}_1) \leq p^e$ for all $\mathbf{v}\in M$ and if this inequality is strict, then $p^{e-1}$ will annihilate $M$.

If we show that $\langle \mathbf{v}_1 \rangle$ is complemented, i.e. $M = \langle\mathbf{v}_1 \rangle \oplus S_1$ for some submodule $S_1$, then since $S_1$ is also a finitely generated primary torsion module over $R$, we can repeat the process to get

$$
  M = \langle\mathbf{v}_1 \rangle \oplus \langle\mathbf{v}_2 \rangle \oplus S_2
$$

where $\operatorname{ann}(\mathbf{v}_i) = \langle p^{e_i} \rangle$. We can continue this decomposition

$$
  M = \left( \oplus_{i=1} \langle \mathbf{v}_1 \rangle \right) \oplus S_n
$$

as long as $S_n \neq \Set{0}$. However, the ascending sequences of submodules
$$
  \langle\mathbf{v}_1 \rangle \subseteq \langle \mathbf{v}_1 \rangle \oplus \langle \mathbf{v}_2 \rangle \subseteq\cdots
$$

must terminate since $M$ is Noetherian and so there is an integer $n$ for which eventually $S_n = \Set{0}$ giving $M = \bigoplus_{i=1}^n \langle \mathbf{v}_i \rangle$.

**(2):** Let $\mathbf{v} = \mathbf{v}_1$. The direct sum $M_1 = \langle \mathbf{v} \rangle \oplus\Set{0}$ clearly exists. Suppose that the direct sum $M_k = \langle \mathbf{v} \rangle \oplus S_k$ exists. We claim that if $M_m \leq M$, the it is possible to find a submodule $S_{k+1}$ for which $S_k < S_{k+1}$ and for which the direct sum $M_{k+1} = \langle \mathbf{v} \rangle \oplus S_{k+1}$ also exists. This process must also stop after a finite number of steps, giving $M = \langle \mathbf{v} \rangle \oplus S$.

If $M_k < M$ and $\mathbf{u}\in M\setminus M_1$, let $S_{k+1} = \langle S_k , \mathbf{u} - \alpha\mathbf{v}\rangle$ for $\alpha\in R$. Then $S_k < S_{k+1}$ since $\mathbf{u}\notin M_k$. We want to show that for some $\alpha\in R$, the direct sum $\langle\mathbf{v}\rangle\oplus S_{k+1}$ exists, i.e.

$$
  \mathbf{x} \in \langle \mathbf{v} \rangle \cap \langle S_k, \mathbf{u} - \alpha\mathbf{v} \rangle \implies \mathbf{x} = 0
$$

Now, there exists scalars $a$ and $b$ for which $\mathbf{x} = a\mathbf{v} = \mathbf{s} + b(\mathbf{u} - \alpha\mathbf{v})$ for $\mathbf{s}\in S_k$ and so if we find a scalar $\alpha$ for which $b(\mathbf{u} - \alpha\mathbf{v})\in S_k$ then $\langle \mathbf{v} \rangle \cap S_k = \Set{0}$ implies that $\mathbf{x} = 0$, completing the proof of existence.

Solving for $b\mathbf{u}$ gives

$$
  b\mathbf{u} = (a + \alpha b)\mathbf{v} - \mathbf{s} \in \langle\mathbf{v} \rangle \oplus S_k = M_k
$$

so let us consider the ideal of all such scalars $\mathcal{I} = \Set{\rho\in R | \rho\mathbf{u} \in M_k}$. Since $p^e \in\mathcal{I}$ and $\mathcal{I}$ is principal, we have $\mathcal{I} = \langle p^f \rangle$ for some $f \leq e$. Also, $f > 0$ since $\mathbf{u}\notin M_k$ implies that $1\notin \mathcal{I}$. Since $b\in\mathcal{I}$, we have $b = \beta p^f$ and there exist $d\in R$ and $\mathbf{t}\in S_k$ for which $p^f \mathbf{u} = d\mathbf{v} + \mathbf{t}$. Hence

$$
  b\mathbf{u} = \beta p^f \mathbf{u} = \beta(d\mathbf{v} + \mathbf{t}) = \beta d\mathbf{v} + \beta\mathbf{t}
$$

Now we need more information about $d$. Multiplying the expression for $p^f \mathbf{u}$ by $p^{e-f}$ gives

$$
  0 = p^{e}\mathbf{u} = p^{e-f}(p^f \mathbf{u}) = p^{e-f} d\mathbf{v} + p^{e-f}\mathbf{t}
$$

and since $\langle\mathbf{v}\rangle \cap S_k = \Set{0}$, it follows that $p^{e-f} d\mathbf{v} = 0$. Hence, $p^e | p^{e-f} d$, i.e. $p^f | d$ and so $d = \delta p^f$ for some $\delta\in R$. Now we can write

$$
  b\mathbf{u} = \beta\delta p^f \mathbf{v} + \beta\mathbf{t}
$$

and so

$$
  b(\mathbf{u} - \delta\mathbf{v}) = \beta\mathbf{t} \in S_k
$$

Thus, we take $\alpha = \delta$ to get $b(\mathbf{u} - \alpha\mathbf{v}) \in S_k$, completing the proof of existence.

For uniqueness, note first that $M$ has orders $p^{e_1}$ and $q^{f_1}$ and so $p$ and $q$ are associates and $e_1 = f_1$. Next, we show that $n = m$. According to **(2)** of the previous lemma $M^{(p)} = \bigoplus_{i=1} \langle \mathbf{v}_i \rangle^{(p)}$ and $M^{(p)} = \bigoplus_{i=1} \langle \mathbf{u}_i \rangle^{(p)}$ where all the summands are nonzero. Since $pM^{(p)} = \Set{0}$, it follows from the previous lemma that $M^{(p)}$ is a vector space over $R\setminus\braket{p}$ and so each of the preceding decompositions expresses $M^(p)$ as a direct sim of one-dimensional vector subspaces. Hence $m = \dim(M^{(p)}) = n$.

Finally, we show that the exponents $e_i$ and $f_i$ are equal using induction on $e_1$. If $e_1 = 1$, then $e_i = 1$ for all $i$ and since $f_1 = e_1$, we also have $f_i = 1$ for all $i$. Suppose that the result is true whenever $e_1 \leq k-1$ and let $e_1 = k$. Write

$$
\begin{align*}
  (e_1,\dots,e_n) =& (e_1,\dots,e_n,1,\dots,1),\; e_s > 1 \\
  (f_1,\dots,f_n) =& (f_1,\dots,f_t,1,\dots,1),\; f_t > 1
\end{align*}
$$

Then $pM = \bigoplus_{i=1}^s p\langle\mathbf{v}_i \rangle$ and $pM = \bigoplus_{i=1}^\top p\langle\mathbf{u}_i \rangle$. However, $p\langle \mathbf{v}_1 \rangle = \langle p\mathbf{v}_1 \rangle$ is a cyclic submodule of $M$ with annihilator $\langle p^{e_i - 1}\rangle$ and so by the induction hypothesis $\mathbf{s} = \mathbf{t}$ and $e_1 = f_1,\dots,e_s = f_s$, concluding the proof of uniqueness.

**(3):** Suppose $g:M\cong N$ and $M$ has annihilator chain

$$
  \operatorname{ann}(\langle\mathbf{v}_1\rangle) \subseteq\cdots\subseteq \operatorname{ann}(\langle\mathbf{v}_n \rangle)
$$

and $N$ has annihilator chain

$$
  \operatorname{ann}(\langle\mathbf{u}_1\rangle) \subseteq\cdots\subseteq \operatorname{ann}(\langle\mathbf{u}_n \rangle)
$$

Then

$$
  N = gM = \bigoplus_{i=1} \langle g\mathbf{v}_1 \rangle
$$

and so $m = n$ and after a suitable reindexing

$$
  \operatorname{ann}(\langle\mathbf{v}_i\rangle) = \operatorname{ann}(\langle g\mathbf{v}_i \rangle) = \operatorname{ann}(\langle\mathbf{u}_i \rangle)
$$

Conversely, suppose $M = \bigoplus_{i=1}^n \mathbf{v}_i$ and $N = \bigoplus_{i=1}^n \langle \mathbf{u}_i \rangle$ have the same annihilator chains, i.e. $m = n$ and 

$$
  \operatorname{ann}(\langle\mathbf{u}_i\rangle) = \operatorname{ann}(\langle\mathbf{v}_i \rangle)
$$

Then

$$
  \langle\mathbf{u}_i \rangle \cong \frac{R}{\operatorname{ann}(\langle\mathbf{u}_i \rangle)} = \frac{R}{\operatorname{ann}(\langle\mathbf{v}_i \rangle)} \cong \langle\mathbf{v}_i \rangle
$$
</details>
</MathBox>

### Primary cyclic decomposition

<MathBox title='Primary cyclic decomposition theorem' boxType='theorem'>
Let $M$ be a finitely generated torsion module over a principal ideal domain $R$.
1. If $M$ has order $\mu = \prod_{i=1}^n p_i^{e_i}$ where the $p_i$ are distinct nonassociate primes in $R$, then $M$ can be uniquely decomposed (up to the order of the summands) into the direct sum $M = \bigoplus_{i=1} M_{p_i}$ where
$$
  M_{p_i} = \frac{\mu}{p_i^{e_i}} M = \Set{\mathbf{v}\in M | p_i^{e_i}\mathbf{v} = 0}
$$
is a primary submodule with annihilator $\langle p_i^{e_i} \rangle$. Finally, each primary submodule $M_{p_i}$ can be written as a direct sum of cyclic submodules, so that
$$
  M = \underbrace{[\langle\mathbf{v}_{1,1}\rangle\oplus\cdots\oplus\langle \mathbf{v}_{i, k_1} \rangle]}_{M_{p_1}} \oplus\cdots\oplus \underbrace{[\langle\mathbf{v}_{1,1}\rangle\oplus\cdots\oplus\langle \mathbf{v}_{i, k_n} \rangle]}_{M_{p_n}}
$$
where $\operatorname{ann}(\langle\mathbf{v}_{i,j}\rangle) = \langle p_i^{e_{i,j}}\rangle$ and the terms in each cyclic decomposition can be arranged so that, for each $i$
$$
  \operatorname{ann}(\langle\mathbf{v}_{i,1}) \subseteq\cdots\subseteq \operatorname{ann}(\langle\mathbf{v}_{i,k}\rangle)
$$
or equivalently $e_i = e_{i,1} \geq\cdots\geq e_{i,k_i}$.
2. As for uniqueness, suppose
$$
  M = \underbrace{[\langle\mathbf{u}_{1,1}\rangle\oplus\cdots\oplus\langle \mathbf{u}_{i, k_1} \rangle]}_{N_{q_1}} \oplus\cdots\oplus \underbrace{[\langle\mathbf{u}_{1,1}\rangle\oplus\cdots\oplus\langle \mathbf{u}_{i, k_n} \rangle]}_{N_{q_n}}
$$
is also a primary cyclic decomposition of $M$. Then
    - The number of summands is the same in both decompositions; in fact, $m = n$ and after possible reindexing $k_u = j_u$ for all $u$.
    - The primary submodules are the same, i.e. after possible reindexing $q_i \sim p_i$ and $N_{q_i} = M_{p_i}$.
    - For each primary submodule pair $N_{q_i} = M_{p_i}$, the cyclic submodules have the same annihilator chains, i.e. after possible reindexing $\operatorname{ann}(\langle \mathbf{u}_{i,j} \rangle) = \operatorname{ann}(\langle \mathbf{v}_{i,j} \rangle)$ for all $i,j$.
In summary, the primary submodules and annihilator chains are uniquely determined by the module $M$. 
3. Two $R$-modules $M$ and $N$ are isomorphic if and only if they have the same annihilator chains.
</MathBox>

### Elementary divisors

Since the chain of annihilators $\operatorname{ann}(\langle \mathbf{v}_{i,j} \rangle) = \langle p_i^{e_{i,j}}\rangle$ is unique except for order, the multiset $\Set{p_i^{e_{i,j}}}$ of generators is uniquely determined up to associate. The generators $p_i^{e_{i,j}}$ are called the *elementary divisors* of $M$. Note that for each prime $p_i$, the elementary divisor $p_i^{e_{i,j}}$ of largest exponent is precisely the factor of $o(M)$ associated to $p_i$.

The multiset of all elementary divisors of $M$ is denoted $\operatorname{ElemDiv}(M)$. Thus, if $r\in\operatorname{ElemDiv}(M)$, then any associate of $r$ is also in $\operatorname{ElemDiv}(M)$. Moreover, the function $M \mapsto \operatorname{ElemDiv}(M)$ is a complete invariant for isomorphism. In theory we could work with a system of distinct representatives for the associates classes of the elementary divisors, but in general, there is no way to single out a special representative.

<MathBox title='' boxType='proposition'>
Let $R$ be a principal ideal domain. The multiset $\operatorname{ElemDiv}(M)$ is a complete invariant for isomorphism of finitely generated torsion $R$-modules, i.e.

$$
  M \cong N \iff \operatorname{ElemDiv}(M) = \operatorname{ElemDiv}(N)
$$
</MathBox>

<MathBox title='' boxType='proposition'>
Let $M$ be a finitely generated torsion module over a principal ideal domain and suppose that

$$
  M = A \oplus B
$$

1. The primary cyclic decomposition of $M$ is the direct sum of the primary cyclic decompositions of $A$ and $B$, i.e. if $A = \bigoplus \langle a_{i,j} \rangle$ and $B = \bigoplus \langle b_{i,j} \rangle$ are primary cyclic decompositions of $A$ and $B$, respectively, then

$$
  M = \left(\bigoplus \langle a_{i,j} \rangle \right) \oplus \left(\bigoplus \langle b_{i,j} \rangle\right)
$$

2. The elementary divisors of $M$ are

$$
  \operatorname{ElemDiv}(M) = \operatorname{ElemDiv}(A) \cup \operatorname{ElemDiv}(B)
$$

where the union is a multiset union (containing duplicate members).
</MathBox>

## Invariant factor decomposition

<MathBox title='Invariant factor decomposition theorem' boxType='theorem'>
Let $M$ be a finitely generated torsion module over a principal ideal domain $R$. Then

$$
  M = \bigoplus_{i=1} D_i
$$

where $D_i$ is a cyclic submodule of $M$ with order $d_i$ satisfying

$$
  d_m | d_{m-1} | \cdots | d_1
$$

or equivalently

$$
  \operatorname{ann}(D_1) \subseteq \operatorname{ann}(D_2) \subseteq\cdots
$$

This decomposition is called an *invariant factor decomposition* of $M$ and the scalars $d_i$ are called the *invariant factors* of $M$.
1. The multiset of invariant factors is uniquely determined up to associate by the module $M$.
2. The multiset of invariant factors is a complete invariant for isomorphism.

<details>
<summary>Proof</summary>

By decomposition of modules, if $S$ and $T$ are cyclic submodules with relatively prime orders, then $S\oplus T$ is a cyclic submodule whose order is the product of the orders of $S$ and $T$. Accordingly, in the primary cyclic decomposition of $M$

$$
  M = \underbrace{[\langle \mathbf{v}_{1,1}\oplus\cdots\oplus\langle \mathbf{v}_{i, k_1} \rangle]}_{M_{p_1}} \oplus\cdots\oplus \underbrace{[\langle \mathbf{v}_{1,1}\oplus\cdots\oplus\langle \mathbf{v}_{i, k_n} \rangle]}_{M_{p_n}}
$$

with elementary divisors $p_i^{e_{i,j}}$ satisfying

$$
  e_i = e_{i,1} \geq\cdots\geq e_{i,k_i}
$$

we can combine cyclic summands with relatively prime orders. One way is to take the leftmost (highest order) cyclic submodules from each group to get

$$
  D_i = \bigoplus_{j=1}^n \langle \mathbf{v}_{j, i} \rangle
$$

Some summands may be missing since different primary modules $M_{p_i}$ do not necessarily have the same number of summands. In any case, the result of this regrouping and combining is a decomposition of the form

$$
  M = \bigoplus_{i=1}^m D_i
$$

As to the orders of the summands, if $D_i$ has order $d_i$, then since the highest powers of each prime $p_i$ are taken for $d_1$, the second highest for $d_2$ and so on, we conclude that

$$
  d_m | d_{m-1} | \cdots | d_1
$$

The process described above that passes from a sequence $p_i^{e_{i,j}}$ of elementary divisors to a sequence of invariant factors is reversible. Then inverse process takes a sequence $d_1,\dots,d_m$ with $d_m | d_{m-1} | \cdots | d_1$, factors each $d_i$ into a product of distinct nonassociate prime powers with the primes in the same order and then "peels off" like prime powers from the left.

This fact together with the decomposition property of cyclic modules implies that primary cyclic decompositions and invariant factors are essentially equivalent. Thus, since the multiset of elementary divisors of $M$ is unique up associate, the multiset of invariant factors of $M$ is also unique up to associate. It follows that the multiset of invariant factors is a complete invariant for isomorphism.
</details>
</MathBox>

The annihilators of an invariant factor decomposition are called the *invariant ideals* of $M$. The chain of invariant ideals is unique, as is the chain of annihilators in the primary cyclic decomposition. Note that $d_1$ is an order of $M$, i.e.

$$
  \operatorname{M} = \langle d_1 \rangle
$$

Note also that the product $\gamma = \prod_{i=1} d_i$ of the invariant factors of $M$ has some nice properties. For example, $\gamma$ is the product of all the elementary divisors of $M$. In the context of a linear operator $\mathrm{T}$ on a vector space, $\gamma$ is the characteristic polynomial of $\mathrm{T}$. 

## Characterizing cyclic modules

<MathBox title='Characterization of cyclic modules' boxType='proposition'>
Let $M$ be a finitely generated torsion module over a principal ideal domain, with order $\mu = \prod_{i=1}^n p_i^{e_i}$. Then the following are equivalent:
1. $M$ is cyclic
2. $M$ is the direct sum $M = \bigoplus_{i=1}^k \langle \mathbf{v}_k \rangle$
3. The elementary divisors of $M$ are precisely the prime power factors of $\mu$
$$
  \operatorname{ElemDiv}(M) = \Set{p_i^{e_i}}_{i=1}^n
$$

<details>
<summary>Proof</summary>

**(1) $\iff$ (2):** Suppose that $M$ is cyclic. Then the primary decomposition of $M$ is a primary *cyclic* decomposition since any submodule of a cyclic module is cyclic. Conversely, if **(2)** holds, then since the orders are relatively prime, the decomposition property of cyclic modules implies that $M$ is cyclic.
</details>
</MathBox>

## Indecomposable modules

<MathBox title='Indecomposable module' boxType='definition'>
A module $M$ is *indecomposable* if it cannot be written as a direct sum of proper submodules.
</MathBox>

<MathBox title='Characterization of indecomposable modules' boxType='proposition'>
Let $M$ be a finitely generated torsion module over a principal ideal domain. The following are equivalent:
1. $M$ is indecomposable
2. $M$ is primary cyclic
3. $M$ has only one elementary divisor
$$
  \operatorname{ElemDiv}(M) = \Set{p^e}
$$
</MathBox>

### Indecomposable submodules of prime order

<MathBox title='Characterization of indecomposable modules' boxType='proposition'>
Let $M$ be a finitely generated torsion module over a principal ideal domain, with order $\mu$. If $p$ is a prime divisor of $\mu$, then $M$ has a cyclic (equivalently, indecomposable) submodule $W$ of prime order $p$.

<details>
<summary>Proof</summary>

If $\mu = pq$, then there is a $\mathbf{v}\in M$ for which $\mathbf{w} = q\mathbf{v} \neq 0$, but $p\mathbf{w} = 0$. Then $W = \braket{\mathbf{w}}$ is annihilated by $p$ and so $o(\mathbf{w})|p$. However, $p$ is prime and $o(\mathbf{w}) \neq 1$ and so $o(\mathbf{w}) = p$. Since $W$ has prime order, the previous proposition implies that $W$ is cyclic if and only if it is indecomposable.
</details>
</MathBox>

# The structure of linear operators

## The module associated with a linear operator

| $\mathbb{F}[x]$-module $V_\mathrm{T}$ | $\mathbb{F}$-vector space $V$ |
| --- | --- |
| Scalar multiplication: $p(x)\mathbf{v}$ | Action of $p(\mathrm{T})$: $p(\mathrm{T})\mathbf{v}$ |
| Annihilator: $\operatorname{ann}(V_\mathrm{T}) = \Set{p(x) : p(x)V_\mathrm{T} = \Set{0}}$ | Annihilator: $\operatorname{ann}(V) = \Set{p(x) p(\mathrm{T})(V) = \Set{0}}$ |
| Monic order $m(x)$ of $V_\mathrm{T}$: $\operatorname{ann}(V_\mathrm{T}) = \langle m(x) \rangle$ | Minimal polynomial of $\mathrm{T}$: $m(x)$ has the smalles degree with $m(\mathrm{T}) = 0$ |
| Cyclic submodule of $V_\mathrm{T}$: $\langle \mathbf{v}\rangle = \Set{p(x)\mathbf{v} : \deg(p(x)) < \deg(m(x))}$ | $\mathrm{T}$-cyclic subspace of $V$: $\langle \mathbf{v}, \mathrm{T}\mathbf{v},\dots, \mathrm{T}^{m-1}(\mathbf{V})\rangle,\; m = \deg(p(x))$ |

Let $V$ be a finite-dimensional vector space. If $\mathrm{T}\in\mathcal{L}(V)$, we will think of $V$ not only as a vector space over a field $\mathbb{F}$, but also as a module over $\mathbb{F}[x]$ with scalar multiplication defined by

$$
  p(x)\mathbf{v} = p(\mathrm{T})(\mathbf{v})
$$

We will write $V_\mathrm{T}$ to indicate the dependence on $\mathrm{T}$. Thus, $V_\mathrm{T}$ and $V_\mathrm{S}$ are modules with the same ring of scalars $\mathbb{F}[x]$, although with different scalar multiplication if $\mathrm{T} \neq \mathrm{S}$.

If $\dim(V) = n$, then $\dim(\mathcal{L}(V)) = n^2$. This implies that $V_\mathrm{T}$ is a torsion module. In fact, then $n^2 + 1$ vectors $\mathrm{I}, \mathrm{T}, \mathrm{T}^2,\dots,\mathrm{T}^{n^2}$ are linearly dependent in $\mathcal{L}(V)$, which implies that $p(\mathrm{T}) = 0$ fomr some nonzero polynomial $p(x) \in\mathbb{F}[x]$. Hence, $p(x)\in\operatorname{ann}(V_\mathrm{T})$ and so $\operatorname{ann}(V_\mathrm{T}) = \Set{p(x) \in\mathbb{F}[x] | p(x)V_\mathrm{T} = \Set{0}}$ is a nonzero principal ideal of $\mathbb{F}[x]$.

Since $V$ is finitely generated as a vector space, it is, a fortiori, finitely generated as an $\mathbb{F}[x]$-module. Thus, $V$ is a finitely generated torision module over a principal ideal domain $\mathbb{F}[x]$ and so we may apply the decomposition theorems for modules.

<MathBox title='' boxType='proposition'>
If $\mathrm{T},\mathrm{S}\in\mathcal{L}(V)$, then

$$
  V_\mathrm{T} \cong V_\mathrm{S}\iff\mathrm{T}\sim\mathrm{S}
$$

In particular, $\mathrm{A}: V_\mathrm{T} \to V_\mathrm{S}$ is a module isomorphism if and only if $\mathrm{A}$ is a vector space automorphism of $V$ satisfying

$$
  \mathrm{S} = \mathrm{ATA}^{-1}
$$

<details>
<summary>Proof</summary>

Suppose that $\mathrm{A}: V_\mathrm{T} \to V_\mathrm{S}$ is a module isomorphism. Then for $\mathbf{v}\in V$

$$
  \mathrm{A}(x\mathbf{v}) = x(\mathrm{A}\mathbf{v})
$$

which is equivalent to $\mathrm{A}(\mathrm{T}\mathbf{v}) = \mathrm{S}(\mathrm{A}\mathbf{v})$. Since $\mathrm{S}$ is bijective, this is equivalent to

$$
  (\mathrm{ATA}^{-1})\mathbf{v} = \mathrm{S}\mathbf{v}
$$

That is, $\mathrm{S} = \mathrm{ATA}^{-1}$. Since a module isomorphism from $V_\mathrm{T}$ to $V_\mathrm{S}$ is a vector space isomorphism as well, the result follows.

Conversely, suppose that $\mathrm{A}$ is a vector space automorphism of $V$ and $\mathrm{S} = \mathrm{ATA}^{-1}$, i.e. $\mathrm{AT} = \mathrm{SA}$. Then

$$
  \mathrm{A}(x^k \mathbf{v}) = \mathrm{A}(\mathrm{T}^k \mathbf{v}) = \mathrm{S}^k (\mathrm{A}\mathbf{v}) = x^k (\mathrm{A}\mathbf{v})
$$

and the $\mathbb{F}$-linearity of $\mathrm{A}$ implies that for any polynomial $p(x) \in \mathbb{F}[x]$

$$
  \mathrm{A}(p(\mathrm{T})\mathbf{v}) = p(\mathrm{S})\mathrm{A}\mathbf{v}
$$

Hence, $\mathrm{A}$ is a module isomorphism from $V_\mathrm{T}$ to $V_\mathrm{S}$.
</details>
</MathBox>

There is a simple connection between the submodules of the $\mathbb{F}[x]$-module $V_\mathrm{T}$ and the subspaces of the vector space $V$. Recall that a subspace $U$ of $V$ is $\mathrm{T}$-invariant if $\mathrm{T}U \subseteq U$.

<MathBox title='' boxType='proposition'>
Let $V$ be a finite-dimensional vector space. A subset $U\subseteq V$ is a submodule of $V_\mathrm{T}$ if and only if $U$ is a $\mathrm{T}$-invariant subspace of $V$.
</MathBox>

### Orders and the minial polynomial

We have seen that the annihilator of $V_\mathrm{T}$ is a nonzero principal ideal of $\mathbb{F}[x]$, say $\operatorname{ann}(V_\mathrm{T}) = \langle m(x) \rangle$. Since the elements of the base ring $\mathbb{F}[x]$ of $V_\mathrm{T}$ are polynomials, there is a logical choice among all scalars in a given associate class: Each associate class contains exactly one monic polynomial.

<MathBox title='Minimal polynomial of linear operators' boxType='definition'>
Let $\mathrm{T}\in\mathcal{L}(V)$. The unique monic order of $V_\mathrm{T}$ is called the *minimal polynomial* for $\mathrm{T}$ and is denoted $m_\mathrm{T}(x)$ or $\min(\mathrm{T})$. Thus

$$
  \operatorname{ann}(V_\mathrm{T}) = \langle m_\mathrm{T}(x) \rangle.
$$

If $\mathrm{T}$ is represented by the matrix $\mathbf{A}\in\mathcal{M}_n (\mathbb{F})$, the minimal polynomial $m_\mathbf{A}(x)$ of $\mathbf{A}$ is defined as the minimal polynomial of the multiplication operator $\mathrm{T}_\mathbf{A}$. 
</MathBox>

In treatments of linear algebra that do nit emphasize the role of the module $V_\mathrm{T}$, the minimal polynomial of a linear operator $\mathrm{T}$ is simply defined as the unique monic polynomial $m_\mathrm{T}(x)$ of smallest degree for which $m_\mathrm{T} = 0$.

<MathBox title='' boxType='proposition'>
1. If $\mathrm{T}\sim\mathrm{S}$ are similar linear operators on $V$, then $m_\mathrm{T}(x) = m_\mathrm{S}(x)$. Thus, the minimal polynomial is an invariant under similarity of operators.
2. If $\mathbf{A}\sim\mathbf{B}$ are similar matrices, then $m_\mathbf{A}(x) = m_\mathbf{B}(x)$. Thus, the minimal polynomial is an invariant under similarity of matrices.
3. The minimal polynomial of $\mathrm{T}\in\mathcal{L}(V)$ is the same as the minimal polynomial of any matrix representing $\mathrm{T}$.
</MathBox>

### Cyclic submodules and cyclic subspaces

The cyclic submodules of $V_\mathrm{T}$ take the form

$$
  \langle\mathbf{v}\rangle = \mathbb{F}[x]\mathbf{v} = \Set{p(\mathrm{T})(\mathbf{v}) | p(x)\in\mathbb{F}[x]}
$$

which are $\mathrm{T}$-invariant subspaces of $V$. Let $m(x)$ be the minimal polynomial of $\mathrm{T}|_{\langle\mathbf{v}\rangle}$ and suppose that $\deg(m(x)) = n$. If $p(x)\mathbf{v} \in\langle\mathbf{v}\rangle$, then writing

$$
  p(x) = q(x)m(x) + r(x)
$$

where $\deg(r(x)) < \deg(m(x))$, gives

$$
  p(x)\mathbf{v} = [q(x)m(x) + r(x)\mathbf{v}] = r(x)\mathbf{v}
$$

and so

$$
  \langle\mathbf{v}\rangle = \Set{r(x)\mathbf{v} | \deg(r(x)) < n}
$$

Hence, the set

$$
  B = \Set{\mathbf{v}, x\mathbf{v},\dots,x^{n-1}\mathbf{v}} = \Set{\mathbf{v},\mathrm{T}\mathbf{v},\dots,\mathrm{T}^{n-1}\mathbf{v}}
$$

spans the *vector space* $\langle\mathbf{v}\rangle$. To see that $B$ is a basis for $\langle\mathbf{v}\rangle$, note that any linear combination of the vectors in $B$ has the form $r(x)\mathbf{v}$ for $\deg(r(x)) < n$ and so is equal to $0$ if and only if $r(x) = 0$. Thus, $B$ is an ordered basis for $\langle\mathbf{v}\rangle$.

<MathBox title='Cyclic basis' boxType='definition'>
Let $\mathrm{T}\in\mathcal{L}(V)$. A $\mathrm{T}$-invariant subspace $U$ of $V$ is $\mathrm{T}$-*cyclic* if $U$ has a basis of the form

$$
  B = \Set{\mathbf{v}, x\mathbf{v},\dots,x^{n-1}\mathbf{v}},\; \mathbf{v}\in V, n > 0
$$

The basis $B$ is called a $\mathrm{T}$-*cyclic basis* for $V$.
</MathBox>

A cyclic submodule $\langle\mathbf{v}\rangle$ of $V_\mathrm{T}$ with order $m(x)$ of degree $n$ is a $\mathrm{T}$-cyclic subspace of $V$ of dimension $n$. The converse is also true, for if

$$
  B = \Set{\mathbf{v}, x\mathbf{v},\dots,x^{n-1}\mathbf{v}}
$$

is a basis for a $\mathrm{T}$-invariant subspace $U\subseteq V$, then $U$ is a submodule of $V_\mathrm{T}$. Moreover, the minimal polynomial of $\mathrm{T}|_U$ has degree $n$, since if

$$
  \mathrm{T}^n\mathbf{v} = \sum_{i=1}^{n-1} -\alpha_i \mathrm{T}^i \mathbf{v}
$$

then $\mathrm{T}|_U$ satisfies the polynomial

$$
  m(x) = \sum_{i=1}^{n-1} \alpha_i x^i
$$

but none of smaller degree since $B$ is linearly independent.

<MathBox title='Cyclic basis' boxType='definition'>
Let $V$ be a finite-dimensional vector space and let $U\subseteq V$. The following are equivalent:
1. $U$ is a cyclic submodule of $V_\mathrm{T}$ with order $m(x)$ of degree $n$
2. $U$ is a $\mathrm{T}$-cyclic subspace of $V$ with $\dim(U) = n$
</MathBox>

## Primary cyclic decomposition

<MathBox title='Elementary divisors and invariant factors of linear operators' boxType='definition'>
Let $\mathrm{T}\in\mathcal{L}(V)$. 
1. The *elementary divisors* and *invariant factors* of $\mathrm{T}$ are the monic elementary divisors and invariant factors, respectively, of the module $V_\mathrm{T}$. The multiset of elementary divisors of $\mathrm{T}$ is denoted $\operatorname{ElemDiv}(\mathrm{T})$, and the multiset of invariant factors of $\mathrm{T}$ is denoted $\operatorname{InvFact}(\mathrm{T})$.
2. The *elementary divisors* and *invariant factors* of a matrix $\mathbf{A}$ are the elementary divisors and invariant factors, respectively, of the multiplication operator $\mathrm{T}_\mathbf{A}$:
$$
\begin{align*}
  \operatorname{ElemDiv}(\mathbf{A}) =& \operatorname{ElemDiv}(\mathrm{T}_\mathbf{A}) \\
  \operatorname{InvFact}(\mathbf{A}) =& \operatorname{InvFact}(\mathrm{T}_\mathbf{A}) \\  
\end{align*}
$$
</MathBox>

<MathBox title='Primary cyclic decomposition theorem for vector spaces' boxType='theorem'>
Let $V$ be a finite-dimensional vector space and let $\mathrm{T}\in\mathcal{L}(V)$ have the minimal polynomial $m_\mathrm{T} (x) = \prod_{i=1}^n p_i^{e_i} (x)$, where the polynomials $p_i(x)$ are distinct monic primes.
1. **Primary decomposition:** The $\mathbb{F}[x]$-module $V_\mathrm{T}$ is the direct sum $V_\mathrm{T} = \bigoplus_{i=1}^n V_{p_i}$, where
$$
  V_{p_i} = \frac{m_\mathrm{T}(x)}{p_i^{e_i}(x)}V = \Set{\mathbf{v}\in V | p_i^{e_i}(\mathrm{T})(\mathbf{v}) = 0}
$$
is a primary submodule of $V_\mathrm{T}$ of order $p_i^{e_i} (x)$. In vector space terms, $V_{p_i}$ is a $\mathrm{T}$-invariant subspace of $V$ and the minimal polynomial of $\mathrm{T}|_{V_{p_i}}$ is $\min(\mathrm{T}|_{V_{p_i}}) = p_i^{e_i} (x)$.
2. **Cyclic decomposition:** Each primary summand $V_{p_i}$ can be decomposed into a direct sum $V_{p_i} = \langle \mathrm{v}_{i,1}\oplus\cdots\oplus\langle\mathbf{v}_{i,k_i}\rangle$ of $\mathrm{T}$-cyclic submodules $\langle\mathbf{v}_{i,j}\rangle$ of order $p_i^{e_{i,j}}(x)$ with
$$
  e_i = e_{i,1} \geq e_{i_{k_i}}
$$
In vector space terms, $\langle\mathbf{v}_{i,j}\rangle$ is a $\mathrm{T}$-cyclic subspace of $V_{p_i}$ and the minimal polynomial of $\mathrm{T}|_{\langle\mathbf{v}_{i,j}\rangle}$ is $\min(\mathrm{T}|_{\langle\mathbf{v}_{i,j}\rangle} = p_i^{e_{i,j}}(x)$.
3. **Complete decomposition:** This yields the docomposition of $V_\mathrm{T}$ into a direct sum of $\mathrm{T}$-cyclic subspaces
$$
  V_\mathrm{T} = (\langle\mathbf{v}_{1,1}\rangle\oplus\cdots\oplus\langle\mathbf{v}_{1,k_1}\rangle)\oplus\cdots\oplus(\langle\mathbf{v}_{n,1}\rangle\oplus\cdots\oplus\langle\mathbf{v}_{n,k_n}\rangle)
$$
4. **Elementary divisors and dimensions:** The multiset of elementary divisors $\Set{p_i^{e_{i,j}}}$ is uniquely determined by $\mathrm{T}$. If $\deg(p_i^{e_{i,j}}(x)) = d_{i,j}$, then the $\mathrm{T}$-cyclic subspace $\langle\mathbf{v}_{i,j}\rangle$ has $\mathrm{T}$-cyclic basis
$$
  B_{i,j} = (\mathbf{v}_{i,j}, \mathrm{T}\mathbf{v}_{i,j},\dots,\mathrm{T}^{d_{i,j}-1}\mathbf{v}_{i,j})
$$
and $\dim(\langle\mathbf{v}_{i,j}\rangle) = \deg(p_i^{e_{i,j}})$. Hence, $\dim(V_{p_i}) = \sum_{j=1}^{k_i} \deg(p_i^{e_{i,j}})$. The basis $R = \bigcup_{i,j} B_{i,j}$ is called the *elementary divisor basis* for $V_\mathrm{T}$. 
</MathBox>

Recall that if $V = A \oplus B$ and if both $A$ and $B$ are $\mathrm{T}$-invariant subspaces of $V$, the pair $(A,B)$ *reduces* $\mathrm{T}$. In module terms, the pair $(A,B)$ reduces $\mathrm{T}$ if $A$ and $B$ are submodules of $V_\mathrm{T}$ and $V_\mathrm{T} = A_\mathrm{T} \oplus B_\mathrm{T}$.

<MathBox title='' boxType='proposition'>
Let $\mathrm{T}\in\mathcal{L}(V)$ and let $V_\mathrm{T} = A_\mathrm{T} \oplus B_\mathrm{T}$.
1. The minimal polynomial of $\mathrm{T}$ is
$$
  m_\mathrm{T}(x) = \operatorname{lcm}(m_{\mathrm{T}|_A}(x), m_{\mathrm{T}|_B} (x))
$$
2. The primary cyclic decomposition of $V_\mathrm{T}$ is the direc sum of the primary cyclic decompositions of $A_\mathrm{T}$ and $B_\mathrm{T}$. That is, if $A_\mathrm{T} = \bigoplus_{i,j} \langle \mathbf{a}_{i,j}\rangle$ and $B_\mathrm{T} = \bigoplus_{i,j} \langle \mathbf{b}_{i,j} \rangle$ are the cyclic decompositions of $A_\mathrm{T}$ and $B_\mathrm{T}$, respectively, then
$$
  V_\mathrm{T} = \left( \bigoplus_{i,j}\langle\mathbf{a}_{i,j}\rangle \right)\oplus \left( \bigoplus_{i,j}\langle\mathbf{b}_{i,j}\rangle\right)
$$
is the primary cyclic decomposition of $V_\mathrm{T}$.
3. The elementary divisors of $\mathrm{T}$ are
$$
  \operatorname{ElemDiv}(\mathrm{T}) = \operatorname{ElemDiv}(\mathrm{T}|_A) \cup \operatorname{ElemDiv}(\mathrm{T}|_B) 
$$
where the union is a multiset union, i.e. we kepp all duplicate members.
</MathBox>

## Characteristic polynomial

<MathBox title='Characteristic polynomial for linear operators' boxType='definition'>
Let $\mathrm{T}\in\mathcal{L}(V)$ be a linear operator. The *characteristic polynomial* $c_\mathrm{T}(x)$ of $\mathrm{T}$ is the product of all the elementary divisors of $\mathrm{T}$

$$
  c_\mathrm{T}(x) = \prod_{i,j} p_i^{e_{i,j}}(x)
$$

Hence, $\deg(c_\mathrm{T}(x)) = \dim(V)$. Similarly the *characteristic polynomial* $c_\mathbf{A}(x)$ of a matrix $\mathbf{A}\in\mathcal{M}_n (\mathbb{F})$ is the product of the elementary divisors of $\mathbf{M}$.
</MathBox>

<MathBox title='The Cayley-Hamilton theorem' boxType='theorem'>
Let $\mathrm{T}\in\mathcal{L}(V)$ be a linear operator.
1. The minimal polynomial of $\mathrm{T}$ divides the characteristic polynomical of $\mathrm{T}$, i.e. $m_\mathrm{T}(x) | c_\mathrm{T}(x)$. Equivalently, $\mathrm{T}$ satisfies its own characterstic polynomial, i.e. $c_\mathrm{T}(\mathrm{T}) = 0$.
2. The minimal polynomial $m_\mathrm{T} = \prod_{i=1}^n p_i^{e_{i,1}}(x)$ and characteristic polynomial $c_\mathrm{T}(x) = \prod_{i,j} p_i^{e_{i,j}}(x)$ of $\mathrm{T}$ have the same set of prime factors $p_i(x)$ and hence the same set of roots (not counting multiplicity).
</MathBox>

<MathBox title='Nonderogatory linear operator' boxType='definition'>
A linear operator $\mathrm{T}\in\mathcal{L}(V)$ is *nonderogatory* if its minimal polynomial is equal to its characteristic polynomal, i.e.

$$
  m_\mathrm{T}(x) = c_\mathrm{T}(x)
$$

or equivalently, if

$$
  \deg(m_\mathrm{T}(x)) = \deg(c_\mathrm{T}(x))
$$

or if

$$
  \deg(m_\mathrm{T}(x)) = \dim(V)
$$

Similar statements hold for matrices.
</MathBox>

## Cyclic and indecomposable modules

<MathBox title='' boxType='proposition'>
Let $\mathrm{T}\in\mathcal{L}(V)$ have minimal polynomial $m_\mathrm{T}(x) = \prod_{i=1}^n p_i^{e_i}(x)$, where $p_i(x)$ are distinct monic primes. The following are equivalent:
1. $V_\mathrm{T}$ is cyclic
2. $V_\mathrm{T}$ is the direct sum $V_\mathrm{T} = \bigoplus_{i=1}^k \langle\mathbf{v}_i \rangle$
3. The elementary divisors of $\mathrm{T}$ are
$$
  \operatorname{ElemDiv}(\mathrm{T}) = \Set{p_i^{e_i}(x)}_{i=1}^n
$$
4. $\mathrm{T}$ is derogatory, i.e. $m_\mathrm{T}(x) = c_\mathrm{T}(x)$.
</MathBox>

### Indecomposable modules

<MathBox title='' boxType='proposition'>
Let $\mathrm{T}\in\mathcal{L}(V)$ be a linear operator and let $p(x)$ be a prime factor of $m_\mathrm{T}(x)$. Then $V_\mathrm{T}$ has a cyclic submodule $W_\mathrm{T}$ of prime order $p(x)$.
</MathBox>

<MathBox title='' boxType='proposition'>
For a module $W_\mathrm{T}$ of prime order $m_\mathrm{T}(x)$, the following are equivalent:
1. $W_\mathrm{T}$ is cyclic
2. $W_\mathrm{T}$ is indecomposable
3. $W_\mathrm{T}$ is irreducible
4. $\mathrm{T}$ is nonderogatory
5. $\dim(W_\mathrm{T}) = \deg(p(x))$
</MathBox>

### Companion matrices

If $\mathrm{T}\in\mathcal{L}(V)$ is a linear operator, we can characterize the cyclic module $V_\mathrm{T}$ via the matrix representation of $\mathrm{T}$. Let $V_\mathrm{T} = \langle\mathbf{v}\rangle$ be a cyclic module with order $m_\mathrm{T}(x) =  \left(\sum_{i=0}^{n-1} a_i x^i\right) + x^n$ and ordered $\mathrm{T}$-cyclic basis $B = (\mathbf{v}, \mathrm{T}\mathbf{v},\dots,\mathrm{T}^{n-1}\mathbf{v})$. Then $\mathrm{T}(\mathrm{T}^i\mathbf{v}) = \mathrm{T}^{i+1}\mathbf{v}$ for $0\leq i \leq n-2$ and

$$
\begin{align*}
  \mathrm{T}(\mathrm{T}^{n-1}\mathbf{v}) =& \mathrm{T}^n\mathbf{v} \\
  =& -\left(\sum_{i=0}^{n-1} a_i \mathrm{T}^i \right)\mathbf{v}
\end{align*}
$$

and so

$$
  [\mathrm{T}]_B = \begin{bmatrix}
    0 & 0 & \cdots & 0 & -a_0 \\
    1 & 0 & \cdots & 0 & -a_1 \\
    0 & 1 & \ddots & & \vdots \\
    \vdots & \vdots & \ddots & 0 & -a_{n-2} \\
    0 & 0 & \cdots & 1 & -a_{n-1} \\
  \end{bmatrix}
$$

This is known as the *companion matrix* for the polynomial $m_\mathrm{T}(x)$.

<MathBox title='Companion matrix' boxType='definition'>
The *companion matrix* of a monic polynomial

$$
  p(x) = \left(\sum_{i=1}^{n-1} a_i x^i \right) + x^n
$$

is the matrix

$$
  C[p(x)] = \begin{bmatrix}
    0 & 0 & \cdots & 0 & -a_0 \\
    1 & 0 & \cdots & 0 & -a_1 \\
    0 & 1 & \ddots & & \vdots \\
    \vdots & \vdots & \ddots & 0 & -a_{n-2} \\
    0 & 0 & \cdots & 1 & -a_{n-1} \\
  \end{bmatrix}
$$
</MathBox>

<MathBox title='Companion matrix' boxType='proposition'>
Let $p(x)\in\mathbb{F}[x]$.
1. A companion matrix $\mathbf{A} = C[p(x)]$ is nonderogatory; in fact
$$
  c_\mathbf{A}(x) = m_\mathbf{A}(x) = p(x)
$$
2. $V_\mathrm{T}$ is cyclic if and only if $\mathrm{T}$ can be represented by a companion matrix, in which case the representing basis is $\mathrm{T}$-cyclic.

<details>
<summary>Proof</summary>

**(1):** Let $E = (\mathbf{e}_i)_{i=1}^n$ be the standard basis for $\mathbb{F}^n$. Since $\mathbf{e}_i = \mathbf{A}^{i-1} \mathbf{e}_1$ for $i \geq 2$, it follows that for any polynomial $f(x)$

$$
  f(\mathbf{A}) = 0 \iff f(\mathbf{A})\mathbf{e}_i = 0\;\forall i \iff f(\mathbf{A})\mathbf{e}_1 = 0
$$

If $p(x) = \left(\sum_{i=0}^{n-1} a_i x^i\right) + x^n$, then

$$
\begin{align*}
  p(\mathbf{A})\mathbf{e}_1 =& \sum_{i=0}^{n-1} a_i \mathbf{A}^i\mathbf{e}_1 + \mathbf{A}^n \mathbf{e}_1 \\
  =& \sum_{i=0}^{n-1} a_i \mathbf{e}_{i+1} - \sum_{i=0}^{n-1} a_i \mathbf{e}_{i+1} = 0
\end{align*}
$$

hence $p(\mathbf{A}) = 0$. Also, if $q(x) = \sum_{i=0}^m b_i x^i$ is nonzero and has degree $m < n$, then

$$
  q(\mathbf{A})\mathbf{e}_1 = \sum_{i=0}^n b_i \mathbf{e}_{i+1} \neq 0
$$

since $E$ is linearly independent. Hence, $p(x)$ has smallest degree among all polynomials satisfied by $\mathbf{A}$ and so $p(x) = m_\mathbf{A}(x)$. Finally,

$$
  \deg(m_\mathbf{A}(x)) = \deg(p(x)) = \deg(c_\mathbf{A}(x))
$$

**(2):** We know that if $V_\mathrm{T}$ with $\mathrm{T}$-cyclic basis $B$, then $[\mathrm{T}]_B = C[p(x)]$. Conversely, if $[\mathrm{T}]_B = C[p(x)]$, then **(1)** implies that $\mathrm{T}$ is nonderogatory. Hence, the previous theorem implies that $V_\mathrm{T}$ is cyclic. It is clear from the form of $C[p(x)]$ that $B$ is a $\mathrm{T}$-cyclic basis for $V$.
</details>
</MathBox>

## Connection between linear operators and their matrix representations

<MathBox title='' boxType='proposition'>
Let $V$ be a finite-dimensional vector space over $\mathbb{F}$. Let $\mathrm{S},\mathrm{T}\in\mathcal{L}(V)$ and $\mathbf{A}, \mathbf{B}\in\mathcal{M}_n (\mathbb{F})$.
1. The multiset of elementary divisors (or invariant factors) is a complete invariant for similarity of operators:
$$
\begin{align*}
  \mathrm{S}\sim\mathrm{T} \iff& V_\mathrm{S} \cong V_\mathrm{T} \\
  \iff& \operatorname{ElemDiv}(\mathrm{S}) = \operatorname{ElemDiv}(\mathrm{T}) \\
  \iff& \operatorname{InvFact}(\mathrm{S}) = \operatorname{InvFact}(\mathrm{T}) 
\end{align*}
$$
A similar statement holds for matrices:
$$
\begin{align*}
  \mathbf{A}\sim\mathbf{B} \iff& \mathbb{F}_\mathbf{A}^n \cong \mathbb{F}_\mathbf{B}^n \\
  \iff& \operatorname{ElemDiv}(\mathbf{A}) = \operatorname{ElemDiv}(\mathbf{B}) \\
  \iff& \operatorname{InvFact}(\mathbf{A}) = \operatorname{InvFact}(\mathbf{B}) 
\end{align*}
$$
2. If $B$ is a basis for $V$, the connection between operators and the representing matrices is
$$
\begin{align*}
  \mathbf{A} = [\mathrm{T}]_B \iff& V_\mathrm{T} \cong \mathbb{F}_\mathbf{A}^n \\
  \iff& \operatorname{ElemDiv}(\mathrm{T}) = \operatorname{ElemDiv}(\mathbf{A}) \\
  \iff& \operatorname{InvFact}(\mathrm{T}) = \operatorname{InvFact}(\mathbf{A})
\end{align*}
$$

<details>
<summary>Proof</summary>

**(1):** This follows from the isomorphic characterization of modules of linear operators and the fact that the elementary divisors form a complete invariant for isomorphism.

**(2):** If $\mathbf{A} = [\mathrm{T}]_B$, then the coordinate map $\phi_B: V\cong\mathbb{F}^n$ is also a module isomorphism $\phi_B: V_\mathrm{T} \to\mathbb{F}_\mathbf{A}^n$. Specifically, we have

$$
  \phi_B (p(\mathrm{T})\mathbf{v}) = [p(\mathrm{T})\mathbf{v}]_B = p([\mathrm{T}]_B)[\mathbf{v}]_B = p(\mathbf{A})\phi_B (\mathbf{v})
$$

and so $\phi_B$ preserves $\mathbb{F}[x]$-scalar multiplication. Hence $\mathbf{A} = [\mathrm{T}]_B \implies V_\mathrm{T} \cong \mathbb{F}_\mathbf{A}^n$.

Conversely, suppose that $\sigma:V_\mathrm{T} \cong \mathbb{F}_\mathbf{A}^n$. If we define $\mathbf{b}_i \in V$ by $\sigma\mathbf{b}_i = \mathbf{e}_i$, where $\mathbf{e}_i$ is the $i$th canonical standard basis vector, then $B = (b_i)_{i=1}^n$ is an ordered basis for $V$ and $\sigma = \phi_B$ is the coordinate map for $B$. Hence, $\phi_B$ is a module isomorphism such that $\phi_B (\mathrm{T}\mathbf{v}) = \mathrm{T}_\mathbf{A}(\phi_B \mathbf{v})$ for all $\mathbf{v}\in B$. Or equivalently, $[\mathrm{T}\mathbf{v}]_B = \mathrm{T}_\mathbf{A}([\mathbf{v}]_B)$ which shows that $\mathbf{A} = [\mathrm{T}]_B$.
</details>
</MathBox>

## Rational canonical form

<MathBox title='Elementary divisor form' boxType='definition'>
A matrix $\mathbf{A}\in\mathcal{M}_n (\mathbb{F})$ is in the *elementary divisor form* of *rational canonical form* if

$$
  \mathbf{A} = \operatorname{diag}(C[r_1^{e_1}(x)],\dots,C[r_n^{e_n}(x)])
$$

where the $r_i (X)$ are monic prime polynomials.
</MathBox>

Let $\mathrm{T}\in\mathcal{V}$ be a linear operator on an $n$-dimensional vector space $V$. The elementary divisor basis $R$ for $V_\mathrm{T}$ that gives the primary cyclic decomposition of $V_\mathrm{T}$

$$
  V_\mathrm{T} = (\langle\mathbf{v}_{1,1}\rangle\oplus\cdots\oplus\langle\mathbf{v}_{1,k_1} \rangle)\oplus\cdots\oplus (\langle\mathbf{v}_{n,1}\rangle\oplus\cdots\oplus\langle\mathbf{v}_{n,k_n}\rangle)
$$

is the union of the bases

$$
  B_{i,j} = (\mathbf{v}_{i,j},\mathrm{T}\mathbf{v}_{i,j},\dots,\mathrm{T}^{d_{i,j}-1}\mathbf{v}_{i,j})
$$

and so the matrix of $\mathrm{T}$ with respect to $R$ is the block diagonal matrix

$$
  [\mathrm{T}]_R = \operatorname{diag}(C[p_1^{e_{1,1}}(x)],\dots,C[p_1^{e_{1,k_1}}(x)],\dots,C[p_n^{e_{n,1}}(x)],\dots,C[p_n^{e_n,k_n}])
$$

with companion matrices on the block diagonal.

<MathBox title='Rational canonical form of a linear operator: elementary divisor version' boxType='proposition'>
Let $V$ be a finite-dimensional vector space and let $\mathrm{T}\in\mathcal{L}(V)$ have minimal polynomial $m_\mathrm{T}(x) = \prod_{i=1}^n p_i^{e_i}(x)$, where the $p_i(x)$ are distinct monic prime polynomials.
1. If $\mathcal{R}$ is an elementary divisor basis for $V_\mathrm{T}$, then $[\mathrm{T}]_\mathcal{R}$ is in the elementary divisor form of rational canonical form
$$
  [\mathrm{T}]_\mathcal{R} = \operatorname{diag}\left(C[p_1^{e_{1,1}}(x)],\dots,C[p_1^{e_{1,k_1}}(x)],\dots,C[p_n^{e_{n,1}}(x)],\dots,C[p_n^{e_{n,k_n}}(x)] \right)
$$
where $p_k^{e_{k,i}}(x)$ are the elementary divisors of $\mathrm{T}$. This block diagonal matrix is called an *elementary divisor version* of a *rational canonical form* of $\mathrm{T}$.

2. Each similarity class $\mathcal{S}$ of matrices contains a matrix $R$ in the elementary divisor form of rational canonical form. Moreover, the set of matrices in $\mathcal{S}$ that have this form is the set of matrices obtained from $M$ by reordering the block diagonal matrices. Any such matrix is called and *elementary divisor version* of a *rational canoncial form* of $\mathbf{A}$.

3. The dimension of $V$ is the sum of the degrees of the elementary divisors of $\mathrm{T}$
$$
  \dim(V) = \sum_{i=1}^n \sum_{j=1} \deg(p_i^{e_{i,j}})
$$
</MathBox>

### Invariant factor version

<MathBox title='Invariant factor form' boxType='definition'>
A matrix $\mathbf{A}\in\mathcal{M}_n (\mathbb{F})$ is in the *invariant factor form* of *rational canonical form* if

$$
  \mathbf{A} = \operatorname{diag}(C[s_1(x)],\dots,C[s_n(x)])
$$

where $s_{k+1}(x) | s_k (x)$ for $k = 1,\dots,n-1$.
</MathBox>

<MathBox title='' boxType='proposition'>
If $p(x), q(x)\in\mathbb{F}[x]$ are relatively prime polynomials, then

$$
  C[p(x)q(x)] \sim \begin{bmatrix} C[p(x)] & \mathbf{0} \\ \mathbf{0} & C[q(x)] \end{bmatrix} 
$$

<details>
<summary>Proof</summary>

If an $m\times m$-matrix $\mathbf{A}\in\mathcal{M}_m (\mathbb{F})$ has minimal polynomial $m_\mathrm{T}(x) = \prod_{i=1}^n p_i^{e_i}(x)$ of degree equal to the size $m$ of the matrix, then the previous theorem implies that the elementary divisors of $\mathbf{A}$ are precisely $\prod_{i=1}^n p_i^{e_i}(x)$. Since the matrices $C[p(x)q(x)]$ and $\operatorname{diag}(C[p(x)],C[q(x)])$ have the same size $m\times m$ and the same minimal polynomial $p(x)q(x)$ of degree $m$, it follows that they have the same multiset of elementary divisors and so are similar.
</details>
</MathBox>

<MathBox title='Rational canonical form of a linear operator: invariant factor version' boxType='proposition'>
Let $V$ be a finite-dimensional vector space and let $\mathrm{T}\in\mathcal{L}(V)$ have minimal polynomial $m_\mathrm{T}(x) = \prod_{i=1}^n p_i^{e_i}(x)$, where the $p_i(x)$ are distinct monic prime polynomials.
1. $V$ has an *invariant factor basis* $B$, i.e. a basis for which
$$
  [\mathrm{T}]_B = \operatorname{diag}(C[s_1(x)],\dots,C[s_n(x)])
$$
where the polynomials $s_k(x)$ are the invariant factors of $\mathrm{T}$ and $s_{k+1}(x) | s_k (x)$. This block diagonal matrix is called an *invariant factor* of a *rational canonical form* of $\mathrm{T}$.

2. Each similarity class $\mathcal{S}$ of matrices contains a matrix $R$ in the invariant factor form of rational canonical form. Moreover, the set of matrices in $\mathcal{S}$ that have this form is the set of matrices obtained from $M$ by reordering the block diagonal matrices. Any such matrix is called and *invariant factor version* of a *rational canoncial form* of $\mathbf{A}$.

3. The dimension of $V$ is the sum of the degrees of the invariant factors of $\mathrm{T}$
$$
  \dim(V) = \sum_{i=1}^n \deg(s_i)
$$
</MathBox>

### Determinant form of characteristic polynomials

<MathBox title='' boxType='lemma'>
For any $p(x)\in\mathbb{F}$

$$
  \det(x\mathrm{I} - C[p(x)]) = p(x)
$$

<details>
<summary>Proof</summary>

Let $\mathbf{A} = C[p_n(x)]$ be the companion matrix of a monic polynomial

$$
  p_n (x; a_0, \dot, a_{n-1}) = \left(\sum_{i=0}^{n-1} a_i x^i \right) + x^n
$$

We want to recovery $p(x) = c_\mathbf{A}(x)$ from $C[p(x)]$ by arithmetic operations. Note that for $n=2$, we can write $p_2(x)$ as

$$
  p_2 (x; a_0, a_1) = a_0 + a_1 x + x^2 = x(x + a_1) + a_0
$$

which corresponds to a determinant

$$
\begin{align*}
  p_2(x; a_0, a_1) =& \det\begin{bmatrix} x & a_0 \\ -1 & x + a_1 \end{bmatrix} \\
  =& \det\left( x\mathrm{I} - \begin{bmatrix} 0 & -a_0 \\ 1 & -a_1 \end{bmatrix}\right) \\
  =& \det(x\mathrm{I} - C[p_2(x)])
\end{align*}
$$

In the general case, define

$$
\begin{align*}
  \mathbf{A}(x;a_0,\dots,a_{n-1}) =& x\mathrm{I} - C[p_n(x)] \\
  =& \begin{bmatrix} 
    x & 0 & \cdots & 0 & a_0 \\ 
    -1 & x & \cdots & 0 & a_1 \\
    0 & -1 & \ddots & & \vdots \\
    \vdots & \vdots & \ddots & x & a_{n-2} \\
    0 & 0 & \cdots & -1 & x + a_{n-1}
  \end{bmatrix}
\end{align*}
$$

where $x$ is an independent variable. The determinant of this matrix is a polynomial in $x$ whose degree equals the number of parameters $a_0,\dots,a_{n-1}$. We have just shown that $\det(\mathbf{A}(x;a_0,a_1)) = p_2(x;a_0,a_1)$, which also trivially true for $n=1$. As a basis for induction, if

$$
  \det(\mathbf{A}(x;a_0,\dots,a_{n-1})) = p_n(x;a_0,\dots,a_{n-1})
$$

then expanding along the first row gives

$$
\begin{align*}
  &\det(\mathbf{A}(x;a_0,\dots,a_{n-1})) \\
  =& x\det(A(x;a_1,\dots,a_n)) + (-1)^n a_0 \det\begin{bmatrix} 
    -1 & x & \cdots & 0 \\ 
    0 & -1 & \ddots & \vdots \\ 
    \vdots & \vdots & \ddots & x \\
    0 & 0 & \cdots & -1  
  \end{bmatrix} \\
  =& x\det(\mathbf{A}(x;a_1,\dots,a_n)) + a_0 \\
  =& x p_n(x; a_1,\dots,a_n) + a_0 \\
  =& \left(\sum_{i=0}^{n} a_i x^i \right) x^{n+1} \\
  =& p_{n+1}(x;a_0,\dots,a_n)
\end{align*}
$$
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $\mathrm{T}\in\mathcal{L}(V)$ be a linear operator. If $\mathbf{A}$ is any matrix representing $\mathrm{T}$, then

$$
  c_\mathrm{T} (x) = c_\mathbf{A} (x) = \det(x\mathbf{I} - \mathbf{A})
$$

<details>
<summary>Proof</summary>

Suppose that $\mathbf{R}$ is a matrix in the elementary divisor form of rational canonical form. Since the determinant of a block diagonal matrix is the product of the determinants of the blocks on the diagonal, it follows that

$$
  \det(x\mathbf{I} - \mathbf{R}) = \prod_{i,j} p_i^{e_{i,j}}(x) = c_\mathbf{R}(x)
$$

Moreover, if $\mathbf{A}\sim\mathbf{R}$, say $\mathbf{A} = \mathbf{PRP}^{-1}$, then

$$
\begin{align*}
  \det(x\mathbf{I} - \mathbf{A}) =& \det(x\mathbf{I} - \mathbf{PRP}^{-1}) \\
  =& \det[\mathbf{P}(x\mathbf{I} - \mathbf{R})\mathbf{P}^{-1}] \\
  =& \det(\mathbf{P})\det(x\mathbf{I} - \mathbf{R})\det(\mathbf{P}^{-1}) \\
  =& \det(x\mathbf{I} - \mathbf{R})
\end{align*}
$$

and so

$$
  \det(x\mathbf{I} - \mathbf{A}) = \det(x\mathbf{I} - \mathbf{R}) = c_\mathbf{R}(x) = c_\mathbf{A}(x)
$$
</details>
</MathBox>

### Changing the base field

<MathBox title='' boxType='proposition'>
Let $\mathbb{F}$ and $\mathbb{K}$ be fields with $\mathbb{F}\subseteq\mathbb{K}$. Suppose that the elementary divisors of a matrix $\mathbf{A}\in\mathcal{M}_n (\mathbb{F})$ are

$$
  A = \Set{p_1^{e_{1,1}},\dots,p_1^{e_{1,k_1}},\dots,p_n^{e_{n,1}},\dots,p_n^{e_{n,k_n}}}
$$

Suppose also that the polynomials $p_i$ can be further factored over $\mathbb{K}$, say

$$
  p_i = \prod_{j=1}^{m_i} a_{i,j}^{d_{i,j}}
$$

where $a_{i,j}$ is prime over $\mathbb{K}$. Then the prime powers

$$
  B = \Set{a_{1,1}^{d_{1,1}e_{1,1}},\dots,a_{1,m_1}^{d_{1,m_1, e_{1,1}}}, \dots,\dots,a_{n,1}^{d_{n,1}e_{n,k_n}},\dots,a_{n,m}^{d_{n,m_0}e_{n,k_n}}}
$$

are the elementary divisors of $\mathbf{A}$ over $\mathbb{K}$.

<details>
<summary>Proof</summary>

Consider the companion matrix $C[p_i^{e_{i,j}}(x)]$ in the rational canonical form of $\mathbf{A}$ over $\mathbb{F}$. This is a matrix over $\mathbb{K}$ as well and it follows that

$$
  C[p_i^{e_{i,j}}(x)] \sim \operatorname{diag}(C[a_{i,1}^{d_{i,1}e_{i,j}}],\dots,C[a_{i,m_i}^{d_{i,m_i}e_{i,j}}])
$$

Hence, $B$ is an elementary divisor basis for $\mathbf{A}$ over $\mathbb{K}$.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $\mathbf{A}\in\mathcal{M}_n (\mathbb{F})$ and let $\mathbb{E}\subseteq\mathbb{F}$ be the smallest subfield of $\mathbb{F}$ that contains the entries of $\mathbf{A}$.
1. The invariant factors of $\mathbf{A}$ are polynomials over $\mathbb{E}$.
2. Two matrices $\mathbf{A},\mathbf{B}\in\mathcal{M}_n (\mathbb{F})$ are similar over $\mathbb{F}$ if and only if they are similar over $\mathbb{E}$.

<details>
<summary>Proof</summary>

**(1):** This follows immediately from the previous proposition, since using either $A$ or $B$ to compute invariant factors gives the same result.

**(2):** This follows from the fact that two matrices are similar over a given field if and only if they have the same multiset of *invariant factors* over the field.
</details>
</MathBox>

# Eigenvalues and eigenvectors

Let $\mathbf{A} = [\mathrm{T}]_B$ be a matrix representing a linear operator $\mathrm{T}\in\mathcal{L}(V)$ on an $\mathbb{F}$-vector space $V$. A scalar $\lambda\in\mathbb{F}$ is a root of the characteristic polynomial $c_\mathrm{T}(x) = c_\mathbf{A}(x) = \det(x\mathrm{A} - \mathbf{A})$ if and only if

$$
  \det(\lambda\mathbf{I} - \mathbf{A}) = 0
$$

or equivalently, if and only if the matrix $\lambda\mathbf{I} - \mathbf{A}$ is singular. In particular, if $\dim(V) = n$, then the characteristic equation holds if and only if there exists a nonzero vector $x\in\mathbb{F}^n$ for which

$$
  (\lambda\mathbf{I} - \mathbf{A})\mathbf{x} = 0
$$

or equivalently

$$
  \mathrm{T}_\mathbf{A}x = \lambda x
$$

If $[\mathbf{v}]_B = x$, then this is equivalent to $[\mathrm{T}]_B [\mathbf{v}]_B = \lambda[\mathbf{v}]_B$, or in operator language

$$
  \mathrm{T}\mathbf{v} = \lambda\mathbf{v}
$$

<MathBox title='Eigenvalue, eigenvector and spectrum' boxType='proposition'>
Let $V$ be a vector space over an algebraically closed field $\mathbb{F}$, which means that every non-constant polynomial has a root in $\mathbb{F}$. 
1. A scalar $\lambda\in\mathbb{F}$ is an *eigenvalue* of an operator $\mathrm{T}\in\mathcal{L}(V)$ if there exists a nonzero vector $\boldsymbol{v}\in V$ for which $\mathrm{T}(\boldsymbol{v}) = \lambda\boldsymbol{v}$. In this case, $\boldsymbol{v}$ is an *eigenvector* of $\mathrm{T}$ associated with $\lambda$.
2. A scalar $\lambda\in\mathbb{F}$ is an eigenvalue for a matrix $\boldsymbol{A}\in\mathcal{M}_{n}(\mathbb{F})$ if there exists a nonzero column vector $\boldsymbol{x}$ for which $\boldsymbol{A}\boldsymbol{x} = \lambda\boldsymbol{x}$. In this case, $\boldsymbol{x}$ is an eigenvector for $\boldsymbol{A}$ associated with $\lambda$.
3. The set of all eigenvectors associated with a given eigenvalue $\lambda$, together with the zero vector, forms a subspace of $V$, called the *eigenspace* of $\lambda$, denoted by $E_\lambda$.
4. The set of all eigenvalues of an operator or matrix is called the *spectrum* of the operator or matrix. The spectrum of $\mathrm{T}$ is denoted $\operatorname{Spec}(\mathrm{T})$.
</MathBox>

<MathBox title='Existence of eigenvectors' boxType='proposition'>
Every $n\times n$ matrix $\boldsymbol{A}\in\mathcal{M}_n (\mathbb{F})$ has an eigenvalue.

<details>
<summary>Proof</summary>

Assume $\boldsymbol{A}$ represent a linear operator $\mathrm{T}_{\boldsymbol{A}}:V\to V$ and choose any nonzero $\boldsymbol{v}\in V$. Consider the vectors $\Set{\boldsymbol{A}^i \boldsymbol{v}}_{i=0}^n$, where $\boldsymbol{A}^0 = \boldsymbol{I}$. Since this set has $n + 1$ vectors it must be linearly dependent. Thus,

$$
\begin{align*}
  \boldsymbol{0} =& \sum_{i=0}^n \alpha_i \boldsymbol{A}^i \boldsymbol{v} = \left(\sum_{i=0}^n \alpha_i \boldsymbol{A}^i\right)\boldsymbol{v} \\
  c(\boldsymbol{A})
\end{align*}
$$
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $\mathrm{T}\in\mathcal{L}(V)$ have minimal polynomial $m_\mathrm{T}(x)$ and characteristic polynomial $c_\mathrm{T}(x)$.
1. The spectrum of $\mathrm{T}$ is the set of all roots of $m_\mathrm{T}(x)$ or of $c_\mathrm{T}(x)$, not counting multiplicity.
2. The eigenvalues of a matrix are invariants under similarity.
3. The eigenspace $E_\lambda$ of the matrix $\mathbf{A}$ is the solution space to the homogeneous system of equations
$$
  (\lambda\mathbf{I} - \mathbf{A})(x) = 0
$$
</MathBox>

<MathBox title='Relation between eigenspaces and eigenvectros of distinct eigenvalues' boxType='proposition'>
Suppose that $\lambda_1,\dots,\lambda_k$ are distinct eigenvalues of a linear operator $\mathrm{T}\in\mathcal{L}(V)$.
1. Eigenvectors associated with distinct eigenvalues are linearly independent. That is, if $\mathbf{v}_i \in E_\lambda$, then the set $\Set{\mathbf{v}_i}_{i=1}^k$ is linearly independent.
2. The sum $\sum_{i=1} E_{\lambda_i}^k$ is direct, i.e. $\bigoplus_{i=1}^k E_{\lambda_i}$ exists.

<details>
<summary>Proof</summary>

**(1):** If $\Set{\mathbf{v}_i}_{i=1}^k$ is linearly dependent, the by renumbering if necessary, we may assume that among all nontrivial linear combinations of these vectors that equal $0$, the equation

$$
  \sum_{i=1}^j r_i \mathbf{v}_i = 0
$$

has the fewest number of terms. Applying $\mathrm{T}$ gives

$$
  \sum_{i=1}^j r_i \mathrm{T}\mathbf{v}_i = \sum_{i=1}^j r_i \lambda_i \mathbf{v}_i = 0
$$

Multiplying by $\lambda_1$ and subtracting gives

$$
  r_2 (\lambda_2 - \lambda_1)\mathbf{v}_2 +\cdots+ r_j (\lambda_j - \lambda_1)\mathbf{v}_j = 0
$$

However, this equation has fewer terms and so all of its coefficients must equal $0$. Since the $\lambda_i$ are distinct, $r_i = 0$ for $i \geq 2$ and so $r_1 = 0$. This contradiction implies that the $\mathbf{v}_i$ are linearly independent.
</details>
</MathBox>

<MathBox title='Spectral mapping theorem' boxType='theorem'>
Let $V$ be a vector space over an algebraically closed field $\mathbb{F}$. Let $\mathrm{T}\in\mathcal{L}(V)$ and let $p(x) \in \mathbb{F}[x]$. Then

$$
  \operatorname{Spec}(p(\mathrm{T})) = p(\operatorname{Spec}(\mathrm{T})) = \Set{p(\lambda) | \lambda\in\operatorname{Spec}(\mathrm{T})}
$$

<details>
<summary>Proof</summary>

If $\lambda$ is an eigenvalue of $\mathrm{T}$, then it can be shown that $p(\lambda)$ is an eigenvalue of $p(\mathrm{T})$. Hence, $p(\operatorname{Spec}(\mathrm{T})) \subseteq \operatorname{Spec}(p(\mathrm{T}))$. For the reverse inclusion, let $\lambda\in\operatorname{Spec}(p(\mathrm{T}))$, i.e.

$$
  (p(\mathrm{T}) - \lambda)\mathbf{v} = 0,\; \mathbf{v}\neq\mathbf{0}
$$

If

$$
  p(x) - \lambda = \prod_{i=1}^n (x-r_i)^{e_i}
$$

where $r_i \in\mathbb{F}$, then writing this as a product of (not necessarily distinct) linear factors, we have

$$
  (\mathrm{T} - r_1)\cdots(\mathrm{T} - r_1)\cdots(\mathrm{T} - r_n)\cdots(\mathrm{T} - r_n)\mathbf{v} = 0
$$

where $r_i \mathrm{I}$ is written $r_i$ for convenience. We can remove factors from the left end of this equation one by one until we arrive at an operator $\mathrm{S}$ (perhaps the identity) for which $\mathrm{S}\mathbf{v} \neq 0$ but $(\mathrm{T} - r_i\mathrm{I})\mathrm{S}\mathbf{v} = 0$. Then $\mathrm{S}\mathbf{v}$ is an eigenvector for $\mathrm{T}$ with eigenvalue $r_i$. However, since $p(r_i) - \lambda = 0$, it follows that $\lambda = p(r_i)\in p(\operatorname{Spec}(\mathrm{T}))$. Hence, $\operatorname{Spec}(p(\mathrm{T})) \subseteq p(\operatorname{Spec}(\mathrm{T}))$.
</details>
</MathBox>

## Trace and determinant


<MathBox title='Relation between eigenvalues and the determinant' boxType='proposition'>
Let $\mathbf{A}\in\mathcal{M}_n (\mathbb{F})$ be an $n\times n$-matrix with characteristic polynomial $c_\mathbf{A} = x^n + \sum_{i=0}^{n-1}$ eigenvalues $\lambda_1,\dots,\lambda_n$. If $\mathbb{F}$ is algebraically closed, then $\det(\mathbf{A})$ is the constant term of $c_\mathbf{A}$ and the product of the eigenvalues of $\mathbf{A}$ 

$$
  \det(\mathbf{A}) = (-1)^n \prod_{i=1}^n
$$

<details>
<summary>Proof</summary>

Let $\mathbb{F}$ be algebraically closed and let $\mathbf{A}\in\mathcal{M}_n (\mathbb{F})$ have characteristic polynomial

$$
\begin{align*}
  c_\mathbf{A}(x) =& x^n + \sum_{i=0}^{n-1} a_i x^i \\
  =& \prod_{i=1}^n (x - \lambda_i)
\end{align*}
$$

where $\lambda_i$ are the eigenvalues of $\mathbf{A}$. Then $c_\mathbf{A} (x) = \det(x\mathbf{I} - \mathbf{A})$ and setting $x = 0$ gives

$$
  \det(\mathbf{A}) = -c_0 = (-1)^{n-1} \prod_{i=1}^n \lambda_i
$$

Hence, if $\mathbb{F}$ is algebraically closed then, up to sign, $\det(\mathbf{A})$ is the constant term of $c_\mathbf{A}(x)$ and the product of the eigenvalues of $\mathbf{A}$, including multiplicity.
</details>
</MathBox>

<MathBox title='Relation between eigenvalues and the trace' boxType='proposition'>
Let $\mathbf{A}\in\mathcal{M}_n (\mathbb{F})$ be an $n\times n$-matrix with characteristic polynomial $c_\mathbf{A} = x^n + \sum_{i=0}^{n-1}$ eigenvalues $\lambda_1,\dots,\lambda_n$. If $\mathbb{F}$ is algebraically closed, then $operatorname{tr}(\mathbf{A})$ is the sum of eigenvalues of $\mathbf{A}$, including multiplicity, i.e.

$$
  \operatorname{tr}(\mathbf{A}) = -c_{n-1} = \sum_{i=1}^n \lambda_i 
$$
</MathBox>

Recall that the coefficients of any polynomial

$$
\begin{align*}
  p(x) =& x^n + \sum_{i=0}^{n-1} c_i x^i \\
  =& \prod_{i=1}^n (x - \lambda_i)
\end{align*}
$$

are the *elementary symmetric functions* of the roots

$$
\begin{align*}
  c_{n-1} =& (-1)^1 \sum_i \lambda_i \\
  c_{n-2} =& (-1)^2 \sum_{i < j} \lambda_i \lambda_j \\
  c_{n-3} =& (-1)^3 \sum_{i < j < k} \lambda_i \lambda_j \lambda_k \\
  \vdots& \\
  c_0 =& (-1)^n \prod_{i=1}^n \lambda_i
\end{align*}
$$

The most important elementary symmetric of the eigenvalues are the first and last ones.

## Algebraic and geometric multiplicities

<MathBox title='Algebraic and geometric multiplicity of eigenvalues' boxType='definition'>
Let $\lambda$ be an eigenvalue of a linear operator $\mathrm{T}\in\mathcal{L}(V)$.
1. The *algebraic multiplicity* of $\lambda$ is the multiplicity of $\lambda$ as a root of the characteristic polynomial $c_\mathrm{T}(x)$.
2. The *geometric multiplicity* of $\lambda$ is the dimension of the eigenspace $E_\lambda$.
</MathBox>

<MathBox title='Relation between algebraic and geometric multiplicity of eigenvalues' boxType='proposition'>
The geometric multiplicity of an eigenvalue $\lambda$ of $\mathrm{T}\in\mathcal{L}(V)$ is less than or equal to its algebraic multiplicity.

<details>
<summary>Proof</summary>

We can extend any basis $B_1 = \Set{\mathbf{v}_i}_{i=1}^k$ of $E_\lambda$ to a basis $B$ for $V$. Since $E_\lambda$ is invariant under $\mathrm{T}$, the matrix of $\mathrm{T}$ with respect to $B$ has the block form

$$
  [\mathrm{T}]_B = \begin{bmatrix} \lambda \mathbf{I}_k & \mathbf{A} \\ \mathbf{0} & \mathbf{B} \end{bmatrix}
$$

where $\mathbf{A}$ and $\mathbf{B}$ are matrices of the appropriate sized and so

$$
\begin{align*}
  c_\mathbf{T} (x) =& \det(x\mathbf{I} - [\mathrm{T}]_B) \\
  =& \det(x\mathbf{I}_k - \lambda\mathbf{I}_k)\det(x\mathbf{I}_{n-k} - \mathbf{B}) \\
  =& (x - \lambda)^k \det(x\mathbf{I}_{n-k} - \mathbf{B})
\end{align*}
$$

Hence, the algebraic multiplicity of $\lambda$ is at least equal to the geometric multiplicity $k$ of $\mathrm{T}$.
</details>
</MathBox>

## Jordan canonical form

Recall that every linear operator $\mathrm{T}\in\mathcal{L}(V)$ on a finite-dimensional $\mathbb{F}$-vector space $V$ has a rational canonical form. When the minimal polynomial $m_\mathrm{T}(x)$ of $\mathrm{T}$ splits over $\mathbb{F}$, i.e.

$$
  m_\mathrm{T} (x) = \prod_{i=1}^n (x - \lambda_i)^{e_i}
$$

there is another set of canonical forms that is arguably simpler than the set of rational canonical forms.

The complexity of the rational canonical form comes from the choice of basis for the cyclic submodules $\langle\mathbf{v}_{i,j}\rangle$. The $\mathrm{T}$-cyclic bases have the form

$$
  B_{i,j} = (\mathbf{v}_{i,j},\mathrm{T}\mathbf{v}_{i,j},\dots,\mathrm{T}^{d_{i,j}-1}\mathbf{v}_{i,j}),\; d_{i,j} = \deg(p_i^{e_{i,j}})
$$

With this basis, all of the complexity arises when expressing

$$
  \mathrm{T}(\mathrm{T}^{d_{i,j}-1} (\mathbf{v}_{i,j})) = \mathrm{T}^{d_{i,j}}(\mathbf{v}_{i,j})
$$

as a linear combination of the basis vectors. However, from the form of $B_{i,j}$ any ordered set of the form

$$
  (p_0 (\mathrm{T})\mathbf{v}, p_1 (\mathrm{T})\mathbf{v},\dots,p_{d-1}(\mathrm{T})\mathbf{v})
$$

where $\deg(p_k(x)) = k$, will also be a basis for $\langle\mathbf{v}_{i,j}\rangle$. In particular, when $m_\mathrm{T}(x)$ splits over $\mathbb{F}$, the elementary divisors are

$$
  p_i^{e_{i,j}}(x) = (x - \lambda_i)^{e_{i,j}}
$$

and so the set

$$
  C_{i,j} = (\mathbf{v}_{i,j}, (\mathrm{T}-\lambda_i)\mathbf{v}_{i,j},\dots,(\mathrm{T}-\lambda_i)^{e_{i,j}-1}\mathbf{v}_{i,j})
$$

is also a basis for $\langle\mathbf{v}_{i,j}\rangle$. Denoting the $k$th basis vector in $C_{i,j}$ by $\mathbf{b}_k$, then for $k=0,\dots,e_{i,j}-2$

$$
\begin{align*}
  \mathrm{T}\mathbf{b}_k =& \mathrm{T}[(\mathrm{T}-\lambda_i)^k(\mathbf{v}_{i,j})] \\
  =& (\mathrm{T} - \lambda_i + \lambda_i)[(\mathrm{T} - \lambda_i)^k (\mathbf{v}_{i,j})] \\
  =& (\mathrm{T} - \lambda_i)^{k+1} (\mathbf{v}_{i,j}) + \lambda_i (\mathrm{T} - \lambda_i)^k (\mathbf{v}_{i,j}) \\
  =& b_{k+1} + \lambda_i b_k
\end{align*}
$$

For $k = e_{i,j} - 1$, using the fact that

$$
  (\mathrm{T} - \lambda_i)^{k+1}(\mathbf{v}_{i,j}) = (\mathrm{T} - \lambda_i)^{e_{i,j}}(\mathbf{v}_{i,j}) = 0
$$

gives

$$
  \mathrm{T}(\mathbf{b}_{e_{i,j} - 1}) = \lambda_i \mathbf{b}_{e_{i,j}-1}
$$

Thus, for this basis the matrix of $\mathrm{T}|_{\langle\mathbf{v}_{i,j}\rangle}$ with respect to $C_{i,j}$ is the $e_{i,j}\times e_{i,j}$ matrix

$$
  \mathbf{J}(\lambda_i, e_{i,j}) = \begin{bmatrix}
    \lambda_i & 0 & \cdots & \cdots & 0 \\
    1 & \lambda_i & \ddots & \vdots \\
    0 & 1 & \ddots & \ddots & \vdots \\
    \vdots & \ddots & \ddots & \ddots & 0 \\
    0 & \cdots & 0 & 1 & \lambda_i
  \end{bmatrix}
$$

called a *Jordan block* associated with the scalar $\lambda_i$. This matrix has $\lambda_i$ on the main diagonal, $1$ on the subdiagonal and $0$ elsewhere. The basis $C = \bigcup_{i,j} C_{i,j}$ is called a *Jordan basis* for $\mathrm{T}$.

<MathBox title='Jordan canonical form' boxType='proposition'>
Let $\mathrm{T}\in\mathcal{L}(V)$ be a linear operator on a finite-dimensional $\mathbb{F}$-vector space $V$. Suppose that the minimal polynomial of $\mathrm{T}\in\mathcal{L}(V)$ splits over the base field $\mathbb{F}$, i.e.

$$
  m_\mathrm{T} (x) = \prod_{i=1}^n (x - \lambda_i)^{e_i}
$$

where $\lambda_i\in\mathbb{F}$ are the eigenvalues of $\mathrm{T}$.
1. The matrix of $\mathrm{T}$ with respect to a Jordan basis $C$ is
$$
  \operatorname{diag}(\mathbf{J}(\lambda_i, e_{1,1}),\dots,\mathbf{J}(\lambda_i,e_{1,k_1}),\dots,\mathbf{J}(\lambda_n, e_{n,1}),\dots,\mathbf{J}(\lambda_n, e_{n,k_n}))
$$
where the polynomials $(x - \lambda_i)^{e_{i,j}}$ are the elementary divisors of $\mathrm{T}$. This block diagonal matrix is called the *Jordan canonical form* of $\mathrm{T}$.

2. If $\mathbb{F}$ is algebraically closed, then up to order of the block diagonal matrices, the set of matrices in Jordan canonical form constitutes a set of canonical forms for similarity.

<details>
<summary>Proof</summary>

**(2):** The companion matrix and corresponding Jordan block are similar, i.e.

$$
  \mathbf{C}[(x-\lambda_i)^{e_{i,j}}] \sim \mathbf{J}(\lambda_i,e_{i,j})
$$

since they both represent the operator $\mathrm{T}$ on the subspace $\langle\mathbf{v}_{i,j}\rangle$. It follows that the rational canonical matrix and the Jordan canonical matrix for $\mathrm{T}$ are similar.
</details>
</MathBox>

## Triangularizability

<MathBox title='Upper triangularizable linear operator' boxType='definition'>
A linear operator $\mathrm{T}\in\mathcal{L}(V)$ on an $n$-dimensional vector space $V$ is *upper triangularizable* if there is an ordered basis $B = \Set{\mathbf{v}_i}_{i=1}^n$ of $V$ for which the matrix $[\mathrm{T}]_B$ is upper triangular, or equivalently, if

$$
  \mathrm{T}\mathbf{v}_i \in \langle\mathbf{v}_1,\dots,\mathbf{v}_n \rangle,\; \forall i=0,\dots,n
$$
</MathBox>

<MathBox title="Schur's theorem" boxType='theorem'>
Let $V$ be a finite-dimensional vector space over a field $\mathbb{F}$.
1. If the characteristic polynomial (or minimal polynomial) of $\mathrm{T}\in\mathcal{L}(V)$ splits over $\mathbb{F}$, then $\mathrm{T}$ is upper triangularizable.
2. IF $\mathbb{F}$ is algebraically closed, then all operators are upper triangularizable.

<details>
<summary>Proof</summary>

**(1):** Using induction by matrix means, we want to show that every square matrix $\mathbf{A}\in\mathcal{M}_n (\mathbb{F})$ whose characteristic polynomial splits over $\mathbb{F}$ is similar to an upper triangular matrix. The base case $n=1$ is trivial since all $1\times 1$ matrices are by definition upper triangularizable. 

For the inductive hypothesis, assume that the result is true for $n - 1$. Let $\mathbf{v}_1$ be an eigenvector associated with the eigenvalue $\lambda_1 \in\mathbb{F}$ of $\mathbf{A}$ and extend $\Set{\mathbf{v}_1}$ to an ordered basis $B = \Set{\mathbf{v}_i}_{i=1}^n$ for $\mathbb{F}^n$. The matrix of $\mathrm{T}_\mathbf{A}$ with respect to $B$ has the form

$$
  [\mathrm{T}_\mathbf{A}]_B = \begin{bmatrix} \lambda_1 & * \\ \mathbf{0} & \mathbf{A}_1 \end{bmatrix}
$$

for some $\mathbf{A}_1 \in \mathcal{M}_{n-1}(\mathbb{F})$. Since $[\mathrm{T}_\mathbf{A}]_B$ and $\mathbf{A}$ are similar, we have

$$
\begin{align*}
  \det(x\mathbf{I} - \mathbf{A}) =& \det(x\mathbf{I} - [\mathrm{T}_\mathbf{A}]_B) \\
  =& (x - \lambda_1)\det(x\mathbf{I} - \mathbf{A})
\end{align*}
$$

Hence, the characteristic polynomial of $\mathbf{A}_1$ also splits over $\mathbb{F}$ and the induction hypothesis implies that there is an invertible matrix $\mathbf{P}\in\mathcal{M}_{n-1}(\mathbb{F})$ for which

$$
  \mathbf{U} = \mathbf{PA}_1\mathbf{P}^{-1}
$$

is upper triangular. Hence if

$$
  \mathbf{Q} = \begin{bmatrix} 1 & 0 \\ 0 & \mathbf{P} \end{bmatrix}
$$

then $\mathbf{Q}$ is invertible and

$$
\begin{align*}
  \mathbf{Q}[\mathbf{A}]_B \mathbf{Q}^{-1} =& \begin{bmatrix} 1 & 0 \\ 0 & \mathbf{P} \end{bmatrix} \begin{bmatrix} \lambda_1 & * \\ 0 & \mathbf{A}_1 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & \mathbf{P}^{-1} \end{bmatrix} \\
  =& \begin{bmatrix} \lambda_1 & * \\ 0 & \mathbf{U} \end{bmatrix}
\end{align*}
$$

is upper triangular.
</details>
</MathBox>

### The real case

<MathBox title='' boxType='proposition'>
Let $\mathrm{T}\in\mathcal{L}(V)$ be a linear operator over $\mathbb{R}$ with characteristic polynomial $c_\mathrm{T}(x)$. If $V_\mathrm{T}$ is a cyclic module and $\deg(c_\mathrm{T}(x)) = 2$, then there is an ordered basis $C$ for which
$$
  [\mathrm{T}]_C = \begin{bmatrix} a & -b \\ b & a \end{bmatrix}
$$

<details>
<summary>Proof</summary>

Suppose that $c_\mathrm{T}(x) = x^2 + sx + t$ is an irreducible quadratic. If $B$ is a $\mathrm{T}-cyclic$ basis for $V_\mathrm{T}$, then

$$
  [\mathrm{T}]_B = \begin{bmatrix} 0 & -t \\ 1 & -s \end{bmatrix}
$$

Let $\mathbf{A} = [\mathrm{T}]_B$. As a complex matrix, $\mathbf{A}$ has two distinct eigenvalues

$$
  \lambda = -\frac{s}{2} \pm i\frac{\sqrt{4t - s^2}}{2}
$$

Now, a matrix of the form $\mathbf{B} = \left[\begin{bmatrix} a & -b \\ b & a \end{bmatrix}\right]$ has characteristic polynomial $q(x) = (x - a)^2 + b$ and eigenvalues $a \pm ib$. If we set

$$
  a = -\frac{s}{2} \quad b = -\frac{\sqrt{4t - s^2}}{2}
$$

then $\mathbf{B}$ has the same two distinc eigenvalues as $\mathbf{A}$, and so $\mathbf{A}$ and $\mathbf{B}$ have the same Jordan canonical form over $\mathbb{C}$. It follows that $\mathbf{A}$ and $\mathbf{B}$ are similar over $\mathbb{C}$ and therefore also over $\R$. Thus, there is an ordered basis $C$ for which $[\mathrm{T}]_C = \mathbf{B}$.
</details>
</MathBox>

<MathBox title='Almost upper triangular matrix' boxType='theorem'>
A matrix $\mathbf{A}\in\mathcal{M}_n (\mathbb{F})$ is *almost upper triangular* if it has the form

$$
  \mathbf{A} = \begin{bmatrix} 
    \mathbf{A}_1 & & * & \\
   & \mathbf{A}_2 & & \\
   & & \ddots & \\
   & \mathbf{0} & & \mathbf{A}_k
  \end{bmatrix}
$$

where $A_i = [a]$ or $A_i = \left[\begin{smallmatrix} a & -b \\ b & a \end{smallmatrix}\right]$ for $a, b\in\mathbb{F}$. A linear operator $\mathrm{T}\in\mathcal{L}(V)$ is *almost upper triangularizable* if there is an ordered basis $B$ for which $[\mathrm{T}]_B$ is almost upper triangular.
</MathBox>

<MathBox title="Schur's theorem: real case" boxType='theorem'>
If $V$ is a real vector space, then every linear operator $\mathrm{T}\in\mathcal{L}(V)$ on $V$ is almost upper triangularizable.

<details>
<summary>Proof</summary>

Suppose that $\mathrm{T}$ has characteristic polynomial $c_\mathrm{T}(x)$. If $p(x)$ is a prime factor of $c_\mathrm{T}(x)$, then $V_\mathrm{T}$ has a cyclic submodule $W_\mathrm{T}$ of order $p(x)$. Hence, $W$ is a $\mathrm{T}$-cyclic subspace of dimension $\deg(p(x))$ and $\mathrm{T}|_W$ has characteristic polynomial $p(x)$.

The minimal polynomial of a real operator $\mathrm{T}\in\mathcal{L}(V)$ factors into a product of linear and irreducible quadratic factors. If $c_\mathrm{T}(x)$ has a linear factor over $\mathbb{F}$, then $V_\mathrm{T}$ has a one-dimensional $\mathrm{T}$-invariant subspace $W$. If $c_\mathrm{T}(x)$ has an irreducible quadratic factor $p(x)$, then $V_\mathrm{T}$ has a cyclic submodule $W_\mathrm{T}$ of order $p(x)$ and so a matrix representation of $\mathrm{T}$ on $W_\mathrm{T}$ is given by the matrix

$$
  \mathbf{A} = \begin{bmatrix} a & -b \\ b & a \end{bmatrix}
$$

Using induction by matrix means, we want to show that every real square matrix $\mathbf{A}\in\mathcal{M}_n (\R)$ is simiar to an almost upper triangular matrix. The base case $n = 1$ is trivial because every $1\times 1$ matrix is by definition almost upper triangularizable. 

For the induction hypothesis, assume $\mathbf{A} \in \mathcal{M}_{n-1}(\R)$ is almost upper triangularizable. We have just seen that $\R^n$ has a one dimensional $\mathrm{T}_\mathbf{A}$-invariant subspace $W$ or a two-dimensional $\mathrm{T}_\mathbf{A}$-cyclic subspace $W$, where $\mathrm{T}_\mathbf{A}$ has irreducible characteristic polynomial on $W$. Hence, we may choose a basis $B$ for $\R^n$ for which the first one or first two vectors are a basis for $W$. Then

$$
  [\mathrm{T}_\mathbf{A}]_B = \begin{bmatrix} \mathbf{A}_1 & * \\ \mathbf{0} & \mathbf{A}_2 \end{bmatrix}
$$

where $\mathbf{A}_1 = [a]$ or $\mathbf{A}_1 = \begin{smallmatrix} a & -b \\ b & a \end{smallmatrix}$ and $\mathbf{A}_2$ has size $k\times k$. Applying the induction hypothesis to $\mathbf{A}_2$ gives an invertible $\mathbf{P}\in\mathcal{M}_k (\R)$ for which

$$
  \mathbf{U} = \mathbf{PA}_2\mathbf{P}^{-1}
$$

is almost upper triangular. Hence if

$$
  \mathbf{Q} = \begin{bmatrix} \mathbf{I}_{n-k} & \mathbf{0} \\ \mathbf{0} & \mathbf{P} \end{bmatrix}
$$

then $\mathbf{Q}$ is invertible and

$$
\begin{align*}
  \mathbf{Q}[\mathbf{A}]_B \mathbf{Q}^{-1} = \begin{bmatrix} \mathbf{I}_{n-k} & \mathbf{0} \\ \mathbf{0} & \mathbf{P} \end{bmatrix} \begin{bmatrix} \mathbf{A}_1 & * \\ \mathbf{0} & \mathbf{A}_2 \end{bmatrix} \begin{bmatrix} \mathbf{I}_{n-k} & \mathbf{0} \\ \mathbf{0} & \mathbf{P}^{-1} \end{bmatrix} \\
  =& \begin{bmatrix} \mathbf{A}_1 & * \\ \mathbf{0} & \mathbf{U} \end{bmatrix}
\end{align*}
$$

is almost upper triangular.
</details>
</MathBox>

### Unitary triangularizability

<MathBox title='Unitary triangularizable operator' boxType='definition'>
A linear operator $\mathrm{T}\in\mathcal{L}(V)$ is *unitarily upper triangularizable* if there is an ordered orthonormal basis with respect to which $\mathrm{T}$ is upper triangular.
</MathBox>

## Diagonalizable operators

<MathBox title='Diagonalizable operator' boxType='definition'>
A linear operator $\mathrm{T}\in\mathcal{L}(V)$ is *diagonalizable* if there is an ordered basis $B = (\mathbf{v}_i)_{i=1}^n$ of $V$ for which the matrix $[\mathrm{T}]_B$ is diagonal, or equivalently, if $\mathrm{T}\mathbf{v}_i = \lambda_i \mathbf{v}_i$ for all $i = 1,\dots,n$.
</MathBox>

<MathBox title='' boxType='proposition'>
Let $\mathrm{T}\in\mathcal{L}(V)$ be a linear operator. The following are equivalent:
1. $\mathrm{T}$ is diagonalizable.
2. $V$ has a basis consisting entirely of eigenvectors of $\mathrm{T}$.
3. $V$ has the form $V = \bigoplus_{i=1}^k E_{\lambda_i}$ where the $\lambda_i$ are the distinct eigenvalues of $\mathrm{T}$. 
</MathBox>

<MathBox title='' boxType='proposition'>
A linear operator $\mathrm{T}\in\mathcal{L}(V)$ an a finite-dimensional vector space is diagonalizable if and only if its minimal polynomial is the product of distinct linear factors.

<details>
<summary>Proof</summary>

If $\mathrm{T}$ is diagonalizable, then $V = \bigoplus_{i=1}^k E_{\lambda_i}$ impying that $m_\mathrm{T}(x)$ is the least common multiple of the minimal polynomials $x - \lambda_i$ of $\mathrm{T}$ restricted to $E_i$. Hence, $m_\mathrm{T}(x)$ is a product of distinct linear factors. Conversely, if $m_\mathrm{T}(x)$ is a product of distinct linear factors, then the primary decomposition of $V$ has the form $V = \bigoplus_{i=1}^k V_i$ where

$$
  V_i = \Set{\mathbf{v}\in V | (\mathrm{T} - \lambda)\mathbf{v} = 0} = E_{\lambda_i}
$$

and so $\mathrm{T}$ is diagonalizable.
</details>
</MathBox>

### Spectral resolutions

<MathBox title='Spectral resolution' boxType='definition'>
Let $\mathrm{T}\in\mathcal{L}(V)$ be a diagonalizable linear operator on an $\mathbb{F}$-vector space $V$. The *spectral resolution* of $\mathrm{T}$ is the form

$$
  \mathrm{T} = \sum_{i=1}^k \lambda_i \mathrm{R}_i
$$

where $\sum_{i=1}^k \mathrm{R}_i = \mathrm{I}$ is a resolution of the identity and the $\lambda_i \in\mathbb{F}$ are distinct.
</MathBox>

<MathBox title='' boxType='proposition'>
A linear operator $\mathrm{T}\in\mathcal{L}(V)$ is diagonalizable if and only if it has a spectral resolution

$$
  \mathrm{T} = \sum_{i=1}^k \lambda_i \mathrm{R}_i
$$

In this case $\Set{\lambda_i}_{i=1}^k$ is the spectrum of $\mathrm{T}$ and $\operatorname{ran}(\mathrm{R}_i) = E_{\lambda_i}$ and $\ker(\mathbf{R}_i) = \bigoplus_{j\neq i} E_{\lambda_i}$.

<details>
<summary>Proof</summary>

If $\Set{\lambda_i}_{i=1}^k$ is the spectrum of $\mathrm{T}$ and $\operatorname{ran}(\mathrm{R}_i) = E_{\lambda_i}$. It follows that $V = \bigoplus_{i=1}^k \operatorname{ran}(\mathrm{R}_i)$. If $\mathrm{R}_i \mathbf{v} \in\operatorname{ran}(\mathrm{R}_i)$, then

$$
  \mathrm{T}(\mathrm{R}_i \mathbf{v}) = \left( \sum_{i=1}^k \lambda_i \mathrm{R}_i \right)\mathrm{R}_i \mathbf{v} = \lambda_i (\mathbf{R}_i \mathbf{v})
$$

and so $\mathrm{R}_i \mathbf{v} \in E_{\lambda_i}$. Hence, $operatorname{\mathrm{R}_i} \subseteq E_{\lambda_i}$ and consequently

$$
  V = \bigoplus_{i=1}^k \operatorname{R_i} \subseteq \bigoplus_{i=1}^k E_{\lambda_i} \subseteq V
$$

which implies that $\operatorname{ran}(\mathrm{R}_i) = E_{\lambda_i}$ and $V = \bigoplus_{i=1}^k E_{\lambda_i}$.

The converse also holds, for if $V = \bigoplus_{i=1}^k E_{\lambda_i}$ and if $\mathrm{R}_i$ is projection onto $E_{\lambda_i}$ along the direct sum of the other eigenspaces then $\sum_{i=1}^k \mathrm{R}_i = \mathrm{I}$. Since $\mathrm{T}\mathbf{R}_i = \lambda_i \mathrm{R}_i$, it follows that

$$
  \mathrm{T} = \mathrm{T}(\sum_{i=1}^k \mathrm{R}_i) = \sum_{i=1}^k \lambda_i \mathrm{R}_i
$$
</details>
</MathBox>

# Inner product space

<MathBox title='Inner product' boxType='definition'>
Let $V$ be an $\mathbb{F}$-vector space. The map $\langle\cdot, \cdot\rangle: V\times V \to\mathbb{F}$ is called an inner product on $V$ if it satisfies for all $\boldsymbol{u},\boldsymbol{v}, \boldsymbol{w}\in V$ and $\alpha, \beta \in\mathbb{F}$

1. **Positive definiteness:** $\langle \boldsymbol{v}, \boldsymbol{v} \rangle \geq 0$ and $\langle \boldsymbol{v}, \boldsymbol{v} \rangle = 0 \iff \boldsymbol{v} = 0$
2. **Linearity in the first argument:** $\langle \alpha\boldsymbol{u} + \beta\boldsymbol{v}, \boldsymbol{w} \rangle = \alpha\langle\boldsymbol{u}, \boldsymbol{w}\rangle + \beta\langle\boldsymbol{v}, \boldsymbol{w}\rangle$
3. **Conjugate symmetry:** $\langle \boldsymbol{v}, \boldsymbol{w}\rangle = \overline{\langle\boldsymbol{w}, \boldsymbol{v}\rangle}$
    - For $\mathbb{F} = \R$, the inner product is symmetric, i.e. $\langle \boldsymbol{v}, \boldsymbol{w}\rangle = \langle\boldsymbol{w}, \boldsymbol{v}\rangle$

The pair $(V,\langle\cdot, \cdot\rangle)$ is called an *inner product space*. Note that the inner product can also be defined with linearity in the second argument instead.
</MathBox>

If $\mathbb{F} = \R$, the inner product is symmetric making it linear in both arguments. In this case, the inner product is a *bilinear form*. However, if $\mathbb{F} = \mathbb{C}$, linearity in the first argument and conjugate symmetry implies conjugate linearity in the second argument, i.e.

$$
\begin{align*}
  \langle \mathbf{u}, \alpha\mathbf{v} + \beta\mathbf{w} \rangle =& \overline{\langle \alpha\mathbf{v} + \beta\mathbf{w}, \mathbf{u}, \rangle} \\
  =& \bar{\alpha}\overline{\langle\mathbf{v},\mathbf{u}\rangle} + \bar{\beta}\overline{\langle\mathbf{w},\mathbf{u}\rangle} \\
  =& \bar{\alpha}\langle\mathbf{u},\mathbf{v}\rangle + \bar{\beta}\langle\mathbf{u},\mathbf{w}\rangle
\end{align*}
$$

Thus, a complex inner product is linear in the first argument and conjugate linear in the second argument. This property is referred to as *sesquilinearity*.

<MathBox title='' boxType='lemma'>
If $V$ is an inner product space and $\langle\mathbf{u},\mathbf{x}\rangle = \langle\mathbf{v},\mathbf{x}\rangle$ for all $\mathbf{x}\in V$, then $\mathbf{u} = \mathbf{v}$.
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V$ be an $\mathbb{F}$-inner product space and let $\mathrm{T}\in\mathcal{L}(V)$ be a linear operator on $V$.
1. $\langle\mathrm{T}\mathbf{v},\mathbf{w}\rangle = 0 \implies \mathrm{T} = 0,\; \forall \mathbf{v},\mathbf{w}\in V$
2. If $\mathbb{F} = \mathbb{C}$, then $\langle\mathrm{T}\mathbf{v},\mathbf{v}\rangle = 0 \implies \mathrm{T} = 0,\; \forall \mathbf{v}\in V$. This does not hold in general for real inner product spaces.

<details>
<summary>Proof</summary>

**(1):** This follows directly from the previous lemma.

**(2):** Let $\mathbf{v} = \alpha\mathbf{x} + \mathbf{y}$ for $\mathbf{x},\mathbf{y}\in V$ and $\alpha\in\mathbb{C}$, then

$$
\begin{align*}
  0 =& \langle \mathrm{T}(\alpha\mathbf{x} + \mathbf{y}), \alpha\mathbf{x} + \mathbf{y} \rangle \\
  =& |\alpha^2| \langle \mathrm{T}\mathbf{x}, \mathbf{x}\rangle + \langle\mathrm{T}\mathbf{y}, \mathbf{y}\rangle + \alpha\langle\mathrm{T}\mathbf{x},\mathbf{y}\rangle + \bar{\alpha}\langle\mathrm{T}\mathbf{y},\mathbf{x}\rangle \\
  =& \alpha\langle\mathrm{T}\mathbf{x},\mathbf{y}\rangle + \bar{\alpha}\langle\mathrm{T}\mathbf{y},\mathbf{x}\rangle
\end{align*}
$$

Setting $r = 1$ and $r = i$ gives, respectively

$$
\begin{align*}
  \langle\mathrm{T}\mathbf{x},\mathbf{y}\rangle + \langle\mathrm{T}\mathbf{y},\mathbf{x}\rangle =& 0 \\
  \langle\mathrm{T}\mathbf{x},\mathbf{y}\rangle - \langle\mathrm{T}\mathbf{y},\mathbf{x}\rangle =& 0 \\
\end{align*}
$$

These two equations imply that $\langle\mathrm{T}\mathbf{x},\mathbf{y}\rangle = 0$ for all $\mathbf{x},\mathbf{y}\in V$ and it follows by **(1)** that $\mathrm{T} = 0$. For the last statement, rotation by $90\degree$ in the $\R^2$ plane has the property that $\langle\mathrm{T}\mathbf{v}, \mathbf{v}\rangle = 0$ for all $\mathbf{v}\in V$.
</details>
</MathBox>

## Norm and distance

<MathBox title='Norm' boxType='definition'>
Let $V$ be an $\mathbb{F}$-inner product space. The *norm* on $V$ is a map $\norm{\cdot}: V\to[0,\infty)$ defined by

$$
  \norm{ \boldsymbol{v} } := \sqrt{\langle \boldsymbol{x}, \boldsymbol{x} \rangle}
$$
</MathBox>

<MathBox title='Unit vector' boxType='definition'>
A vector $\mathbf{v}\in V$ of an inner product space $V$ is a *unit vector* if $\norm{\mathbf{v}} = 1$.
</MathBox>

For any nonzero vector $\mathbf{v}\in V$ of an inner product space $V$, we may obtain a unit vector $\mathbf{u}$ by multiplying $\mathbf{v}$ by the reciprocal of the norm

$$
  \mathbf{u} = \frac{1}{\norm{\mathbf{v}}}\mathbf{v},\; \norm{\mathbf{u}} = 1
$$

<MathBox title='Cauchy-Schwarz inequality' boxType='proposition'>
Let $V$ be an inner product space. Then for all $\boldsymbol{x},\boldsymbol{y}\in V$ 
1. $|\langle\boldsymbol{y}, \boldsymbol{x}\rangle| \leq \norm{\boldsymbol{x}}\cdot\norm{\boldsymbol{y}}$ 
2. $|\langle\boldsymbol{y},\boldsymbol{x}\rangle| = \norm{\boldsymbol{x}}\cdot\norm{\boldsymbol{y}}$ if and only if $\boldsymbol{x}$ and $\boldsymbol{y}$ are linearly dependent

<details>
<summary>Proof</summary>

In the trivial case $\boldsymbol{x} = \boldsymbol{0}$, we get $\langle \boldsymbol{y}, \underbrace{\boldsymbol{x}}_{0\boldsymbol{v}} \rangle = 0\langle \boldsymbol{y},\boldsymbol{x}\rangle = 0$ and $\norm{\boldsymbol{x}}\cdot\norm{\boldsymbol{y}}$.

In the general case $\boldsymbol{x}\neq\boldsymbol{0}$, we want to show that $\left|\langle \boldsymbol{y}, \norm{\hat{\boldsymbol{x}}} \right|\leq\norm{\boldsymbol{y}}$ where $\frac{\boldsymbol{x}}{\norm{\boldsymbol{x}}}$. For any $\lambda\in\R$, we have that

$$
\begin{align*}
  0 \leq& \langle \boldsymbol{y} - \lambda\hat{\boldsymbol{x}}, \boldsymbol{y} - \lambda\hat{\boldsymbol{x}} \rangle \\
  =& \langle \boldsymbol{y},\boldsymbol{y}\rangle - \lambda \hat{\boldsymbol{x},\boldsymbol{y}} \rangle - \lambda\langle\boldsymbol{y},\hat{\boldsymbol{x}}\rangle + \lambda^2 \langle\hat{\boldsymbol{x}},\hat{\boldsymbol{x}}\rangle \\
  =& \lambda^2 + \lambda \underbrace{(-2\Re(\langle\boldsymbol{x},\hat{\boldsymbol{x}}\rangle))}_{p} + \underbrace{\norm{\boldsymbol{y}}^2}_{q}
\end{align*}
$$

The resulting quadratic polynomial has roots $\lambda_{1,2} = -\frac{p}{2} \pm \sqrt{\left(\frac{p}{2}\right)^2 - q}$. For the inequality to hold, the polynomial must either have a single real root or two imaginary roots. Thus, $\left(\frac{p}{2}\right)^2 - q \leq 0$, implying $|\Re(\langle\boldsymbol{y},\hat{\boldsymbol{x}}\rangle)| \leq \norm{\boldsymbol{y}}$. This proves the Cauchy-Schwartz inequality for $\mathbb{F} = \R$. For $\mathbb{F} = \mathbb{C}$, note that $e^{i\phi}\langle \boldsymbol{y}, \hat{\boldsymbol{x}} \rangle = |\langle\boldsymbol{y},\hat{\boldsymbol{x}}\rangle|$. Thus, we get $|\Re(e^{i\phi}\langle\boldsymbol{y},\hat{\boldsymbol{x}}\rangle)| = |\Re(\langle\boldsymbol{y},e^{i\phi}\hat{\boldsymbol{x}}\rangle)| \leq \norm{\boldsymbol{y}}$.
</details>
</MathBox>

<MathBox title='Properties of the norm' boxType='proposition'>
Let $V$ be an $\mathbb{F}$-inner product space.
1. $\norm{\mathbf{v}} \geq 0$ and $\norm{ \mathbf{v} } = 0 \iff \mathbf{v} = \mathbf{0}$
2. $\norm{\alpha \mathbf{v}} = |\alpha|\cdot\norm{ \mathbf{v} }$
3. **Triangle inequality:** $\norm{ \mathbf{u} + \mathbf{v}} \leq \norm{\mathbf{u}} + \lVert\mathbf{v}\Vert$ for all $\mathbf{u},\mathbf{v}\in V$ with equality if and only if $\mathbf{u}$ and $\mathbf{v}$ are linearly dependent
4. $\norm{\mathbf{u} -\mathbf{v}} \leq \norm{\mathbf{u} - \mathbf{x}} + \norm{\mathbf{x} - \mathbf{v}}$ for all $\mathbf{u},\mathbf{v},\mathbf{x}\in V$
5. $\big|\norm{\mathbf{u}} - \norm{\mathbf{v}}\big| \leq \norm{\mathbf{u}-\mathbf{v}}$
6. **Parallelogram law:** $\norm{\mathbf{u} + \mathbf{v}}^2 + \norm{\mathbf{u} - \mathbf{v}}^2 = 2\norm{\mathbf{u}}^2 + 2\norm{\mathbf{v}}^2$

<details>
<summary>Proof</summary>

**(2):**

$$
\begin{align*}
  \norm{\alpha \mathbf{v}} =& \sqrt{\langle\alpha\mathbf{v}, \alpha\mathbf{v}\rangle} = \sqrt{\alpha\bar{\alpha}\langle\mathbf{v},\mathbf{v}\rangle} \\
  =& \sqrt{|\alpha|^2 \langle\mathbf{v},\mathbf{v}\rangle} = |\alpha|\sqrt{\langle\mathbf{v},\mathbf{v}\rangle} \\
  =& |\alpha|\cdot\norm{ \mathbf{v} }
\end{align*}
$$

**(3):** By the Cauchy-Schwarz inequality

$$
\begin{align*}
  \norm{\mathbf{u} + \mathbf{v}}^2 =& \langle\mathbf{u} + \mathbf{v},\mathbf{u} + \mathbf{v}\rangle \\
  =& \langle\mathbf{u},\mathbf{u}\rangle + \langle\mathbf{u},\mathbf{v}\rangle + \langle\mathbf{v},\mathbf{u}\rangle + \langle\mathbf{v},\mathbf{v}\rangle \\
  \leq& \norm{\mathbf{u}}^2 + 2\norm{\mathbf{u}}\cdot\norm{\mathbf{u}} + \norm{\mathbf{v}}^2 \\
  =& (\norm{\mathbf{u}} + \norm{\mathbf{v}})^2
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Polarization identities' boxType='proposition'>
1. If $V$ is a real inner product space, then
$$
  \langle\mathbf{u},\mathbf{v}\rangle = \frac{1}{4}(\norm{\mathbf{u} + \mathbf{v}}^2 - \norm{\mathbf{u} - \mathbf{v}}^2)
$$
2. If $V$ is a complex inner product space, then
$$
\begin{align*}
  \langle\mathbf{u}, \mathbf{v}\rangle =& \frac{1}{4}(\norm{\mathbf{u} + \mathbf{v}}^2 - \norm{\mathbf{u} - \mathbf{v}}^2) \\
  &+ \frac{1}{4}i(\norm{\mathbf{u} + i\mathbf{v}}^2 - \norm{\mathbf{u} - i\mathbf{v}}^2)
\end{align*}
$$
</MathBox>

<MathBox title='Metric' boxType='definition'>
Let $V$ be an $\mathbb{F}$-inner product space. The map $d:V\times V \to [0,\infty)$ defined by $d(\mathbf{u},\mathbf{v}) = \norm{\mathbf{u} - \mathbf{v}}$ is called a *metric* on $V$ if it satisfies for all $\mathbf{u},\mathbf{v},\mathbf{w}\in V$
1. **Positive definiteness:** $d(\mathbf{u},\mathbf{v})\geq 0$ and $d(\mathbf{u},\mathbf{v}) = 0 \iff \mathbf{u} = \mathbf{v}$
2. **Symmetry:** $d(\mathbf{u},\mathbf{v}) = d(\mathbf{v},\mathbf{u})$
3. **Triangle inequality:** $d(\mathbf{u},\mathbf{v}) \leq d(\mathbf{v},\mathbf{w}) + d(\mathbf{w}-\mathbf{v})$

The pair $(V,\mathbf{d})$ is called a *metric space*.
</MathBox>

The introduction of an inner product, and consequently a metric, on a vector space $V$, permits the definition a topology on $V$, and in particular, convergence of infinite sequences. A sequence $(\mathbf{v}_n)$ of vectors in $V$ converges to $\mathbf{v}\in V$ if

$$
  \lim_{n\to\infty} \norm{ \mathbf{v}_n - \mathbf{v}} = 0
$$

Some of the more important concepts related to converges are closedness and closures, completeness and the continuity of linear operators and linear functionals.

In the finite-dimensional case, these concepts are straightforward: All subspaces are closed, all inner product spaces are complete and all linear operators and functionals are continuous. However, in the infinite-dimensional case, things are not as simple.

## Isometries

<MathBox title='Isometry' boxType='definition'>
Let $V$ and $W$ be inner product spaces and let $\mathrm{T}\in\mathcal{L}(V,W)$ be a linear transformation from $V$ to $W$.
1. $\mathrm{T}$ is an *isometry* if it preserves the inner product, i.e. if
$$
  \langle\mathrm{T}\mathbf{u}, \mathrm{T}\mathbf{v}\rangle = \langle\mathbf{u},\mathbf{v}\rangle,\; \forall \mathbf{u},\mathbf{v}\in V
$$
2. A bijective isometry is called an *isometric isomorphism*. When $\mathrm{T}:V\to W$ is an isometric isomorphism, then $V$ and $W$ are *isometrically isomorphic*.
</MathBox>

It is clear that an isometry is injective and so it is an isometric isomorphism provided it is surjective. Moreover, if $\dim(V) = \dim(W) < \infty$ injectivity implies surjectivity and $\mathrm{T}$ is an isometry if and only if $\mathrm{T}$ is an isometric isomorphism.

<MathBox title='Isometries preserve norm' boxType='definition'>
A linear transformation $\mathrm{T}\in\mathcal{L}(V,W)$ is an isometry if and only if it preserves the norm, i.e.

$$
  \norm{\mathrm{T}\mathbf{v}} = \norm{\mathbf{v}}
$$

<details>
<summary>Proof</summary>

Clearly, an isometry preserves the norm. The converse follows from the polarization identities. In the real case, we have

$$
\begin{align*}
  \langle\mathrm{T}\mathbf{u},\mathrm{T}\mathbf{v}\rangle =& \frac{1}{4}(\norm{\mathrm{T}\mathbf{u} + \mathrm{T}\mathbf{v}}^2 - \norm{\mathrm{T}\mathbf{u} - \mathrm{T}\mathbf{v}}^2) \\
  =& \frac{1}{4}(\norm{\mathrm{T}(\mathbf{u} + \mathbf{v})}^2 - \norm{\mathrm{T}(\mathbf{u} - \mathbf{v})}^2) \\
  =& \frac{1}{4}(\norm{\mathbf{u} + \mathbf{v}}^2 - \norm{\mathbf{u} - \mathbf{v}}^2) \\
  =& \langle\mathbf{u},\mathbf{v}\rangle
\end{align*}
$$

showing that $\mathrm{T}$ is an isometry. The complex case is similar.
</details>
</MathBox>

## Orthogonality

<MathBox title='Orthogonal vectors and subsets' boxType='definition'>
Let $V$ be an inner product space. 
1. Two vectors $\boldsymbol{x}, \boldsymbol{v}\in V$ are *orthogonal*, denoted $\boldsymbol{x}\perp\boldsymbol{y}$ if $\langle\boldsymbol{x},\boldsymbol{y}\rangle = 0$.
2. Two subsets $X, Y\subseteq V$ are *orthogonal*, denoted $X\perp Y$, if $\langle X, Y \rangle = \Set{0}$, i.e. if $\mathbf{x}\perp\mathbf{y}$ for all $\mathbf{x}\in X$ and $\mathbf{y}\in Y$
</MathBox>

<MathBox title='Orthogonal complement' boxType='definition'>
Let $\langle\cdot, \cdot\rangle: V\times V \to\mathbb{F}$ be an inner product on the $\mathbb{F}$-vector space $V$. For a nonempty subspace $M\subseteq V$, we define the *orthogonal complement*

$$
  M^\perp := \Set{\boldsymbol{x}\in V |\langle\boldsymbol{x},\boldsymbol{m}\rangle = 0 \forall\boldsymbol{m}\in M}
$$
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V$ be an inner product space.
1. The orthogonal complement $X^\perp$ of any subset $X\subseteq V$ is a subspace of $V$.
2. For any subspace $S$ of $V$, then $S \cap S^\perp = \Set{\boldsymbol{0}}$
</MathBox>

<MathBox title='Orthogonal direct sum' boxType='proposition'>
An inner product space $V$ is the *orthogonal direct sum* of subspaces $S$ and $T$ if

$$
  V = S \oplus T,\; S \perp T
$$

In this case, write $V = S \odot T$. Generally, $V$ is the orthogonal direct sum of subspaces $S_1,\dots,S_n$, written

$$
  V = \bigodot_{i=1}^n S_i
$$

if $V = \bigoplus_{i=1}^n S_i$ and $S_i \perp S_j$ for $i\neq j$.
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V$ be an inner product space. The following are equivalent
1. $V = S \odot T$
2. $V = S \oplus T$ and $T = S^\perp$

<details>
<summary>Proof</summary>

If $V = S \odot T$, then by definition $T \subseteq S^\perp$. However, if $\mathbf{v}\in S^\perp$, then $\mathbf{v} = \mathbf{s} + \mathbf{t}$ where $s\in S$ and $\mathbf{t}\in T$. Then $\mathbf{s}$ is orthogonal to both $\mathbf{t}$ and $\mathbf{v}$ and so $\mathbf{s}$ is orthogonal to itself, which implies that $\mathbf{s} = \mathbf{0}$ and so $\mathbf{v}\in T$. Hence, $T = S^\perp$. The converse is clear.
</details>
</MathBox>

### Orthogonal projection

<MathBox title='Orthogonal projection onto a line' boxType='definition'>
Let $V$ be an inner product space. Suppose $U\subseteq V$ is a 1-dimensional subspace with $U = \operatorname{span}(\boldsymbol{r})$ for $\boldsymbol{r}\neq\boldsymbol{0}$. For $\boldsymbol{x}\in V$ and a decomposition $\boldsymbol{x} = \boldsymbol{p} + \boldsymbol{n}$ with $\boldsymbol{p}\in U$ and $\boldsymbol{n}\in U^\perp$, i.e. $\boldsymbol{n}\perp\boldsymbol{r}$, we call 
- $\boldsymbol{p}$ the *orthogonal projection* of $\boldsymbol{x}$ onto $U$
- $\boldsymbol{n}$ the *normal component* of $\boldsymbol{x}$ with respect to $U$

<details>
<summary>Proof</summary>

To prove the uniqueness of the orthogonal decomposition, assume $\boldsymbol{x} = \boldsymbol{p} + \boldsymbol{n} = \tilde{\boldsymbol{p}} + \tilde{\boldsymbol{n}}$ for $\boldsymbol{p},\tilde{\boldsymbol{p}}\in U$ and $\boldsymbol{n},\tilde{\boldsymbol{n}}\in U^\perp$. Then

$$
\begin{align*}
  & \boldsymbol{p} + \boldsymbol{n} = \tilde{\boldsymbol{p}} + \tilde{\boldsymbol{n}} \\
  \implies& \underbrace{\boldsymbol{p} - \tilde{\boldsymbol{p}}}_{\in U} = \underbrace{\tilde{\boldsymbol{n}} - \boldsymbol{n}}_{\in U^\perp} \\
  \implies& 0 = \langle \boldsymbol{p} - \tilde{\boldsymbol{p}}, \tilde{\boldsymbol{n}} - \boldsymbol{n} \rangle = \begin{cases} \langle \boldsymbol{p} - \tilde{\boldsymbol{p}}, \boldsymbol{p} - \tilde{\boldsymbol{p}} \rangle \\ \langle \tilde{\boldsymbol{n}} - \boldsymbol{n}, \tilde{\boldsymbol{n}} - \boldsymbol{n} \rangle \end{cases} \\
  \implies& \boldsymbol{p} - \tilde{\boldsymbol{p}} = \boldsymbol{0} = \tilde{\boldsymbol{n}} - \boldsymbol{n} \\
\end{align*}
$$

This shows that $\boldsymbol{p} = \tilde{\boldsymbol{p}}$ and $\boldsymbol{n} = \tilde{\boldsymbol{n}}$.

To prove the existence of the orthogonal projection, assume $\boldsymbol{p}\in U = \operatorname{span}(\boldsymbol{r})$ such that $\boldsymbol{p} = \lambda\boldsymbol{r}$ for $\mathbb{F}$. Then

$$
\begin{align*}
  &\langle \boldsymbol{r}, \boldsymbol{x} \rangle = \langle \boldsymbol{r}, \lambda\boldsymbol{r} + \boldsymbol{n} \rangle = \lambda\langle\boldsymbol{r},\boldsymbol{r}\rangle + \underbrace{\langle\boldsymbol{r},\boldsymbol{n}\rangle}_{=0} \\
  \implies& \lambda = \frac{\langle\boldsymbol{r},\boldsymbol{x}\rangle}{\langle\boldsymbol{r},\boldsymbol{r}\rangle}
\end{align*}
$$

Hence, $\boldsymbol{p} = \frac{\langle\boldsymbol{r},\boldsymbol{x}\rangle}{\langle\boldsymbol{r},\boldsymbol{r}\rangle}\boldsymbol{r}$ and $\boldsymbol{n} = \boldsymbol{x} - \boldsymbol{p}$.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V$ be an inner product space. Suppose $U\subseteq V$ is a $k$-dimensional subspace with basis $B = (\boldsymbol{b}_i)_{i=1}^k$. If $\boldsymbol{y}\in V$, then $\boldsymbol{y}\perp\boldsymbol{u}$ for all $\boldsymbol{u}\in U$ if and only if $\boldsymbol{y}\perp\boldsymbol{b}_i$ for all $i\in\Set{1,\dots,k}$.

<details>
<summary>Proof</summary>

The right implication is trivial, since if $\boldsymbol{y}\perp\boldsymbol{u}$ it follows that all basis vectors $\boldsymbol{b}_i$ are in $U$, and hence $\boldsymbol{y}\perp\boldsymbol{b}_i$ for all $i\in\Set{1,\dots,k}$. 

Conversely, assume $\langle \boldsymbol{y},\boldsymbol{b}_i \rangle = 0$ for all $i\in\Set{1,\dots,k}$. Then

$$
\begin{align*}
  &\sum_{i=1}^k \lambda_i \langle\boldsymbol{y}, \boldsymbol{b}_i \rangle = 0 \\
  \implies& \left\langle \boldsymbol{y}, \sum_{i=1}^k \lambda_i \boldsymbol{b}_i \right\rangle = 0
\end{align*}
$$

Since $\sum_{i=1}^k \lambda_i \boldsymbol{b}_i \in U$ for all $\boldsymbol{\lambda}_i \in\mathbb{F}$, it follows that $\boldsymbol{y}\perp\boldsymbol{u}$ for all $\boldsymbol{u}\in U$.
</details>
</MathBox>

<MathBox title='Orthogonal projection onto a subspace' boxType='definition'>
Let $V$ be an inner product space. Suppose $U\subseteq V$ is a $k$-dimensional subspace. For $\boldsymbol{x}\in V$ and a decomposition $\boldsymbol{x} = \boldsymbol{p} + \boldsymbol{n}$ with $\boldsymbol{p}\in U$ and $\boldsymbol{n}\in U^\perp$, i.e. $\boldsymbol{n}\perp\boldsymbol{p}$, we call
- $\boldsymbol{p}$ the *orthogonal projection* of $\boldsymbol{x}$ onto $U$
- $\boldsymbol{n}$ the *normal component* of $\boldsymbol{x}$ with respect to $U$
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V$ be an inner product space. Suppose $U\subseteq V$ is a $k$-dimensional subspace with basis $B = (\boldsymbol{b}_i)_{i=1}^k$. The orthogonal projection of $\boldsymbol{x}\in V$ onto $U$ takes the form

$$
  \boldsymbol{p} = \sum_{i=1}^k \lambda_i \boldsymbol{b}_i,\; \lambda_i \in\mathbb{F}
$$

Taking the orthogonal decomposition $\boldsymbol{x} = \boldsymbol{p} + \boldsymbol{n}$ with $\boldsymbol{p}\in U$ and $\boldsymbol{n}\in U^\perp$, we get for each $\boldsymbol{b}_i$ 

$$
\begin{align*}
  \langle\boldsymbol{b}_i, \boldsymbol{x}\rangle =& \langle \boldsymbol{b}_i,\boldsymbol{p} \rangle + \underbrace{\langle \boldsymbol{b}_i, \boldsymbol{n}\rangle}_{=0} \\
  =& \left\langle \boldsymbol{b}_i, \sum_{j=1}^k \lambda_j \boldsymbol{b}_j \right\rangle \\
  =& \sum_{j=1}^k \lambda_j \langle \boldsymbol{b}_i, \boldsymbol{b}_j \rangle 
\end{align*}
$$

This gives a system of $k$ linear equations of the form

$$
\begin{bmatrix}
  \langle\boldsymbol{b}_1,\boldsymbol{b}_1 \rangle & \cdots & \langle\boldsymbol{b}_1,\boldsymbol{b}_k \rangle \\
  \vdots & \ddots & \vdots \\
  \langle\boldsymbol{b}_k,\boldsymbol{b}_1 \rangle & \cdots & \langle\boldsymbol{b}_k,\boldsymbol{b}_k \rangle
\end{bmatrix}\cdot \begin{bmatrix} \lambda_1 \\ \vdots \lambda_k \end{bmatrix} = \begin{bmatrix} \langle\boldsymbol{b}_1,\boldsymbol{x}\rangle \\ \vdots \\ \langle \boldsymbol{b}_k,\boldsymbol{x} \rangle \end{bmatrix}
$$

The coefficient matrix is called a Gramian matrix, denoted $\boldsymbol{G}(B)$. This system of linear equations has a unique solution if $\boldsymbol{G}(B)$ is invertible, or equivalently $\ker(\boldsymbol{G}(B)) = \Set{\boldsymbol{0}}$.

<details>
<summary>Proof</summary>

To show that $\ker(G(B)) = \Set{\boldsymbol{0}}$, choose $\boldsymbol{\beta}\in\ker(G(B))$. From the resulting equation $\boldsymbol{G}(B)\boldsymbol{\beta} = \boldsymbol{0}$ we get for each $j\in\Set{1,\dots,k}$ that $\sum_{i=1}^k \beta_i \langle \boldsymbol{b}_j, \boldsymbol{b}_i\rangle = \left\langle \boldsymbol{b}_j, \sum_{i=1}^k \beta_i \boldsymbol{b}_i \right\rangle = \boldsymbol{0}$. Note that $\boldsymbol{y} = \sum_{i=1}^k \beta_i \boldsymbol \in U$. This shows that $\boldsymbol{y}$ is orthogonal to all $\boldsymbol{b}_j$, implying that $\boldsymbol{y}\in U^\perp$ as well. Since $U\cap U^\perp = \Set{0}$, it follows that $\boldsymbol{y} = \boldsymbol{0}$ and hence $\boldsymbol{\beta} = \boldsymbol{0}$.
</details>
</MathBox>

### Orthogonal and orthonormal sets

<MathBox title='Orthonormal basis' boxType='definition'>
Let $\langle\cdot, \cdot\rangle: V\times V \to\mathbb{F}$ be an inner product on the $\mathbb{F}$-vector space $V$. Suppose $U\subseteq V$ is a $k$-dimensional subspace. A set $B = \Set{\boldsymbol{b}_i}_{i=1}^{m\leq k}$ is called
- *orthogonal system* (OS) if $\langle\boldsymbol{b}_i,\boldsymbol{b}_j\rangle = 0$ for all $i\neq j$
- *orthonormal system* (ONS) if $\langle\boldsymbol{b}_i,\boldsymbol{b}_j\rangle = \delta_{ij}$, where $\delta$ is the Kronecker delta function
- *orthogonal basis* if it is an OS and a basis of $U$
- *orthonormal basis* if it is an OSN and a basis of $U$
</MathBox>

Note that if $\mathbf{u}\perp\mathbf{v}$, then $\norm{\mathbf{u}+\mathbf{v}}^2 = \norm{\mathbf{u}}^2 + \norm{\mathbf{v}}^2$. The converse also holds if $\mathbb{F} = \R$.

<MathBox title='Orthogonal sets are linearly independent' boxType='proposition'>
Any orthogonal set of nonzero vectors in an $\mathbb{F}$-inner product space $V$ is linearly independent.

<details>
<summary>Proof</summary>

If $O = \Set{\mathbf{u}_i}_{i=1}^n$ is an orthogonal set of nonzero vectors with $\sum_{i=1}^n \alpha_i \mathbf{u} = \mathbf{0}$ for $\alpha_i \in\mathbb{F}$, then

$$
  0 = \left\langle \sum_{i=1}^n \alpha_i \mathbf{u}, \mathbf{u}_j \right\rangle = \alpha_j \langle\mathbf{u}_j, \mathbf{u}_j \rangle
$$

and so $\alpha_j = 0$ for all $j=1,\dots, n$. Hence $O$ is linearly independent.
</details>
</MathBox>

### Gram-Schmidt orthogonalization

<MathBox title='Gram-Schmidt augmentation' boxType='definition'>
Let $V$ be an inner product space and let $O = \Set{\mathbf{u}_i}_{i=1}^n$ be an orthogonal set of vectors in $V$. If $\mathbf{v}\neq \langle\mathbf{u}_1,\dots,\mathbf{u}_n \rangle$, then there is a nonzero $\mathbf{u}\in V$ for which $\Set{\mathbf{u},\dots,\mathbf{u}_n,\mathbf{u}}$ is orthogonal and

$$
  \langle\mathbf{u}_1,\dots,\mathbf{u}_n, \mathbf{u} \rangle = \langle\mathbf{u}_1,\dots,\mathbf{u}_n, \mathbf{v} \rangle
$$

In particular, $\mathbf{u} = \mathbf{v} - \sum_{i=1}^n \lambda_i \mathbf{u}_i$ where

$$
  \lambda_i = \begin{cases}
    0, \quad \mathbf{u}_i \neq \mathbf{0} \\
    \frac{\langle\mathbf{v},\mathbf{u}_i \rangle}{\langle\mathbf{u}_i,\mathbf{u}_i \rangle}, \quad \mathbf{u}_i \neq\mathbf{0}
  \end{cases}
$$

<details>
<summary>Proof</summary>

Set $\mathbf{u} = \mathbf{v} - \left(\sum_{i=1}^n \lambda_i \mathbf{u}_i \right)$ for $\lambda_i \in\mathbb{F}$ and force $\mathbf{u}\perp\mathbf{u}_i$ for all $i$, i.e.

$$
  0 = \langle \mathbf{u},\mathbf{u}_i \rangle = \left\langle \mathbf{v} - \left(\sum_{i=1}^n \lambda_i \mathbf{u}_i \right), \mathbf{u}_i \right\rangle = \langle\mathbf{v},\mathbf{u}_i\rangle - \lambda_i \langle\mathbf{u}_i, \mathbf{u}_i \rangle
$$

This, if $\mathbf{u}_i = \mathbf{0}$, take $\lambda_i = 0$, and if $\mathbf{u}_i \neq 0$, take

$$
  \lambda_i = \frac{\langle\mathbf{v},\mathbf{u}_i \rangle}{\langle\mathbf{u}_i,\mathbf{u}_i \rangle}
$$
</details>
</MathBox>

<MathBox title='Gram-Schmidt orthogonalization process' boxType='definition'>
Let $B = (\mathbf{v}_i)_{i\in I}$ be a sequence of vectors in an $\mathbb{F}$-inner product space $V$ for an index set $I$. Define a sequence $O = (\mathbf{u}_i)_{i\in I}$ by repeated Gram-Schmidt augmentation, i.e.

$$
  \mathbf{u}_j = \mathbf{v}_j 0 \sum_{i=1}^{j-1} \lambda_{i,j}\mathbf{u},\; \lambda_{i,j}\in\mathbb{F}
$$

where $\mathbf{u}_1 = \mathbf{v}_1$ and

$$
  \lambda_{i,j} = \begin{cases}
    0, \quad \mathbf{u}_i =& \mathbf{0} \\
    \frac{\langle \mathbf{v}_k, \mathbf{u}_i \rangle}{\langle \mathbf{u}_i, \mathbf{u}_i \rangle}, \quad \mathbf{u}_i \neq \mathbf{0}
  \end{cases}
$$

Then $O$ is an orthogonal sequence in $V$ with the property that

$$
  \left\langle \sum_{k=1}^j \mathbf{u}_k \right\rangle = \left\langle \sum_{k=1}^j \mathbf{v}_k \right\rangle,\; \forall k\in I
$$

Also $\mathbf{u}_j = \mathbf{0}$ if and only if $\mathbf{v}_j \in \langle \mathbf{v}_1,\dots,\mathbf{v}_j \rangle$.

<details>
<summary>Proof</summary>

This can be proved by induction. The base case $j = 1$ obviously holds. For the inductive hypothesis, assume that it holds for $j - 1$. If $\mathbf{v}_j \in \langle \mathbf{v}_1,\dots,\mathbf{v}_{j-1} \rangle$, then

$$
  \mathbf{v}_j \in \langle \mathbf{v}_1,\dots,\mathbf{v}_{j-1} \rangle = \langle \mathbf{u}_1,\dots,\mathbf{u}_{j-1} \rangle
$$

Writing $\mathbf{v}_j = \sum_{i=1}^{j-1} \alpha_i \mathbf{u}_i$, we have

$$
  \langle \mathbf{v}_j, \mathbf{u}_i \rangle = \begin{cases}
    0, \quad \mathbf{u}_i =& \mathbf{0} \\
    \alpha_i \langle\mathbf{u}_i,\mathbf{u}_i \rangle,\quad \mathbf{u}_i \neq\mathbf{0}
  \end{cases}
$$

Thus $\alpha_i = \lambda_{j,i}$ when $u_i \neq 0$ and so $\mathbf{u}_j = 0$. Hence

$$
\begin{align*}
  \langle\mathbf{u}_1,\dots,\mathbf{u}_j \rangle =& \langle \mathbf{u}_1,\dots,\mathbf{u}_{j-1},\mathbf{0}\rangle \\
  =& \langle \mathbf{v}_1,\dots,\mathbf{v}_{j-1}\rangle \\
  =& \langle \mathbf{v}_1,\dots,\mathbf{v}_j \rangle
\end{align*}
$$

If $\mathbf{v}_j \notin \langle \mathbf{v}_1,\dots,\mathbf{v}_{j-1},\mathbf{0}\rangle$ then

$$
\begin{align*}
  \langle \mathbf{u}_1,\dots,\mathbf{u}_j \rangle = \langle \mathbf{v}_1,\dots,\mathbf{v}_{j-1},\mathbf{u}_j \rangle \\
  =& \langle \mathbf{v}_1,\dots,\mathbf{v}_{j-1},\mathbf{v}_j \rangle
\end{align*}
$$
</details>
</MathBox>

### QR factorization

The Gram-Schmidt process can be used to factor any real or complex matrix into a product of a matrix with orthogonal columns and an uppoer triangular matrix. Suppose that $\mathbf{A} = \left[\begin{smallmatrix} \shortmid & & \shortmid \\ \mathbf{v}_1 & \cdots & \mathbf{v}_n \\ \shortmid & & \shortmid \end{smallmatrix}\right] \in \mathcal{M}_{m,n} (\mathbb{F})$ is an $m\times n$ matrix where $n \leq m$. The Gram-Schmidt process applied to these columns gives orthogonal vectors $\mathbf{O} = \left[\begin{smallmatrix} \shortmid & & \shortmid \\ \mathbf{u}_1 & \cdots & \mathbf{u}_n \\ \shortmid & & \shortmid \end{smallmatrix}\right]$ for which

$$
  \langle \mathbf{u}_1,\dots,\mathbf{u}_k \rangle = \langle \mathbf{v}_1,\dots,\mathbf{v}_k,\; \forall k \leq n
$$

In particular, $\mathbf{v}_k = \mathbf{u}_k + \sum_{i=1}^{k-1} \lambda_{k,i} \mathbf{u}_i$ where

$$
  \lambda_{k,i} = \begin{cases}
    0, \quad \mathbf{u}_i = \mathbf{0} \\
    \frac{\langle\mathbf{v}_k, \mathbf{u}_i}{\langle\mathbf{u}_i, \mathbf{u}_i \rangle}, \quad \mathbf{u}_i \neq \mathbf{0}
  \end{cases}
$$

In matrix terms

$$
  \begin{bmatrix} \shortmid & & \shortmid \\ \mathbf{v}_1 & \cdots & \mathbf{v}_n \\ \shortmid & & \shortmid \end{bmatrix} = \begin{bmatrix} \shortmid & & \shortmid \\ \mathbf{u}_1 & \cdots & \mathbf{u}_n \\ \shortmid & & \shortmid \end{bmatrix} \begin{bmatrix} 1 & \lambda_{2,1} & \cdots & \lambda_{n,1} \\ & 1 & \cdots & \lambda_{n,2} \\ & & \ddots & \vdots \\ & & & 1 \end{bmatrix}
$$

That is $\mathbf{A} = \mathbf{OB}$ where $\mathbf{O}$ has orthogonal columns and $\mathbf{B}$ is upper triangular. We may normalize the nonzero columns $\mathbf{u}_i$ of $\mathbf{O}$ and move the positive constants to $\mathbf{B}$. In particular, if $\alpha_i = \norm{ \mathbf{u}_i }$ for $\mathbf{u}_i \neq \mathbf{0}$ and $\alpha_i = 1$ for $\mathbf{u}_i = \mathbf{0}$, then

$$
  \begin{bmatrix} \shortmid & & \shortmid \\ \mathbf{v}_1 & \cdots & \mathbf{v}_n \\ \shortmid & & \shortmid \end{bmatrix} = \begin{bmatrix} \shortmid & & \shortmid \\ \frac{\mathbf{u}_1}{\alpha_1} & \cdots & \frac{\mathbf{v}_n}{\alpha_n} \\ \shortmid & & \shortmid \end{bmatrix} \begin{bmatrix} \alpha_1 & \alpha_1 \lambda_{2,1} & \cdots & \alpha_1 \lambda_{n,1} \\ & \alpha_2 & \cdots & \alpha_2 \lambda_{n,2} \\ & & \ddots & \vdots \\ & & & \alpha_n \end{bmatrix}
$$

That is $\mathbf{A} = \mathbf{QR}$ where the columns of $\mathbf{Q}$ are orthogonal and each column is either a unit vector or the zero vector and $\mathbf{R}$ is upper triangular with positive entries on the main diagonal. Moreover, if the vectors $\mathbf{v}_1,\dots,\mathbf{v}_n$ are linearly independent, then the columns of $\mathbf{Q}$ are nonzero. Also, if $m = n$ and $\mathbf{A}$ is nonsingular, then $\mathbf{Q}$ is unitary/orthogonal.

If the columns of $\mathbf{A}$ are not linearly independent, we can make one final adjustment to this matrix factorization. If a column $\frac{\mathbf{u}_i}{\alpha_i}$ is zero, then we may replace this column by any vector as long as we replace the $(i,i)$th entry $\alpha_i$ in $\mathbb{R}$ by $0$. Thus, we can take nonzero columns of $\mathbf{Q}$, extend to an orthonormal basis for the span of the columns of $\mathbf{Q}$ and replace the zero columns of $\mathbf{Q}$ by the additional members of this orthogonal basis. In this way, $\mathbf{Q}$ is replaced by a unitary/orthogonal matrix $\mathbf{Q}'$ and $\mathbf{R}$ is replaced by an upper triangular matrix $\mathbf{R}'$ that has nonnegative entries on the main diagonal.

<MathBox title='QR factorization existence, and uniqueness criteria' boxType='proposition'>
Let $\mathbf{A}\in\mathcal{M}_{m,n}(\mathbb{F})$ where $\mathbb{F} = \Set{\mathbb{C},\R}$. There exists a matrix $\mathbf{Q}\in\mathcal{M}_{m,n}(\mathbb{F})$ with orthonormal columns and an upper triangular matrix $\mathbf{R}\in\mathcal{M}_n (\mathbb{F})$ with nonnegative real entries on the main diagonal for which

$$
  \mathbf{A} = \mathbf{QR}
$$

Moreover, if $m = n$, then $\mathbf{Q}$ is unitary/orthogonal. If $\mathbf{A}$ is nonsingular, then $\mathbb{R}$ can be chosen to have positive entries on the main diagonal, in which case the factors $\mathbf{Q}$ and $\mathbf{R}$ are unique. The factorization $\mathbf{A} = \mathbf{QR}$ is called the *QR factorization* of $\mathbf{A}$. If $\mathbf{A}$ is real, then $\mathbf{Q}$ and $\mathbf{R}$ may taken to be real.

<details>
<summary>Proof</summary>

To show the uniqueness criteria, assume $\mathbf{A}$ is nonsingular and $\mathbf{QR} = \tilde{\mathbf{Q}}\tilde{\mathbf{R}}$. Then

$$
  \tilde{\mathbf{Q}}^{-1}\mathbf{Q} = \tilde{\mathbf{R}}\mathbf{R}^{-1}
$$

and the right side is upper triangular with nonzero entries on the main diagonal and the left side is unitary. However, an upper triangular matrix with positive entries on the main diagonal is unitary if and only if it is the identity and so $\tilde{\mathbf{Q}} = \mathbf{Q}$ and $\tilde{\mathbf{R}} = \mathbf{R}$. Finally, if $\mathbf{A}$ is real, then all computations take place in $\R$ and so $\mathbf{Q}$ and $\mathbf{R}$ are real.
</details>
</MathBox>

The QR factorization has important applications. For example, a system of linear equations $\mathbf{Ax} = \mathbf{u}$ can be written in the form

$$
  \mathbf{QRx} = \mathbf{u}
$$

and since $\mathbf{Q}^{-1} = \mathbf{Q}^\dagger$, we have

$$
  \mathbf{Rx} = \mathbf{Q}^\dagger \mathbf{u}
$$

which is an upper triangular system easily solved back by substitutions.

The QR factorization can be use to approximate the eigenvalues of a matrix in a process called the *QR algorithm*. Specifically, if $\mathbf{A} = \mathbf{A}_0$ is an $n\times n$ matrix, define a sequence of matrices as follows:
1. Let $\mathbf{A}_0 = \mathbf{Q}_0 \mathbf{R}_0$ be the QR factorization of $\mathbf{A}_0$ and let $\mathbf{A}_1 = \mathbf{Q}_0 \mathbf{R}_0$
2. Once $\mathbf{A}_k$ has been defined, let $\mathbf{A}_k = \mathbf{Q}_k \mathbf{R}_k$ be the QR factorization of $\mathbf{A}_k$ and let $\mathbf{A}_{k+1} = \mathbf{R}_k \mathbf{Q}_k$

Then $\mathbf{A}_k$ is unitarily/orthogonally similar to $\mathbf{A}$ since

$$
\begin{align*}
  \mathbf{Q}_{k-1}\mathbf{A}_k \mathbf{Q}_{k-1}^* =& \mathbf{Q}_{k-1}(\mathbf{R}_{k-1}\mathbf{Q}_{k-1})\mathbf{Q}_{k-1}^* \\
  =& \mathbf{Q}_{k-1}\mathbf{R}_{k-1} = \mathbf{A}_{k-1}
\end{align*}
$$

For complex matrices, it can be shown that under certain circumstances, such as when the eigenvalues of $\mathbf{A}$ have distinct norms, the sequence $\mathbf{A}_k$ converges (entrywise) to an upper triangular matrix $\mathbf{U}$, which therefore has the eigenvalues of $\mathbf{A}$ on its main diagonal.

### Hilbert basis

<MathBox title='Hilbert basis' boxType='definition'>
A maximal orthonormal set in an inner product space $V$ is called a *Hilbert basis* for $V$.
</MathBox>

Note that a Hilbert basis for an inner product space and a basis for vector space are two different concepts. To avoid confusion, a vector space basis, i.e. a maximal linearly independent set of vectors, is referred to as a *Hamel basis*. Since every orthonormal set is linearly independent, it follows that an orthonormal basis is a Hilbert basis, since it cannot be properly contained in an orthonormal set.

<MathBox title='Finite-dimensional orthonormal Hamel bases are also Hilbert bases' boxType='definition'>
Let $V$ be an innert product space. A finite subset $O = \Set{\mathbf{u}_i}_{i=1}^n$ of $V$ is an orthonormal (Hamel) basis for $V$ if and only if it is a Hilbert basis for $V$.

<details>
<summary>Proof</summary>

Any orthonormal basis is a Hilbert basis by definition. Conversely, if $O$ is a finite maximal orthonormal set and $O \subseteq P$, where $P$ is linearly independent, then we may extend $O$ to a strictly larger orthonormal set, in contradiction to the maximality of $O$. Hence, $O$ is maximal linearly independent.
</details>
</MathBox>

<MathBox title='A Hilbert basis for $\ell^2$ is not a Hamel basis' boxType='example'>
Let $V = \ell^2$ and let $M$ be the set of all vectors of the form $\mathbf{e}_i = (0,\dots,0,1,0,\dots)$, where $\mathbf{e}_i$ has a $1$ in the $i$th coordinate and $0$ elsewhere. Clearly, $M$ is an orthonormal set. Moreover, it is maximal: If $\mathbf{v} = (x_n)\in\ell^2$ has the property that $\mathbf{v}\perp M$, then $x_i = \langle \mathbf{v}, \mathbf{e}_i \rangle = 0$ for all $i$ and so $\mathbf{v} = \mathbf{0}$. Hence, no nonzero vector $\mathbf{v}\notin M$ is orthogonal to $M$. This shows that $M$ is a Hilbert basis for the inner product space $\ell^2$.

On the other hand, the vector space span of $M$ is the subspace $S$ of all sequences in $\ell^2$ that have finite support, i.e. have only a finite number of nonzero terms. Since $\operatorname{span}(M) = S \neq \ell^2$, we see that $M$ is not a Hamel basis for the vector space $\ell^2$.
</MathBox>

### The projection theorem and best approximations

Orthonormal bases have a great practical advantage over arbitrary bases. If $B = \Set{\mathbf{v}_i}_{i=1}^n$ is a basis for a vector space $V$, then each $\mathbf{v}\in V$ has the form

$$
  \mathbf{v} = \sum_{i=1}^n \alpha_i \mathbf{v}_i
$$

In general, determining the coordinates $\alpha_i$ requires solving a system of linear equations of size $n\times n$. On the other hand, if $O = \Set{\mathbf{u}_i}_{i=1}^n$ is an orthonormal basis for an inner product space $V$ and

$$
  \mathbf{v} = \sum_{i=1}^n \alpha_i \mathbf{u}_i
$$

then the coefficients $\alpha_i$ are easily computed

$$
  \langle\mathbf{v}, \mathbf{u}_i \rangle = \left\langle \sum_{i=1}^n \alpha_i \mathbf{u}_i, u_i \right\rangle = \alpha_i \langle\mathbf{u_i},\mathbf{u}_i \rangle = \alpha_i
$$

<MathBox title='Fourier expansion' boxType='definition'>
Let $O = \Set{\mathbf{b}_i}_{i=1}^k$ be an orthonormal subset of an inner product space $V$, and let $S = \langle O \rangle$. The *Fourier expansion* with respect to $O$ of a vector $\mathbf{v}\in V$ is

$$
  \tilde{\mathbf{v}} = \sum_{i=1}^k \langle \mathbf{v},\mathbf{b}_i \rangle\mathbf{b}_i
$$

Each coefficient $\langle\mathbf{v},\mathbf{b}_i \rangle$ is called a *Fourier coefficient* of $\mathbf{v}$ with respect to $O$.
</MathBox>

<MathBox title='Properties of Fourier expansion' boxType='definition'>
Let $V$ be an inner product space $V$. The Fourier expansion $\tilde{\mathbf{v}} = \sum_{i=1}^k \langle \mathbf{v},\mathbf{b}_i \rangle\mathbf{b}_i$ of a vector $\mathbf{v}\in V$ with respect to an orthonormal basis $O = \Set{\mathbf{b}_i}_{i=1}^n$ has the following properties.
1. $\tilde{\mathbf{v}}$ is the unique vector $\mathbf{s}\in S = \langle O \rangle$ for which $(\mathbf{v} - \mathbf{s}) \perp S$
2. $\tilde{\mathbf{v}}$ is the *best approximation* to $\mathbf{v}$ from within $S$, i.e. $\tilde{\mathbf{v}}$ is the unique vector $\mathbf{s}\in S$ that is closest to $\mathbf{v}$, in the sense that
$$
  \norm{\mathbf{v} - \tilde{\mathbf{v}}} < \norm{\mathbf{v} - \mathbf{s}}, \mathbf{s}\in S\setminus\Set{\tilde{\mathbf{v}}}
$$
3. **Bessel's inequality:** $\norm{\tilde{\mathbf{v}}} \leq \norm{\mathbf{v}}$

<details>
<summary>Proof</summary>

**(1):** Since

$$
  \langle\mathbf{v} - \tilde{\mathbf{v}}, \mathbf{u}_i \rangle = \langle\mathbf{v}, \mathbf{u}_i \rangle - \langle\tilde{\mathbf{v}}, \mathbf{u}_i \rangle - \langle\tilde{\mathbf{v}}, \mathbf{u}_i \rangle = 0
$$

it follows that $\mathbf{v} - \tilde{\mathbf{v}} \in S^\perp$. Also, if $\mathbf{v} - \mathbf{s} \in S^\perp$ for $\mathbf{s}\in S$, then $\mathbf{s} - \tilde{\mathbf{v}}\in S$ and

$$
  \mathbf{s} - \tilde{\mathbf{v}} = (\mathbf{v} - \tilde{\mathbf{v}}) - (\mathbf{v} - \mathbf{s})\in S^\perp
$$

and so $\mathbf{s} = \tilde{\mathbf{v}}$.

**(2):** If $\mathbf{s}\in S$, then $\mathbf{v} - \tilde{\mathbf{v}}\in S^\perp$ implies that $(\mathbf{v} - \tilde{\mathbf{v}})\perp (\tilde{\mathbf{v}} - \mathbf{s})$ and so

$$
  \norm{ \mathbf{v} - \mathbf{s} }^2 = \norm{ \mathbf{v} - \tilde{\mathbf{v}} + \tilde{\mathbf{v}} - \mathbf{s} }^2 = \norm{ \mathbf{v} - \tilde{\mathbf{v}} }^2 + \norm{ \tilde{\mathbf{v}} - \mathbf{s} }^2
$$

Hence, $\norm{ \mathbf{v} - \mathbf{s} }$ is smallest if and only if $\mathbf{s} = \tilde{\mathbf{v}}$ and the smallest value is $\norm{ \mathbf{v} - \tilde{\mathbf{v}} }$.
</details>
</MathBox>

<MathBox title='Projection theorem' boxType='theorem'>
If $S$ is a finite-dimensional subspace of an inner product space $V$, then $S = S \odot S^\perp$. In particular, if $\mathbf{v}\in V$, then

$$
  \mathbf{v} = \tilde{\mathbf{v}} + (\mathbf{v} - \tilde{\mathbf{v}}) \in S \odot S^\perp
$$

It follows that $\dim(V) = \dim(S) + \dim(S^\perp)$.

<details>
<summary>Proof</summary>

We have seen that $\mathbf{v} - \tilde{\mathbf{v}} \in S^\perp$ and so $V = S + S^\perp$. However, $S \cap S^\perp = \Set{\mathbf{0}}$ and so $V = S \odot S^\perp$.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V$ be an inner product space and let $S$ be a finite-dimensional subspace of $V$.
1. $S^{\perp\perp} = S$
2. If $X\subseteq V$ and $\dim(\langle X \rangle) < \infty$, then $X^{\perp\perp} = \langle X \rangle$

<details>
<summary>Proof</summary>

**(1):** It is clear that $S\subseteq S^{\perp\perp}$. On the other hand, if $\mathbf{v}\in S^{\perp\perp}$, then the projection theorem implies that $\mathbf{v} = \mathbf{s} + \mathbf{s}'$ where $\mathbf{s}\in S$ and $\mathbf{s}' \in S^\perp$. Then $\mathbf{s}'$ is orthogonal to both $\mathbf{s}$ and $\mathbf{v}$ and so $\mathbf{s}'$ is orthogonal to itself. Hence, $\mathbf{s}' = \mathbf{0}$ and $\mathbf{v} = \mathbf{s}\in S$ and so $S = S^{\perp\perp}$.
</details>
</MathBox>

<MathBox title='Approximation formula' boxType='proposition'>
Let $\langle\cdot, \cdot\rangle: V\times V \to\mathbb{F}$ be an inner product on the $\mathbb{F}$-vector space $V$. Suppose $U\subseteq V$ is a $k$-dimensional subspace. The distance of any $\boldsymbol{x}\in V$ to $U$ is given by

$$
  \operatorname{dist}(\boldsymbol{x}, U) := \inf\Set{\norm{ \boldsymbol{x}-\boldsymbol{u} } : \boldsymbol{u}\in U} = \norm{ \boldsymbol{x} - \boldsymbol{x}|_U }
$$

<details>
<summary>Proof</summary>

For all $\boldsymbol{u}\in U$ we have 

$$
\begin{align*}
  \norm{ \boldsymbol{x}-\boldsymbol{u} }^2 =& \norm{ \underbrace{(\boldsymbol{x} - \boldsymbol{x}|_U)}_{=\boldsymbol{n}} + \underbrace{\tilde{u}}_{(\boldsymbol{x}|_U - \boldsymbol{u})}_{\tilde{\boldsymbol{u}}\in U} }^2 \\
  =& \langle \boldsymbol{n} + \tilde{\boldsymbol{u}}, \boldsymbol{n} + \tilde{\boldsymbol{u}} \rangle \\
  =& \langle\boldsymbol{n},\boldsymbol{n}\rangle + \underbrace{\langle\boldsymbol{n},\tilde{\boldsymbol{u}}\rangle}_{=\boldsymbol{0}} + \underbrace{\langle\tilde{\boldsymbol{u}},\boldsymbol{n}\rangle}_{=\boldsymbol{0}} + \langle\tilde{\boldsymbol{u}},\tilde{\boldsymbol{u}}\rangle \\
  =& \norm{ \boldsymbol{n} }^2 + \underbrace{\norm{\tilde{\boldsymbol{u}}}^2}_{\geq 0} \geq \norm{\boldsymbol{n}}^2
\end{align*}
$$

This shows that $\inf\Set{\norm{\boldsymbol{x}-\boldsymbol{u}} | \boldsymbol{u}\in U} \geq \norm{\boldsymbol{n}}$ with equality if and only iff $\tilde{\boldsymbol{u}} = \boldsymbol{0} \iff \boldsymbol{u} = \boldsymbol{x}|_U$.
</details>
</MathBox>

### Characterizing orthonormal bases

<MathBox title='Characterization of orthonormal bases' boxType='proposition'>
Let $O = \Set{\mathbf{u}_i}_{i=1}^k$ be an orthonormal subset of an inner product space $V$ and let $S = \langle O \rangle$. The following are equivalent:
1. $O$ is an orthonormal basis for $V$
2. $\langle O \rangle^\perp = \Set{\mathbf{0}}$
3. Every vector is equal to its Fourier expansion, i.e. $\tilde{\mathbf{v}} = \mathbf{v}$ for all $\mathbf{v} \in V$
4. **Bessel's identity:** $lVert\tilde{\mathbf{v}}\rVert = \norm{\mathbf{v}}$
5. **Parseval's identity:** $\langle \mathbf{v},\mathbf{w} \rangle = [\tilde{\mathbf{v}}]_O \cdot [\tilde{\mathbf{w}}]_O$ for all $\mathbf{v}, \mathbf{w}\in V$ where $[\tilde{\mathbf{v}}]_O \cdot [\tilde{\mathbf{w}}]_O = \sum_{i=1}^k \langle\mathbf{v},\mathbf{u}_i \rangle \overline\langle\mathbf{w},\mathbf{u}_i \rangle$ is the standard dot product in $\mathbb{F}^k$

<details>
<summary>Proof</summary>

**(1) $\iff$ (2):** If $\mathbf{v}\in\langle O \rangle^\perp$ is nonzero, then $O \cup \Set{\frac{\mathbf{v}}{\norm{\mathbf{v}}}}$ is orthonormal and so $O$ is not maximal. Conversely, if $O$ is not maximal, there is an orthonormal set $P$ for which $O\subset P$. Then any nonzero $\mathbf{v} \in P\setminus O$ is in $\langle O \rangle^\perp$.
</details>
</MathBox>

## Riesz representation theorem

<MathBox title='Conjugate linear map' boxType='proposition'>
A function $\mathrm{T}: V\to W$ on complex vector spaces $V$ and $W$ is *conjugate linear* if it is additive

$$
  \mathrm{T}(\mathbf{v}_1 + \mathbf{v}_2) = \mathrm{T}\mathbf{v}_1 + \mathrm{T}\mathbf{v}_2
$$

and

$$
  \mathrm{T}(\alpha\mathbf{v}) = \bar{\alpha}\mathrm{T}\mathbf{v},\; \alpha\in\mathbb{C}
$$

A *conjugate isomorphism* is a bijective conjugate linear map.
</MathBox>

<MathBox title='Riesz representation theorem' boxType='theorem'>
Let $V$ be a finite-dimensional inner product space.
1. The map $\mathrm{T}:V\to V^*$ defined by $\mathrm{T}\mathbf{x} = \langle \cdot, \mathbf{x} \rangle$ is a conjugate isomorphism. In particular, for each $f\in V^*$, there exists a unique vector $\mathbf{x}\in V$ for which $f = \langle\cdot,\mathbf{x}\rangle$, i.e. $f\mathbf{v} = \langle\mathbf{v},\mathbf{x}\rangle$ for all $\mathbf{v}\in V$. We call $\mathbf{x}$ the *Riesz vector* for $f$ and denote it by $\mathrm{R}_f$.
2. The map $\mathrm{R}: V^* \to V$ defined by $\mathrm{R}f = \mathrm{R}_f$ is also a conjugate isomorphism, being the inverse of $\mathrm{T}$. We call $\mathrm{R}$ the *Riesz map*.

<details>
<summary>Proof</summary>

**(1):** Assume $\mathrm{T}$ is surjective. If $f = 0$, then $\mathrm{R}_f = 0$, so let us assume $f \neq 0$. Then $K = \ker(f)$ has codimension $1$ and so

$$
  V = \langle\mathbf{w}\rangle \odot K,\; \mathbf{w}\in K^\perp
$$

Letting $\mathbf{x} = \alpha\mathbf{w}$ for $\alpha\in\mathbb{F}$, we require that $f(\mathbf{v}) = \langle\mathbf{v},\alpha\mathbf{w}\rangle$. Since this clearly holds for any $\mathbf{v}\in K$, it is sufficient to show that it holds for $\mathbf{v}  = \mathbf{w}$, i.e.

$$
  f(\mathbf{w}) = \langle\mathbf{w},\alpha\mathbf{w}\rangle = \bar{\alpha}\langle\mathbf{w},\mathrm{w}\rangle
$$

Thus, $\alpha = \frac{\overline{f(\mathbf{w})}}{\norm{\mathbf{w}}^2}$ and

$$
  \mathrm{R}_f = \frac{\overline{f(\mathbf{w})}}{\norm{\mathbf{w}}^2}\mathbf{w}
$$

**(2):** For $f, g\in V^*$ and $\alpha, \beta \in\mathbb{F}$ we have

$$
\begin{align*}
  \langle \mathbf{v}, \mathrm{R}_{\alpha f + \beta g} \rangle =& (\alpha f + \beta g)(\mathbf{v}) \\
  =& \alpha f(\mathbf{v}) + \beta g(\mathbf{v}) \\
  =& \langle\mathbf{v}, \bar{\alpha}\mathrm{R}_f \rangle + \langle \mathbf{v}, \bar{\beta}\mathrm{R}_g \rangle \\
  =& \langle\mathbf{v}, \bar{\alpha}\mathrm{R}_f + \bar{\beta}\mathrm{R}_g \rangle
\end{align*}
$$

for all $\mathbf{v}\in V$ and so

$$
  \mathrm{R}_{\alpha f + \beta g} = \bar{\alpha} R_f + \bar{\beta} R_g
$$
</details>
</MathBox>

# Structure theory for normal operators

## The adjoint of a linear operator

<MathBox title='Hermitian adjoint' boxType='proposition'>
Let $V$ and $W$ be finite-dimensional inner product spaces over $\mathbb{F}$ and let $\mathrm{T}\in\mathcal{L}(V,W)$. Then there is a unique function $\mathrm{T}^\dagger: W\to V$ defined by the condition

$$
  \langle \mathrm{T}\mathbf{v}, \mathbf{w}\rangle = \langle \mathbf{v}, \mathrm{T}^\dagger \mathbf{w} \rangle,\; \mathbf{v}\in V, \mathbf{w}\in w
$$

The function $\mathrm{T}^\dagger \mathcal{L}(W, V)$ is the *adjoint* of $\mathrm{T}$.

<details>
<summary>Proof</summary>

If $\mathrm{T}^\dagger$ exists, then it is unique, for if

$$
  \langle\mathrm{T}\mathbf{v},\mathbf{w}\rangle = \langle\mathbf{v},\mathrm{S}\mathbf{w}\rangle
$$

then $\langle\mathbf{v},\mathrm{S}\mathbf{w}\rangle = \langle\mathbf{v},\mathrm{T}^* \mathbf{w}\rangle$ for all $\mathbf{v}\in V$ and $\mathbf{w}\in W$ and so $\mathrm{S} = \mathrm{T}^\dagger$.

We seek a linear map $\mathrm{T}^\dagger : W\to V$ for which $\langle \mathrm{T}\mathbf{v}, \mathbf{w}\rangle = \langle \mathbf{v}, \mathrm{T}^\dagger \mathbf{w} \rangle$. By the Riesz representation theorem, for each $\mathbf{w}\in W$, the linear functional $f_\mathbf{w} \in V^*$ defined by

$$
  f_\mathbf{w} \mathbf{v} = \langle\mathrm{T}\mathbf{v},\mathbf{w}\rangle
$$

has the form

$$
  f_\mathbf{w} \mathbf{v} = \langle\mathbf{v}, \mathrm{R}_{f_\mathbf{w}}\rangle
$$

where $\mathrm{R}_{f_\mathbf{w}} \in V$ is the Riesz vector for $f_\mathbf{w}$. If $\mathrm{T}^\dagger :W\to V$ is defined by

$$
  \mathrm{T}^\dagger \mathbf{w} = \mathrm{R}_{f_\mathbf{w}} = \mathrm{R}(f_\mathbf{w})
$$

where $\mathrm{R}$ is the Riesz map, then

$$
  \langle\mathbf{v},\mathrm{T}^\dagger \mathbf{w}\rangle = \langle\mathbf{v},\mathrm{R}_{f_\mathbf{w}}\rangle = f_\mathrm{w}\mathbf{v} = \langle\mathrm{T}\mathbf{v},\mathbf{w}\rangle
$$

Finally, since $\mathrm{T}^\dagger = \mathrm{R}\circ f$ is the composition of the Riesz map $\mathrm{R}$ and the map $f:\mathbf{w}\mapsto f_\mathrm{w}$ and since both of these maps are conjugate linear, their composition is linear.
</details>
</MathBox>

<MathBox title='Properties of the Hermitian adjoint' boxType='proposition'>
Let $V$ and $W$ be finite-dimensional $\mathbb{F}$-inner product spaces. For every $\mathrm{S},\mathrm{T}\in\mathcal{L}(V,W)$ and $\alpha\in\mathbb{F}$
1. $(\mathrm{S} + \mathrm{T})^\dagger = \mathrm{S}^\dagger + \mathrm{T}^\dagger$
2. $(\alpha\mathrm{T})^\dagger = \bar{\alpha}\mathrm{T}^\dagger$
3. $T^{\dagger\dagger} = \mathrm{T}$ and so $\langle\mathrm{T}^\dagger \mathbf{v},\mathbf{w}\rangle = \langle\mathbf{v},\mathrm{T}\mathbf{w}\rangle$
4. If $V = W$ then $(\mathrm{S}\mathrm{T})^\dagger = \mathrm{T}^\dagger \mathrm{S}^\dagger$
5. If $\mathrm{T}$ is invertible, then $(\mathrm{T}^{-1})^\dagger = (\mathrm{T}^\dagger)^{-1}$
6. If $V = W$ and $p(x)\in\R[x]$, then $p(\mathrm{T})^\dagger = p(\mathrm{T}^\dagger)$

Moreover, if $\mathrm{T}\in\mathcal{L}(V)$ is a linear operator and $S$ is a subspace of $V$, then
7. $\mathrm{S}$ is $\mathrm{T}$-invariant if and only if $S^\perp$ is $\mathrm{T}^\dagger$-invariant
8. $(S,S^\perp)$ reduces $\mathrm{T}$ if and only if $S$ is both $\mathrm{T}$-invariant and $\mathrm{T}^\dagger$-invariant, in which case
$$
  (\mathrm{T}|_S)^\dagger = (\mathrm{T}^\dagger)|_S
$$

<details>
<summary>Proof</summary>

**(7):** Let $\mathbf{s}\in S$ and $\mathbf{z}\in S^\perp$ and write

$$
  \langle\mathrm{T}^\dagger \mathbf{z}, \mathbf{s}\rangle = \langle \mathbf{z}, \mathrm{T}\mathbf{s}\rangle
$$

If $S$ is $\mathrm{T}$-invariant, then $\langle\mathrm{T}^\dagger \mathbf{z}, \mathbf{s}\rangle = 0$ for all $\mathbf{s}\in S$ and so $\mathrm{T}^\dagger \mathbf{z} \in S^\perp$ and $S^\perp$ is $\mathrm{T}^\dagger$-invariant. Conversely, if $S^\perp$ is $\mathrm{T}^\dagger$-invariant, then $\langle\mathbf{z},\mathrm{T}\mathbf{s}\rangle = 0$ for all $\mathbf{z}\in S^\perp$ and so $\mathrm{T}\mathbf{s}\in S^{\perp\perp} = S$. Hence $S$ is $\mathrm{T}$-invariant.

**(8):** The first statement follows from **(7)** applied to both $S$ and $S^\perp$. For the second statement, since $S$ is both $\mathrm{T}$-invariant and $\mathrm{T}^\dagger$-invariant, if $\mathbf{s},\mathbf{t}\in S$, then

$$
  \langle \mathbf{s}, (\mathrm{T}^\dagger)|_S (\mathbf{t}) \rangle = \langle \mathbf{s}, \mathrm{T}^\dagger \mathbf{t} \rangle = \langle \mathrm{T}\mathbf{s}, \mathbf{t} \rangle = \langle\mathrm{T}|_S (\mathbf{s}), \mathbf{t} \rangle
$$

Hence, by definition of adjoint, $(\mathrm{T}^\dagger)|_S = (\mathrm{T}|_S)^\dagger$.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $\mathrm{T}\in\mathcal{L}(V,W)$, where $V$ and $W$ are finite-dimensional inner product spaces.
1. $\ker(\mathrm{T}^\dagger) = \operatorname{ran}(\mathrm{T})^\perp$ and $\operatorname{ran}(\mathrm{T}^\dagger) = \ker(\mathrm{T})^\perp$, and so
$$
\begin{align*}
  \mathrm{T} \text{ surjective} \iff& \mathrm{T}^\dagger \text{ injective} \\
  \mathrm{T} \text{ injective} \iff& \mathrm{T}^\dagger \text{ surjective}
\end{align*}
$$
2. $\ker(\mathrm{T}^\dagger \mathrm{T}) = \ker(\mathrm{T})$ and $\ker(\mathrm{T}\mathrm{T}^\dagger) = \ker(\mathrm{T}^\dagger)$
3. $\operatorname{ran}(\mathrm{T}^\dagger \mathrm{T}) = \operatorname{ran}(\mathrm{T}^\dagger)$ and $\operatorname{ran}(\mathrm{T}\mathbf{T}^\dagger) = \operatorname{ran}(\mathrm{T})$
4. $(\mathrm{P}_{S,T})^\dagger = \mathrm{P}_{T^\perp, S^\perp}$

<details>
<summary>Proof</summary>

**(1):** We have

$$
\begin{align*}
  \mathbf{u} \in\ker(\mathrm{T}^\dagger) \iff& \mathrm{T}^\dagger \mathbf{u} = \mathbf{0} \\
  \iff& \langle \mathrm{T}^\dagger \mathbf{u}, V \rangle = \Set{\mathbf{0}} \\
  \iff& \langle \mathbf{u}, \mathrm{T}V \rangle = \Set{\mathrm{0}} \\
  \iff& \mathbf{u}\in\operatorname{ran}(\mathrm{T})^\perp
\end{align*}
$$

and so $\ker(\mathrm{T}^\dagger) = \operatorname{ran}(\mathrm{T})^\perp$. The second identity follows by replacing $\mathrm{T}$ and $\mathrm{T}^\dagger$ and taking complements.

**(2):** It is clear that $\ker(\mathrm{T}) \subseteq\ker(\mathrm{T}^\dagger \mathrm{T})$. For the reverse inclusion, we have

$$
\begin{align*}
  \mathrm{T}^\dagger \mathrm{T}\mathbf{u} = \mathbf{0} \implies& \langle \mathrm{T}^\dagger \mathrm{T}\mathbf{u}, \mathbf{u} \rangle = 0 \\
  \implies& \langle \mathrm{T}\mathbf{u}, \mathrm{T}\mathbf{u} \rangle = 0 \\
  \implies& \mathrm{T}\mathbf{u} = \mathbf{0}
\end{align*}
$$

and so $\ker(\mathrm{T}^\dagger \mathrm{T}) \subseteq\ker(\mathrm{T})$. The second identity follows from the first by replacing $\mathrm{T}$ with $\mathrm{T}^\dagger$.
</details>
</MathBox>

### Relation between the algebraic adjoint and Hermitian adjoint

<MathBox title='' boxType='proposition'>
Let $\mathrm{T}\in\mathcal{L}(V,W)$, where $V$ and $W$ are finite-dimensional inner product spaces.
1. The algebraic adjoint $\mathrm{T}^* :W^* \to V^*$ and the Hermitian adjoint $\mathrm{T}^\dagger : W\to V$ are related by
$$
  \mathrm{T}^* = (\mathrm{R}^V)^{-1} \circ \mathrm{T}^\dagger \circ \mathrm{R}^W
$$
where $\mathrm{R}^V$ and $\mathrm{R}^W$ are the conjugate Riesz isomorphisms on $V$ and $W$, respectively.

2. If $B$ and $C$ are ordered orthonormal bases for $V$ and $W$, respectively, then
$$
  [\mathrm{T}^\dagger]_{C,B} = (\mathrm{T}_{B,C})
$$
In other words, the matrix of the adjoint $\mathrm{T}^\dagger$ is adjoint (conjugate transpose) of the matrix of $\mathrm{T}$.

$$
\begin{CD}
  V^* @<{\mathrm{T}^*}<< W^* \\
  @V{\mathrm{R}^V}VV @VV{\mathrm{R}^W}V \\
  V @<{\mathrm{T}^\dagger}<< W
\end{CD}
$$

<details>
<summary>Proof</summary>

**(1):** Consider the composite $\mathrm{S}:W^* \to V^*$ defined by

$$
  \mathrm{S} = (\mathrm{R}^V)^{-1} \circ\mathrm{T}^\dagger \circ\mathrm{R}^W
$$

is linear. Moreover, for all $f\in W^*$ and $\mathbf{v}\in V$

$$
\begin{align*}
  (\mathrm{T}^* (f))\mathbf{v} =& f(\mathrm{T}\mathbf{v}) \\
  =& \langle\mathrm{T}\mathbf{v}, \mathrm{R}^W (f)\rangle \\
  =& \langle \mathbf{v}, \mathrm{T}^\dagger \mathrm{R}^W (f) \rangle \\
  =& [(\mathrm{R}^V)^{-1} (\mathrm{T}^\dagger \mathrm{R}^W (f))](\mathbf{v}) \\
  =& (\mathrm{S}f)\mathbf{v}
\end{align*}
$$

showing that $\mathrm{S} = \mathrm{T}^*$. Hence, the relationship between $\mathrm{T}^*$ an $\mathrm{T}^\dagger$ is

$$
  \mathrm{T}^* = (\mathrm{R}^V)^{-1} \circ \mathrm{T}^\dagger \circ \mathrm{R}^W
$$

Loosely speaking, the Riesz functions are like "change of variables" functions from linear functionals to vectors, and we can say that $\mathrm{T}^\dagger$ does to Riesz vectors what $\mathrm{T}^*$ does to the corresponding linear functions. Put another way, $\mathrm{T}$ and $\mathrm{T}^\dagger$ are the same, up to conjugate Riesz isomorphism.

**(2):** Suppose that $B = (\mathbf{b}_i)_{i=1}^n$ and $C = (\mathbf{c}_i)_{i=1}^m$ are ordered orthonormal bases for $V$ and $W$, respectively, then

$$
\begin{align*}
  ([\mathrm{T}^\dagger]_{C,B})_{i,j} =& \langle \mathrm{T}^\dagger \mathbf{c}_j, \mathbf{b}_i \rangle \\
  =& \langle \mathbf{c}_j, \mathrm{T}\mathbf{b}_i \rangle \\
  =& \overline{\langle \mathrm{T}\mathbf{b}_i, \mathrm{c}_j \rangle} \\
  =& \overline{([\mathrm{T}]_{B,C})_{j,i}}
\end{align*}
$$

showing that $[\mathrm{T}^\dagger]_{C,B}$ and $[\mathrm{T}]_{B,C}$ are matrix adjoints (conjugate transposes).
</details>
</MathBox>

## Orthogonal projections

<MathBox title='Orthogonal projection' boxType='definition'>
A projection of the form $\mathrm{P}_{S,S^\perp}$ is called *orthogonal*. Equivalently, a projection $\mathrm{P}$ is orthogonal if $\ker(\mathrm{P}) \perp \operatorname{ran}(\mathrm{P})$.
</MathBox>

Note that orthogonal projections differ in concept from two projections $\mathrm{P}, \mathrm{S}$ that are orthogonal to each other, i.e. $\mathrm{PS} = \mathrm{SP} = 0$ are not necessarily the same as.

<MathBox title='Characterizations of orthogonal projections' boxType='proposition'>
Let $V$ be a finite-dimensional inner product space. The following are equivalent for an operator $\mathrm{P}$ on $V$:
1. $\mathrm{P}$ is an orthogonal projections
2. $\mathrm{P}$ is idempotent and self-adjoint
3. $\mathrm{P}$ is idempotent and does not expand lengths, i.e.
$$
  \norm{\mathrm{P}\mathbf{v}}\leq\norm{\mathbf{v}},\; \mathbf{v}\in V
$$

<details>
<summary>Proof</summary>

**(1) $\iff$ (2):** Since $(\mathrm{P}_{S,T})^\dagger = \mathrm{P}_{T^\perp, S^\perp}$ it follows that $\mathrm{P} = \mathrm{P}^\dagger$.

**(1) $\iff$ (3):** Let $\mathrm{P} = \mathrm{P}_{S,S^\perp}$. Then, if $\mathbf{v} = \mathbf{s} + \mathbf{t}$ for $\mathbf{s}\in S$ and $\mathbf{t}\in S^\perp$, it follows that

$$
  \norm{\mathbf{v}}^2 = \norm{\mathbf{s}}^2 + \norm{\mathbf{t}}^2 \geq \rVert\mathbf{s}\norm{^2 = }\mathbf{P}\mathbf{v}\lVert^2
$$

Suppose that **(3)** holds, then

$$
  \operatorname{ran}(\mathrm{P})\oplus\mathrm{P}) = V = \ker(\mathrm{P})^\perp \odot \ker(\mathrm{P})
$$

and we want to show that the first direct sum is orthogonal. If $\mathbf{w}\in\operatorname{ran}(\mathrm{P})$, then $\mathbf{w} = \mathbf{x} + \mathbf{y}$, where $\mathbf{x}\in\ker(\mathrm{P})$ and $\mathbf{y}\in\ker(\mathrm{P})^\perp$. Hence

$$
  \mathbf{w} = \mathrm{P}\mathbf{w} = \mathrm{P}\mathbf{x} + \mathrm{P}\mathbf{y} = \mathrm{P}\mathbf{y}
$$

and so the orthogonality of $\mathbf{x}$ and $\mathbf{y}$ implies that

$$
  \norm{\mathbf{x}}^2 + \norm{\mathbf{y}}^2 = \norm{\mathbf{w}}^2 = \norm{\mathrm{P}\mathbf{v}}^2 \leq \norm{\mathbf{y}}^2
$$

Hence, $\mathbf{x} = \mathbf{0}$, and so $\operatorname{ran}(\mathrm{P}) \subseteq\ker(\mathrm{P})^\perp$, which implies that $\operatorname{ran}(\mathrm{P}) = \ker(\mathrm{P})^\perp$.
</details>
</MathBox>

### Orthogonal resolutions of the identity

<MathBox title='Orthogonal resolution of identity' boxType='definition'>
An *orthogonal resolution of the identity* is a resolution of the identity $\sum_{i=1}^k \mathrm{P}_k = \mathrm{I}$ in which each projection $\mathrm{P}_i$ is orthogonal.
</MathBox>

<MathBox title='Relation between orthogonal direct sum decompositions and orthogonal resolution of identity' boxType='theorem'>
Let $V$ be an inner product space. Orthogonal resolutions of the identity on $V$ corresponds to orthogonal direct sum decompositions of $V$ as follows:
1. If $\sum_{i=1}^k \mathrm{P}_i = \mathrm{I}$ is an orthogonal resolution of the identity, then
$$
  V = \bigodot_{i=1}^k \operatorname{ran}(\mathrm{P}_i)
$$
where $\mathrm{P}_i$ is an orthogonal projections onto $\operatorname{ran}(\mathrm{P}_i)$.

2. Conversely, if $V = \bigodot_{i=1}^k S_i$ and if $\mathrm{P}_i$ is an orthogonal projection onto $S_i$, then $\sum_{i=1}^k \mathrm{P}_i = \mathrm{I}$.

<details>
<summary>Proof</summary>

**(1):** If $\sum_{i=1}^k \mathrm{P}_i = \mathrm{I}$ is an orthogonal resolution of the identity, it follows that

$$
  V = \bigoplus_{i=1}^k \operatorname{ran}(\mathrm{P}_i)
$$

However, since the $\mathrm{P}_i$ are pairwise orthogonal and self-adjoint, it follows that

$$
  \langle\mathrm{R}_i \mathbf{v}, \mathrm{R}_j \mathbf{w} \rangle = \langle\mathbf{v}, \mathrm{R}_i \mathrm{R}_j \mathbf{w} \rangle = \langle \mathbf{v}, 0 \rangle = 0
$$

and so

$$
  V = \bigodot_{i=1}^k \operatorname{ran}(\mathrm{P}_i)
$$

**(2):** For the converse, we know that $\sum_{i=1}^k \mathrm{P}_i = \mathrm{I}$ is a resolution of the identity, where e$\mathrm{P}_i$ is a projection onto $\operatorname{ran}(\mathrm{P}_i)$ along

$$
  \ker(\mathrm{P}_i) = \bigodot_{j\neq i} \operatorname{ran}(\mathrm{P}_j) = \operatorname{ran}(\mathrm{P}_i)^\perp
$$

Hence, $\mathrm{P}_i$ is orthogonal.
</details>
</MathBox>

## Unitary diagonalizability

<MathBox title='Unitarily/orthogonally diagonalizable linear operator' boxType='definition'>
A linear operator $\mathrm{T}\in\mathcal{L}(V)$ is *unitarily diagonalizable* (when $V$ is complex) and *orthogonally diagonalizable* (when $V$ is real) if there is an ordered orthonormal basis $O = (\mathbf{u}_i)_{i=1}^n$ of $V$ for which the matrix $[\mathrm{T}]_O$ is diagonal, or equivalently if

$$
  \mathrm{T}\mathbf{u}_i = \lambda_i \mathbf{u}_i,\; \forall i=1,\dots,n
$$
</MathBox>

<MathBox title='Characterization of unitarily/orthogonally diagonalizable linear operators' boxType='definition'>
Let $V$ be a finite-dimensional inner product space and let $\mathrm{T}\in\mathcal{L}(V)$. The following are equivalent:
1. $\mathrm{T}$ is unitarily (orthogonally) diagonalizable
2. $V$ has an orthonormal basis that consists entirely of eigenvectors of $\mathrm{T}$
3. $V$ has the form $V = \bigodot_{i=1}^k E_{\lambda_i}$ where $E_{\lambda_i}$ is the eigenspace of the eigenvalue $\lambda_i$
</MathBox>

## Normal operators

<MathBox title='Normal operators and matrices' boxType='definition'>
1. A linear operator $\mathrm{T}\in\mathcal{L}(V)$ on an inner product space $V$ is *normal* if it commutes with its adjoint
$$
  \mathrm{TT}^\dagger = \mathrm{T}^\dagger \mathrm{T}
$$
2. A matrix $\mathbf{A}\in\mathcal{M}_n (\mathbb{F})$ is *normal* if $\mathbf{A}$ commutes with its adjoint $\mathbf{A}^\dagger$.
</MathBox>

If $\mathrm{T}$ is normal and $O$ is an ordered orthonormal basis of $V$, then

$$
  [\mathrm{T}]_O [\mathrm{T}]_O^\dagger = [\mathrm{T}]_O [\mathrm{T}^\dagger]_O = [\mathrm{TT}^\dagger]_O
$$

and

$$
  [\mathrm{T}]_O^\dagger [\mathrm{T}]_O = [\mathrm{T}^\dagger]_O [\mathrm{T}]_O = [\mathrm{T}^\dagger \mathrm{T}]_O
$$

and so $\mathrm{T}$ is normal if and only if $[\mathrm{T}]_O$ is normal for some, and hence all, orthonormal bases for $V$. Note that this does not hold for bases that are not orthonormal.

<MathBox title='Properties of normal operators' boxType='proposition'>
Let $\mathrm{T}\in\mathcal{L}(V)$ be a normal operator.
1. The following are also normal operators:
    - $\mathrm{T}|_S$ if $\mathrm{T}$ reduces $(S,S^\perp)$
    - $\mathrm{T}^\dagger$
    - $\mathrm{T}^{-1}$ if $\mathrm{T}$ is invertible
    - $p(\mathrm{T})$ for any polynomial $p(x) \in\mathbb{F}[x]$
2. For any $\mathbf{v},\mathbf{w}\in V$
$$
  \langle\mathrm{T}\mathbf{v},\mathrm{T}\mathbf{w}\rangle = \langle\mathrm{T}^\dagger \mathbf{v}, \mathrm{T}^\dagger \mathbf{w}\rangle
$$
and in particular 
$$
  \norm{\mathrm{T}\mathbf{v}} = \norm{\mathrm{T}^\dagger \mathbf{v}}
$$
and so
$$
  \ker(\mathrm{T}^\dagger) = \ker(\mathrm{T})
$$
3. For any integer $k\geq 1$
$$
  \ker(\mathrm{T}^k) = \ker(\mathrm{T})
$$
4. The minimal polynial $m_\mathrm{T}(x)$ is a product of distinct prime monic polynomials
5. $\mathrm{T}\mathbf{v} = \lambda\mathbf{v} \iff \mathrm{T}^\dagger \mathbf{v} = \bar{\lambda}\mathbf{v}$
6. If $S$ and $T$ are submodules of $V_\mathrm{T}$ with relatively prime orders, then $S\perp T$
7. If $\lambda$ and $\mu$ are distinct eigenvalues of $\mathrm{T}$, then $E_\lambda \perp E_\mu$

<details>
<summary>Proof</summary>

**(2):** Normality implies that

$$
  \langle \mathrm{T}\mathbf{v},\mathrm{T}\mathbf{w}\rangle = \langle\mathrm{T}^\dagger \mathrm{T}\mathbf{v}, \mathbf{v}\rangle = \langle \mathrm{TT}^\dagger \mathbf{v}, \mathbf{v}\rangle = \langle \mathrm{T}^\dagger \mathbf{v}, \mathrm{T}^\dagger \mathbf{v} \rangle
$$

**(3):** Consider the operator $\mathrm{S} = \mathrm{T}^\dagger \mathrm{T}$ which is self-adjoint, i.e.

$$
  \mathrm{S}^\dagger = (\mathrm{T}^\dagger \mathrm{T})^\dagger = \mathrm{T}^\dagger \mathrm{T} = \mathrm{S}
$$

If $\mathrm{S}^k \mathbf{v} = \mathbf{0}$ for $k > 1$, then

$$
  0 = \langle\mathrm{S}^k \mathbf{v}, \mathrm{S}^{k-2} \mathbf{v} \rangle = \langle \mathrm{S}^{k-1}\mathbf{v}, \mathrm{S}^{k-1}\mathbf{v} \rangle
$$

and so $\mathrm{S}^{k-1} \mathbf{v} = \mathbf{0}$. Continuing in this way gives $\mathrm{S}\mathbf{v} = \mathbf{0}$. If $\mathrm{T}^k \mathbf{v} = \mathbf{0}$ for $k > 1$, then

$$
  \mathrm{S}^k \mathbf{v} = (\mathrm{T}^\dagger \mathrm{T})^k \mathbf{v} = (\mathrm{T}^\dagger)^k \mathrm{T}^k \mathbf{v} = \mathbf{0}
$$
and so $\mathrm{S}\mathbf{v} = \mathbf{0}$. Hence

$$
  0 = \langle\mathrm{S}\mathbf{v},\mathbf{v}\rangle = \langle\mathrm{T}^\dagger \mathrm{T}\mathbf{v},\mathbf{v}\rangle = \langle\mathrm{T}\mathbf{v},\mathrm{T}\mathbf{v}\rangle
$$

and so $\mathrm{T}\mathbf{v} = \mathbf{0}$

**(4):** Suppose that $m_\mathrm{T} (x) = p^e (x)q(x)$ where $p(x)$ is monic and prime. Then for any $\mathbf{v}\in V$

$$
  p^e (\mathrm{T})[q(\mathrm{T})\mathbf{v}] = 0
$$

and since $p(\mathrm{T})$ is also normal, **(3)** implies that

$$
  p(\mathrm{T})[q(\mathrm{T})\mathbf{v}] = 0
$$

for all $\mathbf{v}\in V$. Hence, $p(\mathrm{T})q(\mathrm{T}) = 0$, which implies that $e = 1$. Thus, the prime factors of $m_\mathrm{T}(x)$ appear only to the first power.

**(5):** This follows from **(2)**

$$
  \ker(\mathrm{T} - \lambda\mathrm{I}) = \ker[(\mathrm{T} - \lambda\mathrm{I})^\dagger] = \ker(\mathrm{T}^\dagger - \bar{\lambda}\mathrm{I}) 
$$

**(6):** If $o(S) = p(x)$ and $o(T) = q(x)$, then there are polynomials $a(x)$ and $b(x)$ for which $a(x)p(x) + b(x)q(x) = 1$ and so

$$
  a(\mathrm{T})p(\mathrm{T}) + b(\mathrm{T})q(\mathrm{T}) = \mathrm{I}
$$

Now, $\mathrm{A} = a(\mathrm{T})p(\mathrm{T})$ annihilates $S$ and $\mathrm{B} = b(\mathrm{T})q(\mathrm{T})$ annihilates $T$. Thus, $\mathrm{B}^\dagger$ also annihilates $T$ and so

$$
  \langle S, T \rangle = \langle (\mathrm{A} + \mathrm{B})S, T\rangle = \langle \mathrm{B}S, T \rangle = \langle S, \mathrm{B}^\dagger T \rangle = \Set{0}
$$

**(7):** This follows from **(6)**, since $o(E_\lambda) = x - \lambda$ and $o(E_\lambda) = x - \mu$ are relatively prime when $\lambda\neq\mu$. Alternatively, for $\mathbf{v}\in E_\lambda$ and $\mathbf{w}\in E_\mu$, we have

$$
  \lambda\langle\mathbf{v},\mathbf{w}\rangle = \langle\mathrm{T}\mathbf{v},\mathbf{w}\rangle = \langle\mathbf{v},\mathrm{T}^\dagger \mathbf{w}\rangle = \langle\mathbf{v}, \bar{\mu}\mathbf{v}\rangle = \mu\langle\mathbf{v},\mathbf{w}\rangle
$$

and so $\lambda\neq\mu$ implies that $\langle\mathbf{v},\mathbf{w}\rangle = 0$.
</details>
</MathBox>

### The spectral theorem for normal operators

<MathBox title='The spectral theorem for normal operators: the complex case' boxType='theorem'>
Let $V$ be a finite-dimensional complex inner product space and let $\mathrm{T}\in\mathcal{L}(V)$. The following are equivalent:
1. $\mathrm{T}$ is normal
2. $\mathrm{T}$ is unitarily diagonalizable, i.e. $V_\mathrm{T} = \bigodot_{i=1}^k E_{\lambda_i}$
3. $\mathrm{T}$ has an orthogonal spectral resolution $\mathrm{T} = \sum_{i=1}^k \lambda_i \mathrm{P}_i$ where $\sum_{i=1}^k \mathrm{P}_i = \mathrm{I}$ and $\mathrm{P}_i$ is orthogonal for all $i$, in which case $\Set{\lambda_i}_{i=1}^k$ is the spectrum of $\mathrm{T}$ and $\operatorname{ran}(\mathrm{P}_i) = E_{\lambda_i}$ and $\ker(\mathrm{P}_i)  = \bigodot_{j\neq i} E_{\lambda_j}$

<details>
<summary>Proof</summary>

**(2) $\iff$ (3):** From the diagonalizability characterization, we know that $V_\mathrm{T} = \bigoplus_{i=1}^k E_{\lambda_i}$ if and only if $\mathrm{T} = \sum_{i=1}^k \lambda_i \mathrm{P}_i$. In this case $\operatorname{ran}(\mathrm{P}_i) = E_{\lambda_i}$ and $\ker(\mathrm{P}_i) = \bigoplus_{j\neq i} E_{\lambda_j}$. However, $E_{\lambda_i} \perp E_{\lambda_j}$ for $i\neq j$ if and only if

$$
  \operatorname{ran}(\mathrm{P}_i) \perp \ker(\mathrm{P}_i)
$$

That is, if and only if each $\mathrm{P}_i$ is orthogonal. Hence, the direct sum $V_\mathrm{T} = \bigoplus_{i=1}^k$ is an orthogonal sum if and only if each projection is orthogonal.
</details>
</MathBox>

<MathBox title='The spectral theorem for normal operators: the real case' boxType='theorem'>
A linear operator $\mathrm{T}$ on a finite-dimensional real inner product space is normal if and only if

$$
  V = \left(\bigodot_{i=1}^k E_{\lambda_i} \right) \odot \left(\bigodot_{j=1}^m W_j \right) 
$$

where $\Set{\lambda_i}_{i=1}^k$ is the spectrum of $\mathrm{T}$ and each $W_j$ is an indecomposable two-dimensional $\mathrm{T}$-invariant subspace with an ordered basis $B_i$ for which

$$
  [\mathrm{T}]_{B_i} = \begin{bmatrix} a_i & -b_i \\ b_i & a_i \end{bmatrix}
$$

<details>
<summary>Proof</summary>

We only need to show that if $V$ has such a decomposition, then $\mathrm{T}$ is normal. However,

$$
  [\mathrm{T}]_{B_i} [\mathrm{T}]_{B_i}^\top = (a_i^2 + b_i^2)\mathbf{I}_2 = [\mathrm{T}]_{B_i}^\top [\mathrm{T}]_{B_i}
$$

and so $[\mathrm{T}]_{B_i}$ is normal. It follows that $\mathrm{T}$ is normal.
</details>
</MathBox>

## Self-adjoint operators

<MathBox title='Self-adjoint (Hermitian) operator' boxType='definition'>
A linear operator $\mathrm{T}\in\mathcal{L}(V)$ on an $\mathbb{F}$-inner product space $V$ is *self-adjoint* if 

$$
  \mathrm{T}^\dagger = \mathrm{T}
$$

A self-adjoint operator is also called *Hermitian* when $\mathbb{F} = \mathbb{C}$ and *symmetric* when $\mathbb{F} = \R$, in which case $\mathrm{T} = \mathrm{T}^\top$.
</MathBox>

<MathBox title='Skew self-adjoint (skew-Hermitian) operator' boxType='definition'>
A linear operator $\mathrm{T}\in\mathcal{L}(V)$ on an $\mathbb{F}$-inner product space $V$ is *skew self-adjoint* if 

$$
  \mathrm{T}^\dagger = -\mathrm{T}
$$

A skew self-adjoint operator is also called *skew-Hermitian* when $\mathbb{F} = \mathbb{C}$ and *skew-symmetric* when $\mathbb{F} = \R$, in which case $\mathrm{T} = -\mathrm{T}^\top$.
</MathBox>

<MathBox title='Quadratic form' boxType='definition'>
Let $\mathrm{T}\in\mathcal{L}(V)$ be a linear operator on an $\mathbb{F}$-inner product space $V$. The *quadratic form* associated with $\mathrm{T}$ is the function $q_\mathrm{T}: V\to\mathbb{F}$ defined by

$$
  q_\mathrm{T} (\mathbf{v}) = \langle\mathrm{T}\mathbf{v}, \mathbf{v}\rangle
$$
</MathBox>

<MathBox title='Properties of self-adjoint operators' boxType='proposition'>
Let $V$ be a finite-dimensional inner product space and let $\mathrm{S},\mathrm{T}\in\mathcal{L}(V)$ be linear operators on $V$.
1. If $\mathrm{S}$ and $\mathrm{T}$ are self-adjoint, then so are the following
    - $\mathrm{S} + \mathrm{T}$
    - $\mathrm{T}^{-1}$ if $\mathrm{T}$ is invertible
    - $p(\mathrm{T})$, for any real polynomial $p(x)\in\R[x]$
2. A complex operator $\mathrm{T}$ is Hermitian if and only if the quadratic form $q_\mathrm{T}$ is real for all $\mathbf{v}\in V$
3. If $\mathrm{T}$ is a complex operator or a real symmetric operator, then $\mathrm{T} = 0 \iff q_\mathrm{T} = 0$
4. The characteristic polynomial $c_\mathrm{T}(x)$ of a self-adjoint operator $\mathrm{T}$ splits over $\R$, i.e. all complex roots of $c_\mathrm{T}(x)$ are real. Hence, the minimal polynomial $m_\mathrm{T}(x)$ of $\mathrm{T}$ is the product of distinct monic linear factors over $\R$.

<details>
<summary>Proof</summary>

**(2):** If $\mathrm{T}$ is Hermitian, then

$$
  \langle\mathrm{T}\mathbf{v}, \mathbf{v}\rangle = \langle\mathbf{v},\mathrm{T}\mathbf{v}\rangle = \overline{\langle\mathrm{T}\mathbf{v},\mathbf{v}}
$$

and so $q_\mathrm{T}(\mathbf{v}) = \langle\mathrm{T}\mathbf{v},\mathbf{v}\rangle$ is real. Conversely, if $\langle\mathrm{T}\mathbf{v},\mathbf{v}\rangle\in\R$, then

$$
  \langle\mathbf{v},\mathrm{T}\mathbf{v}\rangle = \langle\mathrm{T}\mathbf{v},\mathbf{v}\rangle = \langle\mathbf{v}, \mathrm{T}^\dagger \mathbf{v}\rangle
$$

and so $\mathrm{T} = \mathrm{T}^\dagger$.

**(3):** We only need to prove that $q_\mathrm{T} = 0$ implies $\mathrm{T} = 0$ when $\R$. If $q_\mathrm{T} = 0$, then

$$
\begin{align*}
  0 =& \langle\mathrm{T}(\mathbf{x} + \mathbf{y}), \mathbf{x} + \mathbf{y}\rangle \\
  =& \langle\mathrm{T}\mathbf{x}, \mathbf{x}\rangle + \langle\mathrm{T}\mathbf{y}, \mathbf{y}\rangle + \langle\mathrm{T}\mathbf{x}, \mathbf{y}\rangle + \langle\mathrm{T}\mathbf{y}, \mathbf{x}\rangle \\
  =& \langle\mathrm{T}\mathbf{x}, \mathbf{y}\rangle + \langle\mathrm{T}\mathbf{y}, \mathbf{x}\rangle \\
  =& \langle\mathrm{T}\mathbf{x}, \mathbf{y}\rangle + \langle\mathbf{x}, \mathrm{T}\mathbf{x}\rangle \\
  =& \langle\mathrm{T}\mathbf{x}, \mathbf{y}\rangle + \langle\mathrm{T}\mathbf{x}, \mathbf{y}\rangle \\
  =& 2\langle\mathrm{T}\mathbf{x}, \mathbf{y}\rangle
\end{align*}
$$

and so $\mathrm{T} = 0$.

**(4):** If $\mathrm{T}$ is Hermitian for $\mathbb{F} = \mathbb{C}$ and $\mathrm{T}\mathbf{v} = \lambda\mathbf{v}$, then

$$
  \lambda\mathbf{v} = \mathrm{T}\mathbf{v} = \mathrm{T}^\dagger \mathbf{v} = \bar{\lambda}\mathbf{v}
$$

and so $\lambda = \bar{\lambda}$ is real. 

If $\mathrm{T}$ is symmetric for $\mathbb{F} = \R$, note that a nonreal root of $c_\mathrm{T}(x)$ is not an eigenvalue of $\mathrm{T}$. If $\mathbf{A} = [\mathrm{T}]_O$ for any ordered orthonormal basis $O$ for $V$, then $c_\mathrm{T}(x) = c_\mathbf{A}(x)$. Now, $\mathbf{A}$ is a real symmetric matrix, but can be thought of as a complex Hermitian matrix with real entris. As such, it represents a Hermitian linear operator on the complex space $\mathbb{C}^n$ and so, by what we have just shown, all (complex) roots of its characteristic polynomial are real. However, the characteristic polynomial of $\mathbf{A}$ is the same, whether we think of $\mathbf{A}$ as a real or complex matrix and so the result follows.
</details>
</MathBox>

### Positive definite matrices

<MathBox title='Positive definite matrix' boxType='definition'>
An $n\times n$ matrix $\boldsymbol{A}\in\mathcal{M}_{n}(\mathbb{F})$ is *positive definite* if
1. $\boldsymbol{A}$ is Hermitian or self-adjoint, i.e. $\boldsymbol{A}^* = \boldsymbol{A}$
2. $\langle \boldsymbol{v}, \boldsymbol{A}\boldsymbol{v} \rangle > 0$ for all $\boldsymbol{v}\in \mathbb{F}^n \setminus\Set{\boldsymbol{0}}$

If $\boldsymbol{A}$ is a positive definite matrix, then the map $\langle\cdot, \cdot\rangle: \mathbb{F}\times\mathbb{F}\to\mathbb{F}$ defined by

$$
  \langle\boldsymbol{y}, \boldsymbol{x} \rangle := \langle\boldsymbol{y}, \boldsymbol{A}\boldsymbol{x}\rangle
$$

defines an inner product in $\mathbb{F}^n$.
</MathBox>

<MathBox title="Sylvester's criterion" boxType='criterion'>
A Hermitian matrix is positive definite if and only if all leading principle minors have a positive determinant.

<details>
<summary>Proof</summary>

Suppose $\boldsymbol{M}$ is an $n\times n$ Hermitian matrix and let $\boldsymbol{M}_k$ for $k=\Set{1,\dots,n}$ be the leading principal minors. First assume $\boldsymbol{M}$ is positive definite. If $\boldsymbol{x} = \left[\begin{smallmatrix} x_1 & \cdots & x_k & 0 & \cdots & 0 \end{smallmatrix}\right]^* = \left[\begin{smallmatrix} \boldsymbol{x}_k & \boldsymbol{0} \end{smallmatrix}\right]^*$ we note that $0 < \boldsymbol{x}^* \boldsymbol{M}\boldsymbol{x} = \boldsymbol{x}_k^* \boldsymbol{M}_k \boldsymbol{x}_k$. Equivalently, the eigenvalues of $\boldsymbol{M}_k$ are positive, implying that $\det(\boldsymbol{M}_k) > 0$ since the determinant is the product of the eigenvalues.

The inverse implication can be shown inductively. The general form of an $(n+1)\times(n+1)$ Hermitian matrix is $\boldsymbol{M}_{n+1} = \left[\begin{smallmatrix} \boldsymbol{M}_n & \boldsymbol{v} \\ \boldsymbol{v}^* & d \end{smallmatrix}\right]$ where $\boldsymbol{v}\in\mathbb{F}^n$ and $d\in\mathbb{F}$. Suppose the criterion holds for $\boldsymbol{M}_n$. Assuming that all the leading principle minors of $\boldsymbol{M}$ have positive determinants implies that $\det(\boldsymbol{M}_{n+1}) > 0$, $\det(\boldsymbol{M}) > 0$, and that $\boldsymbol{M}_n$ is positive definite by the inductive hypothesis. Denote $\boldsymbol{x} = \left[\begin{smallmatrix} \boldsymbol{x}_n \\ x_{n+1} \end{smallmatrix}\right]$, then

$$
  \boldsymbol{x}^* M_{n+1} \boldsymbol{x} = \boldsymbol{x}_n^* \boldsymbol{M}_n \boldsymbol{x}_n + x_{n+1}\boldsymbol{x}_n^* \boldsymbol{v} + \bar{x}_{n+1}\boldsymbol{v}^* \boldsymbol{x}_n + d|x_{n+1}|^2
$$

Completing the squares, we get

$$
\begin{align*}
  &(\boldsymbol{x}^* + \boldsymbol{v}^* \boldsymbol{M}_n^{-1} \bar{x}_{n+1})\boldsymbol{M}_n (\boldsymbol{x} + x_{n+1}\boldsymbol{M}_n^{-1}\boldsymbol{v}) \\
  &- |x_{n+1}|^2 \boldsymbol{v}^* \boldsymbol{M}_n^{-1}\boldsymbol{v} + d|x_{n+1}|^2 \\
  =& (\boldsymbol{x} + \boldsymbol{c})^* \boldsymbol{M}_n (\boldsymbol{x} + \boldsymbol{c}) + |x_{n+1}|^2 (d - \boldsymbol{v}^* \boldsymbol{M}_n^{-1}\boldsymbol{v})
\end{align*}
$$

where $\boldsymbol{c} = \boldsymbol{x}_{n+1} M_n^{-1} \boldsymbol{v}$. The first term is positive by the inductive hypothesis. It remains to check the sign of the second terms. Using the block determinant formula $\left|\begin{smallmatrix} \boldsymbol{A} & \boldsymbol{B} \\ \boldsymbol{C} & \boldsymbol{D} \end{smallmatrix}\right| = \det(\boldsymbol{A})\det(\boldsymbol{D} - \boldsymbol{C}\boldsymbol{A}^{-1}\boldsymbol{B})$ on $\boldsymbol{M}_{n+1}$ we obtain

$$
  \det(\boldsymbol{M}_{n+1}) = \det(\boldsymbol{M}_n)\det(d - \boldsymbol{v}^* \boldsymbol{M}_n^{-1}\boldsymbol{v}) > 0
$$

which implies $d - \boldsymbol{v}^* \boldsymbol{M}_n^{-1} \boldsymbol{v} > 0$. Hence, $\boldsymbol{x}^* \boldsymbol{M}\boldsymbol{x} > 0$.
</details>
</MathBox>

<MathBox title='Properties of positive definite matrices' boxType='proposition'>
For a self-adjoint $n\times n$ matrix $\boldsymbol{A}\in\mathcal{M}_{n}(\mathbb{F})$, the following claims are equivalent
1. $\boldsymbol{A}$ is positive definite
2. All eigenvalues of $\boldsymbol{A}$ are positive
3. After Gaussian elimination (without scaling and exchanging rows) only with row operations $\boldsymbol{Z}_{i + \lambda j}$, all pivots in the row echelon are positive.
4. The determinants of leading principal minors of $\boldsymbol{A}$ are positive. **(Sylvester's criterion)**
</MathBox>

## Unitary operators and isometries

<MathBox title='Unitary/orthogonal operator' boxType='definition'>
A linear operator $\mathrm{T}\in\mathcal{L}(V)$ on an $\mathbb{F}$-inner product space $V$ is *unitary* for $\mathbb{F} = \mathbb{C}$ and orthogonal for $\mathbb{F} = \R$ if $\mathrm{T}$ is invertible and 

$$
  \mathrm{T}^\dagger = \mathrm{T}^{-1}
$$

Equivalent, $\mathrm{T}$ is unitary if and only if

$$
  \langle\mathrm{T}\mathbf{v},\mathbf{w}\rangle = \langle\mathbf{v}, \mathrm{T}^{-1}\mathbf{w}\rangle,\; \forall \mathbf{v},\mathbf{w}\in V
$$
</MathBox>

<MathBox title='Properties of unitary operators' boxType='proposition'>
Let $V$ be a finite-dimensional inner product space and let $\mathrm{S},\mathrm{T}\in\mathcal{L}(V)$
1. If $\mathrm{S}$ and $\mathrm{T}$ are unitary/orthogonal, then so are the following:
    - $\alpha\mathrm{T}$ for $\alpha\in\mathbb{C}$ with $|\alpha| = 1$
    - $\mathrm{S}\mathrm{T}$
    - $\mathrm{T}^{-1}$ if $\mathrm{T}$ is invertible
2. $\mathrm{T}$ is unitary/orthogonal if and only if it is an isometric isomorphism
3. $\mathrm{T}$ is unitary/orthogonal if and only if it preserves orthonormal bases
4. If $\mathrm{T}$ is unitary/orthogona, then the eigenvalues $\lambda_i$ of $\mathrm{T}$ satisfy $|\lambda_i| = 1$

<details>
<summary>Proof</summary>

**(2):** A unitary/orthogonal map is injective and since $V$ is finite-dimensional, it is bijective. Moreover, for a bijective linear map $\mathrm{T}$, we have

$$
\begin{align*}
  \mathrm{T} \textrm{ isometric} \iff& \langle\mathrm{T}\mathbf{v},\mathrm{T}\mathbf{w}\rangle = \langle\mathbf{v},\mathbf{w}\rangle,\; \forall \mathbf{v},\mathbf{w}\in V \\
  \iff& \langle\mathbf{v},\mathrm{T}^\dagger \mathrm{T}\mathbf{w}\rangle = \langle\mathbf{v},\mathbf{w}\rangle \\
  \iff& \mathrm{T}^\dagger \mathrm{T} = \mathrm{I} \\
  \iff& \mathrm{T}^\dagger = \mathrm{T}^{-1} \\
  \iff& \mathrm{T} \text{ unitary/orthogonal}
\end{align*}
$$

**(3):** Assume $\mathrm{T}$ is unitary/orthogonal and that $O = \Set{\mathbf{b}_i}_{i=1}^n$ is an orthonormal basis for $V$. Then

$$
  \langle\mathrm{T}\mathbf{b}_i, \mathrm{T}\mathbf{b}_j \rangle = \langle\mathbf{b}_i,\mathbf{b}_j\rangle = \delta_{i,j}
$$

and so $\mathrm{T}O$ is an orthonormal basis for $V$. Conversely, suppose $O$ and $\mathrm{T}O$ are orthonormal bases for $V$. Then

$$
  \langle\mathrm{T}\mathbf{b}_i,\mathrm{T}\mathbf{b}_j \rangle = \delta_{i,j} = \langle\mathbf{b}_i, \mathbf{b}_j\rangle
$$

which implies that $\langle\mathrm{T}\mathbf{v},\mathrm{T}\mathbf{w} = \langle\mathbf{v},\mathbf{w}\rangle$ for all $\mathbf{v},\mathbf{w}\in V$ and so $\mathrm{T}$ is unitary/orthogonal.

**(4):** If $\mathrm{T}$ is unitary and $\mathrm{T}\mathbf{v} = \lambda\mathbf{v}$, then

$$
  \lambda\bar{\lambda}\langle\mathbf{v},\mathbf{v}\rangle = \langle\lambda\mathbf{v},\lambda\mathbf{v}\rangle = \langle\mathrm{T}\mathbf{v},\mathrm{T}\mathbf{v}\rangle = \langle\mathbf{v},\mathbf{v}\rangle
$$

and so $|\lambda|^2 = \lambda\bar{\lambda} = 1 \iff |\lambda| = 1$.
</details>
</MathBox>

<MathBox title='Properties of unitary matrices' boxType='proposition'>
Let $\mathbf{A}$ be an $n\times n$ matrix over $\mathbb{F} = \mathbb{C}$ or $\mathbb{F} = \R$.
1. The following are equivalent:
    - $A$ is unitary/orthogonal
    - The columns of $\mathbf{A}$ form an orthonormal set in $\mathbb{F}^n$
    - The rows of $\mathbf{A}$ form an orthonormal set in $\mathbb{F}^n$
2. If $\mathbf{A}$ is unitary, then $|\det(\mathbf{A})| = 1$. If $\mathbf{A}$ is orthogonal, then $\det(\mathbf{A}) = \pm 1$.

<details>
<summary>Proof</summary>

**(1):** The matrix $\mathbf{A}$ is unitary if and only if $\mathbf{AA}^* = \mathbf{I}$, which is equivalent to the rows of $\mathbf{A}$ being orthonormal. Similarly, $\mathbf{A}$ is unitary if and only if $\mathbf{A}^* \mathbf{A} = \mathbf{I}$, which is equivalent to the columns of $\mathbf{A}$ being orthonormal.

**(2):** We have

$$
  \mathbf{AA}^* = \mathbf{I} \implies \det(\mathbf{A})\det(\mathbf{A}^*) = 1 \implies \det(\mathbf{A})\det(\bar{\mathbf{A}}) = 1
$$

from which it follows that $|\det(\mathbf{A})| = 1$.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
If we are given any two of the following:
1. A unitary/orthogonal $n\times n$ matrix $\mathbf{A}$
2. An ordered orthonormal basis $B$ for $\mathbb{F}^n$
3. An ordered orthonormal basis $C$ for $\mathbb{F}^n$
then the third is uniquely determined by the equation

$$
  \mathbf{A} = \mathbf{M}_{B,C}
$$

<details>
<summary>Proof</summary>

Let $B = \Set{\mathbf{b}_i}_{i=1}^n$ be a basis for $V$. If $C$ is an orthonormal basis for $V$, then

$$
  \langle\mathbf{b}_i, \mathbf{b}_j \rangle = [\mathbf{b}_i]_C \cdot [\mathbf{b}_j]_C
$$

where $[\mathbf{b}_i]_C$ is the $i$th column of $\mathbf{A} = \mathbf{M}_{B,C}$. Hence, $\mathbf{A}$ is unitary if and only if $B$ is orthonormal.
</details>
</MathBox>

### Unitary similarity

<MathBox title='Unitary/orthogonally similarity' boxType='proposition'>
1. Two complex matrices $\mathbf{A}$ and $\mathbf{B}$ are *unitarily similar* if there exists a unitary matrix $\mathbf{U}$ for which
$$
  \mathbf{B} = \mathbf{UAU}^{-1} = \mathbf{UAU}^*
$$
The equivalence classes associated with unitary similarity are called *unitary similarity classes*.
2. Two real matrices $\mathbf{A}$ and $\mathbf{B}$ are *orthogonally similar* if there exists an orthogonal matrix $\mathbf{O}$ for which
$$
  \mathbf{B} = \mathbf{OAO}^{-1} = \mathbf{OAO}^*
$$
The equivalence classes associated with unitary similarity are called *orthogonal similarity classes*.
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V$ be an inner product space of dimension $n$. Then two $n\times n$ matrices $\mathbf{A}$ and $\mathbf{B}$ are unitarily/orthogonally similar if and only if they represent the same linear operator $\mathrm{T}\in\mathcal{L}(V)$ with respect to (possibly different) ordered orthonormal bases. In this case, $\mathbf{A}$ and $\mathbf{B}$ represent exactly the same set of linear operators in $\mathcal{L}(V)$ with respect to ordered orthonormal bases.

<details>
<summary>Proof</summary>

If $\mathbf{A}$ and $\mathbf{B}$ represent $\mathrm{T}\in\mathcal{L}(V)$, i.e. if $\mathbf{A} = [\mathrm{T}]_B$ and $\mathbf{B} = [\mathrm{T}]_C$ for ordered orthonormal bases $B$ and $C$, then

$$
  \mathbf{B} = \mathbf{M}_{B,C} \mathbf{A} \mathbf{M}_{C,B}
$$

and by the previous proposition, $\mathbf{M}_{B,C}$ is unitary/orthogonal.

Conversely, suppose that $\mathbf{A}$ and $\mathbf{B}$ are unitarily/orthogonally similar, say

$$
  \mathbf{B} = \mathbf{UAU}^{-1}
$$

where $\mathbf{U}$ is unitary/orthogonal. Suppose also that $\mathbf{A}$ represents a lienar operator $\mathrm{T}\in\mathcal{L}(V)$ for some ordered orthonormal basis $B$, i.e. $\mathbf{A} = [\mathrm{T}]_B$. The previous proposition implies that there is a unique ordered orthonormal basis $C$ for $V$ for which $\mathbf{U} = \mathbf{M}_{B,C}$. Hence

$$
  \mathbf{B} = \mathbf{M}_{B,C}[\mathrm{T}]_B \mathbf{M}_{B,C}^{-1} = [\mathrm{T}]_C
$$

and so $\mathbf{B}$ also represents $\mathrm{T}$. By symmetry, we see that $\mathbf{A}$ and $\mathbf{B}$ represent the same set of linear opeartors, under all possible ordered orthonormal bases.
</details>
</MathBox>

### Reflections

<MathBox title='Reflection' boxType='proposition'>
Let $V$ be an inner product space. For a nonzero $\mathbf{v}\in V$, the unique operator $\mathrm{H}_\mathbf{v}$ for which

$$
  \mathrm{H}_\mathbf{v} = -\mathbf{v},\; \mathrm{H}_\mathbf{v}\mathbf{w} = \mathbf{w} \; \forall \mathbf{w}\in\langle\mathbf{v}\rangle^\perp
$$

is called a *reflection* or a *Householder transformation*.
</MathBox>

It is easiy to verify that

$$
  \mathrm{H}_\mathbf{v} x = \mathbf{x} - \frac{2\langle\mathbf{x},\mathbf{v}\rangle}{\langle\mathbf{v},\mathbf{v}\rangle}\mathbf{v}
$$

Moreover, $\mathrm{H}_\mathbf{v} \mathbf{x} = -\mathbf{x}$ for $\mathbf{x}\neq\mathbf{0}$ if and only if $\mathbf{x} = \alpha\mathbf{v}$ for some $\alpha\in\mathbb{F}$ and so we can uniquely identify $\mathbf{v}$ by the behaviour of the reflection on $V$.

If $\mathrm{H}_\mathbf{v}$ is a reflection and if we extend $\mathbf{v}$ to an ordered orthonormal basis $B$ for $V$, then $[\mathrm{H}_\mathbf{v}]_B$ is the matrix obtained from the identity matrix by replacing the upper left entry by $-1$

$$
  [\mathrm{H}_\mathbf{v}]_B = \begin{bmatrix} -1 & & & \\ & 1 & & \\ & & \ddots & \\ & & & 1 \end{bmatrix}
$$

Thus, a reflection is both unitary and Hermitian, i.e.

$$
  \mathrm{H}_\mathbf{v}^\dagger = \mathrm{H}_\mathbf{v}^{-1} = \mathrm{H}_\mathbf{v}
$$

<MathBox title='' boxType='proposition'>
Let $\mathbf{v},\mathbf{w}\in V$ be distinct nonzero vector of equal length in an inner product space $V$. Then $\mathrm{H}_{\mathbf{v} - \mathbf{w}}$ is the unique reflection sending $\mathbf{v}$ to $\mathbf{w}$ and $\mathbf{w}$ to $\mathbf{v}$.

<details>
<summary>Proof</summary>

If $\Vert\mathbf{v}\rVert = \norm{\mathbf{w}}$, then $(\mathbf{v} - \mathbf{w}) \perp (\mathbf{v} + \mathbf{w})$ and so

$$
\begin{align*}
  \mathrm{H}_{\mathbf{v} - \mathbf{w}} (\mathbf{v} - \mathbf{w}) =& \mathbf{w} - \mathbf{v} \\
  \mathrm{H}_{\mathbf{v} - \mathbf{w}}(\mathbf{v} + \mathbf{w}) = \mathbf{v} + \mathbf{w}
\end{align*}
$$

From which it follows that $\mathrm{H}_{\mathbf{v} -\mathbf{w}} = \mathbf{w}$ and $\mathrm{H}_{\mathbf{v}-\mathbf{w}}(\mathbf{w}) = \mathbf{v}$. As to uniqueness, suppose $\mathrm{H}_\mathbf{x}$ is a reflection for which $\mathrm{H}_\mathbf{x} (\mathbf{v}) = \mathbf{w}$. Since $\mathrm{H}_\mathbf{x}^{-1} = \mathrm{H}_\mathbf{v}$, we have $\mathrm{H}_\mathbf{x}(\mathbf{w}) = \mathbf{v}$ and so

$$
  \mathrm{H}_\mathbf{x} (\mathbf{v} - \mathbf{w}) = -(\mathbf{v} - \mathbf{w})
$$

which implies that $\mathrm{H}_\mathbf{x} = \mathrm{H}_{\mathbf{v} - \mathbf{w}}$.
</details>
</MathBox>

<MathBox title='Characterization of unitary operators by reflections' boxType='proposition'>
Let $V$ be a finite-dimensional inner product space. The following are equivalent for an operator $\mathrm{T}\in\mathcal{L}(V)$
1. $\mathrm{T}$ is unitary/orthogonal
2. $\mathrm{T}$ is a product of reflections

<details>
<summary>Proof</summary>

Since reflections are unitary/orthogonal and the product of unitary/orthogonal operators is unitary, it follows that **(2)** implies **(1)**. The converse can be shown by induction. Let $\mathrm{T}$ be unitary and let $B = (\mathbf{b}_i)_{i=1}^n$ be ordered orthonormal basis for $V$, then in the base case $n = 1$ we have

$$
  \mathrm{H}_{\mathrm{T}\mathbf{b}_1 - \mathbf{b}_1} (\mathrm{T}\mathbf{b}_1) = \mathbf{b}_1
$$

and so if $\mathbf{x}_1 = \mathrm{T}\mathbf{b}_1 - \mathbf{b}_1$, then

$$
  (\mathrm{H}_{\mathbf{x}_1} \mathrm{T}) = \mathbf{b}_1 = \mathbf{b}_1
$$

That is, $\mathrm{T}_1 := \mathrm{H}_{\mathbf{x}_1}\mathrm{T}$ is the identity on $\langle\mathbf{b}_1\rangle$. For the inductive hypothesis $n = k-1$, suppose we have found reflections $\mathrm{H}_{\mathbf{x}_{k-1}},\dots,\mathrm{H}_{\mathbf{x}_1}$ for which $\mathrm{T}_{k-1} := \left(\prod_{i=1}^{k-1} \mathrm{H}_{\mathbf{x}_i} \right)\mathrm{T}$ is the identity on $\langle\mathbf{b}_1,\dots,\mathbf{b}_k \rangle$, then

$$
  \mathrm{H}_{\mathrm{T}_{k-1}\mathbf{b}_k - \mathbf{b}_k} (\mathrm{T}_{k-1} \mathbf{b}_k) = \mathbf{b}_k
$$

Moreover, we claim that $(\mathrm{T}_{k-1} \mathbf{b}_k - \mathbf{b}_k) \perp \mathbf{b}_i$ for $i < k$, since

$$
\begin{align*}
  \langle \mathrm{T}_{k-1}\mathbf{b}_k - \mathbf{b}_k, \mathbf{u}_i \rangle =& \langle (\mathbf{H}_{\mathbf{x}_{k-1}}\cdots\mathrm{H}_{\mathbf{x}_1}\mathrm{T})\mathbf{b}_k, \mathbf{b}_i \rangle \\
  =& \langle \mathrm{T}\mathbf{b}_k, \mathbf{H}_{\mathbf{x}_{k-1}}\cdots\mathrm{H}_{\mathbf{x}_1}\mathrm{T} \mathbf{b}_i \rangle \\
  =& \langle \mathrm{T}\mathbf{b}_k, \mathrm{T}\mathbf{b}_i \rangle \\
  =& \langle \mathbf{b}_k, \mathbf{b}_i \rangle \\
  =& 0
\end{align*}
$$

Hence, if $\mathbf{x}_k = \mathrm{T}_{k-1}\mathbf{b}_k - \mathbf{b}_k$, then

$$
  (\mathbf{H}_{\mathbf{x}_k} \cdots\mathrm{H}_{\mathbf{x}_1}\mathrm{T})\mathbf{b}_i = \mathrm{H}_{\mathbf{x}_k}\mathbf{b}_i = \mathbf{b}_i
$$

showing that $\mathrm{T}_k := \left(\prod_{i=1}^k \mathrm{H}_{\mathbf{x}_i} \right)\mathrm{T}$ is the identity on $\langle\mathbf{b}_1,\dots,\mathbf{b}_k \rangle$. Thus, for $k = n$ we have $\left(\prod_{i=1}^k \mathrm{H}_{\mathbf{x}_i} \right)\mathrm{T} = \mathrm{I}$ and so $\mathrm{T} = \prod_{i=1}^{k-1} \mathrm{H}_{\mathbf{x}_i}$.
</details>
</MathBox>

## The structure of normal operators

<MathBox title='The structure theorem for normal operators: complex case' boxType='theorem'>
Let $V$ be a finite dimensional complex inner product space.
1. The following are equivalent for $\mathrm{T}\in\mathcal{L}(V)$:
    - $\mathrm{T}$ is normal
    - $\mathrm{T}$ is unitarily diagonalizable
    - $\mathrm{T}$ has an orthogonal spectral resolution $\mathrm{T} = \sum_{i=1}^k \lambda_i \mathrm{P}_i$
2. Among the normal operators, the Hermitian operators are precisely those for which all complex eigenvalues are real.
3. Among the normal operators, the unitary operators are precisely those for which all complex eigenvalues have norm $1$

<details>
<summary>Proof</summary>

**(2):** It is only necessary to look at a diagonal matrix $\mathbf{A}$ representing $\mathrm{T}$. This matrix has the eigenvalue of $\mathrm{T}$ on its main diagonal and it is Hermitian if and only if the eigenvalues of $\mathrm{T}$ are real. Similarly, $\mathbf{A}$ is unitary if and only if the eigenvalues of $\mathrm{T}$ have norm $1$.
</details>
</MathBox>

<MathBox title='The structure theorem for normal operators: real case' boxType='theorem'>
Let $V$ be a finite dimensional real inner product space.
1. $\mathrm{T}\in\mathcal{L}(V)$ is normal if and only if $V = \left(\bigodot_{i=1}^k E_{\lambda_i} \right)\odot\left(\bigodot_{j=1}^m W_j \right)$, where $\Set{\lambda_i}_{i=1}^k$ is the spectrum of $\mathrm{T}$ and each $W_j$ is a two-dimensional indecomposable $\mathrm{T}$-invariant subspace with an ordered basis $B_j$, for which
$$
  [\mathrm{T}]_{B_j} = \begin{bmatrix} a_j & -b_j \\ b_j & a_j \end{bmatrix}
$$
2. Among the real normal operators, the symmetric operators are those for which there are no subspaces $W_j$ in the decomposition in **(1)**. Hence the following are equivalent for $\mathrm{T}\in\mathcal{L}(V)$:
    - $\mathrm{T}$ is symmetric
    - $\mathrm{T}$ is orthogonally diagonalizable
    - $\mathrm{T}$ has the orthogonal spectral resolution $\mathrm{T} = \sum_{i=1}^k \lambda_i \mathrm{P}_i$
3. Among the real normal operators, the orthogonal operators are precisely those for which the eigenvalues are equal to $\pm 1$ and the matrices $[\mathrm{T}]_B$ described in **(1)** have rows (and columns) of norm $1$, i.e. for some $\theta\in\R$
$$
  [\mathrm{T}]_{B_i} = \begin{bmatrix} \sin\theta & -\cos\theta \\ \cos\theta & \sin\theta \end{bmatrix}
$$

<details>
<summary>Proof</summary>

**(1)** and **(2)** follow by looking at the matrix $\mathbf{A} = [\mathrm{T}]_B$ where $B = \bigcup_j B_j$. This matrix is symmetric if and only if $\mathbf{A}$ is diagonal and $\mathbf{A}$ is orthogonal if and only if $\lambda_i = \pm 1$ and the matrices $[\mathrm{T}]_{B_j}$ have orthonormal rows. 
</details>
</MathBox>

<MathBox title='The structure theorem for normal matrices: complex case' boxType='theorem'>
Let $\mathbf{A}\in M_n (\mathbb{C})$ be a complex $n\times n$ matrix
1. $\mathbf{A}$ is normal if and only if it is unitarily diagonalizable, i.e. if and only if there is a unitary matrix $\mathbf{U}$ for which
$$
  \mathbf{UAU}^\dagger = \operatorname{diag}(\lambda_1,\dots,\lambda_k)
$$
2. $\mathbf{A}$ is Hermitian if and only if **(1)** holds, where all eigenvalues $\lambda_i \in\R$ are real
3. $\mathbf{A}$ is unitary if and only if **(1)** holds, where all eigenvalues $\lambda_i$ have norm $1$, i.e. $|\lambda_i| = 1$
</MathBox>

<MathBox title='The structure theorem for normal matrices: real case' boxType='theorem'>
Let $\mathbf{A}\in M_n (\R)$ be a real $n\times n$ matrix
1. $\mathbf{A}$ is normal if and only if it is unitarily diagonalizable, i.e. if and only if there is an orthogonal matrix $\mathbf{O}$ for which
$$
  \mathbf{OAO}^\top = \operatorname{diag}\left(\lambda_1,\dots,\lambda_k, \left[\begin{smallmatrix} a_1 & -b_1 \\ b_1 & a_1 \end{smallmatrix}\right],\dots,\left[\begin{smallmatrix} a_m & -b_m \\ b_m & a_m \end{smallmatrix}\right] \right)
$$
2. $\mathbf{A}$ is symmetric if and only if it is orthogonally diagonalizable, i.e. if and only if there is an orthogonal matrix $\mathbf{O}$ for which
$$
  \mathbf{OAO}^\top = \operatorname{diag}(\lambda_1,\dots,\lambda_k)
$$
3. $\mathbf{A}$ is orthogonal if and only if there is an orthogonal matrix $\mathbf{O}$ for which
$$
  \mathbf{OAO}^\top = \operatorname{diag}\left(\lambda_1,\dots,\lambda_k, \left[\begin{smallmatrix} \sin\theta_1 & -\cos\theta_1 \\ \cos\theta_1 & \sin\theta_1 \end{smallmatrix}\right],\dots,\left[\begin{smallmatrix} \sin\theta_m & -\cos\theta_m \\ \cos\theta_m & \sin\theta_m \end{smallmatrix}\right] \right)
$$
for some $\theta_1,\dots,\theta_m \in\R$.
</MathBox>

## Functional calculus

Let $\mathrm{T}$ be a normal operator on a finite-dimensional inner product space $V$ and let $\mathrm{T}$ have spectral resolution $\mathrm{T} = \sum_{i=1}^k \lambda_i \mathrm{P}_i$. Since each $\mathrm{P}_i$ is idempotent, we have $\mathrm{P}_i^m = \mathrm{P}_i$ for all $m\geq 1$. The pairwise orthogonality of the projections implies that

$$
  \mathrm{T}^n = \left(\sum_{i=1}^k \right)^n = \sum_{i=1}^k \lambda_i^n \mathrm{P}_i
$$

More generally, for any polynomial $p(x)$ over $\mathbb{F}$

$$
  \mathrm{p}(\mathrm{T}) = \sum_{i=1}^k p(\lambda_i)\mathrm{P}_i
$$

Note that a polynomial of degree $k-1$ is uniquely determined by specifying an arbitrary set of $k$ of its values at the distinct points $\alpha_1,\dots,\alpha_k$. This follows from the *Lagrange interpolation formula*

$$
  p(x) = \sum_{i=0}^{k-1} p(\alpha_i) \left[\prod_{j\neq i} \frac{x - \alpha_j}{\alpha_i - \alpha_j} \right]
$$

Therefore, we can define a unique polynomial $p(x)$ by specifying the values $p(\lambda_i)$ for $i = 1,\dots,k$. For example, for a given $1\leq j \leq k$, if $p_j (x)$ is a polynomial for which $p_j (\lambda_i) = \delta_{i,j}$ for $i=1,\dots,k$, then

$$
  p_j (\mathrm{T}) = \mathrm{P}_j
$$

and so each projection $\mathrm{P}_j$ is a polynomial function of $\mathrm{T}$. As another example, if $\mathrm{T}$ is invertible and $p(\lambda_i) = \lambda_i^{-1}$ then

$$
  p(\mathrm{T}) = \sum_{i=1}^k \lambda_i^{-1}\mathrm{P}_i = \mathrm{T}^{-1}
$$

as can easily be verified by direct calculation. Finally, if $p(\lambda_i) = \bar{\lambda_i}$, then since each $\mathrm{P}_i$ is self-adjoint, we have

$$
  p(\mathrm{T}) = \sum_{i=1}^k \bar{\lambda_i}\mathbf{P}_i = \mathrm{T}^\dagger
$$

and so $\mathrm{T}^\dagger$ is a polynomial in $\mathrm{T}$.

We can extend this idea further by defining, for any function $f:\Set{\lambda_i}_{i=1}^k \to\mathbb{F}$, then linear operator $f(\mathrm{T})$ by

$$
  f(\mathrm{T}) = \sum_{i=1}^k f(\lambda_i)\mathrm{P}_i
$$

For example, we may define $\sqrt{\mathrm{T}}$, $\mathrm{T}^{-1}$, $e^\mathrm{T}$ and so on. Notice, however, that since the spectral resolution of $\mathrm{T}$ is a finite sum, we gain nothing (but convenience) by using functions other than polynomials, for we can always find a polynomial $p(x)$ for which $p(\lambda_i) = f(\lambda_i)$ for $i = 1,\dots,k$ and so $f(\mathrm{T}) = p(\mathrm{T})$. The study of the properties of functions on an operator $\mathrm{T}$ is referred to as the *functional calculus* of $\mathrm{T}$.

According to the spectral theorem, if $V$ is a complex inner product space and $\mathrm{T}$ is normal, then $f(\mathrm{T})$ is a normal operator whose eigenvalues are $f(\lambda_i)$. Similarly, if $V$ is real and $\mathrm{T}$ is symmetric, then $f(\mathrm{T})$ is symmetric, with eigenvalues $f(\lambda_i)$.

### Commutativity
 
<MathBox title='' boxType='proposition'>
Let $V$ be a finite-dimensional complex inner product space. For $\mathrm{S},\mathrm{T}\in\mathcal{L}(V)$, we write $\mathrm{T}\leftrightarrow\mathrm{S}$ to denote the fact that $\mathrm{T}$ and $\mathrm{S}$ commute. Let $\mathrm{T}$ and $\mathrm{S}$ have spectral resolutions

$$
\begin{align*}
  \mathrm{T} =& \sum_{i=1}^k \lambda_i \mathrm{P}_i \\
  \mathrm{S} =& \sum_{j=1}^m \mu_j \mathrm{Q}_j
\end{align*}
$$

Then
1. For any $\mathrm{U}\in\mathcal{L}(V)$,
$$
  \mathrm{U}\leftrightarrow\mathrm{T} \iff \mathrm{U}\leftrightarrow\mathrm{P}_i,\; \forall i
$$
2. $\mathrm{T}\leftrightarrow\mathrm{S} \iff \mathrm{P}_i\leftrightarrow\mathrm{Q}_j,\; \forall i,j$
3. If $f:\Set{\lambda_i}_{i=1}^k \to\mathbb{F}$ and $g:\Set{\mu_j}_{j=1}^m \to\mathbb{F}$ are injective functions, then
$$
  f(\mathrm{T})\leftrightarrow g(\mathrm{S}) \iff \mathrm{T}\leftrightarrow\mathrm{S}
$$

<details>
<summary>Proof</summary>

**(1):** If $\mathrm{U}\leftrightarrow\mathrm{P}_i$ for all $i$, then $\mathrm{U}\leftrightarrow\mathrm{T}$ and the converse follows from the fact that $\mathrm{P}_i$ is a polynomial in $\mathrm{T}$.

**(2):** This is shown similarly to **(1)**.

**(3):** Note that $\mathrm{T}\leftrightarrow\mathrm{S}$ clearly implies $f(\mathrm{T})\leftrightarrow g(\mathrm{T})$. Conversely, let $\Lambda = \Set{\lambda_i}_{i=1}^k$. Since $f$ is injective, the inverse function $f^{-1}: f(\Lambda)\to\Lambda$ is well-defined and $f^{-1}(f(\mathrm{T})) = \mathrm{T}$. Thus, $\mathrm{T}$ is a function of $f(\mathrm{T})$. Similarly, $\mathrm{S}$ is a function of $g(\mathrm{S})$. It follows that $f(\mathrm{T})\leftrightarrow g(\mathrm{S})$ implies $\mathrm{T}\leftrightarrow\mathrm{S}$.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $\mathrm{T}$ and $\mathrm{S}$ be normal operators on a finite-dimensional complex inner product space $V$. Then $\mathrm{T}$ and $\mathrm{S}$ commute if and only if they have the form

$$
\begin{align*}
  \mathrm{T} =& p(r(\mathrm{T},\mathrm{S})) \\
  \mathrm{S} =& q(r(\mathrm{T},\mathrm{S}))
\end{align*}
$$

where $p(x)$, $q(x)$ and $r(x)$ are polynomials.

<details>
<summary>Proof</summary>

If $\mathrm{T}$ and $\mathrm{S}$ are polynomials in $\mathrm{U} = r(\mathrm{T},\mathrm{S})$, then they clearly commute. Conversely, suppose $\mathrm{TS} = \mathrm{ST}$ and let 

$$
\begin{align*}
  \mathrm{T} =& \sum_{i=1}^k \lambda_i \mathrm{P}_i \\
  \mathrm{S} =& \sum_{j=1}^m \mu_j \mathrm{Q}_j 
\end{align*}
$$

be the orthogonal spectral resolutions for $\mathrm{T}$ and $\mathrm{S}$. The previous proposition implies that $\mathrm{P}_i \mathrm{Q}_j = \mathrm{Q}_j \mathrm{P}_i$. Hence

$$
\begin{align*}
  \mathrm{T}^r \mathrm{S}^s =& \left(\sum_{i=1}^k \lambda_i \mathbf{P}_i \right)^r \left(\sum_{j=1}^m \mu_j \mathrm{Q}_j \right)^s \\
  =& \left(\sum_{i=1}^k \lambda_i^r \mathbf{P}_i \right) \left(\sum_{j=1}^m \mu_j^s \mathrm{Q}_j \right) \\
  =& \sum_{i,j} \lambda_i^r \mu_j^s \mathrm{P}_i \mathrm{Q}_j
\end{align*}
$$

It follows that for any polynomial $r(x,y)$ in two variables that

$$
  r(\mathrm{T},\mathrm{S}) = \sum_{i,j} r(\lambda_i, \mu_j) \mathrm{P}_i \mathrm{Q}_i
$$

If we choose $r(x,y)$ with the property that $\alpha_{i,j} = r(\lambda_i, \mu_j)$ are distinct, then

$$
  r(\mathrm{T},\mathrm{S}) = \sum_{i,j} \alpha_{i,j}\mathrm{P}_i \mathrm{Q}_j
$$

and we can also choose $p(x)$ and $q(x)$ so that $p(\alpha_{i,j}) = \lambda_i$ for all $j$ and $q(\alpha_{i,j}) = \mu_j$ for all $i$. Then

$$
\begin{align*}
  p(r(\mathrm{T},\mathrm{S})) =& \sum_{i,j} p(\alpha_{i,j})\mathrm{P}_i \mathrm{Q}_j = \sum_{i,j} \lambda_i \mathrm{P}_i \mathrm{Q}_j \\
  =& \left(\sum_i \lambda_i \mathrm{P}_i \right)\left(\sum_j \mathrm{Q}_j \right) \\
  =& \sum_i \lambda_i \mathrm{P}_i
  = \mathrm{T}
\end{align*}
$$

and similarly, $q(r(\mathrm{T},\mathrm{S})) = \mathrm{S}$.
</details>
</MathBox>

## Positive operators

<MathBox title='Positive and positive definite operators' boxType='definition'>
A self-adjoint linear operator $\mathrm{T}\in\mathcal{V}$ on a finite-dimensional inner product space $V$ is
- *positive* if the quadratic form $q_\mathrm{T}(\mathbf{v}) = \langle\mathrm{T}\mathbf{v},\mathbf{v}\rangle \geq 0$ for all $\mathbf{v}\in V$
- *positive definite* if $q_\mathrm{T}(\mathbf{v}) > 0$ for all $\mathbf{v} \neq 0$
</MathBox>

<MathBox title='Characterization of positive and positive definite operators' boxType='proposition'>
A self-adjoint operator $\mathrm{T}\in\mathcal{L}(V)$ on a finite-dimensional inner product space $V$ is
1. positive if and only if all of its eigenvalues are nonnegative
2. positive definite if and only if all of its eigenvalues are positive

<details>
<summary>Proof</summary>

**(1):** If $q_\mathbf{v}(\mathbf{v}) \geq 0$ and $\mathrm{T}\mathbf{v} = \lambda\mathbf{v}$, then

$$
  0 \leq \langle\mathrm{T}\mathbf{v},\mathbf{v}\rangle = \lambda\langle\mathbf{v},\mathbf{v}\rangle
$$

and so $\lambda > 0$. Conversely, if all eigenvalues of $\mathrm{T}$ are nonnegative, then

$$
  \mathrm{T} = \sum_{i=1}^k \lambda_i \mathrm{P}_i,\; \lambda_i \geq 0
$$

and since $\mathrm{I} = \sum_{i=1}^k \mathrm{P}_i$, then

$$
  \langle\mathrm{T}\mathbf{v},\mathbf{v}\rangle = \sum_{i,j} \lambda_i \langle\mathrm{P}_i\mathbf{v},\mathrm{P}_j \mathbf{v} \rangle = \sum_i \lambda_i \norm{ \mathrm{P}_i \mathbf{v} }^2 \geq 0
$$

and so $\mathrm{T}$ is positive. **(2)** is proved similarly.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $\mathrm{T}\in\mathcal{L}(V)$ be a self-adjoint linear operator in a finite-dimensional inner product space $V$.
1. $\mathrm{T}$ is positive if and only if it has a positive square root
2. $\mathrm{T}$ is positive if and only if it has the form $\mathrm{T} = \mathrm{S}^\dagger \mathrm{S}$ for some $\mathrm{S}\in\mathcal{L}(V)$

<details>
<summary>Proof</summary>

**(1):** If $\mathrm{T}$ is a positive operator, with spectral resolution 

$$
  \mathrm{T} = \sum_{i=1}^k \lambda_i \mathrm{P}_i,\; \lambda_i \geq 0
$$

then the positive square root of $\mathrm{T}$ is $\sqrt{\mathrm{T}} = \sum_{i=1}^k \sqrt{\lambda_i} \mathrm{P}_i$. It is clear that $\sqrt{\mathrm{T}}$ is the only positive operator whose square is $\mathrm{T}$. Conversely, if $\mathrm{T}$ has a positive square root, i.e. if $\mathrm{T} = \mathrm{S}^2$ for some positive operator $\mathrm{S}$, then $\mathrm{T}$ is positive.

**(2):** If $\mathrm{T}$ is positive, then $\sqrt{\mathrm{T}}$ is self-adjoint and so

$$
  (\sqrt{\mathrm{T}})^\dagger \sqrt{\mathrm{T}} = \mathrm{T}
$$

Conversely, if $\mathrm{T} = \mathrm{S}^\dagger \mathrm{S}$ for some operator $\mathrm{S}$, then $\mathrm{T}$ is positive, since it is clearly self-adjoint and

$$
  \langle\mathrm{T}\mathbf{v},\mathbf{v}\rangle = \langle\mathrm{S}^\dagger \mathrm{S}\mathbf{v},\mathbf{v}\rangle = \langle\mathrm{S}\mathbf{v},\mathrm{S}\mathbf{v}\rangle \geq 0
$$
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
If $\mathrm{T}$ and $\mathrm{S}$ are positive operators and $\mathrm{TS} = \mathrm{ST}$, then $\mathrm{TS}$ is positive.

<details>
<summary>Proof</summary>

Since $\mathrm{T}$ is a positive operator, it has a positive square root $\sqrt{\mathrm{T}}$, which is a polynomial in $\mathrm{T}$. A similar statement holds for $\mathrm{S}$. Thus, since $\mathrm{T}$ and $\mathrm{S}$ commute, so do $\sqrt{\mathrm{T}}$ and $\sqrt{\mathrm{S}}$. Hence,

$$
  (\sqrt{\mathrm{T}}\sqrt{\mathrm{S}})^2 = (\sqrt{\mathrm{T}})^2 (\sqrt{\mathrm{S}})^2 = \mathrm{TS}
$$

Since $\sqrt{\mathrm{T}}$ and $\sqrt{\mathrm{S}}$ are self-adjoint and commute, their product is self-adjoint and so $\mathrm{TS}$ is positive.
</details>
</MathBox>

# The polar decomposition of an operator

<MathBox title='' boxType='proposition'>
Let $\mathrm{T}$ be a nonzero linear operator on a finite-dimensional complex inner product space $V$.
1. There exists a positive operator $\mathrm{P}$ and a unitary operator $\mathrm{Q}$ for which $\mathrm{T} = \mathrm{QP}$. Moreover, $\mathrm{P}$ is unique and if $\mathrm{T}$ is invertible, then $\mathrm{Q}$ is also unique.
2. Similarly, there exists a positive operator $\mathrm{S}$ and a unitary operator $\mathrm{U}$ for which $\mathrm{T} = \mathrm{SU}$. Moreover, $\mathrm{S}$ is unique and if $\mathrm{T}$ is invertible, then $\mathrm{U}$ is also unique.

<details>
<summary>Proof</summary>

**(1):** Suppose $\mathrm{T} = \mathrm{QP}$. Then

$$
  \mathrm{T}^\dagger = (\mathrm{QP})^\dagger = \mathrm{P}^\dagger \mathrm{Q}^\dagger = \mathrm{PQ}^{-1}
$$

and so

$$
  \mathrm{T}^\dagger \mathrm{T} = \mathrm{PQ}^{-1}\mathrm{QP} = \mathrm{P}^2
$$

Also, if $\mathbf{v}\in V$, then

$$
  \mathrm{T}\mathbf{v} = \mathrm{Q}(\mathrm{P}\mathbf{v})
$$

These equations give us an idea of how to define $\mathrm{P}$ and $\mathrm{Q}$. Let us define $\mathrm{P}$ to be the unique positive square root of the positive operator $\mathrm{T}^\dagger \mathrm{T}$, then

$$
  \norm{\mathrm{p}\mathbf{v}}^2 = \langle\mathrm{P}\mathbf{v},\mathrm{P}\mathbf{v}\rangle = \langle\mathrm{P}^2 \mathbf{v},\mathbf{v}\rangle = \langle\mathrm{T}^\dagger \mathrm{T}\mathbf{v},\mathbf{v}\rangle = \norm{\mathrm{T}\mathbf{v}}^2
$$

Define $\mathrm{Q}$ on $\operatorname{ran}(\mathrm{P})$ by $\mathrm{Q}(\mathrm{P}\mathbf{v}) = \mathrm{T}\mathbf{v}$ for all $\mathbf{v}\in V$. The equation above shows that $\mathrm{P}\mathbf{x} = \mathrm{P}\mathbf{y}$ implies that $\mathrm{T}\mathbf{x} = \mathrm{T}\mathbf{v}$ and so this definition of $\mathrm{Q}$ on $\operatorname{ran}(\mathrm{P})$ is well-defined.

Moreover, $\mathrm{Q}$ is an isometry on $\operatorname{ran}(\mathrm{P})$, since the equation above gives

$$
  \norm{ \mathrm{Q}(\mathrm{P}\mathbf{v}) } = \norm{ \mathrm{T}\mathbf{v} } = \norm{\mathrm{P}\mathbf{v}}
$$

Thus, if $B = \Set{b_i}_{i=1}^k$ is an orthonormal basis for $\operatorname{ran}(\mathrm{P})$, then $\mathrm{Q}B = \Set{\mathrm{Q}\mathbf{b}_i}_{i=1}^k$ is an orthonormal basis for $\mathrm{Q}(\operatorname{ran}(\mathrm{P})) = \operatorname{ran}(\mathrm{T})$. Finally, we may extend both orthonormal bases to orthonormal bases for $V$ and then extend the definition of $\mathrm{Q}$ to an isometry on $V$ for which $\mathrm{T} = \mathrm{QP}$.

As for the uniqueness, we have seen that $\mathrm{P}$ must satisfy $\mathrm{P}^2 = \mathrm{T}^\dagger \mathrm{T}$ and since $\mathrm{P}^2$ has a unique positive square root, we deduce that $\mathrm{P}$ is uniquely defined. Finally, if $\mathrm{T}$ is invertible, then so is $\mathrm{P}$ since $\ker(\mathrm{P})\subseteq\ker(\mathrm{T})$. Hence, $\mathrm{Q} = \mathrm{TP}^{-1}$ is uniquely determined by $\mathrm{T}$.

**(2):** This can be proved by the applying the previous proposition to the map $\mathrm{T}^\dagger$, to get

$$
  \mathrm{T} = (\mathrm{T}^\dagger)^\dagger = (\mathrm{QP})^\dagger = \mathrm{PQ}^{-1} = \mathrm{PU}
$$

where $\mathrm{U}$ is unitary.
</details>
</MathBox>

<MathBox title='Polar decomposition' boxType='corollary'>
Let $\mathrm{T}$ be a nonzero linear operator on a finite-dimensional complex inner product space. Then there is a positive operator $\mathrm{P}$ and a self-adjoint operator $\mathrm{S}$ for which $\mathrm{T}$ has the *polar decomposition*

$$
  \mathrm{T} = \mathrm{P}e^{i\mathrm{S}}
$$

Moreover, $\mathrm{P}$ is unique and if $\mathrm{T}$ is invertible, then $\mathrm{S}$ is also unique.
</MathBox>

<MathBox title='' boxType='proposition'>
Let $\mathrm{T} = \mathrm{P}e^{i\mathrm{S}}$ be a polar decomposition of a nonzero linear operator $\mathrm{T}$. Then $\mathrm{T}$ is normal if and only if $\mathrm{PS} = \mathrm{SP}$.

<details>
<summary>Proof</summary>

Since

$$
  \mathrm{TT}^\dagger = \mathrm{P}e^{i\mathrm{S}} e^{-i\mathrm{S}} \mathrm{P} = \mathrm{P}^2
$$

and

$$
  \mathrm{T}^\dagger \mathrm{T} = e^{-i\mathrm{S}}\mathrm{PP}e^{i\mathrm{S}} = e^{-i\mathrm{S}}\mathrm{P}^2 e^{i\mathrm{S}}
$$

we see that $\mathrm{T}$ is noraml if and only if

$$
  e^{-i\mathrm{S}}\mathrm{P}^2 e^{i\mathrm{S}} = \mathrm{P}^2
$$

or equivalently,

$$
  \mathrm{P}^2 e^{i\mathrm{S}} = e^{i\mathrm{S}}\mathrm{P}^2
$$

Since $\mathrm{P}$ is a polynomial in $\mathrm{P}^2$ and $\mathrm{S}$ is a polynomial in $e^{i\mathrm{S}}$, this holds if and only if $\mathrm{PS} = \mathrm{PS}$.
</details>
</MathBox>

# Singular values

<MathBox title='' boxType='proposition'>
Let $U$ and $V$ be finite-dimensional inner product spaces over $\mathbb{F}$, and let $\mathrm{T}\in\mathcal{L}(U,V)$ have rank $r$. Then there are ordered orthonormal bases $B$ and $C$ for $U$ and $V$, respectively, for which

$$
\begin{align*}
  B =& (\underbrace{\mathbf{u}_1,\dots,\mathbf{u}_r}_{\text{ONB for }\operatorname{ran}(\mathrm{T}^*)},\underbrace{\mathbf{u}_r,\dots,\mathbf{u}_n}_{\text{ONB for}\ker(\mathrm{T})}) \\
  C =& (\underbrace{\mathbf{v}_1,\dots,\mathbf{v}_r}_{\text{ONB for }\operatorname{ran}(\mathrm{T})},\underbrace{\mathbf{v}_r,\dots,\mathbf{v}_m}_{\text{ONB for}\ker(\mathrm{T}^*)})
\end{align*}
$$

Moreover, for $1\leq i\leq r$

$$
\begin{align*}
  \mathrm{T}\mathbf{u}_i =& s_i \mathbf{v} \\
  \mathrm{T}\mathbf{v}_i =& s_i \mathbf{u}
\end{align*}
$$

where $s_i > 0$ are called the *singular values* of $\mathrm{T}$ defined by

$$
  \mathrm{T}^* \mathrm{T} \mathbf{u}_i = s_i^2 \mathbf{u}_i,\; s_i > 0,\; i \leq r
$$

The vectors $\mathbf{u}_1,\dots,\mathbf{u}_r$ are called the *right singular vectors* for $\mathrm{T}$ and the vectors $\mathbf{v}_1,\dots,\mathbf{v}_r$ are called the *left-singular vectors* for $\mathrm{T}$.

<details>
<summary>Proof</summary>

Note that $\mathrm{T}^* \mathrm{T} \in\mathcal{L}(U)$ is a positive Hermitian operator. Hence, if $r = \operatorname{rank}(\mathrm{T})) = \operatorname{rank}(\mathrm{T}^* \mathrm{T})$, then $U$ has an ordered orthonormal basis

$$
  B = (\mathbf{u}_1,\dots,\mathbf{u}_r,\mathbf{u}_{r+1},\dots,\mathbf{u}_n)
$$

of eigenvectors for $\mathrm{T}^* \mathrm{T}$, where the corresponding eigenvalues can be arranged so that

$$
  \lambda_1 \geq\cdots\geq\lambda_r > 0 = \lambda_{r+1} = \cdot = \lambda_n
$$

The set $(\mathbf{u}_{r+1},\cdots,\mathbf{u}_n)$ is an ordered orthonormal basis for $\ker(\mathrm{T}^* \mathrm{T}) = \ker(\mathrm{T})$, and so $(\mathbf{u}_1,\dots,\mathbf{u}_r)$ is an ordered orthonormal basis for $\ker(\mathrm{T})^\perp = \operatorname{ran}(T^*)$. Definining

$$
  s_i = \begin{cases}
    \sqrt{\lambda_i},\quad& 1 \leq i \leq r \\
    0,\quad& i > r
  \end{cases}
$$

we get 

$$
  \mathrm{T}^* T\mathbf{u}_i = s_i^2 \mathbf{u}_i,\; i=1,\dots,n
$$

Setting $\mathbf{v}_i = \frac{1}{s_i}\mathrm{T}\mathbf{u}_i$ for each $i\leq r$, we get

$$
  \mathrm{T}\mathbf{u}_i = \begin{cases}
    s_i \mathbf{v}_i,\quad& i\leq r \\
    0,\quad& i > r
  \end{cases}
$$

and

$$
  \mathrm{T}^* \mathbf{v}_i = \begin{cases}
    s_i \mathbf{u}_i,\quad& i\leq r \\
    0,\quad& i > r
  \end{cases}
$$

The vectors $\mathbf{v}_1,\dots,\mathbf{v}_r$ are orthonormal, since if $i,j\leq r$, then

$$
  \langle\mathbf{v}_i,\mathbf{v}_j\rangle = \frac{1}{s_i s_j} \langle\mathrm{T}\mathbf{u}_i, \mathrm{T}\mathbf{u}_j\rangle = \frac{1}{s_i s_j}\langle\mathrm{T}^* \mathrm{T}\mathbf{u}_i,\mathrm{u}_j\rangle = \frac{s_i}{s_j}\langle\mathrm{u}_i,\mathrm{u}_j\rangle = \delta_{ij}
$$

Hence $(\mathbf{v}_1,\dots,\mathbf{v}_r)$ is an orthonormal basis for $\operatorname{ran}(\mathrm{T}) = \ker(\mathrm{T}^*)^\perp$, which can be extended to an orthonormal basis $C = (\mathbf{v}_i)_{i=1}^m$ for $V$, the extension $(\mathbf{v}_{r+1},\dots,\mathbf{v}_m)$ being an orthonormal basis for $\ker(T^*)$. Moreover, since

$$
  \mathrm{TT}^* \mathbf{v}_i = s_i \mathrm{T}\mathbf{u}_i = s_i^2 \mathbf{v}_i
$$

the vectors $\mathbf{v}_1,\dots,\mathbf{v}_r$ are eigenvectors for $\mathrm{TT}^*$ with the same eigenvalues $\lambda_i = s_i^2$ as for $\mathrm{T}^* T$.
</details>
</MathBox>

Let $\mathbf{A}\in\mathcal{M}_{m,n}(\mathbb{F})$ be the matrix for the operator $\mathrm{T}\in\mathcal{L}(U,V)$ between inner product spaces, and let $B = (\mathbf{u}_i)_{i=1}^n$ and $C = (\mathbf{v}_i)_{i=1}^m$ be the orthonormal bases for $U$ and $V$, respectively. Then

$$
  [T]_{B,C} = \boldsymbol{\Sigma} = \operatorname{diag}(s_1,\dots,s_r,0,\dots,0)
$$

A change of orthonormal bases from the standard bases to $B$ and $C$ gives

$$
  \mathbf{A} = [\mathrm{T}_\mathbf{A}]_{E_m, E_m} = \mathbf{M}_{C,E_m} [\mathrm{T}_\mathbf{A}]_{B,C} \mathbf{M}_{E_n, B}= \mathbf{P}\boldsymbol{\Sigma}\mathbf{Q}^*
$$

where $\mathbf{P} = \mathbf{M}_{C, E_m}$ and $\mathbf{Q} = \mathbf{M}_{B, E_n}$ are unitary. This is the singular value decomposition of $\mathbf{A}$.

As to uniqueness, if $\mathbf{A} = \mathbf{P}\boldsymbol{\Sigma}\mathbf{Q}^*$, where $\mathbf{P}$ and $\mathbf{Q}$ are unitary and $\boldsymbol{\Sigma}$ is diagonal with entries $\lambda_i$, then

$$
  \mathbf{A}^* \mathbf{A} = (\mathbf{P}\boldsymbol{\Sigma}\mathbf{Q}^*)^* \mathbf{P}\boldsymbol{\Sigma}\mathbf{Q}^* = \mathbf{Q}\boldsymbol{\Sigma}^* \boldsymbol{\Sigma} \mathbf{Q}^*
$$

and since $\boldsymbol{\Sigma}^* \boldsymbol{\Sigma} = \operatorname{diag}(\lambda_i^2)_{i=1}^n$, it follows that the $\lambda_i^2$ are eigenvalues of $\mathbf{A}^* \mathbf{A}$, i.e. they are the squares of the singular values along with a sufficient number of zeroes. Hence, $\boldsymbol{\Sigma}$ is uniquely determined by $\mathbf{A}$, up to the order of the diagonal elements.

If $n\leq m$ and if the eigenvalues $\lambda_i$ are distinct, then $\mathbf{P}$ is uniquely determined up to multiplication on the right by a diagonal matrix of the form $\mathbf{D} = \operatorname{diag}(z_i)_{i=1}^m$ with $|z_i| = 1$. If $n < m$, then $\mathbf{Q}$ is never uniquely determined. If $m = n = r$, then for any given $\mathbf{P}$, there is a unique $\mathbf{Q}$. Thus, we see that, in general, the singular value decomposition is not unique.

## The Moore-Penrose generalized inverse (pseudoinverse)

<MathBox title='Pseudoinverse of linear transformations' boxType='definition'>
Let $U$ and $V$ be finite-dimensional inner product spaces over $\mathbb{F}$, and let $\mathrm{T}\in\mathcal{L}(U,V)$ have rank $r$. Then there are ordered orthonormal bases $B$ and $C$ for $U$ and $V$, respectively, for which

$$
\begin{align*}
  B =& (\underbrace{\mathbf{u}_1,\dots,\mathbf{u}_r}_{\text{ONB for }\operatorname{ran}(\mathrm{T}^*)},\underbrace{\mathbf{u}_r,\dots,\mathbf{u}_n}_{\text{ONB for}\ker(\mathrm{T})}) \\
  C =& (\underbrace{\mathbf{v}_1,\dots,\mathbf{v}_r}_{\text{ONB for }\operatorname{ran}(\mathrm{T})},\underbrace{\mathbf{v}_r,\dots,\mathbf{v}_m}_{\text{ONB for}\ker(\mathrm{T}^*)})
\end{align*}
$$

The *pseudoinverse* of $\mathrm{T}$ is defined as a linear transformation $\mathrm{T}^+ \in \mathcal{L}(V,U)$ by

$$
  \mathrm{T}^+ = \begin{cases}
    \frac{1}{s_i}\mathbf{u}_1,\quad& i\leq r \\
    0,\quad& i > r
  \end{cases}
$$

where $s_i$ are the nonzero singular values of $\mathrm{T}$. The pseudoinver $\mathrm{T}^+$ satisfies

$$
\begin{align*}
  (\mathrm{T}^+ \mathrm{T})|_{\langle\mathbf{u}_1,\dots,\mathbf{u}_r \rangle} =& \mathrm{I} \\
  (\mathrm{T}^+ \mathrm{T})|_{\langle\mathbf{u}_{r+1},\dots,\mathbf{u}_n \rangle} =& 0
\end{align*}
$$

and

$$
\begin{align*}
  (\mathrm{T}\mathrm{T}^+ )|_{\langle\mathbf{v}_1,\dots,\mathbf{v}_r \rangle} =& \mathrm{I} \\
  (\mathrm{T}\mathrm{T}^+)|_{\langle\mathbf{v}_{r+1},\dots,\mathbf{v}_m \rangle} =& 0
\end{align*}
$$

Hence, if $n = m = r$ ,then $\mathrm{T}^+ = \mathrm{T}^{-1}$.
</MathBox>

<MathBox title='Moore-Penrose conditions for pseudoinverses' boxType='proposition'>
Let $U$ and $V$ be finite-dimensional inner product spaces over $\mathbb{F}$. A linear transformation $\mathrm{T}\in\mathcal{L}(U,V)$ has a unique pseudoinverse $\mathrm{T}^+ \in\mathcal{L}(V,U)$ that satisfies the four Moore-Penrose conditions:
1. $\mathbf{TT}^+ \mathrm{T} = \mathrm{T}$
2. $\mathrm{T}^+ \mathrm{TT}^+ = \mathrm{T}^+$
3. $(\mathrm{TT}^+)^* = \mathrm{TT}^+$
4. $(\mathrm{T}^+ \mathrm{T})^* = \mathrm{T}^+ \mathrm{T}$

<details>
<summary>Proof</summary>

As for uniqueness suppose $\mathrm{R}$ and $\mathrm{S}$ satisfy **(1)**-**(2)** when substituted for $\mathrm{T}^+$. Then

$$
\begin{align*}
  \mathrm{R} =& \mathrm{RTR} \\
  =& (\mathrm{RT})^* \mathrm{R} \\
  =& \mathrm{T}^* \mathrm{R}^* \mathrm{R} \\
  =& (\mathrm{TRT})^* \mathrm{R}^* \mathrm{R} \\
  =& \mathrm{T}^* \mathrm{S}^* \mathrm{T}^* \mathrm{R}^* \mathrm{R} \\
  =& (\mathrm{ST})^* \mathrm{T}^* \mathrm{R}^* \mathrm{R} \\
  =& \mathrm{STT}^* \mathrm{R}^* \mathrm{R} \\
  =& \mathrm{STRTR} \\
  =& \mathrm{STR}
 \end{align*}
$$

and

$$
\begin{align*}
  \mathrm{S} =& \mathrm{STS} \\
  =& \mathrm{S}(\mathrm{TR})^* \\
  =& \mathrm{SS}^* \mathrm{T}^* \\
  =& \mathrm{SS}^* (\mathrm{TRT})^* \\
  =& \mathrm{SS}^* \mathrm{T}^* \mathrm{R}^* \mathrm{T}^* \\
  =& \mathrm{SS}^* \mathrm{T}^* (\mathrm{TR})^* \\
  =& \mathrm{SS}^* \mathrm{T}^* \mathrm{TR} \\
  =& \mathrm{STSTR} \\
  =& \mathrm{STR}
 \end{align*}
$$

which shows that $\mathrm{R} = \mathrm{S}$.
</details>
</MathBox>

<MathBox title='Matrix pseudoinverse' boxType='definition'>
The pseudoinverse of a matrix $\mathbf{A}\in\mathcal{M}_{m,n}(\mathbb{F})$ is the matrix $\mathbf{A}^+ \in\mathcal{M}_{m,n}(\mathbb{F})$, satisfying the Moore-Penrose conditions
1. $\mathbf{AA}^+ \mathbf{A} = \mathbf{A}$
2. $\mathbf{A}^+ \mathbf{AA}^+ = \mathbf{A}^+$
3. $(\mathbf{AA}^+)^* = \mathbf{AA}^+$
4. $(\mathbf{A}^+ \mathbf{A})^* = \mathbf{A}^* \mathbf{A}$

If $\mathbf{A}$ is full rank, i.e. $\operatorname{rank}(\mathbf{A}) = \min\Set{m,n}$, the pseudoinverse takes the following forms
- When $\mathbf{A}$ has linearly independent columns, i.e. $\mathbf{A}^* \mathbf{A}$ is invertible, then $\mathbf{A}^+$ becomes the left-inverse
$$
  \mathbf{A}^+ = (\mathbf{A}^* \mathbf{A})^{-1} \mathbf{A}^*
$$
- When har linearly independent rows, i.e. $\mathbf{AA}^*$ is invertible, then $\mathbf{A}^+$ becomes the right-inverse
$$
  \mathbf{A}^+ = \mathbf{A}^* (\mathbf{AA}^*)^{-1}
$$
</MathBox>

Let $\mathbf{A}\in\mathcal{M}_{m,n} (\mathbb{F})$ be the matrix representation of a linear transformation $\mathrm{T}\in\mathcal{L}(U,V)$ of rank $r$. If $\mathbf{A}$ has singular value decomposition

$$
  \mathbf{A} = \mathbf{U}_1 \boldsymbol{\Sigma} \mathbf{U}_2^*
$$

then the pseudoinverse of $\mathbf{A}$ is given by

$$
  \mathbf{A}^+ = \mathbf{U}_2 \boldsymbol{\Sigma}' \mathbf{U}_1^*
$$

where $\boldsymbol{\Sigma}'$ is obtained from $\boldsymbol{\Sigma}$ by replacing all nonzero entries by their multiplicative inverses. This follows from the Moore-Penrose condtions and also from the fact that for $i\leq r$

$$
  \mathbf{U}_2 \boldsymbol{\Sigma}' \mathbf{U}_1^* \mathbf{v}_i = \mathbf{U}_2 \boldsymbol{\Sigma}' \mathbf{e}_i = \frac{1}{s_i} \mathbf{U}_2 \mathbf{e}_i = \frac{1}{s_i}\mathbf{u}_i
$$

and for $i > r$

$$
  \mathbf{U}_2 \boldsymbol{\Sigma}' \mathbf{U}_1^* \mathbf{v}_i = \mathbf{U}_2 \boldsymbol{\Sigma}' \mathbf{e}_i = 0
$$

where 
- $\Set{\mathbf{u}}_{i=1}^n$ is an orthonormal basis for $U$
- $\Set{\mathbf{v}}_{i=1}^m$ is an orthonormal basis for $V$
- $\Set{\mathbf{e}_i}_{i=1}^n$ is the standard basis for $U$ 

## Least squares approximation

Let $\mathbf{A}\in\mathcal{M}_{m,n}(\mathbb{F})$ be an $m\times n$-matrix and consider the system of linear equations

$$
  \mathbf{Ax} = \mathbf{v} \in \mathbb{F}^m
$$

This system has a solution if and only if $\mathbf{v}\in\operatorname{ran}(\mathrm{T}_\mathbf{A})$. If the system has no solution, we can solve the system

$$
  \mathbf{Ax} = \hat{\mathbf{v}}
$$

where $\mathbf{v}\in\operatorname{ran}(\mathrm{T}_\mathbf{A})$ is the unique vector that is closest to $\mathbf{v}$, as measured by the Euclidean distance. This problem is called the *linear least squares* problem. Any solution to the system $\mathbf{Ax} = \hat{\mathbf{v}}$ is called a *least squares solution* to the system $\mathbf{Ax} = \mathbf{v}$. In other words, a least squares solution to $\mathbf{Ax} = \mathbf{v}$ is a vector for which $\norm{\mathbf{Ax} - \mathbf{v}}$ is minimized.

Suppose that $\mathbf{w}$ and $\mathrm{z}$ are least squares solutions to $\mathbf{Ax} = \mathbf{v}$. Then

$$
  \mathbf{Aw} = \hat{\mathbf{v}} = \mathbf{Az}
$$

and so $\mathrm{w} - \mathbf{z}\in\ker(\mathrm{T}_\mathbf{A})$. Thus, if $\mathbf{w}$ is a particular least squares solution, then the set of all least squares solutions is $\mathbf{w} + \ker(\mathrm{T}_\mathbf{A})$. Among all the solutions, the most interesting is the solution of minimum norm. Note that if there is a least squares solution $\mathbf{w}$ that lieas in $\ker(\mathbf{A})^\perp$, then for any $\mathbf{z}\in\ker(\mathbf{A})$, we have

$$
  \norm{ \mathbf{w} + \mathbf{z}}^2 = \norm{ \mathbf{w}}^2 + \norm{\mathbf{z}}^2 \geq \norm{\mathbf{w}}^2
$$

and so $\mathbf{w}$ wil be the unique least squares solution of minimum norm.

<MathBox title='' boxType='proposition'>
Let $\mathbf{A}\in\mathcal{M}_{m,n}(\mathbb{F})$. Among the least squares solutions to the system

$$
  \mathbf{Ax} = \hat{\mathbf{v}}
$$

there is a unique solution of minimum norm, given by $\mathbf{A}^+\mathbf{v}$, where $\mathbf{A}^+ \in\mathcal{M}_{n,m}(\mathbb{F})$ is the pseudoinverse of $\mathbf{A}$.

<details>
<summary>Proof</summary>

Suppose that $\mathbf{A}$ represents a linear transformation $\mathrm{T}\in\mathcal{L}(U,V)$. A vector $\mathbf{w}$ is a least squares solutions if and only if $\mathbf{Aw} = \hat{\mathbf{v}}$. Using the characterization of the best approximation $\hat{\mathbf{v}}$, we see that $\mathbf{w}$ is a solution to $\mathrm{Aw} = \hat{\mathbf{v}}$ if and only if

$$
  \mathbf{Aw} - \mathbf{v} \perp \operatorname{ran}(\mathbf{A})
$$

Since $\operatorname{ran}(\mathbf{A})^\perp = \ker(\mathbf{A}^*)$, this is equivalent to

$$
  \mathbf{A}^* (\mathbf{Aw} - \mathbf{v}) = 0
$$

or

$$
  \mathrm{A}^* \mathbf{Aw} = \mathbf{A}^* \mathbf{v}
$$

This system of equation is called the *normal equations* for $\mathbf{Ax} = \mathbf{v}$. Its solutions are precisely the least squares solutions to the system $\mathbf{Ax} = \mathbf{v}$.

To se that $\mathbf{w} = \mathbf{A}^+ \mathbf{v}$ is a least squares solution, note that if $\mathbf{A}$ has rank $r$, then

$$
  \mathbf{AA}^+ \mathbf{v}_i = \begin{cases}
    \mathbf{v}_i,\quad& i\leq r \\
    0,\quad& i > r
  \end{cases}
$$

and so

$$
  \mathbf{A}^* \mathbf{A}(\mathbf{A}^+ \mathbf{v}_i) = \begin{cases}
    \mathbf{A}^* \mathbf{v}_i,\quad& i\leq r \\
    0,\quad i > r
  \end{cases}
$$

and since $C = (\mathbf{v}_i)_{i=1}^m$ is an orthonormal basis for $V$, we conclude that $\mathbf{A}^+ \mathbf{v}$ satisfies the normal equations. Finally, since $\mathbf{A}^+ \mathbf{v} \in\ker(\mathbf{A})^\perp$, it follows that $\mathbf{A}^+ \mathbf{v}$ is the unique least squares solution of minimum norm. 
</details>
</MathBox>

# Metric vector space

## Matrices

Every linear map between finite-dimensional vector spaces can be represented by a matrix. This can be shown as follows for a transform $L : V \subseteq \mathbb{F}^n \to W \subseteq \mathbb{F}^m$. For simplicity, it is assumed that the bases $B^V = \Set{\hat{v}_i}_{i \in I \subseteq \N}$ and $B^W = \Set{\hat{w}_i}_{i \in I \subseteq \N}$ are orthonormal

$$
\begin{align*}
  L(v) &= L\left( \sum_{i=1}^n \hat{v}_i \right) = \sum_{i=1}^n L(\hat{v}_i) \\
  &= \sum_{i=1}^n \sum_{j=1}^m \left( \sum_{k=1}^n L_k^j \hat{v}^k \right)\hat{w}_j, \quad \sum_{k=1}^n L_k^j \hat{v}^k = L_k^j \delta^{ik} \\
  &= \sum_{j=1}^m \sum_{i=1}^n L_i^j \hat{w}_j = \sum_{j=1}^m w^j \hat{w}_j = w
\end{align*}    
$$

The output vector $w$ is given by the matrix multiplication

$$
  w^j = \sum_{i=1}^n L_i^j
$$

### Eigenvectors

Let $L$ be an endomorphic linear transformation $L: V \to V$. A non-zero vector $v \neq 0 \in V$ is an eigenvector of $T$ if

$$
  L(v) = \lambda v
$$

where $\lambda \in \mathbb{F}$ is the eigenvalue corresponding to $v$.

The eigenvalue equation can be rearranged as

$$
  (L - \lambda \cdot \operatorname{id})v = 0
$$

If $V$ is a finite-dimensional vector space, the determinant of the composed transformation $L - \lambda \cdot \operatorname{id}$ vanishes

$$
  \det(L - \lambda \cdot \operatorname{id}) = 0
$$

The determinant gives a polynomial function in $\lambda$, called the characteristic polynomial of $L$.

### Linear map transform

A linear map can be transformed into a new basis through the steps
1. Transform the input vector (contravariant) from the new to the old basis through a forward transform
2. Apply the linear map to the transformed vector
3. Transform the output vector back into the new basis

This can be shown as follows for a linear map $L$

$$
\begin{align*}
  L(\tilde{e}_i) &= L\left( \sum_{j=1}^n A_i^j e_j \right) = \sum_{j=1}^n A_i^j L(e_j) \\
  &= \sum_{j=1}^n A_i^j \sum_{k=1}^n L_j^k e_k \\
  &= \sum_{j=1}^n \sum_{k=1}^n A_i^j t_j^k \sum_{l=1}^n \tilde{A}_k^l \tilde{e}_l \\
  &= \sum_{l=1}^n \sum_{j=1}^n \sum_{k=1}^n \tilde{A}_k^l L_j^k A_i^j   \tilde{e}_l = \sum_{l=1} \tilde{L}_i^l \tilde{e}_l
\end{align*}
$$

The linear map gets transformed as follows

$$
\begin{gather*}
  \tilde{L}_i^l = \sum_{j=1}^n \sum_{k=1}^n \tilde{A}_k^l L_j^k A_i^j = \tilde{A}_k^l L_j^k A_i^j \\
\begin{aligned}
  \tilde{L} &= A^{-1}LA = \tilde{A}LA \\
  A\tilde{L}A^{-1} &= AA^{1}LAA^{-1} = L
\end{aligned}
\end{gather*}
$$

In tensor product notation, this can be derived as follows

$$
\begin{align*}
  L &= L_j^i e_i \otimes \boldsymbol{\varepsilon}^j \\
  &= L_j^i \left(\tilde{A}_i^k \tilde{e}_k \right) \otimes \left(A_l^j \tilde{\boldsymbol{\varepsilon}}^l \right) \\
  &= \left( \tilde{A}_i^k L_j^i A_l^j \right) \tilde{e}_k \otimes \tilde{\boldsymbol{\varepsilon}}^l \\
  &= \tilde{L_j^i} \tilde{e}_i \otimes \tilde{\boldsymbol{\varepsilon}}^j
\end{align*}
$$


## Inner product (dot product)

$$
  \langle v, w \rangle = v \cdot w = \norm{ v} \norm{ w} \cos{\theta} = g_{ij}_ v^j
$$

## Norm

The norm of a vector $\boldsymbol{v} \in V$ for a generalized basis $B = \Set{ e_i }_{i \in I \subset \N}$ is given by

$$
  \norm{ v}^2 = \langle v, v \rangle = v^* G v = g_{ij}_ v^j
$$

where $g_{ij}$ is the metric tensor

$$
  g_{ij} = \langle e_i, e_j \rangle
$$

The norm is invariant of coordinate changes

$$
\begin{align*}
  \norm{ v}^2 &= \tilde{g}_{ij} \tilde{v}^i \tilde{v}^j \\
  &= \left( A_i^k A_j^l g_{kl} \right) \left( \tilde{A}_m^i v^m \right) \left( \tilde{A}_n^j v^n \right) \\
  &= g_{kl} v^m v^n \left( \tilde{A}_m^i A_i^k  \tilde{A}_n^j A_j^l \right) \\
  &= g_{kl} v^m v^n \left(\delta_m^k \delta_n^l \right) \\
  &= g_{ij} _ v^j 
\end{align*}
$$

## Cross product (wedge product)
$$
\begin{gather*}
  a \times b = \begin{vmatrix} e_1 & e_2 & e_3\\
  a_1 & a_2 & a_3 \\
  b_1 & b_2 & b_3 \end{vmatrix} = \varepsilon^i_{jk}e_i a^j b^k  \\
  \norm{ a \times b} = \norm{ a} \norm{ b} \sin\theta \\
  \norm{ a \times b}^2 = \norm{ a}^2 \norm{ b}^2 - \left( a \cdot b \right)^2 = \norm{ a}^2 \norm{ b}^2 \left(1 - \cos^2\theta \right)
\end{gather*}
$$

Properties
- Jacobi identity
$$
  a \times \left( b \times c \right) + b \times \left( c \times a \right) + c \times \left( a \times b \right) = 0
$$
- Scalar triple product
$$
  A \cdot \left( B \times C \right) = B \cdot \left( C \times A \right) = C \cdot \left( A \times B \right)
$$

### Triple product

$$
\begin{align*}
  \left[a \times \left( b \times c \right)\right]^i &= \varepsilon^i_{jk} a^j \varepsilon^k_{mn} b^m c^n \\ 
  &= \left(\delta^{im}\delta^{jn} - \delta^{in}\delta^{jm}\right)a^j b^m c^n \\
  &= b^i a^j c^j - c^i a^j b^j \\
  &= \left[b \left(a \cdot c \right) - c\left( a \cdot b \right) \right]^i
\end{align*}
$$

## Outer product (tensor product)

Given an $m$-dimensional vector $u$ and an $n$-dimensional vector $v$, their outer product is an $m \times n$ matrix 

$$
  \left(u \otimes v\right)_{ij} = \left(u v^{\dagger}\right)_{ij} = u_{i} v_{j}^*
$$

The outer product has the following properties $\forall u, \boldsymbol{v} \in V$, $\psi, \varphi \in V^*$  and $a \in \mathbb{F}$

- $a \left( v \otimes  \right) = \left(a v \right) \otimes \psi = v \otimes \left(a \psi \right)$
- $v \otimes \left( \psi + \varphi \right) = v \otimes \psi + v \otimes \varphi$
- $\left(u + v \right) \otimes \psi = u \otimes \psi + v \otimes \psi$

## Linear form/functional (covector)

A linear form is a linear map from a vector space to its field of scalars, $f : V \to \mathbb{F}$ (1-form).

Linear functionals are represented as row vectors in $\R^n$.

### Bilinear form

A bilinear form is a map $B: V \times V \to \mathbb{F}$ (2-form) that is linear in each argument separately $\forall u, v, w \in V$ and $\lambda \in \mathbb{F}$

- $\lambda B(u, v) = B(\lambda u, v) = B(u, \lambda u)$
- $B(u + v, w) = B(u, w) + B(v, w)$
- $B(u, v + w) = g(u, v) + g(u, w)$

Bilinear forms are formed by linear combinations of covector-covector pairs

$$
  B = B_{ij} \left( e^i \otimes e^j \right)
$$

With this definition a bilinear map can be expressed as follows

$$
\begin{align*}
  s &= B(u, v) \\
  &= B_{ij}e^i e^j \left(u^k e_k, v^l e_l \right) \\
  &= B_{ij}e^i \left(u^k e_k \right) e^j \left( v^l e_l \right) \\
  &= B_{ij} u^k v^l e^i \left(e_k \right) e^j \left(e_l \right) \\
  &= B_{ij} u^k v^l \delta_k^i \delta_l^j \\
  &= B_{ij} u^i v^j
\end{align*}
$$

Bilinear forms are transformed as follows

$$
\begin{align*}
  B &= B_{ij} \left( e^i \otimes e^j \right) = B_{ij} \left(A_k^i \tilde{e}^k \right) \left( A_l^j \tilde{e}^l \right) \\
  &= \left( A_k^i A_l^j B_{ij} \right) \tilde{e}^k  \tilde{e}^l \\
  &= \tilde{B}_{ij}  \tilde{e}^i \tilde{e}^j
\end{align*}
$$

giving the transformation rules

$$
\begin{align*}
  \tilde{B}_{ij} &= A_i^k A_j^l B_{kl} \\
  B_{ij} &= \tilde{A}_i^k \tilde{A}_j^l \tilde{B}_{kl}
\end{align*}
$$

## Dual vector space

Given a vector space $V$ over a field $\mathbb{F}$, the dual space $V^*$ is defined as the set of all linear functionals (covectors) $\varphi : V \to \mathbb{F}$.

The dual space $V^*$ itself becomes a vector space over $\mathbb{F}$ when equipped with the operations of addition and scalar multiplication satisfying $\forall \varphi, \psi \in V^*, x \in V$ and $a \in \mathbb{F}$ 

$$
\begin{gather*}
  (\varphi + \psi)(x) = \varphi(x) + \psi(x) \\
  (a\varphi)(x) = a(\varphi(x))
\end{gather*}
$$

If $V$ is finite-dimensional, then $V^*$ has the same dimension as $V$.

## Basis

A basis $B$ of a vector space $V$ over a field $\mathbb{F}$ is a linearly independent subset of $V$ that spans $V$.

Given a basis $B = \Set{ e_i }_{i \in I \subseteq \N}$, a new basis $\tilde{B} = \Set{\tilde{e}_i}_{i \in I \subseteq \N}$ can be formed by transforming the old basis vectors, and vice versa

$$
\begin{align*}
  \tilde{e}_i &= \sum_{j=1}^n A_i^je_j = A_i^je_j \\
  e_i &= \sum_{j=1}^n \tilde{A}_i^j \tilde{e}_j = \tilde{A}_i^j \tilde{e}_j 
\end{align*}
$$

Basis vectors transform covariantly. The transforms are invertible (bijections) such that the compositions give the identity transform 

$$
\begin{gather*}
  A\tilde{A} = AA^{-1} = I \\
  \sum_{j=1}^n A_{ij}\tilde{A}_{ji} = \sum_{j=1}^n \tilde{A}_{ij}A_{ji} = \delta_{ij} = \begin{cases} 1, \quad i = j \\ 0, \quad i \neq j \end{cases}
\end{gather*}
$$

A vector $v$ can be expressed as a linear combination of basis vectors

$$
\begin{align*}
  v = \sum_{j=1}^n v_j e_j = \sum_{j=1}^n v_j \left( \sum_{i=1}^n \tilde{A}_{ij} \tilde{e}_i \right) = \sum_{i=1}^n \left( \sum_{j=1}^n \tilde{A}_{ij}v_j \right)\tilde{e}_i = \sum_{j=1}^n \tilde{v}_j \tilde{e}_j \\ 
  v = \sum_{j=1}^n \tilde{v}_j \tilde{e}_j = \sum_{j=1}^n \tilde{v}_j \left( \sum_{i=1}^n A_{ij} e_i \right) = \sum_{i=1}^n \left( \sum_{j=1}^n A_{ij} \tilde{v}_j \right)e_i = \sum_{j=1}^n v_j e_j
\end{align*}
$$

Vector components are said to be contravariant as they transform inversely of the basis vectors. To signify this, vector components are denoted with superscript indices. 

$$
\begin{align*}
  v^j &= \sum_{j=1}^n A_{ij} \tilde{v}^j \\
  \tilde{v}^j &= \sum_{j=1}^n \tilde{A}_{ij}v^j
\end{align*}
$$

### Dual basis

Given a basis $B = \Set{ e_i }_{i \in I \subset \N}$ for a finite-dimensional vector space $V$, a basis $B^* = \Set{e^i}_{i \in I \subset \N}$, called the dual basis, can be formed through the bi-orthogonality property

$$
  e^i \left( e_j \right) = \delta_j^i
$$

A new dual basis $\tilde{B}^*$ can be formed by transforming the old dual basis, $\tilde{e}^i = \sum_{i=1}^n t_{ij}e^j$. By the bi-orthogonality property

$$
\begin{align*}
  \tilde{e}^i (\tilde{e}_k) &= \sum_{j=1} t_{ij} e^j \left( \tilde{e}_k\right) \\
  &= \sum_{j=1}^n t_{ij} e^j \left( \sum_{l=1}^n a_{lk} e_l \right) \\
  &= \sum_{j=1}^n \sum_{l=1}^n t_{ij} a_{lk} e^j (e_l) \\
  &= \sum_{j=1}^n \sum_{l=1}^n t_{ij} a_{lk} \delta_j^l \\
  &= \sum_{j=1}^n \sum_{l=1}^n t_{ij} a_{jk} = \delta_k^i \quad \Rightarrow t_{ij} = \tilde{a}_{ij}
\end{align*}
$$

This shows that dual basis vectors transform covariantly, hence the superscripts

$$
\begin{gather*}
  \tilde{e}^i = \sum_{j=1}^n \tilde{a}_{ij} e^j \\
  e^i = \sum_{j=1}^n a_{ij} \tilde{e}^j
\end{gather*}
$$

A covector $\varphi \in V^*$ can be represented as a linear combination of the dual basis vectors

$$
  \varphi = \sum_{i=1}^n \varphi_i e^i = \sum_{i=1}^n \varphi_i \left( a_{ij}e^j \right) = \sum_{j=1}^n \left( \sum_{i=1}^n \varphi_i a_{ij} \right) \tilde{e}^j
$$

Covectors transform covariantly, hence the subscripts.

$$
\begin{gather*}
  \varphi_j = \sum_{i=1}^n \tilde{a}_{ij}\tilde{\varphi}_i \\
  \tilde{\varphi}_j = \sum_{i=1}^n a_{ij}\varphi_i
\end{gather*}
$$


