---
title: 'Linear Algebra'
subject: 'Mathematics'
showToc: true
---

# Matrices

The set of $m\times n$ matrices with entries in a field $\mathbb{F}$ is denoted $\mathcal{M}_{m,n}(\mathbb{F})$. For $n\times n$ matrices this set is denoted $\mathcal{M}_{n,n}(\mathbb{F})$. The $(i, j)$-th entry of an $m\times n$ matrix $A\in\mathcal{M}_{m,n}(\mathbb{F})$ is denoted $[A]_{i,j} = a_{i,j}$.

The *main diagonal* of an $m\times n$ matrix $A$ is sequence of entries $(a_{i,i})_{i=1}^{\min\{m,n\}}$. An $m\times n$ matrix $A$ is called diagonal if its off-diagonal entries are zero, and denoted $A = \mathrm{diag}(a_{i,i})_{i=1}^n$. A square matrix is *upper triangular* if all of its entries below the main diagonal are $0$. Similarly, a square matrix is *lower triangular* if all of its entries above the main diagonal are $0$. 

The transpose of $A\in\mathcal{M}_{m,n}$ is the matrix $A^T$ defined by $[A^T]_{i,j} = [A]_{j,i}$. A matrix is symmetric if $A = A^T$ and skew-symmetric if $A^T = -A$. All $n\times n$ diagonal matrices are symmetric.

The $n\times n$ identity matrix is defined as $I_n := \mathrm{diag}(1)_{i=1}^n$ with $[I_n]_{i,j} = \delta_{i,j}$ where $\delta_{i,j}$ is the Kronecker delta

$$
  \delta_{i,j} = \begin{cases} 1,\quad& i=j \\ 0,\quad& i\neq j \end{cases}
$$

<MathBox title='Properties of the transpose' boxType='proposition'>
The transpose operation has the following properties for $A,B\in\mathcal{M}_{m,n}$ and $\lambda\in\mathbb{F}$
1. $(A^T)^T = A$ **(injection)**
2. $(A + B)^T = A^T + B^T$
3. $(\lambda A)^T = \lambda A^T$
4. $(AB)^T = B^T A^T$
5. $\det(A^T) = \det(A)$
</MathBox>

## Matrix multiplication

If $A$ is an $m\times n$ matrix and $B$ and $p\times q$ matrix, the matrix product $AB$ is defined if $n = p$, resulting in an $m \times q$ matrix given by the dot product of the corresponding row of $A$ and the corresponding column of $B$

$$
  [AB]_{i,j} = \sum_{r=1}^n a_{i,r} b_{r,j}
$$

The matrix product $BA$ is only defined if $m = q$ resulting in an $p \times n$ matrix. If both products are defined, they generally need not be equal, meaning that matrix multiplication is not commutative.

Matrix multiplication has the following properties for matrices $A, B, C$ with appropriate sizes
- $cA = Ac$ for a scalar $c$ **(scalar commutativity)**
- $c(AB) = (cA)B$ **(left scalar associativity)**
- $(AB)c = A(Bc)$ **(right scalar associativity)**
- $(AB)C = A(BC)$ **(associativity)**
- $C(A + B) = CA + CB$ **(left distributivity)**
- $(A + B)C = AC + BC$ **(left distributivity)**

## Partioning and matrix multiplication

Let $M$ be an $m\times n$ matrix. If $B\subseteq \{1,\dots,m \}$ and $C\subseteq\{1,\dots,n\}$ then the submatrix $M[B,C]$ is the matrix obtained from $M$ by keeping only the rows index in $B$ and the columns with index in $C$ such that $M[B,C]$ has size $|B|\cdot|C|$.

Suppose that $M\in\mathcal{M}_{m,n}$ and $N\in\mathcal{M}_{n,k}$. For
1. $\mathcal{P} = \{B_1,\dots, B_p\}$ be a partition of $\{1,\dots,m\}$
2. $\mathcal{Q} = \{C_1,\dots, C_q\}$ be a partition of $\{1,\dots,n\}$
3. $\mathcal{R} = \{D_1,\dots, D_r\}$ be a partition of $\{1,\dots,k\}$

we have

$$
  [MN][B_i, D_i] = \sum_{C_h \in\mathcal{Q}} M[B_i, C_h] N[C_h, D_j]
$$

When the partions contain only single-element block, this reduces to the usual formula for matrix multiplication

$$
  [MN]_{i,j} = \sum_{h=1}^m M_{i,h} N_{h,j}
$$

## Block matrices

If $B_{i,j}$ are submatrices of $M\in\mathcal{M}_{m,n}$ with appropriate sizes, then $M$ can be written as the block matrix

$$
  M = \begin{bmatrix} 
    B_{1,1} & \cdots & B_{1,n} \\
    \vdots & \ddots & \vdots \\
    B_{m,1} & \cdots & B_{m,n}
  \end{bmatrix}
$$

A square matrix of the form

$$
  M = \begin{bmatrix} 
    B_1 & 0 &\cdots & 0 \\
    0 & \ddots & \ddots & \vdots \\
    \vdots & \ddots & \ddots & 0 \\
    0 & \cdots & 0 & B_n
  \end{bmatrix}
$$

where each $B_i$ is square and $0$ is a zero submatrix, is called a *block diagonal matrix*.

## Elementary row operations

There are three elementary row operations on matrices
- a row within the matrix can be switched with another row (row switching)
- each element in a row can be multiplied by a non-zero constant (row scaling)
- a row can be replaced by the sum of that row and a multiple of another row (row addition)

<MathBox title='Reduced row echelon form' boxType='definition'>
A matrix $R$ is said to be in *reduced row echelon* form if
1. All zero rows appear at the bottom of the matrix.
2. In any nonzero row, the first nonzero entry is $1$, called a leading entry.
3. For any two consecutive rows, the leading entry of the lower is to the right of the leading entry of the upper row.
4. Any column that contains a leading entry has zeroes in all other positions.
</MathBox>

<MathBox title='Row equivalence' boxType='proposition'>
Two matrices $A, B\in\mathcal{M}_{m,n}$ are row equivalent, denoted $A\sim B$, if eiter one can be obtained from the other by a series of elementary row operations, i.e. there is an invertible matrix $P$ such that $A = PB$.

A matrix $A$ is row equivalent to one and only one matrix $R$ in reduced row echelon form, i.e. $A = E_1\cdots E_k R$ where $E_i$ are elementary matrices required to reduce $A$ to reduced row echelon form.

A matrix $A$ is invertible if and only if its reduced row echelon form is an identity matrix. Hence a matrix is invertible if and only if it is the product of elementary matrices.
</MathBox>

An $m\times n$ matrix $R$ that is in both reduced row echelon form and reduced column echelon form must have the block form

$$
  J_k = \begin{bmatrix} 
    I_k & 0_{k,n-k} \\
    0_{m-k, k} & 0_{m-k, n-k}
  \end{bmatrix}
$$

<MathBox title='Similar matrices' boxType='definition'>
Two matrices $A, B\in\mathcal{M}_{m,n}$ are *similar* if there exists an invertible matrix $P$ such that 

$$
  A = PBP^{-1}
$$
</MathBox>

<MathBox title='Congruent matrices' boxType='definition'>
Two matrices $A, B\in\mathcal{M}_{m,n}$ are *congruent* if there exists an invertible matrix $P$ such that 

$$
  A = PBP^{T}
$$
</MathBox>

### Row-switching transformations

The corresponding elementary matrix for row switching is obtained by swapping row $i$ and row $j$ of the identity matrix.
$$
  T_{i,j} = \begin{bmatrix}
    1 & & & & & & \\
    & \ddots & & & & & \\
    & & 0 & & 1 & & \\
    & & & \ddots & & & \\
    & & 1 & & 0 & & \\
    & & & & & \ddots & \\
    & & & & & & 1
  \end{bmatrix}
$$

### Row scaling transformation

The corresponding elementary matrix for scaling the $i$th row by $m \neq 0$ is a diagonal matrix with diagonal entries $1$ except in the $i$th position

$$
  D_i (m) =   T_{i,j} = \begin{bmatrix}
    1 & & & & & & \\
    & \ddots & & & & & \\
    & & 1 & & & & \\
    & & & m & & & \\
    & & & & 1 & & \\
    & & & & & \ddots & \\
    & & & & & & 1
  \end{bmatrix}
$$

The entries of $D_i(m)$ are given by

$$
  [D_i(m)]_{k,l} = \begin{cases} 
    0 \quad& k\neq l \\
    1 \quad& k = l, k\neq i \\
    m \quad& k = l, k = i
  \end{cases}
$$

### Row addition transformation

The corresponding elementary matrix for adding the $j$th row multiplied by a scalar $m$ to the $i$th row is the identity matrix with an $m$ in the $(i,j)$th entry

$$
  L_{i,j} (m) =   T_{i,j} = \begin{bmatrix}
    1 & & & & & & \\
    & \ddots & & & & & \\
    & & 1 & & & & \\
    & & & \ddots & & & \\
    & & m & & 1 & & \\
    & & & & & \ddots & \\
    & & & & & & 1
  \end{bmatrix}
$$

The entries of $L_{i,j}(m)$ are given by

$$
  [L_{i,j}(m)]_{k,l} = \begin{cases} 
    0 \quad& k\neq l, k\neq i, l\neq j \\
    1 \quad& k = l \\
    m \quad& k = i, l = j
  \end{cases}
$$

## Determinants

<MathBox title='Determinant' boxType='proposition'>
The determinant is a function $\mathrm{det}:\mathcal{M}_{n}(\mathbb{F}) \to\mathbb{F}$ defined by

$$
  \mathrm{det}(A) = \sum_{\pi\in S_n} \left( \sgn(\sigma) \prod_{i=1}^n A_{i,\sigma(i)} \right)
$$

Where $\sigma$ is a permutation bijection and $S_n$ is the set of all permutations of $\{1,\dots,n\}$. The signature of a permutation $\sigma$ is given by

$$
  \mathrm{sgn}(\sigma) = \begin{cases} 
    +1,\quad& \sigma \text{ is even} \\
    -1,\quad& \sigma \text{ is odd}
  \end{cases}
$$
</MathBox>

<MathBox title='Properties of the determinant' boxType='proposition'>
The determinant has the following properties for $A,B\in\mathbb{M}_{n}(\mathbb{F})$
1. $\mathrm{det}(AB) = \mathrm{det}(A)\mathrm{det}(B)$
2. $A$ is invertible (nonsingular) if and only if $\mathrm{det}(A) \neq 0$
3. if $A$ is an upper/lower triangular matrix, then $\mathrm{det}(A) = \prod_{i=1}^n A_{i,j}$
4. if a square matrix $M$ has the block diagonal form

$$
  M = \begin{bmatrix} 
    B_1 & 0 &\cdots & 0 \\
    0 & \ddots & \ddots & \vdots \\
    \vdots & \ddots & \ddots & 0 \\
    0 & \cdots & 0 & B_n
  \end{bmatrix}
$$

then $\mathrm{det}(M) = \prod_{i=1}^n B_i$.
</MathBox>

# Vector space

<MathBox title='Vector space' boxType='definition'>
A vector space is a set $V$ over a field $\mathbb{F}$ equipped with the two closed operations 
- $+: V \times V \to V$ **(vector addition)**
- $\cdot: \mathbb{F} \times V \to V$ **(scalar multiplication)**

and has the following properties

- $V$ is an abelian group under vector addition, i.e. for all $u, v, w \in V$
  - $u + (v + w) = (u + v) + w$ **(associativity)
  - $\exists 0 \in V : v + 0 = v$ **(identity element)**
  - $\forall v \; \exists -v : v + (-v) = 0$ **(additive inverse)**
  - $u + v = v + u$ **(commutativity)**
- Scalar multiplication is compatible, satisfying for all $\alpha, \beta\in F$
  - $\alpha(\beta v) = (\alpha\beta)v$
  - $1v = v$
- Vector addition and scalar multiplication are related by distributivity
  - $\alpha (u + v) = \alpha u + \alpha v$
  - $(\alpha + \beta)v = \alpha v + \beta v$

The elements of $V$ are called vectors, while the elements of $\mathbb{F}$ are called scalars.
</MathBox>

<MathBox title='Linear combination' boxType='definition'>
A linear combination of vectors $\{ v_i \}_{i=1}^k \subseteq V$ for $k\in\N_+$ is a vector of the form

$$
  \sum_{i=1}^k \lambda_i v_i,\; \lambda_i \in\mathbb{F}
$$

If at least one of the scalars $\lambda_i$ is nonzero, the linear combination is nontrivial.
</MathBox>

<MathBox title='Linear subspace' boxType='definition'>
A linear subspace of an $\mathbb{F}$-vector space $V$ is a set $U\subseteq V$ that is itself an $\mathbb{F}$-vector. This means that $U$ must satisfy
- $0\in U$
- $u, v\in U \implies u + v \in U$ (closed under vector addition)
- $u\in U, \alpha\in\mathbb{F} \implies \alpha u \in U$ (closed under scalar multiplication)

The conditions imply that $U$ is closed under linear combinations, i.e. $\alpha u + \beta v \in U$ for all $\alpha,\beta\in\R$ and all $u, v \in U$.
</MathBox>

## Direct sums and products

### External direct sums and products
<MathBox title='External direct sum' boxType='definition'>
Let $V_1,\dots,V_n$ be $\mathbb{F}$-vector spaces. The external direct sum of $V_1,\dots,V_n$ denoted

$$
  V = V_1 \boxplus \cdots \boxplus V_n
$$

is the vector space $V$ whose elements are ordered $n$-tuples

$$
  V = \left\{ (v_i)_{i=1}^n \mid v_i \in V_i \right\}
$$

and whose vector operations apply componentwise

$$
\begin{gather*}
  (u_i)_{i=1}^n + (v_i)_{i=1}^n = (u_i + v_i)_{i=1}^n,\; \mathbf{u_i}, \mathbf{v_i}\in V_i \\
  \lambda (u_i)_{i=1}^n = (\lambda u_i)_{i=1}^n,\; \lambda\in\mathbb{F}
\end{gather*}
$$
</MathBox>

<MathBox title='Direct product' boxType='definition'>
Let $\mathcal{F} = \{ V_i \}_{i\in I}$ be a collection of $\mathbb{F}$-vector spaces for an index set $I$. The direct product of $\mathcal{F}$ is the vector space

$$
  \prod_{i\in I} V_i = \left\{ f: I\to\bigcup_{i\in I} V_i \mid f(i) \in V_i \right\}
$$

which is subspace of the vector space of all functions from $I$ to $\bigcup_{i\in I} V_i$.
</MathBox>

<MathBox title='Generalized external direct sum' boxType='definition'>
Let $\mathcal{V} = \{ V_i \}_{i\in I}$ be a collection of $\mathbb{F}$-vector spaces for an index set $I$. The external direct sum of $\mathcal{F}$ is the vector space

$$
  \bigoplus_{i\in I} V = \left\{ f: I\to\bigcup_{i\in I} V_i \mid f(i) \in V_i, f \text{ has finite support} \right\}
$$

which is subspace of the vector space of all functions from $I$ to $\bigcup_{i\in I} V_i$. The support of $f:I\to\bigcup_{i\in I} V_i$ is the set

$$
  \mathrm{supp}(f) = \{i\in I \mid f(i) \neq 0\}
$$

Thus, $f$ has finite support if $f(i) = 0$ for all but a finite number of $i\in I$.
</MathBox>

An important case occurs when $V_i = V$ for all $i\in I$. If $_$ denotes the set of all functions from $I$ to $V$ and $(_)_0$ denote the set of all functions in $_$ that have finite support then

$$
  \prod_{i\in I} V = V^K \quad \bigoplus_{i\in I} V = (V^K)_0
$$

### Internal direct sums

<MathBox title='Internal direct sum' boxType='definition'>
An $\mathbb{F}$-vector space $V$ is the (internal) direct sum of a collection $\mathcal{U} = \{ U_i \}_{i\in I}$ of subspaces of $V$ if every vector $v\in V$ can be uniquely written (except for order) as a finite sum of vectors from the subspaces in $\mathcal{F}$. That is, if for all $v\in V$

$$
  v = \sum_{i=1}^{n\in I} u_i = \sum_{i=1}^{m\in I} w_j,\; u_i, w_i \in U_i
$$

then $m = n$ and $u_i = w_i$ (after reindexing if necessary) for all $i=1,\dots,n$.

If $V$ is the direct sum of $\mathcal{F}$ we write

$$
  V = \bigoplus_{i\in I} U_i
$$

where each $U_i$ is a direct summand of $V$. If $V = U \oplus W$, then $W$ is called a complement of $U$ in $V$.
</MathBox>

<MathBox title='All subspaces have a complement' boxType='proposition'>
Any subspace $U$ of a vector space $V$ has a complement $W$ for which $V = U \oplus W$.
</MathBox>

<MathBox title='' boxType='proposition'>
Let $I\subseteq\N_+$ be an index set. A vector space $V$ is the direct sum of a collection of subspaces $\mathcal{U} = \{ U_i \}_{i\in I}$ if and only if
1. $V = \sum_{i\in I} U_i$
2. $U_i \cap \left(\sum_{j\neq i} U_i \right) = \{ 0 \}$

<details>
<summary>Proof</summary>

Suppose first that $V$ is the direct sum of $\mathcal{U}$. Then $(1)$ clearly holds and if

$$
  v \in U_i \cap \left(\sum_{j\neq i} U_i \right)
$$

then $v = u_i$ for some $u_i \in U_i$ and

$$
  v = \sum_{k=1}^n u_{j_k},\, u_{j_k}\in U_{j_k}
$$

where $j_k \neq 1$ for all $k=1,\dots,n$. Hence, by the uniqueness of direct sum representations, $u_i = 0$ and $v = 0$.

Conversely, suppose that $(1)$ and $(2)$ hold. We need only verify the uniqueness condition. If

$$
  v = \sum_{i=1}^m u_{j_i}\ = \sum_{i=1}^n w_{k_i},\; u_{j_i}\in U_{j_i}, w_{k_i}\in U_{k_i}
$$

then by including additional zero terms we may assume that the index sets $\{j_i\}_{i=1}^m$ and $\{k_i\}_{i=1}^n$ are the same set $\{ i_j \}_{j=1}^p$, giving

$$
  \sum_{j=1}^p (u_{i_j} - w_{i_j}) = 0
$$

Thus each term $u_{i_j} - t_{i_j}\in S_{i_j}$ is a sum of vectors from subspaces other than $S_{i_j}$, which can happen only if $u_{i_j} - t_{i_j} = 0$. Hence $u_{i_j} = t_{i_j}$ for all $i_j$ and $V$ is a direct sum of $\mathcal{U}$.
</details>
</MathBox>

## Span

<MathBox title='Linear dependence and independence' boxType='definition'>
A set of vectors $S =\{ v_i \}_{i=1}^k$ for $k\in\N_+$ of an $\mathbb{F}$-vector space $V$ is linearly dependent if there is a non-trivial linear combination for $0\in V$. That is, there is a sequence of scalars $(\lambda_i \in\mathbb{F})_{i=1}^k$ that are not all equal to zero such that

$$
\begin{gather*}
  \sum_{i=1}^k \lambda_i \mathbf{v_i} = 0 \\
  \iff \mathbf{v_j} = \sum_{i=1, i\neq j}^k \tilde{\lambda}_i \mathbf{v_i},\; 1 \leq j \leq k, \lambda_j \neq 0, \tilde{\lambda}_i = \frac{-\lambda_i}{\lambda_j}
\end{gather*}  
$$

Equivalently, $S$ is linearly dependent if and only if one of its vectors is a linear combination of the others. The set $S$ is linearly independent if it is not linearly dependent, i.e. 

$$
  \sum_{i=1}^k \lambda_i \mathbf{v_i} = 0 \implies \lambda_i = 0
$$
</MathBox>

<MathBox title='Span' boxType='definition'>
Given a subset $U$ of a $\mathbb{F}$-vector space $V$, the subspace spanned by $U$ is the the smallest set of all linear combinations of vectors in $U$. If $U$ is finite, i.e. for $k\in\N_+$ we can write $U = \{ u_i \}_{i=1}^k$, the span of $U$ is the set

$$
  \mathrm{span}(U) := \left\{ v\in V \mid \exists (\lambda_i \in F)_{i=1}^k : v = \sum_{i=1}^k \lambda_i u_i \right\}
$$

In particular $\mathrm{span}(\emptyset) := \{0\}$. 

The subset $U$ is said to span (generate) $V$ if every $v\in V$ is a linear combination of vectors in $U$, in which case we write $\mathrm{span}(U) = V$.
</MathBox>

## Basis

<MathBox title='Basis' boxType='definition'>
A basis $B$ of an $\mathbb{F}$-vector space $V$ is a linearly independent subset of $V$ spanning $V$. 
</MathBox>

<MathBox title='' boxType='proposition'>
A finite subset $U = \{u_i\}_{i=1}^n$ of an $\mathbb{F}$-vector space $V$ for $n\in\N$ is a basis for $V$ if and only if

$$
  V = \bigoplus_{i=1}^n \mathrm{span}(u_i)
$$
</MathBox>

<MathBox title='Basis properties' boxType='proposition'>
If $B$ is a subset of an $\mathbb{F}$-vector space $V$, the following are equivalent
1. $B$ is a basis of $V$ 
2. $S$ is a minimal spanning set, i.e. $B$ spans $V$ while no proper subsets of $B$ does
3. $S$ is a maximal linearly independent set, i.e. $S$ is linearly independent while no proper supersets of $S$ is linearly dependent.

<details>
<summary>Proof</summary>

**$(1)\iff(2)$**
Suppose that $B$ is a basis of $V$, i.e. $B$ is linearly independent with $\mathrm{span}(B) = V$. If $\mathrm{\tilde{B}} = V$ for some $\tilde{B}\subset B$, then any vector in $B\setminus \tilde{B}$ should be a linear combination of the vectors in $\tilde{B}$, contradicting the fact that the vectors in $B$ are linearly independent. Hence $B$ must be a minimal spanning set.

Conversely, if $B$ is a minimal spanning set, then it must be linearly independent. If not some $b\in B$ would be a linear combination of the other vectors in $B$ and so $B\setminus\{b\}$ would be a proper spanning subset of $B$, which is a contradiction. Hence $B$ must be a basis. 

**$(1)\iff(3)$**
Suppose that $B$ is a bsis of $V$. If $B$ is not maximal, there should be a vector $v\in V\setminus{B}$ for which the set $B\cup\{v\}$ is linearly independent. However, then $v\not\in \mathrm{span}(B)$, contradicting the fact that $B$ is a spanning set. Hence, $B$ is a maximal linearly independent set.

Conversely, if $B$ is a maximal linearly independent set then $\mathrm{span}(B) = V$. If not, we could find a vector $v\in V\setminus{B}$ that is not a linear combination of the vectors in $B$. In this case, $B\cup\{v\}$ would be a linearly independent proper superset of $B$, which is a contradiction. Hence, $B$ must be a basis.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V$ be a nonzero vector space. Let $I$ be a linearly independent set in $V$ and let $S$ be a spanning set in $V$ containing $I$. Then there is a basis $B$ for $V$ for which $I\subseteq B\subseteq S$. In particular
1. any vector space, except $\{0\}$ has a basis
2. any linearly independent set in $V$ is contained in a basis
4. any spanning set in $V$ contains a basis

<details>
<summary>Proof</summary>

Consider the collection $\mathcal{A}$ of all linearly independent subsets of $V$ containing $I$ and contained in $S$. Clearly, $\mathcal{A}$ is not empty since $I\in\mathcal{A}$. If $\mathcal{C} = \{ I_j\}_{j\in J}$ for some index set $J$ is a chain in $\mathcal{A}$ then the union $U = \bigcup_{j\in J} I_j$ is linearly independent and satisfies $I\subseteq U\subseteq S$, i.e. $U\in\mathcal{A}$. Thus, every chain in $\mathcal{A}$ has an upper bound in $\mathcal{A}$ and by Zorn's lemma, $\mathcal{A}$ must contain a maximal element $B$, which is linearly independent.

The set $B$ is a basis for the vector space $\mathrm{span}(S) = V$, for if any $s\in S$ is not a linear combination of the elements of $B$, then $B\cup\{s\} \subseteq S$ is linearly independent, contradicting the maximality of $B$. Hence $S\subseteq\mathrm{span}(B)$ and so $V = \mathrm{span}(S) \subseteq\mathrm{span}(B)$. 
</details>
</MathBox>

## Dimension

<MathBox title='' boxType='proposition'>
Let $V$ be a vector space and assume that the vectors $\{v_i\}_{i=1}^n$ for $n\in\N_+$ are linearly independent and the vectors $\{ s_i \}_{i=1}^m$ for $m\in\N_+$ span $V$. Then $n \leq m$. 

<details>
<summary>Proof</summary>

List the two set of vectors with the spanning set followed by the linearly independent set

$$
  s_1,\dots,s_m;v_1,\dots,v_n
$$

Move the first vector $v_1$ to the front of the list

$$
  v_1, s_1,\dots,s_m;v_2,\dots,v_n
$$

Since $\mathrm{span}\{ s_i \}_{i=1}^m = V$, it follows that $v_1$ is a linear combination of the $s_i$'s. This implies that we may remove one the $s_i$'s, which by reindexing if necessary can be $s_1$, from the list and still have a spanning set

$$
  v_1, s_2,\dots,s_m;v_2,\dots,v_n
$$

Note that the first set of vectors still spans $V$ and the second set is still linearly independent. Repeat the process, moving $v_2$ from the second list to the first list

$$
  v_1, v_2, s_2,\dots,s_m;v_3,\dots,v_n
$$

As before, the vectors in the first list are linearly independent, since they spanned $V$ before the inclusion of $v_2$. However, since the $v_i$'s are linearly independent, any nontrivial linear combination of the vectors in the first list that equals 0 must involve at least one of the $s_i$'s. Thus, we may remove that vector, which by reindexing if necessary can be $s_2$, and still have a spanning set

$$
  v_1, v_2, s_3,\dots,s_m;v_3,\dots,v_n
$$

If $m < n$, this process will eventually exhaust the $s_i$'s and lead to the list

$$
  v_1,\dots,v_m;v_{m+1},\dots,v_n
$$

where $\mathrm{span}\{v_i\}_{i=1}^m = V$, which is contradictory since any $v_i$ for $i > m$ is not in the span of $\{v_i\}_{i=1}^m$. Hence, $n\leq m$. 
</details>
</MathBox>

<MathBox title='All bases have same cardinality' boxType='theorem'>
All bases of an $\mathbb{F}$-vector space $V$ have the same cardinality, called the dimension of $V$, denoted $\dim(V)$.

<details>
<summary>Proof</summary>

For an index set $I\in\N$, let $B = \{ b_i \}_{i\in I}$ be a basis for $V$ and suppose that $C$ is another basis for $V$. Then any vector $c\in C$ can be written as finite linear combination of the vectors in $B$

$$
  c = \sum_{i\in U_c} \lambda_i b_i,\; \lambda_i\in\mathbb{F}\setminus\{0\}
$$

Because $C$ is basis, we must have $\bigcup_{c\in C} U_c = I$. If the vectors in $C$ can be expressed as finite linear combinations of the vectors in a proper subset $B' \subset B$ then $\mathrm{span}(B') = V$, which is contradictory.

Since $U_c$ is finite, i.e. $|U_c| < \aleph_0$, for all $c\in C$, it follows that 

$$
  |B| = |I| \leq \aleph_0 |C| = |C|
$$

Reversing the roles of $B$ and $C$, we may also conclude that $|C| \leq |B|$ and so $|B| = |C|$ by the Schröder-Bernstein theorem.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
A vector space $V$ is *finite-dimensional* if it is the zero space $\{0\}$ or if it has a finite basis. Otherwise, $V$ is infinite-dimensional. If $V$ has a basis of cardinality $n\in\N$ we say that $V$ is $n$-dimensional and write $\dim(V) = n$. In particular $\dim(\{0\}) = 0$.
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V$ be a vector space, then
1. if $B$ is a basis for $V$ and if $B = B_1 \cup B_2$ with $B_1 \cap B_2 = \emptyset$ then

$$
  V = \mathrm{span}(B_1) \oplus\mathrm{span}(B_1)
$$
2. let $V = S\oplus T$. If $B_1$ is a basis for $S$ and $B_2$ is a basis for $T$ then $B_1 \cap B_2 = \emptyset$ and $B = B_1 \cup B_2$ is a basis for $V$. 
</MathBox>

<MathBox title='' boxType='proposition'>
Let $S$ and $T$ be subspaces of a vector space $V$. Then

$$
  \dim(S) + \dim(T) = \dim(S + T) + \dim(S \cap T)
$$

In particular, if $T$ is any complement of $S$ in $V$, i.e. $S\oplus T = V$, then

$$
  \dim(S\oplus T) + \dim(S) + \dim(T) = dim(V)
$$

<details>
<summary>Proof</summary>

Suppose that $B = \{b_i\}_{i\in I}$ is a basis for $S\cap T$. Extend this to a basis $A\cup B$ for $S$ where $A = \{a_j\}_{j\in J}$ is disjoint from $B$. Also, extend $B$ to a basis $B\cup C$ for $T$ where $C = \{c_k\}_{k\in K}$ is disjoint from $B$. We claim that $A\cup B\cup C$ is a basis for $S+T$. It is clear that $\mathrm{span}(A\cup B\cup C) = S + T$.

To see that $A\cup B\cup C$ is linearly independent, suppose the opposite that

$$
  \sum_{i=1}^n \alpha_i v_i = 0,\;\alpha_i \in\mathbb{F}\setminus\{0}, v_i\in A\cup\B\cup C
$$

There must be vectors $v_i$ in this expression from $A$ and $C$ since $A\cup B$ and $B\cup C$ are linearly independent. Isolating the terms involving the vectors from $A$ on one side of the equality shows that there is a nonzero vector $x\in\mathrm{span}(A) \cap \mathrm{span}(B\cup C)$. However, then $x\in S\cap T$ and so $x\in \mathrm{span}(A) \cap \mathrm{span}(B)$, which implies that $x = 0$, a contradiction. Hence $A\cup B\cup C$ is linearly independent and a basis for $S + T$, giving

$$
\begin{align*}
  \dim(S) + \dim(T) =& |A\cup B| + |B\cup A| \\
  =& |A| + |B| + |B| + |C| \\
  =& |A| + |B| + |C| + \dim(S\cap T) \\
  =& \dim(S + T) + \dim(S\cap T)
\end{align*}
$$
</details>
</MathBox>

## Coordinates

<MathBox title='Ordered basis' boxType='definition'>
An ordered basis for an $n$-dimensional vector space $V$ is an ordered $n$-tuple $(n_i)_{i=1}^n$ of vectors for which the set $\{v_i\}_{i=1}^n$ is a basis for $V$.
</MathBox>

<MathBox title='Coordinate map' boxType='definition'>
If $B = \{b_i\}_{i=1}^n$ is an ordered basis for a vector space $V$ over $\mathbb{F}$ then for each $v\in V$ there is a unique ordered $n$-tuple $(\lambda_i)_{i=1}^n \in \mathbb{F}^n$ such that $v = \sum_{i=1}^n \lambda_i b_i$.

Accordingly we can define the *coordinate map* $\varphi_B: V\to\mathbb{F}^n$ by

$$
  \varphi_B (v) = [v]_B := \begin{bmatrix} \lambda_i \\ \vdots \\ \lambda_n \end{bmatrix}
$$

where the column matrix $[v]_B$ is known as the *coordinate matrix* of $v$ with respect to the ordered basis $B$. The coordinate is an isomorphism (bijection) with inverse $\varphi_B^{-1}:\mathbb{F}^n \to V$ defined by

$$
  (\lambda_i)_{i=1}^n \mapsto \sum_{i=1}^n \lambda_i b_i
$$
</MathBox>

## Row and column spaces of matrices

Let $A\in\mathcal{M}_{m,n}(\mathbb(F))$ be and $m\times n$ matrix. The rows of $A$ span a subspace of $\mathbb{F}^n$ called the *row space* of $A$, dented $\mathrm{rs}(A)$, and the columns of $A$ span a subspace of $\mathbb{F}^m$ called the *column space* of $A$, denoted $\mathrm{cs}(A)$. The dimensions of these spaces are called the *row rank* and *column rank*, denoted $\mathrm{rrk}(A)$ and $\mathrm{crk}(A)$, respectively.

<MathBox title='' boxType='proposition'>
Let $A$ be an $m\times n$ matrix. Then elementary column operations do not affect the row rank of $A$. Similarly, elementary row operations do not affect the column rank of $A$.

<details>
<summary>Proof</summary>

The rowspace of $A$ is 

$$
  \mathrm{rs}(A) = \mathrm{span}(e_i A)_{i=1}^n
$$

where $e_i$ are the standard basis vectors in $\mathbb{F}$. Perferming an elementary column operation on $A$ is equivalent to multiplying $A$ on the right by an elementary matrix $E$. Thus, the row space of $AE$ is

$$
  \mathrm{rs}(A) = \mathrm{span}(e_i EA)_{i=1}^n
$$

and since $E$ is invertible

$$
  \mathrm{rrk}(A) = \dim(\mathrm{rs}(A)) = \dim(\mathrm{rs}(AE)) = \mathrm{rrk}(AE)
$$

The second statement follows from the first by taking transposes. 
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
If $A\in\mathcal{M}_{m,n}$, then $\mathrm{rrk}(A) = \mathrm{crk}(A)$. This number is called the rank of $A$ and is denoted by $\mathrm{rank}(A)$.

<details>
<summary>Proof</summary>
 
According to the previous result, $A$ can be transformed into a reduced column echelon form without affecting the row rank. This reduction does not affect the column either. The matrix $A$ can be further transformed into a reduced row echelon form without affecting either rank. The resultiing matrix $M$ has the same row and column ranks ars $A$. However, $M$ is a matrix with $1$'s followed by $0$' on the main diagonals entries and $0$'s elsewhere. Hence

$$
  \mathrm{rrk}(A) = \mathrm{rrk}(M) = \mathrm{crk}(M) = \mathrm{crk}(A)
$$
</details>
</MathBox>

# Linear transformations

<MathBox title='Linear transformation' boxType='definition'>
Let $V$ and $W$ be vector spaces over the same field $\mathbb{F}$. A function $f: V \to W$ is called a *linear transformation* if it preserve vector space operations

$$
  f(\alpha v + \beta w) = \alpha f(v) + \beta f(w)
$$

for all $\alpha, \beta\in\mathbb{F}$ and $v, w\in V$. A linear transformation $f: V\to V$ is called a *linear operator* on $V$. The set of all linear transformations from $V$ to $W$ is denoted $\mathcal{L}(V, W)$ and the set of all linear operators on $V$ is denoted $\mathcal{L}$. 

The following terms are used to classify linear transformations and operators
- **homomorphism:** linear transformation
- **endomorphism:** linear operator
- **monomorphism (embedding):** injective linear transformation
- **epimorphism**: surjective linear transformation
- **isomorphism**: bijective linear transformation
- **automorphism**: bijective linear operator
</MathBox>

<MathBox title='' boxType='proposition'>
1. The set $\mathcal{L}(V, W)$ is a vector space under ordinary addition of funtions and scalar multiplication of functions by elements of $\mathcal{F}$.
2. If $f\in\mathcal{L}(U, V)$ and $g\in\mathcal{L}(V, W)$ then the composition $g\circ f: V\to W$
3. If $f\in\mathcal{L}(V, W)$ is bijective, then $f^{-1}\in\mathcal{L}(W, V)$
4. The vector space $\mathcal{L}$ is an algebra, where multiplication is composition of functions. The identity map $\mathrm{id}\in\mathcal{L}(V)$ is the multiplicative identity, and the zero map $0\in\mathcal{L}(V)$ is the additive identity.

<details>
<summary>Proof</summary>

Let $f:V\to W$ be a bijective linear transformation. Then $f^{-1}: W\to V$ is well-defined and since any two vectors $w_1, w_2\in W$ have the form $w_1 = f(v_1)$ and $w_2 = f(v_2)$ we have for $\alpha, \beta\in\mathbb{F}$

$$
\begin{align*}
  f^{-1}(\alpha w_1 + \beta w_2) =& f^{-1}[\alpha f(v_1) + \beta f(v_2)] \\
  =& f^{-1}[f(\alpha v_1 + \beta v_2)] \\
  =& \alpha v_1 + \beta v_2 \\
  =& \alpha f^{-1}(w_1) + \beta f^{-1}(w_2)
\end{align*}
$$
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V$ and $W$ be vector spaces and let $B = \{v_i\}_{i\in I}$ be a basis for $V$. Then we can define a linear transformation $f\in\mathcal{L}(V,W)$ by specfiying the values of $f(v_i)\in W$ arbitrarily for all $v_i\in B$ and extending the domain of $f$ to $V$ using linearity, i.e.

$$
  f(\lambda_i v_i)_{i\in I} = \sum_{i\in I} \lambda_i f(v_i)
$$

This process uniquely defines a linear transformation. If $f,g\in\mathcal{L}(V,W)$ satisfy $f(v_i) = g(v_i)$ for all $v_i\in B$ then $f = g$.

<details>
<summary>Proof</summary>

</details>
</MathBox>

<MathBox title='Kernel and range' boxType='definition'>
A linear transformation $f\in\mathcal{L}(V, W)$ has the following two principal subspaces:
- the *kernel* (null space) of $f$ is the set $\mathrm{ker}(f):= \{v\in V \mid f(v) = 0\}$
- the *range* (image) of $f$ is the set $\mathrm{ran}(f):= \{f(v) \mid v\in V\}$

The dimension of $\mathrm{ker}(f)$ is called the *nullity* of $f$ and is denoted $\mathrm{null}(f)$. The dimension of $\mathrm{ran}(f)$ is called the *rank* of $f$ is denoted $\mathrm{rank}(f)$.
</MathBox>

<MathBox title='' boxType='proposition'>
Let $f\in\mathcal{L}(V, W)$, then
1. $f$ is surjective if and only if $\mathrm{ran}(f) = W$
2. $f$ is injective if and only if $\mathrm{ker}(f) = \{0\}$

<details>
<summary>Proof</summary>

The first statement is merely a restatement of the definition of surjectivity. To show the second statement, note that

$$
  f(u) = f(v) \iff f(u - u) = 0 \iff u - v \in\mathrm{ker}(f)
$$

Thus, if $\mathrm{ker}(f) = \{0\}$ then $f(u) = f(v) \iff u = v$, showing that $f$ is injective. Conversely, if $f$ is injective and $u\in\mathrm{ker}(f)$, then $f(u) = f(0) \iff u = 0$. Hnece $\mathrm{ker}(f) = \{0\}$.
</details>
</MathBox>

## Isomorphism

<MathBox title='Isomorphism' boxType='definition'>
A bijective linear transformation $f:V\to W$ is an isomorphism from $V$ to $W$. The vector space $V$ and $W$ are isomorphic, denoted $V \simeq W$, if there is an isomorphism from $V\to W$.
</MathBox>

For any ordered basis $B$ of an $\mathbb{F}$-vector space $V$ with $\dim(V) = n$, the coordinate map $\varphi_B: V\to\mathbb{F}^n$ is an isomorphism. Hence, any $n$-dimensional vector space over $\mathbb{F}$ is isomorphic to $\mathbb{F}^n$.

An isomorphism can be characterized as a linear transformation $f:V\to W$ that maps a basis for $V$ to a basis for $W$.

<MathBox title='Properties of isomorphisms' boxType='definition'>
Let $f\in\mathcal{L}(V, W)$ be an isomorphism. For $S\subseteq V$, then
1. $S$ spans $V$ if and only if $f(S)$ spans $W$.
2. $S$ is linearly independent in $V$ if and only if $f(S)$ is linearly independent in $W$.
3. $S$ is a basis for $V$ if and only if $f(S)$ is a basis for $W$.
</MathBox>

<MathBox title='Isomorphic spaces have same dimension' boxType='definition'>
Let $V$ and $W$ be vector spaces, then $V\simeq W$ if and only if $\dim(V) = \dim(W)$.
</MathBox>

<MathBox title='' boxType='definition'>
For $n\in\N$, any $n$-dimensinoal vector space over $\mathbb{F}$ is isomorphic to $\mathbb{F}^n$. If $B$ is a set of cardinality $|B| = \kappa$, then any $\kappa$-dimensional vector space over $\mathbb{F}$ is isomorphic to the vector space $(\mathbb{F}^B)_0$ of all functions from $B$ to $\mathbb{F}$ with finite support.
</MathBox>

## Rank-nullity theorem

<MathBox title='' boxType='proposition'>
Let $f\in\mathcal{L}(V,W)$. Any complement of $\mathrm{ker}(f)$ is isomorphic to $\mathrm{ran}(f)$.

<details>
<summary>Proof</summary>

Let $f\in\mathcal{L}(V,W)$. Since any subspace of $V$ has a complement, we can write

$$
  V = \mathrm{ker}(f) \oplus\mathrm{ker}(f)^c
$$

where $\mathrm{ker}(f)^c$ is the complement of $\mathrm{ker}(f)$. The restriction of $f$ to $\mathrm{ker}(f)^c$, denoted $f^c:\mathrm{ker}(f)^c \to W$ is injective since

$$
  \mathrm{ker}(f^c) = \mathrm{ker}(f) \cap\mathrm{ker}(f)^c = \{0\}
$$

Also, $\mathrm{ran}(f^c)\subseteq \mathrm{ran}(f)$. For the reverse inclusion, if $f\in\mathrm{ran}(f)$ then since $v = u + w$ for $u\in\mathrm{ker}(f)$ and $w\in\mathrm{ker}(f)^c$, we have

$$
  f(v) = f(u) + f(w) = f(w) = f^c(w) \in\mathrm{ran}(f^c)
$$

Thus $\mathrm{ran}(f^c) = \mathrm{ran}(f)$ and it follows that $\mathrm{ker}(f)^c \simeq \mathrm{ran}(f)$.
</details>
</MathBox>

<MathBox title='Rank-nullity theorem' boxType='theorem'>
For any $f\in\mathcal{L}(V,W)$

$$
\begin{gather*}
  \dim(\mathrm{ker}(f)) + \dim(\mathrm{ran}(f)) = \dim(V) \\
  \mathrm{rank}(f) + \mathrm{null}(f) = \dim(V)
\end{gather*}
$$
</MathBox>

<MathBox title='' boxType='corollary'>
Let $f\in\mathcal{L}(V,W)$, where $\dim(V) = \dim(W) < \infty$. Then $f$ is injective if and only if it is surjective.
</MathBox>

## Finite-dimensional linear transformations

Any $m\times n$ matrix $A$ over $\mathbb{F}$ defines a linear transformation $f_A:\mathbb{F}^n\to\mathbb{F}^m$ in the form of the multiplication map $v\mapsto Av$. 

<MathBox title='' boxType='lemma'>
1. If $A$ is an $m\times n$ matrix over $\mathbb{F}$, then the multiplication function $f_A:\mathbb{F}^n \to\mathbb{F}^m$ defined by $v \mapsto Av$ is a linear map, i.e. $\tau_A \in\mathcal{L}(\mathbb{F}^n,\mathbb{F}^m)$.
2. If $f\in\mathcal{L}(\mathbb{F}^n,\mathbb{F}^m)$ then $f = f_A$ where

$$
  A = \begin{bmatrix} f(e_1) & \cdots & f(e_n) \end{bmatrix} \in\mathcal{M}_{m,n}(\mathbb{F})
$$

is the matrix of $f$.

<details>
<summary>Proof</summary>

**(1)**: For a matrix $A\in\mathcal{M}_{m,n}(\mathbb{F})$, vectors $v, w \in \mathbb{F}^n$ and scalar $\alpha, \beta \in\mathbb{F}$ the associativity and distributivity properties of matrix multiplication gives

$$
  A(\alpha v + \beta w) = A(\alpha v) + A(\beta v) = \alpha Av + \beta Aw
$$

showing that $f_A \in\mathcal{L}(\mathbb{F}^n,\mathbb{F}^m)$.

**(2)**: Let $E = \{ e_i \}_{i=1}^n$ be the standard basis of $\mathbb{F}^n$. Then any $v\in\mathbb{F}^n$ is given by 
linear combination

$$
  v = \sum_{i=1}^n v_i e_i = \begin{bmatrix} e_1 & \cdots & e_n \end{bmatrix} \begin{bmatrix} v^1 \\ \vdots \\ v^n \end{bmatrix} = E[v]_E 
$$

By the linearity of $f$

$$
\begin{align*}
  f(v) =& f\left(\sum_{i=1}^n _ e_i \right) = \sum_{i=1}^n f(e_i)_ \\
  =& \begin{bmatrix} f(e_1) & \cdots & f(e_n) \end{bmatrix} [v]_E \\
  =& A[v]_E = f_A (v)
\end{align*}
$$

Hence $A = \begin{bmatrix} f(e_1) & \cdots & f(e_n) \end{bmatrix}$
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $A$ be an $m\times n$ matrix over $F$.
1. $f_A:\mathbb{F}^n \to\mathbb{F}^m$ is injective if and only if $\mathrm{rank}(A) = n$.
2. $f_A:\mathbb{F}^n \to\mathbb{F}^m$ is surjective if and only if $\mathrm{rank}(A) = m$.
</MathBox>

### Change of basis matrices

<MathBox title='Change of basis operator' boxType='definition'>
Let $B $ and $C$ be ordered bases for an $n$-dimensional vector space $V$. For any $v\in V$, the map $\varphi_{B,C} = \varphi_C \varphi_B^{-1}$ given by $[v]_B \mapsto [v]_C$ is called the change of basis operator. 
</MathBox>

<MathBox title='' boxType='proposition'>
Let $B = \{b_i\}_{i=1}^n$ and $C$ be ordered bases for an $n$-dimensional vector space $V$. The change of basis operator $\varphi_{B,C} = \varphi_C \varphi_B^{-1}$ from $B$ to $C$ is an automorphism of $\mathbb{F}^n$ whose standard matrix is

$$
  M_{B,C} = \begin{bmatrix} [b_1]_C & \cdots & [b_n]_C \end{bmatrix}
$$

Hence $[v]_C = M_{B,C}[v]_B$ and $M_{C,B} = M_{B,C}^{-1}$.

<details>
<summary>Proof</summary>

Since $\varphi_{B,C}$ is an operator on $\mathbb{F}^n$ it has the form $f_M$ where $M\in\mathcal{M}_n$

$$
\begin{align*}
  M =& \begin{bmatrix} \varphi_{B,C}(e_1) & \cdots & \varphi_{B,C}(e_n) \end{bmatrix} \\
  =& \begin{bmatrix} \varphi_C \varphi_B^{-1}([b_1]_B) & \cdots & \varphi_C \varphi_B^{-1}([b_n]_B) \end{bmatrix} \\
  =& \begin{bmatrix} [b_1]_C & \cdots & [b_n]_C \end{bmatrix}
\end{align*}
$$
</summary>
</MathBox>

<MathBox title='Functional dependency of bases and transformation matrices' boxType='proposition'>
If given any two of the following
1. an invertible $n\times n$ matrix $A$
2. an ordered basis $B$ for $\mathbb{F}^n$
3. an ordered basis $C$ for $\mathbb{F}^n$

then the third is uniquely determined by the equation

$$
  A = M_{B,C}
$$

<details>
<summary>Proof</summar>

The result is clear if $B$ and $C$ are given or if $A$ and $C$ are given. If $A$ and $B$ are given, then there is a unique $C$ for which $A^{-1} = M_{C,B}$ and so there is a unique $C$ for which $A = M_{B,C}$.
</details>
</MathBox>

### The matrix of a linear transformation

<MathBox title='' boxType='proposition'>
Let $f:V\to W$ be a linear transformation, where $\dim(V) = n$ and $\dim(W) = m$, and let $B = \{b_i\}_{i=1}^n$ be an ordered basis for $V$ and $C$ nad ordered basis for $W$. Then $f$ can be represented with respect to $B$ and $C$ as the matrix product $[f(v)]_C = [f]_{B,C}[v]_B$ where

$$
  [f]_[B,C] = \begin{bmatrix} [f(b_1)]_C & \cdots & [f(b_n)]_C \end{bmatrix}
$$

is called the matrix of $f$ with respect to $B$ and $C$. If $V = W$ and $B = C$ we denote $[f]_{B,B}$ by $[f]_{B}$ and so

$$
  [f(v)]_B = [f]_B [v]_B
$$

<details>
<summary>Proof</summary>

Let $f\in\mathcal{L}(V, W)$ and let $B = \{b_i\}_{i=1}^n$ and $C$ be ordered bases for $V$ and $W$, respectively. Then the map $\theta:[v]_B \mapsto [f(v)]_C$ is a representation of $f$ as a linear transformation from $\mathbb{F}^n$ to $\mathbb{F}^m$ in the sense that knowing $\theta$ (along with $B$ and $C$) is equivalent to knowing $f$.

Since $\theta$ is a linear transformation from $\mathbb{F}^n$ to $\mathbb{F}^m$, it is simply multiplication by an $m \times n$ matrix $A$, i.e. $[f(v)]_C = A[v]_B$. Since $[b_i]_B = e_i$ we get the columns of $A$

$$
  A[v]_B = \begin{bmatrix} [f(b_i)]_C & \cdots & [f(b_n)]_C  \end{bmatrix} [v]_B
$$
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $f\in\mathcal{L}(V, W)$, and let $V$ and $W$ be vector spaces over $\mathbb{F}, with ordered bases $B = \{b_i\}_{i=1}^n$ and $C = \{c_i\}_{i=1}^m$, respectively.
1. The map $g:\mathcal{L}(V, W)\to\mathcal{M}_{m,n}(\mathbb{F})$ defined by $g(f) = [f]_{B,C}$ is an isomorphism and so $\mathcal{L}(V,W)\simeq\mathcal{M}_{m,n}(\mathbb{F})$.
2. If $h\in\mathcal{L}(U,V)$ and $f\in(V,W)$ and if $B$, $C$ and $D$ are ordered bases for $U$, $V$ and $W$, respectively, then $[fh]_{BC} = [f]_{C,D} [h]_{B,C}$. Thus, the matrix of the product (composition) $fh$ is the product of the matrices of $f$ and $g$ respectively.

<details>
<summary>Proof</summary>

**(1)**: To see that $g$ is linear, note that for all $i\in\{1,\dots,n\}$

$$
\begin{align*}
  [\alpha h + \beta f]_{B,C} [b_i]_B =& [(\alpha h + \beta f)(b_i)]_C \\
  =& [\alpha h(b_i) + \beta f(b_i)]_C \\
  =& \alpha[h(b_i)]_C + \beta[f(b_i)]_C \\
  =& \alpha[h]_{B,C}[b_i]_B + \beta[f]_{B,C}[b_i]_B \\
  =& (\alpha[h]_{B,C} + \beta[f]_{B,C})[b_i]_B
\end{align*}
$$

since $[b_i]_B = e_i$ is a standard basis vector, it follows that

$$
  [\alpha h + \beta f]_{B,C} = \alpha[h]_{B,C} + \beta[f]_{B,C}
$$

showing that $g$ is linear. If $A\in\mathcal{M}_{m,n}$, we define $f$ by the condition $[f(b_i)]_C = A_i$ giving $g(f) = A$ which is surjective. Since $\dim(\mathcal{L}(V,W)) = \dim(\mathcal{M}_{m,n})$, the map $g$ is an isomorphism.

**(2)**: We have

$$
\begin{align*}
  [fh]_{B,D}[v]_B =& [f(h(v))]_D = [f]_{C,D}[h(v)]_{C} \\
  =& [f]_{C,D}[h]_{B,C}[v]_B
\end{align*}
$$
</details>
</MathBox>

### Change of bases for linear transforms

<MathBox title='Change of bases equivalence' boxType='proposition'>
Let $f\in\mathcal{L}(V,W)$ and let $(B,C)$ and $(B',C')$ be pairs of ordered bases of $V$ and $W$, respectively, then

$$
  [f]_{B',C'} = M_{C,C'}[f]_{B,C}M_{B',B}
$$

in which case $[f]_{B',C'} \sim [f]_{B,C}$.

If $f\in\mathcal{L}(V)$ and $B$ and $C$ are ordered bases for $V$, then the matrix of $f$ reduce to

$$
  [f]_C = M_{B,C}[f]_B M_{B,C}^{-1}
$$

in which case $[f]_C \sim [f]_B$.

<details>
<summary>Proof</summary>


Multiplication by $[f]_{B',C'}$ sends $[v]_{B'}$ to $[f(v)]_{C'}$. This can be reproduced by first switching from $B'$ to $B$, then applying $[f]_{B,C}$, and finally switching from $C$ to $C'$, i.e.

$$
\begin{align*}
  [f]_{B',C'} =& M_{C,C'}[f]_{B,C} M_{B',B} \\
  =& M_{C,C'} [f]_{B,C} M_{B,B'}^{-1}
\end{align*}
$$
</details>
</MathBox>

### Equivalence of matrices

<MathBox title='Equivalence of matrices' boxType='defintion'>
Two matrices $A$ and $B$ are *equivalent* if there exist invertible matrices $P$ and $Q$ for which

$$
  B = PAQ^{-1}
$$
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V$ and $W$ be vector spaces with $\dim(V) = n$ and $\dim(W) = m$. Then two $m\times n$ matrices $A$ and $B$ are equivalent if and only if they represent the same linear transformation $f\in\mathcal{F}(V,W)$, possibly with respec to different ordered bases. In this case, $A$ and $B$ represent exactly the same set of linear transformation in $\mathcal{L}(V,W)$.

<details>
<summary>Proof</summary>

If $A$ and $B$ represent $f$, i.e. if $A = [f]_{B,C}$ and $B_{B',C'}$ for ordered bases $B,C,B'$ and $C'$, then by the change of basis equivalence, $A$ and $B$ are equivalent. 

Conversely, suppose that $A$ and $B$ are eqiuvalent, i.e. $B = PAQ^{-1}$ where $P$ an $Q$ are invertible. Suppose that $A$ represents a linear transformation $f\in\mathcal{L}(V,W)$ for some ordered basis $B$ and $C$, i.e.

$$
  A = [f]_{B,C}
$$

By the functional dependency property, there is a unique ordered basis $B'$ for $V$ for which $Q = M_{B,B'}$ and a unique ordered basis $C'$ for $W$ for which $P = M_{C,C'}$. Thus

$$
  B = M_{C,C'}[f]_{B,C} M_{B', B} = [f]_{B',C'}
$$

showing that $B$ also represents $f$. By symmetry, we se that $A$ and $B$ represent the same set of linear transformation.
</details>
</MathBox>

### Similarity of matrices

<MathBox title='Similarity of matrices' boxType='proposition'>
Two matrices are *similar* if there exists an invertible matrix $P$ for which $B = PAP^{-1}$. The equivalence classes associated with similarity are called similarity classes.
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V$ be a vector spaces with $\dim(V) = n$. Then two $n\times n$ matrices $A$ and $B$ are similar if and only if they represent the same linear transformation $f\in\mathcal{F}(V)$, possibly with respec to different ordered bases. In this case, $A$ and $B$ represent exactly the same set of linear transformation in $\mathcal{L}(V)$.

<details>
<summary>Proof</summary>

If $A$ and $B$ represent $f\in\mathcal{L}(V)$, i.e. if $A = [f]_B$ and $B = [f]_C$ for ordered bases $B$ and $C$, then by the change of bases equivalence $A$ and $B$ are similar.

Conversely, suppose that $A$ and $B$ are similar, i.e. $B = PAP^{-1}$. Suppose that $A$ represents a linear operator $\mathcal{L}(V)$ for some ordered basis $B$, i.e. $A = [f]_B$. By the functional dependency relation, there is a unique ordered basis $C$ for $V$ for which $P = M_{B,C}$. Thus 

$$
  B = M_{B,C}[f]_{B}M_{B,C}^{-1} = [f]_C
$$

Hence, $B$ also represents $f$. By symmetry, we that $A$ and $B$ represent the same set of linear operators.
</details>
</MathBox>

### Similarity of operators

<MathBox title='Similarity of operators' boxType='proposition'>
Two linear operators $f, g\in\mathcal{L}(V)$ are *similar* if there exists an automorphism $\h\in\mathcal{L}(V)$ for which 

$$
  g = hfh^{-1}
$$

The equivalence classe associated with similarity are called *similarity classes*.
</MathBox>

<MathBox title='' boxType='proposition'>
Let $V$ be a vector spaces with $\dim(V) = n$. Then two linear operators $f,g\in\mathcal{L}(V)$ are similar if and only if there is a matrix $A\in\mathcal{M}_n$ that represents both operators with respect to possibly different ordered bases. In this case, $f$ and $g$ are represented by exactly the same set of matrices in $\mathcal{M}_n$.

<details>
<summary>Proof</summary>

If $f$ and $g$ are represented by $A\in\mathcal{M}_{n}$, i.e. if $[f]_B = A = [g]_C$ for ordered bases $B$ and $C$ then

$$
  [g]_C = [f]_B = M_{C,B}[f]_C M_{B_C}
$$

Let $h\in\mathcal{L}(V)$ be the automorphism of $V$ defined by $f(c_i) = b_i$ where $B = \{b_i\}_{i=n}$ and $C = \{c_i\}_{i=1}^n$, then

$$
\begin{align*}
  [h]_C =& \begin{bmatrix} [h(c_1)]_C & \cdots & [h(c_n)]_C \end{bmatrix} \\
  =& \begin{bmatrix} [b_2]_C & \cdots & [b_n]_C \end{bmatrix} \\
  =& M_{B,C}
\end{align*}
$$

and so

$$
  [g]_C = [h]_C^{-1}[f]_C [h]_C = [h^{-1}fh]_C
$$

from which it follows that $f$ and $g$ are similar.

Conversely, suppose that $f$ and $g$ are similar, i.e. $g = hfh^{-1}$. Suppose also that $f$ is represented by the matrix $A\in\mathcal{M}_n$, i.e. $A = [f]_B$ for some ordered basis $B$. Then

$$
  [g]_B = [hfh^{-1}]_B = [h]_B [f]_B [h]_B^{-1}
$$

Setting $c_i = h(b_i)$ then $C = \{ c_i \}_{i=1}^n$ is an ordered basis for $V$ and

$$
\begin{align*}
  [h]_B =& \begin{bmatrix} [h(b_1)]_B & \cdots & [h(b_n)]_B \end{bmatrix} \\
  =& \begin{bmatrix} [c_1]_B & \cdots & [c_n]_C \end{bmatrix} \\
  =& M_{C,B}
\end{align*}
$$

Thus, $[g]_B = M_{C,B}[f]_B M_{C,B}^{-1}$ and it follows that

$$
  A = [f]_B = M_{B,C}[g]_B M_{B,C}^{-1} = [g]_C
$$

and so $A$ also represents $g$. By symmetry, we see that $f$ and $g$ are represented by the same set of matrices.
</details>
</MathBox>

## Invariant subspaces and reducing pairs

The restriction of a linear operator $f\in\mathcal{L}(V)$ to a subspace $S\subseteq V$ is not necessarily a liear operator on $S$.

<MathBox title='Invatiant subspaces' boxType='definition'>
Let $f\in\mathcal{L}(V)$. A subspace $S\subseteq V$ is *invariant* under $f$, or $f$-invariant if $f(S) \subseteq S$. That is, $S$ is invariant under $f$ if the restriction $f|_S$ is a linear operator on $S$.
</MathBox>

If $V = S \oplus T$ then the fact that $S$ is $f$-invariant does not imply that the complement $T$ is also $f$-invariant.

<MathBox title='Reducing pair' boxType='definition'>
Let $f\in\mathcal{L}(V)$. If $V = S\oplus T$ and if both $S$ and $T$ are $f$-invariant, we say that the pair $(S,T)$ *reduces* $f$.
</MathBox>

<MathBox title='Direct sum of linear operators' boxType='definition'>
Let $f\in\mathcal{L}(V)$. If $(S,T)$ reduces $f$ we write

$$
  f = f|_S \oplus f|_T
$$

and call $f$ the *direct sum* of $f|_S$ and $f|_T$. Thus, the expression $h = g \oplus f$ means that there exists subspaces $S$ and $T$ of $V$ for which $(S,T)$ reduces $h$ with $g = h|_S$ and $f = h|_T$.
</MathBox>

## Topological vector space

The standard topology on $\R^n$ is the topology induced by the Euclidean metric on $\R^n$ for which the set of open rectangles

$$
  B = \left\{ \prod_{i=1}^n \mid I_i \text{ is open an open interval in } \R \right\}
$$

is a basis, i.e. a subset of $\R^n$ is open if and only if it is a union of sets in $B$. The standard topology on $\R^n$ has the properties that the any linear functional $f:\R^n \to\R^n$ together with the addition function

$$
  \mathcal{A}:\R^n\times \R^n \ni (v,w) \mapsto v + w \in\R^n
$$

and the scalar multiplication function

$$
  \mathcal{M}:\R \times \R^n \ni (\lambda,v) \mapsto \lambda v \in\R^n
$$

are continuous. As such $\R^n$ is a topological vector space.

Generally, any real vector space $V$ endowed with a topology $\mathcal{T}$ is called a *topological vector space* if the operations of addition $\mathcal{A}:V\times V\to V$ and scalar multiplication $\mathcal{M}:\R\times V\to V$ are continuous under $\mathcal{T}$.

<MathBox title='Topological vector space' boxType='proposition'>
Let $V$ be a real vector space with $\dim(V) = n$. There is a unique topology on $V$, called the *natural topology* for which $V$ is a topological vector space and for which all linear functionals on $V$ are continuous. This topology is determined by the fact that the coordinate map $\varphi: V\to\R^n$ is a homeomorphism.

<details>
<summary>Proof</summary>

Let $V$ be any real vector space with $\dim(V) = n$ and fix and ordered basis $B = \{ v_i \}_{i=1}^n$ for $V$. Consider the coordinate map

$$
  \varphi = \varphi_B: V\ni v \mapsto [v]_B \in \R^n
$$

and its inverse

$$
  \psi_B = \varphi_B^{-1}:\R^n \ni (\lambda_i)_{i=1}^n \mapsto sum_{i=1}^n \lambda_i v_i
$$

**$\psi$ is continuous under $\mathcal{T}$**<br/>
First we show that if $V$ is a topological vector space under a topology $\mathcal{T}$ then $\psi$ is continuous. Since $\psi = \sum_{i=1}^n \psi$ where $\psi_i:\R\to V$ is defined by $\psi_i (\lambda_i)_{i=1}^n = \lambda_i v_i$, it is sufficient to show that these maps continuous, as the sum of continuous maps is continuous.

Let $O$ be an open set in $\mathcal{T}$. Then

$$
  \mathcal{M}^{-1}(O) = \{(\alpha, v)\in\R\times V \mid \alphaLet $V$ be any real vector space with $\dim(V) = n$ and fix and ordered basis $B = \{ v_i \}_{i=1}^n$ for $V$. Consider the coordinate map

$$
  \varphi = \varphi_B: V\ni v \mapsto [v]_B \in \R^n
$$

and its inverse

$$
  \psi_B = \varphi_B^{-1}:\R^n \ni (\lambda_i)_{i=1}^n \mapsto sum_{i=1}^n \lambda_i v_i
$$
 v \in O\}
$$

is an open set in $\R\times V$. We need to show that the set

$$
  \psi_i^{-1}(O) = \{(\lambda_i)_{i=1}^n \in\R^n \mid \lambda_i v_i \in O \}
$$

is open in $\R^n$. Let $(\lambda_i)_{i=1}^n \in\psi_i^{-1}(O)$ so that $\lambda_i v_i \in O$. It follows that $(\lambda_i, v_i)\in\mathcal{M}^{-1}(O)$, which is open. Thus, there is an open interval $I\subseteq\mathbb{R}$ and an open set $B\in\mathcal{T}$ of $V$ for which

$$
  (\lambda_i, v_i) \in I \times B \subseteq\mathcal{M}^{-1}(O)
$$

Then the open set $U = \R\times\cdot\R\times I \times\R\times\cdots\times\R$ where $I$ is in the $i$th position, has the property that $\psi_i(U)\subseteq O$. Thus

$$
  (\lambda_i)_{i=1}^n \in U \subseteq \psi_i^{-1}(O)
$$

and so $\psi_i^{-1}(O)$ is open. Hence, $\psi_i$, and therefore $\psi$, is continuous.

**$\varphi$ is continuous under $\mathcal{T}$**<br/>
Next we show that if every linear functional on $V$ is continuous under a topology $\mathcal{T}$ on $V$ then the coordinate map $\varphi$ is continuous. For $v\in V$ let $[v]_{B,i}$ denote the $i$th coordinate of $[v]_B$. The map $\mu:V \to\R$ defined by $\mu(v) = [v]_{B,i}$ is a linear functional and so is continuous by assumption. Thus, for any open interval $I_i \in\R$ the set

$$
  A_i = \{ v\in V \mid [v]_{B,i} \in I_i \}
$$

is open. If $I_i$ are open intervals in $\R$ then

$$
  \varphi^{-1}(\prod_{i=1}^n I_i) = \{ v\in V \mid [v]_B \in \prod_{i=1}^n I_i \} = \bigcap_{i=1}^n A_i
$$

is open. Hence $\varphi$ is continuous.

**\mathcal{T} is unique topology on $V$**
If a topology $\mathcal{T}$ has the property that $V$ is a topological vector space and every linear functional is continuous, then $\varphi$ and $\psi = \varphi^{-1}$ are homeomorphisms. This means that $\mathcal{T}$, if it exists, must be unique.

It remains to prove that the topology $\mathcal{T}$ on $V$ that makes $\varphi$ a homeomorphism has the property that $V$ is a topological space under $\mathcal{T}$ and that any linear functional $f$ on $V$ is continuous.

**Addition $\mathcal{A}$ is continuous under $\mathcal{T}$**
As to addition, the maps $\varphi: V\to\R^n$ and $(\varphi\times\varphi):V\times V\to\R^n \times\R^n$ are homeomorphisms and the map $\mathcal{A}':\R^n\times \R^n \to\R^n$ is continuous and so the map $\mathcal{A}:V\times V\to V$, being equal to $\varphi^{-1}\circ\mathcal{A}'\circ(\varphi\times\varphi)$,, is also continuous.

**Scalar multiplication $\mathcal{M}$ is continuous under $\mathcal{T}$**
As to scalar multiplication, the maps $\varphi:V\to\R^n$ and $(\iota\times\varphi):\R\times V\to \R\times\R^n$ are homeomorphisms and the map $\mathcal{M}:V\times V\to V$, being equal to $\varphi^{-1}\circ\mathcal{M}'\circ(\iota\times\varphi)$, is also continuous.

**Any linear functional $f:V\to\R$ is continuous**<br/>
Let $f$ be a linear functional. Since $\varphi$ is continuous if and only if $f\circ\varphi^{-1}$ is continuous, we can confine attention to $V=\R^n$. In this case, if $\{e_i\}_{i=1}^n$ is the standard basis for $\R^n$ and $|f(e_i)|\leq M$, then for any $x=(\lambda_i)\in\R^n$ we have

$$
\begin{align*}
  |f(x)| =& \left| \sum_{i=1}^n \lambda_i f(e_i) \right| \\
  \leq& \sum_{i=1}^n |\lambda_i|\cdot|f(e_i)| \leq M\sum_{i=1}^n |\lambda_i|
\end{align*}
$$

If $|x| < \frac{\varepsilon}{Mn}$ then $|\lambda_i| < \frac{\varepsilon}{Mn}$ and so $|f(x)| < \varepsilon$, which implies that $f$ is continuous.

By the Riesz representation theorem and the Cauchy-Schwarz inequality we have

$$
  \lVert f(x) \rVert \leq \lVert \mathcal{R}_f \rVert \cdot \lVert x\rVert
$$

Hence, $x_n \to 0$ implies $f(x_n)\to 0$ and so by linearity, $x_n \to x$ implies $f(x_n)\to x$ and so $f$ is continuous.
</details>
</MathBox>




We claim that there is precisely one topology $\mathcal{T} = \mathcal{T}_V$, called the *natural topology*, on $V$ for which $V$ becomes a topological vector space and for which all linear functionals are continuous. The natural topology is the topology for which $\varphi_B$ (and therefore also $\psi_B$) is a homeomorphism for any basis $B$. Recall that a homeomorphism is a bijective map that is continuous and has a continuous inverse.


For a vector space $V$, the outer product of its basis $B = \left\{ e_i \right\}_{i \in I \subset \N}$ and its dual basis $B^* = \left\{ e^i \right\}_{i \in I \subset \N}$ spans all endomorphic linear transforms. Every linear transform $L : V \to V$ can be writen as a linear combination of vector-covector pairs 

$$
  L = L_j^i \left(e_i \otimes e^j \right)
$$

The outer product itself forms a linear map

$$
\begin{align*}
  w &= L(v) = L_j^i \left(e_i \otimes e^j \right)(v^k e_k) \\
  &= L_j^i v^k e_i e^j (e_k) \\
  &= L_j^i v^k e_i \delta_k^j \\
  &= L_j^i v^j e_i
\end{align*}
$$



### Matrices

Every linear map between finite-dimensional vector spaces can be represented by a matrix. This can be shown as follows for a transform $L : V \subseteq \mathbb{F}^n \to W \subseteq \mathbb{F}^m$. For simplicity, it is assumed that the bases $B^V = \{\hat{v}_i\}_{i \in I \subseteq \N}$ and $B^W = \{\hat{w}_i\}_{i \in I \subseteq \N}$ are orthonormal

$$
\begin{align*}
  L(v) &= L\left( \sum_{i=1}^n _ \hat{v}_i \right) = \sum_{i=1}^n _ L(\hat{v}_i) \\
  &= \sum_{i=1}^n _ \sum_{j=1}^m \left( \sum_{k=1}^n L_k^j \hat{v}^k \right)\hat{w}_j, \quad \sum_{k=1}^n L_k^j \hat{v}^k = L_k^j \delta^{ik} \\
  &= \sum_{j=1}^m \sum_{i=1}^n L_i^j _ \hat{w}_j = \sum_{j=1}^m w^j \hat{w}_j = w
\end{align*}    
$$

The output vector $w$ is given by the matrix multiplication

$$
  w^j = \sum_{i=1}^n L_i^j _
$$

### Eigenvectors

Let $L$ be an endomorphic linear transformation $L: V \to V$. A non-zero vector $v \neq 0 \in V$ is an eigenvector of $T$ if

$$
  L(v) = \lambda v
$$

where $\lambda \in \mathbb{F}$ is the eigenvalue corresponding to $v$.

The eigenvalue equation can be rearranged as

$$
  (L - \lambda \cdot \mathrm{Id})v = 0
$$

If $V$ is a finite-dimensional vector space, the determinant of the composed transformation $L - \lambda \cdot \mathrm{Id}$ vanishes

$$
  \mathrm{det}(L - \lambda \cdot \mathrm{Id}) = 0
$$

The determinant gives a polynomial function in $\lambda$, called the characteristic polynomial of $L$.

### Linear map transform

A linear map can be transformed into a new basis through the steps
1. Transform the input vector (contravariant) from the new to the old basis through a forward transform
2. Apply the linear map to the transformed vector
3. Transform the output vector back into the new basis

This can be shown as follows for a linear map $L$

$$
\begin{align*}
  L(\tilde{e}_i) &= L\left( \sum_{j=1}^n A_i^j e_j \right) = \sum_{j=1}^n A_i^j L(e_j) \\
  &= \sum_{j=1}^n A_i^j \sum_{k=1}^n L_j^k e_k \\
  &= \sum_{j=1}^n \sum_{k=1}^n A_i^j t_j^k \sum_{l=1}^n \tilde{A}_k^l \tilde{e}_l \\
  &= \sum_{l=1}^n \sum_{j=1}^n \sum_{k=1}^n \tilde{A}_k^l L_j^k A_i^j   \tilde{e}_l = \sum_{l=1} \tilde{L}_i^l \tilde{e}_l
\end{align*}
$$

The linear map gets transformed as follows

$$
\begin{gather*}
  \tilde{L}_i^l = \sum_{j=1}^n \sum_{k=1}^n \tilde{A}_k^l L_j^k A_i^j = \tilde{A}_k^l L_j^k A_i^j \\
\begin{aligned}
  \tilde{L} &= A^{-1}LA = \tilde{A}LA \\
  A\tilde{L}A^{-1} &= AA^{1}LAA^{-1} = L
\end{aligned}
\end{gather*}
$$

In tensor product notation, this can be derived as follows

$$
\begin{align*}
  L &= L_j^i e_i \otimes \mathbf{\varepsilon}^j \\
  &= L_j^i \left(\tilde{A}_i^k \tilde{e}_k \right) \otimes \left(A_l^j \tilde{\mathbf{\varepsilon}}^l \right) \\
  &= \left( \tilde{A}_i^k L_j^i A_l^j \right) \tilde{e}_k \otimes \tilde{\mathbf{\varepsilon}}^l \\
  &= \tilde{L_j^i} \tilde{e}_i \otimes \tilde{\mathbf{\varepsilon}}^j
\end{align*}
$$


## Inner product (dot product)

$$
  \langle v, w \rangle = v \cdot w = \lVert v \rVert \lVert w \rVert \cos{\theta} = g_{ij}_ v^j
$$

## Norm

The norm of a vector $v \in V$ for a generalized basis $B = \left\{ e_i \right\}_{i \in I \subset \N}$ is given by

$$
  \lVert v \rVert^2 = \langle v, v \rangle = v^* G v = g_{ij}_ v^j
$$

where $g_{ij}$ is the metric tensor

$$
  g_{ij} = \langle e_i, e_j \rangle
$$

The norm is invariant of coordinate changes

$$
\begin{align*}
  \lVert v \rVert^2 &= \tilde{g}_{ij} \tilde{v}^i \tilde{v}^j \\
  &= \left( A_i^k A_j^l g_{kl} \right) \left( \tilde{A}_m^i v^m \right) \left( \tilde{A}_n^j v^n \right) \\
  &= g_{kl} v^m v^n \left( \tilde{A}_m^i A_i^k  \tilde{A}_n^j A_j^l \right) \\
  &= g_{kl} v^m v^n \left(\delta_m^k \delta_n^l \right) \\
  &= g_{ij} _ v^j 
\end{align*}
$$

## Cross product (wedge product)
$$
\begin{gather*}
  a \times b = \begin{vmatrix} e_1 & e_2 & e_3\\
  a_1 & a_2 & a_3 \\
  b_1 & b_2 & b_3 \end{vmatrix} = \varepsilon^i_{jk}e_i a^j b^k  \\
  \lVert a \times b \rVert = \lVert a \rVert \lVert b \rVert \sin\theta \\
  \lVert a \times b \rVert^2 = \lVert a \rVert^2 \lVert b \rVert^2 - \left( a \cdot b \right)^2 = \lVert a \rVert^2 \lVert b \rVert^2 \left(1 - \cos^2\theta \right)
\end{gather*}
$$

Properties
- Jacobi identity
$$
  a \times \left( b \times c \right) + b \times \left( c \times a \right) + c \times \left( a \times b \right) = 0
$$
- Scalar triple product
$$
  A \cdot \left( B \times C \right) = B \cdot \left( C \times A \right) = C \cdot \left( A \times B \right)
$$

### Triple product

$$
\begin{align*}
  \left[a \times \left( b \times c \right)\right]^i &= \varepsilon^i_{jk} a^j \varepsilon^k_{mn} b^m c^n \\ 
  &= \left(\delta^{im}\delta^{jn} - \delta^{in}\delta^{jm}\right)a^j b^m c^n \\
  &= b^i a^j c^j - c^i a^j b^j \\
  &= \left[b \left(a \cdot c \right) - c\left( a \cdot b \right) \right]^i
\end{align*}
$$

## Outer product (tensor product)

Given an $m$-dimensional vector $u$ and an $n$-dimensional vector $v$, their outer product is an $m \times n$ matrix 

$$
  \left(u \otimes v\right)_{ij} = \left(u v^{\dagger}\right)_{ij} = u_{i} v_{j}^*
$$

The outer product has the following properties $\forall u, v \in V$, $\psi, \varphi \in V^*$  and $a \in \mathbb{F}$

- $a \left( v \otimes  \right) = \left(a v \right) \otimes \psi = v \otimes \left(a \psi \right)$
- $v \otimes \left( \psi + \varphi \right) = v \otimes \psi + v \otimes \varphi$
- $\left(u + v \right) \otimes \psi = u \otimes \psi + v \otimes \psi$

## Linear form/functional (covector)

A linear form is a linear map from a vector space to its field of scalars, $f : V \to \mathbb{F}$ (1-form).

Linear functionals are represented as row vectors in $\R^n$.

### Bilinear form

A bilinear form is a map $B: V \times V \to \mathbb{F}$ (2-form) that is linear in each argument separately $\forall u, v, w \in V$ and $\lambda \in \mathbb{F}$

- $\lambda B(u, v) = B(\lambda u, v) = B(u, \lambda u)$
- $B(u + v, w) = B(u, w) + B(v, w)$
- $B(u, v + w) = g(u, v) + g(u, w)$

Bilinear forms are formed by linear combinations of covector-covector pairs

$$
  B = B_{ij} \left( e^i \otimes e^j \right)
$$

With this definition a bilinear map can be expressed as follows

$$
\begin{align*}
  s &= B(u, v) \\
  &= B_{ij}e^i e^j \left(u^k e_k, v^l e_l \right) \\
  &= B_{ij}e^i \left(u^k e_k \right) e^j \left( v^l e_l \right) \\
  &= B_{ij} u^k v^l e^i \left(e_k \right) e^j \left(e_l \right) \\
  &= B_{ij} u^k v^l \delta_k^i \delta_l^j \\
  &= B_{ij} u^i v^j
\end{align*}
$$

Bilinear forms are transformed as follows

$$
\begin{align*}
  B &= B_{ij} \left( e^i \otimes e^j \right) = B_{ij} \left(A_k^i \tilde{e}^k \right) \left( A_l^j \tilde{e}^l \right) \\
  &= \left( A_k^i A_l^j B_{ij} \right) \tilde{e}^k  \tilde{e}^l \\
  &= \tilde{B}_{ij}  \tilde{e}^i \tilde{e}^j
\end{align*}
$$

giving the transformation rules

$$
\begin{align*}
  \tilde{B}_{ij} &= A_i^k A_j^l B_{kl} \\
  B_{ij} &= \tilde{A}_i^k \tilde{A}_j^l \tilde{B}_{kl}
\end{align*}
$$

## Dual vector space

Given a vector space $V$ over a field $\mathbb{F}$, the dual space $V^*$ is defined as the set of all linear functionals (covectors) $\varphi : V \to \mathbb{F}$.

The dual space $V^*$ itself becomes a vector space over $\mathbb{F}$ when equipped with the operations of addition and scalar multiplication satisfying $\forall \varphi, \psi \in V^*, x \in V$ and $a \in \mathbb{F}$ 

$$
\begin{gather*}
  (\varphi + \psi)(x) = \varphi(x) + \psi(x) \\
  (a\varphi)(x) = a(\varphi(x))
\end{gather*}
$$

If $V$ is finite-dimensional, then $V^*$ has the same dimension as $V$.

## Basis

A basis $B$ of a vector space $V$ over a field $\mathbb{F}$ is a linearly independent subset of $V$ that spans $V$.

Given a basis $B = \left\{ e_i \right\}_{i \in I \subseteq \N}$, a new basis $\tilde{B} = \left\{ \tilde{e}_i \right\}_{i \in I \subseteq \N}$ can be formed by transforming the old basis vectors, and vice versa

$$
\begin{align*}
  \tilde{e}_i &= \sum_{j=1}^n A_i^je_j = A_i^je_j \\
  e_i &= \sum_{j=1}^n \tilde{A}_i^j \tilde{e}_j = \tilde{A}_i^j \tilde{e}_j 
\end{align*}
$$

Basis vectors transform covariantly. The transforms are invertible (bijections) such that the compositions give the identity transform 

$$
\begin{gather*}
  A\tilde{A} = AA^{-1} = I \\
  \sum_{j=1}^n A_{ij}\tilde{A}_{ji} = \sum_{j=1}^n \tilde{A}_{ij}A_{ji} = \delta_{ij} = \begin{cases} 1, \quad i = j \\ 0, \quad i \neq j \end{cases}
\end{gather*}
$$

A vector $v$ can be expressed as a linear combination of basis vectors

$$
\begin{align*}
  v = \sum_{j=1}^n v_j e_j = \sum_{j=1}^n v_j \left( \sum_{i=1}^n \tilde{A}_{ij} \tilde{e}_i \right) = \sum_{i=1}^n \left( \sum_{j=1}^n \tilde{A}_{ij}v_j \right)\tilde{e}_i = \sum_{j=1}^n \tilde{v}_j \tilde{e}_j \\ 
  v = \sum_{j=1}^n \tilde{v}_j \tilde{e}_j = \sum_{j=1}^n \tilde{v}_j \left( \sum_{i=1}^n A_{ij} e_i \right) = \sum_{i=1}^n \left( \sum_{j=1}^n A_{ij} \tilde{v}_j \right)e_i = \sum_{j=1}^n v_j e_j
\end{align*}
$$

Vector components are said to be contravariant as they transform inversely of the basis vectors. To signify this, vector components are denoted with superscript indices. 

$$
\begin{align*}
  v^j &= \sum_{j=1}^n A_{ij} \tilde{v}^j \\
  \tilde{v}^j &= \sum_{j=1}^n \tilde{A}_{ij}v^j
\end{align*}
$$

### Dual basis

Given a basis $B = \left\{ e_i \right\}_{i \in I \subset \N}$ for a finite-dimensional vector space $V$, a basis $B^* = \left\{ e^i \right\}_{i \in I \subset \N}$, called the dual basis, can be formed through the bi-orthogonality property

$$
  e^i \left( e_j \right) = \delta_j^i
$$

A new dual basis $\tilde{B}^*$ can be formed by transforming the old dual basis, $\tilde{e}^i = \sum_{i=1}^n t_{ij}e^j$. By the bi-orthogonality property

$$
\begin{align*}
  \tilde{e}^i (\tilde{e}_k) &= \sum_{j=1} t_{ij} e^j \left( \tilde{e}_k\right) \\
  &= \sum_{j=1}^n t_{ij} e^j \left( \sum_{l=1}^n a_{lk} e_l \right) \\
  &= \sum_{j=1}^n \sum_{l=1}^n t_{ij} a_{lk} e^j (e_l) \\
  &= \sum_{j=1}^n \sum_{l=1}^n t_{ij} a_{lk} \delta_j^l \\
  &= \sum_{j=1}^n \sum_{l=1}^n t_{ij} a_{jk} = \delta_k^i \quad \Rightarrow t_{ij} = \tilde{a}_{ij}
\end{align*}
$$

This shows that dual basis vectors transform covariantly, hence the superscripts

$$
\begin{gather*}
  \tilde{e}^i = \sum_{j=1}^n \tilde{a}_{ij} e^j \\
  e^i = \sum_{j=1}^n a_{ij} \tilde{e}^j
\end{gather*}
$$

A covector $\varphi \in V^*$ can be represented as a linear combination of the dual basis vectors

$$
  \varphi = \sum_{i=1}^n \varphi_i e^i = \sum_{i=1}^n \varphi_i \left( a_{ij}e^j \right) = \sum_{j=1}^n \left( \sum_{i=1}^n \varphi_i a_{ij} \right) \tilde{e}^j
$$

Covectors transform covariantly, hence the subscripts.

$$
\begin{gather*}
  \varphi_j = \sum_{i=1}^n \tilde{a}_{ij}\tilde{\varphi}_i \\
  \tilde{\varphi}_j = \sum_{i=1}^n a_{ij}\varphi_i
\end{gather*}
$$

