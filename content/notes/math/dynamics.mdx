---
title: 'Dynamics'
subject: 'Mathematics'
showToc: true
---

# Dynamical systems

A dynamical system consists of a state space $\mathcal{M}$, a time set $T \subseteq \R$, being an additive semigroup, and an evolution operator $\Phi : \mathcal{M}\times T \to\mathcal{M}$ satisfying the group property. 

## Phase space and flows

Dynamics is the study of motion through phase space. The phase space of a given dynamical system is described as an $n$-dimensional manifold $\mathcal{M}$. The dynamics of the system is described by an evolution operator

$$
  \Phi (x, t) : \mathcal{M}\times\R\to\mathcal{M}
$$

The evolution operator satisfies the group properties
1. $\Phi(x, 0) = x$
2. $\Phi\left[ \Phi\left( x, s \right), t \right] = \Phi\left(x, s + t \right)$

For a fixed $x \in \mathcal{M}$, called the initial state, the evolution operator forms a map $t \mapsto\Phi^t (x) = \Phi(x, t)$ that defines a flow in the phase space. The map $t \mapsto\Phi^t$ is a group morphism of $\R$, as an additive group, to the group of bijections of $\mathcal{M}$, where the group operation is composition of maps. Thus $\Phi^t$ has the following properties

1. Identity: $\Phi^0 = \mathrm{id}(\mathcal{M})$
2. Closure: $\Phi^s \circ \Phi^t = \Phi^{s + t}$
3. Associativity: $\Phi^r \circ \left(\Phi^s \circ \Phi^t \right) = \left(\Phi^r \circ \Phi^s \right) \circ \Phi^t $
4. Inverse: $\Phi^t \circ \Phi^{-t} = \Phi^{-t} \circ \Phi^t = \Phi^0$

In some cases, the evolution operator also satisfies the commutative property $\Phi^s \circ \Phi^t = \Phi^t \circ \Phi^s$. If the evolution operator $\Phi$ is differentiable, the flow $\Phi^t$ is a diffeomorphism.

### Vector fields

If the flow $\Phi^t$ is a diffeomorphism, a velocity vector can be defined from the derivative

$$
  \mathbf{V}(x) = \left.\frac{\mathrm{d}}{\mathrm{d}t} \Phi^t (x) \right|_{t=0}
$$

The velocity $\mathbf{V}(x)$ is an element of the tangent space to $\mathcal{M}$ at $x$, abbreviated $T\mathcal{M}_x$. Both $\mathcal{M}$ and $T\mathcal{M}_x$ have same dimensions, but may differ topologically.

By forming a vector space on $\mathcal{M}$ and taking a flow is twice differentiable, ie. $\Phi^t \in C^2$, we may define the $C^1$-map $f: \mathcal{M}\to\mathcal{M}$ by

$$
  f(x) = \frac{\partial \Phi(x, 0)}{\partial t}
$$

It can be shown that a curve $t \mapsto x(t)$ is an evolution of the dynamical system defined by $\Phi$ if and only if it is a solution of the differential equation

$$
  \frac{\mathrm{d}x(t)}{\mathrm{d}t} = \dot{x} = f\left[x(t)\right]
$$

which shows that dynamical systems in general can be described by a system of first order differential equations. The system is called autonomous if $f$ is independent of $t$. Otherwise it is called non-autonomous.

In general, any $n^{\textrm{th}}$ order ordinary differential equation of the explicit form

$$
  \frac{\mathrm{d}^n x}{\mathrm{d}t^n} = F\left(x, \frac{\mathrm{d}x}{\mathrm{d}t}, \dots, \frac{\mathrm{d}^{n-1} x}{\mathrm{d}t^{n-1}} \right)
$$

can be converted to a first order system $\dot{x} = f(x)$. To see this, define $x_k = \frac{\mathrm{d}^{k-1}x}{\mathrm{d}t^{k-1}}$ for $k=1,\dots,n$. Thus, for $j < n$ we have $\dot{x}_j = x_{j+1}$ and $\dot{x}_n = F$ giving

$$
\begin{gather*}
  \frac{\mathrm{d}}{\mathrm{d}t}\begin{bmatrix} x_1 \\ \vdots \\ x_{n-1} \\ x_n \end{bmatrix} = \begin{bmatrix} x_2 \\ \vdots \\ x_n \\ F\left(x_1,\dots, x_n \right) \end{bmatrix} \\
  \dot{x} = f(x)
\end{gather*}
$$

### Evolution of phase space volume

Consider an autonomous vector field $\dot{x} = f(x),\, x \in \R^n$ that generates a flow $\Phi^t (x)$. Let $D_0 \subseteq \R$, then $\Phi^t (D_0)$ is the evolution of $D_0$ under the flow. If $V(t)$ is the volume of $D$, then the rate of change of volume is given as

$$
  \left.\frac{\mathrm{d}V}{\mathrm{d}t} \right|_{t=0} = \int_{D_0} \nabla \cdot f \mathrm{d}x
$$

The volume $V(t)$ can be expressed in the following form using the definition of a Jacobian matrix

$$
  V(t) = \int_{D_0} \left| \frac{\partial \Phi(x, t)}{\partial x} \right| \mathrm{d}x
$$

Taylor series expansion of $\Phi(x, t)$ about $t = 0$ gives

$$
\begin{gather*}
  \Phi(x, t) = x + f(x)t + \mathcal{O}(t^2) \\
  \implies \frac{\partial \Phi}{\partial x} = I + \frac{\partial f}{\partial x}t + \mathcal{O}(t^2) \\
  \left| \frac{\partial \Phi}{\partial x} \right| = \left| I + \frac{\partial f}{\partial x}t \right| + \mathcal{O}(t^2) = 1 + \mathrm{tr}\left( \frac{\partial f}{\partial x} \right)t + \mathcal{O}(t^2)
\end{gather*}
$$

Since $\mathrm{tr}\left( \frac{\partial f}{\partial x} \right) = \nabla \cdot f$, we get

$$
\begin{gather*}
  V(t) = V(0) + \int_{D_0} t\nabla \cdot f \mathrm{d}x + \mathcal{O}(t^2) \\
  \left.\frac{\mathrm{d}V}{\mathrm{d}t} \right|_{t=0} = \int_{D_0} \nabla \cdot f \mathrm{d}x
\end{gather*}
$$

Liouville's theorem states that the flow generated by a time independent system is volume preserving. Suppose $\nabla \cdot f = 0$ for a vector field $f$. Then for any region $D_0 \subseteq \R$, the volume $V(t)$ generated by the flow $\Phi^t(x)$ is $V(t) = V(0)$, where $V(0)$ is the volume of $D_0$.

Suppose $\nabla \cdot f = c$ for some constant $c$. For arbitrary time, the volume evolves as

$$
  \dot{V} = cV \implies V(t) = V(0)e^{ct}
$$

When $f$ is divergence free, then $\dot{V} = 0 \implies V(t) = V(0)$ which is constant.

## Constructions of dynamical systems

### Discretisation

A dynamical system with time set $T = \R$ can be discretized by introducing a new system with time set $\tilde{T} = \Z$ and evolution operator $\tilde{\Phi}: \mathcal{M}\times\tilde{T}\to\mathcal{M}$ defined by

$$
  \tilde{\Phi} (x, n) = \Phi (x, hn), \quad h \in \R
$$

In the case where $\Phi$ is determined by a differential equation

$$
  \dot{x} = f(x), \quad x \in \R^n
$$

the simplest approximation of $\Phi^h$ is given by

$$
  \Phi^h (x) = x + hf(x)
$$

which corresponds to numerically solving the differential equation through the Euler method with step size $h$.

### Suspension

A dynamical system $(\mathcal{M}, T, \Phi)$ with time set $T = \Z$ can be suspended to a system with time set $T = \Z$. The state space of the suspended system is defined as the quotient space

$$
  \tilde{\mathcal{M}} = \mathcal{M} \times \R / \sim
$$

where $\sim$ is the equivalence relation given by 

$$
  \left(x_1, s_1 \right) \sim \left( x_2, s_2 \right) \iff s_2 - s_1 \in \Z\textrm{ and } \Phi^{s_2 - s_1} (x_2) = x_1
$$

Another way to construct $\tilde{\mathcal{M}}$ is to take $\mathcal{M} \times [0, 1]$, and to identify the boundaries $\mathcal{M}\times \Set{0}$ and $\mathcal{M}\times \Set{1}$ such that the points $(x, 1)$ and $\left( \Phi^1 (x), 0 \right)$ are glued together.

The fact that the two construction give the same result can be seen as follows. The equivalence class in $\mathcal{M} \times \R$ containing the point $(x, s)$ contains exactly one element $(x', s')$ with $s' \in [0, 1)$. The elements in $\mathcal{M}\times\Set{1}$ and $\mathcal{M}\times\Set{1}$ are pairwise equivalent such that $(x, 1) \sim \left(\Phi^1 (x) , 0 \right)$. 

The evolution operator of the suspended system $\tilde{\Phi}: \tilde{\mathcal{M}} \times \R\to\mathcal{M}$ is defined by

$$
  \tilde{\Phi}\left( [x, s ], t \right) = [x, s + t]
$$

where $[\cdot]$ indicates the $\sim$-equivalence class. If $\mathcal{M}$ is connected, then after suspension, the state space is not simply connected, which means that there exists a continuous curve $\alpha: \mathbb{S}^1 \to\mathcal{M}$ that, within $\tilde{\mathcal{M}}$ cannot be deformed continuously to a point.

#### Suspension to cylinder or Möbius strip

Consider two dynamical systems with state space $M = (-1, 1)$, time set $T = \Z$ and evolution operators $\Phi_\pm^1 = \pm\mathrm{id}$. To construct the suspension we consider $M \times [0, 1] = (-1, 1) \times [0, 1]$, which forms a rectangle.

In the case with $\Phi_+^1 = \mathrm{id}$, the boundary points are $(x, 0)$ and $(x, 1)$, so that $\tilde{\mathcal{M}}$ forms a cylinder. In the case with $\Phi_-^1 = -\mathrm{id}$, the boundary points are $(x, 0)$ and $(-x, 1)$, which results in $\tilde{\mathcal{M}}$ forming the Möbius strip.



In both cases, the suspended evolutions are periodic. In the first case where $\Phi_+^1 = \mathrm{id}$, we have $\Phi^n (x) = x$. For any evolution starting at $[x, 0]$, the pairs $(x, t)$ and $(x', t')$ require a single revolution around the cylinder to become equivalent. This implies that the evolution has period $1$. In the second case where $\Phi_-^1 = -\mathrm{id}$, we have $\Phi^n (x) = (-1)^n x$. For an evolution starting at $[0, 0]$, the pairs $(0, t)$ and $(0, t')$ require a single revolution along the Möbius strip to become equivalent. However, for any evolution starting at $[x, 0]$, with $x \neq 0$, the pairs $(x, t)$ and $(x', t')$ require two revolutions along the Möbius strip to become equivalent. This implies that the evolution has period $2$.

### Poincaré map

A Poincaré map can be thought of as an inverse of the suspension construction. Considering a dynamical system with time set $\R$, given by a differential equation $x' = f(x)$. Further assuming that in the state space $\mathcal{M}$ there is a submanifold $S \subset \mathcal{M}$ of codimension $1$ (meaning that $\dim \mathcal{M} = \dim S + 1$), with the following properties

1. For all $x \in S$ the vector $f(x)$ is transversal to $S$, ie. not tangent to $S$.
2. For any point $x \in S$ there exist real numbers $t_-(x) < 0 < t_+ (x)$ such that $\Phi(x, t_-) \in S$ and $\Phi(x, t_+) \in S$. We call $t_-(x)$ and $t_+(x)$ return times and subsequently we assume that both have been choosen with minimal absolute value. This means that, due to transversality (and the Implicit Function Theorem) both $t_-$ and $t_+$ are differentiable. 

In the above setting we define diffeomorphism

$$
  \tilde{\varphi}(x) = \Phi\left[x, t_+(x) \right]: S \to S 
$$

which is indeed a diffeomorphism since an inverse of the this map simply is given by $x \mapsto \Phi\left[x ,t_-(x) \right]$. We call $\tilde{\varphi}$ the Poincaré map of $S$ (also called the return map). The corresponding dynamical system on $S$ has an evolution operator given by $\tilde{\Phi}(x, n) = \tilde{\varphi}^n (x)F$

## Fixed points (steady state)

A point $x_0 \in \R^n$ of the flow generated by an autonomous system $\dot{x} = f(x)$ is a fixed point iff $\Phi^t (x) = x\quad \forall t \in T$. By definition a fixed point satisfies $\dot{x} = f(x) = 0$. A fixed point is also known as
- critical point
- equilibrium point
- stationary point

A fixed point $x_0$ is stable if for a given $\epsilon > 0$, there exists a $\delta > 0$ depending upon $\epsilon$ such that for all $t \geq t_0$, then $\lVert x(t) - x_0 (t) \rVert < \epsilon$ whenever $\lVert x(t_0) - x_0 (t_0) \rVert < \delta$. Otherwise the fixed point is called unstable. A stable fixed point is also known as a sink or attractor, while an unstable point is also known as a source or repellor. 

Consider a small pertubation quantity $\xi(t)$ away from a fixed point $x_0$ such that $x(t) = x_0 + \xi(t)$. Inserting into the differential equation gives

$$
  \dot{\xi} = \dot{x} = f(x) = f(x_0 + \xi)
$$

applying Taylor series expansion of $f(x_0 + \xi)$ we get

$$
  \dot{\xi} = f(x_0) + \xi f'(x_0) + \frac{\xi^2}{2}f''(x_0) + \dots
$$

In linear stability theory only the linear pertubation terms are considered, hence $\dot{x} = \xi f'(x_0)$. Assuming $f'(x_0) \neq 0$, the perturbation grows exponentially if $f'(x) > 0$ and decays exponentially if $f'(x_0) < 0$. If $f'(x_0) = 0$, linear stability fails and higher order pertubation terms must be considered. 

If $f$ is continuously differentiable, the stability of a fixed point is determined by the eigenvalues $\lambda_i$ of the Jacobian matrix $J_x (f)$

- If $\lVert \lambda_i \rVert < 1$ for all $i$, the $x_0$ is a stable fixed point
- If $\lVert \lambda_i \rVert > 1$ for at least one $i$, the $x_0$ is an unstable point.
- If $\lVert \lambda_i \rVert = 1$ for some $i$, then the Jacobian test is inconclusive.

## Asymptotic behaviour (limit sets/cycles)

The asymptotic behaviour of a flow $\Phi (x, t)$ is related to $\omega$- and $\alpha$-limit points. A point $p \in \R^n$ is called an $\omega$-limit point of there exists a sequence $\Set{ t_i }_{i\in \N}$ with $t_i \to \infty$ (or $t_i \to -\infty$ for $\alpha$-limit points) such that $\Phi(x, t_i) \overset{i \to \infty}{\longrightarrow} p$

The $\omega$-limit set/cycle is defined as

$$
  \omega(x) = \Set{ x \in \mathbf{R}^n \,|\, \exists \Set{ t_i }_{i\in \N} \subset T :\, \Phi(x, t_i) \xrightarrow{t_i \to \infty} p }
$$

Similarly, the $\alpha$-limit set/cycle is defined as

$$
  \alpha(x) = \Set{ x \in \mathbf{R}^n | \exists \Set{ t_i }_{i\in \N}: \, \Phi(x, t_i) \xrightarrow{t_i \to -\infty} p }
$$

The trajectory of a system through a point $x$ is the set 

$$
\begin{align*}
  \gamma(x) &= \bigcup_{t\in T} \Phi (x, t) \\
  &= \left[ \bigcup_{t \leq 0} \Phi (x, t) \right] \cup \left[ \bigcup_{t \geq 0} \Phi (x, t) \right] \\
  &= \gamma^+ (x) \cup \gamma^- (x)
\end{align*}
$$

A lemma states that the set

$$
  \bigcap_{x \in \gamma(x)} \bar{\gamma}^+(x) = \omega(x) 
$$

is the $\omega$-limiting set.

### Invariant set

A subset $A \subset \mathcal{M}$ is called invariant under the given dynamics if $\Phi (A \times T) = A$. An interval is called trapping if it is mapped into itself and is said to be invariant if it is mapped exactly onto itself. Moreover, if a bounded interval is trapping, then all of its trajectories are trapped inside and must converge to a closed, invariant and bounded limit set. The following properties hold for an invariant set $A$

- $A$ is invariant iff $\gamma(x) \subset A \quad \forall x \in A$
- $A$ is invariant iff $\mathcal{M}\backslash A$ is invariant
- Let $(A_i)$ be a countable collection of invariant subsets of $\mathcal{M}$. Then $\bigcup_i A_i$ and $\bigcap_i A_i$ are also invariant subsets of $\mathcal{M}$

### Non-wandering point

A point $p$ is called a non-wandering point if for any neighbourhood $U$ of $p$ and for any $\tau > 0$ there exists some $|t| > \tau$ such that $\Phi^t (U) \cap U \neq \emptyset$. Otherwise it is called a wandering point. The non-wandering set, denoted $\Sigma$, contains all such points and is closed. Non-wandering points give asymptotic behaviour of the orbit. Non-wandering points include fixed points and periodic orbits.

### Attracting set (attractor)

The $\omega$-limit set $\omega(x)$ is an attractor whenever there exists arbitrarily small neighbourhoods $U$ of $\omega(x)$ such that

$$
  \Phi (U \times \Set{t}) \subset U \quad \forall\, 0 < t \in T
$$

and 

$$
  \bigcap_{0 \leq t \in T} \Phi^t (U) = \omega(x)
$$

#### Basin of attraction

The domain of an attracting set $A \subset \mathcal{M}$ is called a basin of attraction and is defined as

$$
    B(A) = \Set{ y \in \mathcal{M} \,|\, \omega(y) \subset A }
$$

### Absorbing set

A positive invariant compact subset $B \subseteq \mathcal{M}$ is said to be an absorbing set if there exists a bounded subset $C$ of $\mathcal{M}$ with $B \subset C$ such that $t_C > 0 \implies \Phi^t (C) \subset B\, \forall t\geq t_C$

### Trapping zone

An open set $U$ in an invariant set $A \subset \mathcal{M}$ in an attracting set for a flow generated by a system is called a trapping zone. Let a set $A$ be closed and invariant. The set $A$ is said to be stable iff every neighbourhood of $A$ contains a neighbourhood $U$ of $A$ which is trapping. 

## Lyapunov functions

For a general dynamical system $\dot{\boldsymbol{\phi}} = \mathbf{V}(\boldsymbol{\phi})$, a Lyapunov function $L(\boldsymbol{\phi})$ is a function which satisfies

$$
  \nabla L(\boldsymbol{\phi}) \cdot \mathbf{V}(\boldsymbol{\phi}) \leq 0
$$

There is no simple way to determine wheter a Lyapunov function exists for a given dynamical system, or, if it does exist, what the Lyapunov function is. However, if a Lyapunov function can be found, the behaviour of the system get severly constrained because $L(\boldsymbol{\phi})$ is monotonic function of time

$$
  \frac{\mathrm{d}}{\mathrm{d}t}L[\boldsymbol{\phi}(t)] = \nabla L \cdot \dot{\boldsymbol{\phi}} = \nabla L(\boldsymbol{\phi}) \cdot \mathbf{V}(\boldsymbol{\phi}) \leq 0
$$

Thus, the system evolves toward a local minimum of the Lyapunov function. In general, this means that oscillations are impossible in systems for which a Lyapunov exists.

### Lyapunov characteristic exponents

Suppose $\boldsymbol{\phi}(t)$ is an integral curve, i.e. a solution of $\dot{\boldsymbol{\varphi}}(t) = \mathbf{V}(\boldsymbol{\varphi})$. To get nearby trajectories, we form

$$
  \tilde{\boldsymbol{\varphi}}(t) \equiv \boldsymbol{\varphi}(t) + \boldsymbol{\eta}
$$

in which case

$$
  \frac{\mathrm{d}}{\mathrm{d}t}\eta_i (t) = M_{ij}(t) \eta_j (t) + \mathcal{O}(n^2)
$$

where 

$$
  M_{ij}(t) = \left. \frac{\partial V_i}{\partial \varphi_j} \right|_{\boldsymbol{\varphi(t)}}
$$

The solution, valid to first order in $\delta \varphi$ is

$$
  \eta_i (t) = Q_{ij}(t, t_0)\eta_j (t_0)
$$

where the matrix $Q(t, t_0)$ is given by the path ordered exponential

$$
\begin{align*}
  Q\left(t, t_0\right) &= \mathcal{P}\exp\Set{ \int_{t_0}^t \mathrm{d}t' M(t') } \\
  &\equiv \lim_{N\to\infty} \left[ 1 + \frac{\Delta t}{N} M\left(t_{N-1}\right) \right]\dots \left[ 1 + \frac{\Delta t}{N} M\left(t_1 \right) \right] \left[ 1 + \frac{\Delta t}{N} M\left(t_0 \right) \right]
\end{align*}
$$

with $\Delta t = t - t_0$ and $t_j = t_0 + \frac{j}{N}\Delta t$. The path order operator $\mathcal{P}$ places earlier times to the right

$$
  \mathcal{P}A(t)B(t') = \begin{cases} A(t)B(t') \quad t > t' \\ B(t')A(t) \quad t < t' \end{cases}
$$

Note that $Q$ satisfies the composition property

$$
  Q\left(t, t_0\right) = Q\left(t, t_1\right) Q\left(t_1, t_0 \right)
$$

for any $t_1 \in \left[t_0, t \right]$. When $M$ is time independent, as in the case of a fixed point where $\mathbf{V}(\boldsymbol{\varphi^*}) = 0$, the path ordered exponential reduces to the ordinary exponential, and $Q\left(t, t_0 \right) = e^{M\left(t - t_0 \right)}$.

The Lyapunov exponents are defined as

$$
  \Lambda \left(\boldsymbol{\varphi}_0 , \hat{\mathbf{e}} \right) \equiv \lim_{t\to\infty}\lim_{b\to\infty} \frac{1}{t - t_0} \ln\left( \frac{\lVert \boldsymbol{\eta}(t) \rVert}{\lVert \boldsymbol{\eta}(t_0) \rVert} \right)_{\boldsymbol{\eta}(t_0) = b\hat{\mathbf{e}}}
$$

for an $N$-dimensional unit vector $\hat{\mathbf{e}}$, and where $\boldsymbol{\varphi}_0 = \boldsymbol{\varphi}\left(t_0\right)$. Oseledec's theorem (multiplicative ergodic theorem) guarantees that there are $N$ such values $\Lambda_i \left( \boldsymbol{\varphi}_0 \right)$ depending on the choice of $\hat{\mathbf{e}}$ for a given $\boldsymbol{\varphi}_0$. Specifically, the theorem guarantees that the matrix 

$$
  \hat{Q} \equiv (Q^t Q)^{\frac{1}{t - t_0}}
$$

converges in the limit $t \to\infty$ for almost all $\boldsymbol{\varphi}_0$.

The eigenvalues $\Lambda_i$ corresponds to different eigenspaces of $\mathbf{R}$. Oseledec's theorem guarantees that the eigenspaces of $Q$ either grow ($\Lambda_i > 1$) or shrink ($\Lambda_i < 1$) exponentially fast. That is, the norm of any vector lying in the $i^\textrm{th}$ eigenspace of $Q$ will behave as $ e^{\Lambda\left(t - t_0 \right)}$ for $t \to \infty$.

Note that while $\hat{Q} = \hat{Q}^t$ is symmetric by construction, $Q$ is simply a general real-valued $N\times N$ matrix. The left and right eigenvectors of a matrix $M \in GL(N, \R)$ will in general be different. The set of eigenvalues $\lambda_\alpha$ is, however, common to both sets of eigenvectors.

Let $\Set{ \lambda_\alpha }$ be the right eigenvectors and $\Set{ \chi_\alpha^* }$ the left eigenvectors, such that

$$
\begin{gather*}
  M_{ij} \psi_{\alpha, j} = \lambda_{\alpha} \psi_{\alpha, i} \\
  \chi_{a,i}^{*} M_{ij} = \lambda_\alpha \chi_{\alpha,j}^*
\end{gather*}
$$

We can always chose the left and right eigenvectors to be orthonormal

$$
  \langle \chi_\alpha, \psi_\beta \rangle = \chi_{\alpha,i}^* \psi_{\beta, j} = \delta_{\alpha, \beta}
$$

Furtermore, we can define the matrix $S_{i\alpha} = \phi_{\alpha, i}$, in which case $S_{\alpha j}^{-1} = \chi_{\alpha,j}^*$ and

$$
  S^{-1}MS = \mathrm{diag}\left(\lambda_1,\dots,\lambda_N \right)
$$

The matrix $M$ can always be decomposed into its eigenvectors as

$$
  M_{ij} = \sum_\alpha \lambda_\alpha \psi_{\alpha, i}\chi_{\alpha, j}^*
$$

If we expand $\mathbf{u}$ in terms of the right eigenvectors

$$
  \boldsymbol{\eta}(t) = \sum_\beta C_\beta (t) \boldsymbol{\psi}_\beta (t)
$$

then upon taking the inner product with $\boldsymbol{\chi}_\alpha$, we find that $C_\alpha$ obeys

$$
  \dot{C}_\alpha + \langle \chi_\alpha, \psi_\beta \rangle C_\beta = \lambda_\alpha C_\alpha
$$

If $\dot{\boldsymbol{\psi}}_\beta = 0$, e.g. if $M$ is time-independent, then $C_\alpha (t) = C_\alpha (0) e^{\lambda_\alpha t}$ and

$$
  \eta_i (t) = \sum_\alpha \sum_j \eta_j (0) \chi_{\alpha, j}^* e^{\lambda_\alpha t} \psi_{\alpha, i} = \sum_\alpha C_\alpha (0) e^{\lambda_\alpha t}$
$$

Thus, the components of $\boldsymbol{\eta} (t)$ along $\boldsymbol{\psi}_\alpha$ increases exponentially with time if $\Re \left( \lambda_\alpha \right) > 0 $ and decreases exponentially if $\Re \left( \lambda_\alpha \right) < 0 $

## Attractors

An attractor of a dynamical system $\hat{\boldsymbol{\phi}} = \mathbf{V} (\boldsymbol{\phi})$ is the set of $\boldsymbol{\phi}$ values that the system evolves to after a sufficiently long time. For one-dimensional systems the only possible attractors are fixed points. For two-dimensional systems attractors appear as stable nodes, spirals and limit cycles. For higher dimensional systems, strange attractors emerge.

An attractor is called strange if it has a fractal structure. A strange attractor is a bounded set on which nearby orbits diverge exponentially (i.e. there exists at least one positive Lyapunov exponent). If a strange attractor is chaotic, exhibiting sensitivity dependence on initial conditions, then any two arbitrarily close alternative initial points on the attractor, after any various numbers of iterations, will lead to points that are arbitrarily far apart, and after any of various other numbers of iterations will lead to points that are arbitrarily close together.

# Bifurcations

## One-dimensional systems

In one-dimensional systems, bifurcations are derived from scalar differential equations of the form

$$
  \dot{x}(t) = f(x, \mu),\quad x, \mu \in \R
$$

where $\mu$ is the control parameter and $f:\R \times \R\to\R$ is a smooth function. If $\mu_0$ is a bifurcation point, the corresponding fixed point $x_0$ satisfies $f(x_0, \mu_0) = 0$. Thus, bifurcations of a one-dimensional system are associated with the stabilities of its equilibrium points. Furthermore, we are interested in nonhyperbolic fixed points, meaning that $x_0$ satisfies 

$$
  \frac{\partial f}{\partial x}(x_0, \mu_0) = 0
$$

### Saddle-node bifurcation (fold bifurcation)

Suppose the system $\dot{x} = f(x, \mu)$ has a nonhyperbolic fixed point at $(x_0, \mu_0)$. This fixed point is a saddle-node bifurcation if the system satisfies

$$
\begin{aligned}
  \frac{\partial f}{\partial\mu}(x_0, \mu_0) \neq 0
\end{aligned}\quad
\begin{aligned}
  \frac{\partial^2 f}{\partial x^2}(x_0, \mu_0) \neq 0
\end{aligned}
$$

These conditions can be found from assuming that

$$
  \frac{\partial f}{\partial \mu}(x_0, \mu_0) \neq 0
$$

then by the implicit function theorem, there exists a unique smooth funtion $\mu(x)$ with $\mu(x_0) = \mu_0$, in the neighbourhood of $(x_0, \mu_0)$ such that $f\left[x, \mu(x) \right] = 0$. Differentiating the equation wrt. $x$

$$
  0 = \frac{\mathrm{d}f}{\mathrm{d}x}\left[x, \mu(x) \right] = \frac{\partial f}{\partial x}[x, \mu(x)] + \frac{\partial f}{\partial \mu}[x, \mu(x)]\frac{\mathrm{d}\mu}{\mathrm{d}x}(x)
$$

Therefore at $(x_0, \mu_0)$ we get

$$
\begin{gather*}
  \frac{\partial f}{\partial \mu}(x_0, \mu_0) + \frac{\partial f}{\partial x}(x_0, \mu_0) \frac{\mathrm{d}\mu}{\mathrm{d}x}(x_0) = 0 \\
  \implies \frac{\mathrm{d}\mu}{\mathrm{d}x}(x_0) = -\frac{\frac{\partial f}{\partial x}(x_0, \mu_0)}{\frac{\partial f}{\partial \mu}(x_0, \mu_0)}
\end{gather*}
$$

Differentiating again gives

$$
\begin{align*}
  0 = \frac{\mathrm{d}^2 f}{\mathrm{d}x^2}[x, \mu(x)] &= \frac{\mathrm{d}}{\mathrm{d}x}\left( \frac{\partial f}{\partial x}[x, \mu(x)] + \frac{\partial f}{\partial \mu}[x, \mu(x)]\frac{\mathrm{d}\mu}{\mathrm{d}x}(x) \right) \\
  &= \frac{\partial^2 f}{\partial x^2}[x, \mu(x)] + 2\frac{\partial^2 f}{\partial x \partial\mu}[x, \mu(x)] \frac{\mathrm{d}\mu}{\mathrm{d}x}(x) + \frac{\partial^2 f}{\partial x^2}[x, \mu(x)]\left( \frac{\mathrm{d}\mu}{\mathrm{d}x}(x) \right)^2 \\
  &\quad + \frac{\partial f}{\partial \mu}[x, \mu(x)] \frac{\mathrm{d}^2\mu}{\mathrm{d}x^2}(x)
\end{align*}
$$

Therefore at $(x_0, \mu_0)$

$$
  \frac{\partial^2 f}{\partial x^2}(x_0, \mu_0) + \frac{\partial f}{\partial\mu}(x_0, \mu_0) \frac{\mathrm{d}^2\mu}{\mathrm{d}x^2}(x_0) = 0
$$

Since $\mu(x_0)$ is a local extrema, then $\frac{\mathrm{d}^2\mu}{\mathrm{d}x^2}(x_0) \neq 0$ as evident from the bifurcation diagram. Consequently we must have 

$$
  \frac{\partial^2 f}{\partial x^2}(x_0, \mu_0) \neq 0
$$

#### Normal form

Expanding $f(x, \mu)$ in a Taylor series about $(x_0, \mu_0)$ and applying the conditions for a saddle-node bifurcation gives

$$
\begin{align*}
  f(x, \mu) &= f(x_0, \mu_0) + (x - x_0)\left.\frac{\partial f}{\partial x}\right|_{(x_0, \mu_0)} + (\mu - \mu_0)\left.\frac{\partial f}{\partial \mu}\right|_{(x_0, \mu_0)} + \frac{1}{2!}(x - x_0)^2\left.\frac{\partial^2 f}{\partial x^2}\right|_{(x_0, \mu_0)} \\
  &\quad + (x - x_0)(\mu - \mu_0)\left.\frac{\partial^2 f}{\partial x \partial \mu}\right|_{(x_0, \mu_0)} + \frac{1}{2!}(\mu - \mu_0)^2\left.\frac{\partial^2 f}{\partial \mu^2}\right|_{(x_0, \mu_0)} + \dots \\
  &= (\mu - \mu_0)\left.\frac{\partial f}{\partial \mu}\right|_{(x_0, \mu_0)} + \frac{1}{2!}(x - x_0)^2\left.\frac{\partial^2 f}{\partial x^2}\right|_{(x_0, \mu_0)} + \dots \\
  &= \alpha(\mu - \mu_0) + \beta(x - x_0)^2 + \dots
\end{align*}
$$

Keeping the terms of lowest order in the deviations of $x$ and $\mu$ and rescaling $u \equiv \sqrt{\beta/\alpha}(x - x_0)$, $r \equiv \mu - \mu_0$ and $\tau = \alpha t$, we obtain the normal form of the saddle-node bifurcation

$$
  \frac{\mathrm{d}u}{\mathrm{d}\tau} = r + u^2
$$

The fixed points of the system is obtained by solving $0 = \frac{\mathrm{d}u}{\mathrm{d}\tau} = r + u^2$. The behaviour of the system depends on the control parameter $r$.

- For $r < 0$, the systems has two fixed points, one stable at $u_- = -\sqrt{-r}$ and one unstable at $u_+ = \sqrt{-r}$
- For $r = 0$, the two fixed points coalesce and annihilate each other, giving a half-stable fixed point at $u^* = 0$.
- For $r > 0$ there are no fixed points.

### Transcritical bifurcation

A transcritical bifurcation occurs when there is an exchange in stability between the fixed points as they cross the bifurcation point. Suppose the system $\dot{x} = f(x, \mu)$ has a nonhyperbolic fixed point at $(x_0, \mu_0)$. This fixed point is a transcritical bifurcation if the system satisfies

$$
\begin{aligned}
  \frac{\partial f}{\partial\mu}(x_0, \mu_0) = 0
\end{aligned}\quad
\begin{aligned}
  \frac{\partial^2 f}{\partial x^2}(x_0, \mu_0) \neq 0
\end{aligned}\quad
\begin{aligned}
  \frac{\partial^2 f}{\partial x \partial\mu}(x_0, \mu_0) \neq 0
\end{aligned}
$$

#### Normal form

Expanding $f(x, \mu)$ in a Taylor series about $(x_0, \mu_0)$ and applying the conditions for a transcritical bifurcation gives

$$
\begin{align*}
  f(x, \mu) &= f(x_0, \mu_0) + (x - x_0)\left.\frac{\partial f}{\partial x}\right|_{(x_0, \mu_0)} + (\mu - \mu_0)\left.\frac{\partial f}{\partial \mu}\right|_{(x_0, \mu_0)} + \frac{1}{2!}(x - x_0)^2\left.\frac{\partial^2 f}{\partial x^2}\right|_{(x_0, \mu_0)} \\
  &\quad + (x - x_0)(\mu - \mu_0)\left.\frac{\partial^2 f}{\partial x \partial \mu}\right|_{(x_0, \mu_0)} + \frac{1}{2!}(\mu - \mu_0)^2\left.\frac{\partial^2 f}{\partial \mu^2}\right|_{(x_0, \mu_0)} + \dots \\
  &= (x - x_0)(\mu - \mu_0)\left.\frac{\partial^2 f}{\partial x \partial \mu}\right|_{(x_0, \mu_0)} + \frac{1}{2!}(x - x_0)^2\left.\frac{\partial^2 f}{\partial x^2}\right|_{(x_0, \mu_0)} + \dots \\
  &= \alpha(x - x_0)(\mu - \mu_0) + \beta(x - x_0)^2 + \dots
\end{align*}
$$

Keeping the terms of lowest order in the deviations of $x$ and $\mu$ and rescaling $u \equiv \sqrt{\beta/\alpha}(x - x_0)$, $r \equiv \mu - \mu_0$ and $\tau = \alpha t$, we obtain the normal form of the transcritical bifurcation

$$
  \frac{\mathrm{d}u}{\mathrm{d}\tau} = ru - u^2 = u(r - u)
$$

The two fixed points are at $x = 0$ and $x = r$. For $r < 0$, the fixed point $x = 0$ is stable and the fixed point $x = r$ is unstable. The stability gets interchanged when $r > 0$.

### Pitchfork bifurcation

The pitchfork bifurcation is encountered in systems in which there is an overall parity symmetry $(u \mapsto -u)$. Pitchforks are divided into supercritical and subrcritical bifurcations. Suppose the system $\dot{x} = f(x, \mu)$ has a nonhyperbolic fixed point at $(x_0, \mu_0)$. This fixed point is a pitchfork bifurcation if the system satisfies

$$
\begin{aligned}
  \frac{\partial f}{\partial\mu}(x_0, \mu_0) = 0
\end{aligned}\quad
\begin{aligned}
  \frac{\partial^2 f}{\partial x^2}(x_0, \mu_0) = 0
\end{aligned}\quad
\begin{aligned}
  \frac{\partial^2 f}{\partial x \partial\mu}(x_0, \mu_0) \neq 0
\end{aligned}\quad
\begin{aligned}
  \frac{\partial^3 f}{\partial x^3}(x_0, \mu_0) \neq 0
\end{aligned}
$$

If $\frac{\partial^3 f}{\partial x^3}(x_0, \mu_0) < 0$, the system has a supercritical bifurcation. Otherwise if $\frac{\partial^3 f}{\partial x^3}(x_0, \mu_0) > 0$, the system has a subcritical bifurcation.

#### Normal form

Expanding $f(x, \mu)$ in a Taylor series about $(x_0, \mu_0)$ and applying the conditions for a pitchfork bifurcation gives

$$
\begin{align*}
  f(x, \mu) &= f(x_0, \mu_0) + (x - x_0)\left.\frac{\partial f}{\partial x}\right|_{(x_0, \mu_0)} + (\mu - \mu_0)\left.\frac{\partial f}{\partial \mu}\right|_{(x_0, \mu_0)} + \frac{1}{2!}(x - x_0)^2\left.\frac{\partial^2 f}{\partial x^2}\right|_{(x_0, \mu_0)} \\
  &\quad + (x - x_0)(\mu - \mu_0)\left.\frac{\partial^2 f}{\partial x \partial \mu}\right|_{(x_0, \mu_0)} + \frac{1}{2!}(\mu - \mu_0)^2\left.\frac{\partial^2 f}{\partial \mu^2}\right|_{(x_0, \mu_0)} + \dots + \frac{1}{3!}(x - x_0)^3 \frac{\partial^3 f}{\partial x^3} \\
  &= (x - x_0)(\mu - \mu_0)\left.\frac{\partial^2 f}{\partial x \partial \mu}\right|_{(x_0, \mu_0)} + \frac{1}{3!}(x - x_0)^3\left.\frac{\partial^3 f}{\partial x^3}\right|_{(x_0, \mu_0)} + \dots \\
  &= \alpha(x - x_0)(\mu - \mu_0) + \beta(x - x_0)^3 + \dots
\end{align*}
$$

Keeping the terms of lowest order in the deviations of $x$ and $\mu$ and rescaling $u \equiv \sqrt{\beta/\alpha}(x - x_0)$, $r \equiv \mu - \mu_0$ and $\tau = \alpha t$, we obtain the normal form of the supercritical pitchfork bifurcation

$$
  \dot{u} = ru - u^3 = u\left(u - \sqrt{r}\right)\left(u + \sqrt{r}\right)
$$

which has fixed points at $x_0 = 0$ and $x_0 = \pm \sqrt{r}$. The subcritical pitchfork is obtained by sending $u \mapsto -u$, $r \mapsto -r$ and $t \mapsto -t$. The normal form of the subcritical pitchfork bifurcation is

$$
  \dot{u} = ru + u^3
$$

### Imperfect bifurcation

While the saddle-node bifurcation is structurally stable, the transcritical and pitchfork bifurcations are not. The two latter can alter under arbitrarily small perturbations and produce new bifurcations, called imperfect bifurcations. The perturbation quantity $\varepsilon$ is known as the imperfection parameter.

A perturbed transcritical bifurcation is given by

$$
  \dot{x} = \varepsilon + \mu x - x^2
$$

The fixed points are given by

$$
  x = \frac{\mu \pm \sqrt{\mu^2 + 4\varepsilon}}{2}
$$

If $\varepsilon < 0$, the system has two distinct fixed points when $\mu \in \left(-\infty, -2\sqrt{-\varepsilon} \right)\cup\left(-2\sqrt{\varepsilon}, \infty \right)$. These two equilibrium points merge at $x_c = \mu/2$ for $\mu = \pm 2\sqrt{-\varepsilon}$ and disappear when $\mu \in \left( -2\sqrt{-\varepsilon}, 2\sqrt{-\varepsilon} \right)$. Thus for $\varepsilon < 0$, the transcritical bifurcation perturbs into two saddle-node bifurcations at $(-\sqrt{-\varepsilon}, \pm 2\sqrt{-\varepsilon}$.

If $\varepsilon > 0$, then $\mu^2 + 4\varepsilon > 0$ for all $\mu$ resulting in two nonintersecting solution curves, one stable and the other unstable.

## Two-dimensional systems

### Limit cycles

Consider a two-dimensional dynamical system of the form $\dot{\mathbf{x}}(t) = f(\mathbf{x})$. A trajectory $\mathbf{x}(t)$ solving the equation, is called closed/periodic if there exists some $t_0 > t$ such that $\mathbf{x}\left(t + t_0 \right) = \mathbf{x}(t)$ forall $t\in \R$. A closed orbit/cycle is the image of a closed trajectory. A limit cycle is a cycle which is the limit set of some other trajectory. There are three types of limit cycles

- stable that attracts all neighbouring trajectories)
- unstable that repels all neighbouring trajectories)
- semistable that attracts trajectories from one side, and repels from the other

#### Poincaré-Bendixson theorem

The Poincaré-Bendixson theorem guarantees the existence of limit cycles for two-dimensional systems. Let $\mathbf{f}: \R^2 \to\R^2$ be a $C^1$ vector field in $\R^2$ and consider the system $\dot{\mathbf{x}} = \mathbf{f}(\mathbf{x})$. Suppose $K$ is a set in $\R^2$ such that

1. $K$ is closed and bounded, ie. compact
2. the system has no fixed points in $K$
3. $K$ contains a forward trajectory of the system, ie. there exists $\mathbf{x}_0 \in K$ such that $\Phi^t (\mathbf{x}_0) \in K$ for any $t \geq 0$.

Then $\mathbf{f}$ is either itself a closed orbit or it spirals approaches a closed orbit as $t \to \infty$. In either case, the system has a non-trivial closed orbit in $K$.

### Saddle-node bifurcation

The normal form of the saddle-node bifurcation in two dimensions is the system

$$
\begin{align*}
  \dot{x} &= \mu - x^2 \\
  \dot{y} &= -y
\end{align*}
$$

with the Jacobian

$$
  J(x, y) = \begin{bmatrix} -2x & 0 \\ 0 & -1 \end{bmatrix}
$$

For $\mu > 0$, the system has two distinct fixed points $(\pm\sqrt{\mu}, 0)$ that merge when $\mu = 0$ and vanish when $\mu < 0$. To determine the stability of the system, we evaluate the eigenvalues of the Jacobian of the system.

We first consider the case $\mu > 0$. The eigenvalues of $J(\sqrt{\mu}, 0)$ are $\lambda_1 = -2\sqrt{\mu}$ and $\lambda_2 = -1$, which are real and negative, indicating that the fixed point $(\sqrt{\mu}, 0)$ is a stable node. The eigenvalues of $J(-\sqrt{\mu}, 0)$ are $\lambda_1 = 2\sqrt{\mu}$ and $\lambda_2 = -1$, which are opposite in signs. This tells that the fixed point $(-\sqrt{\mu}, 0)$ is a saddle point and therefore unstable.

For $\mu = 0$, the eigenvalues of $J(0, 0)$ are $\lambda_1 = 0$ and $\lambda_2 = -1$, showing that the fixed point $(0, 0)$ is a semistable node.

### Transcritical bifurcation

The normal form of the transcritical bifurcation in two dimensions is the system

$$
\begin{align*}
  \dot{x} &= \mu x - x^2 \\
  \dot{y} &= -y
\end{align*}
$$

with the Jacobian

$$
  J(x, y) = \begin{bmatrix} \mu - 2x & 0 \\ 0 & -1 \end{bmatrix}
$$

This system has two distinct fixed points $(0, 0)$ and $(\mu, 0)$ for $\mu \neq 0$. For $\mu = 0$ these two fixed points merge at $(0,0)$. 

The eigenvalues of $J(0,0)$ are $\lambda_1 = \mu$ and $\lambda_2 = -1$, indicating that the fixed point $(0, 0)$ is a stable node if $\mu < 0$ and a saddle point if $\mu > 0$. For $\mu = 0$, the eigenvalues of $J(\mu, 0)$ are $\lambda_1 = -\mu$ and $\lambda_2 = -1$, showing that the fixed point $(\mu, 0)$ is a stable node if $\mu > 0$ and a saddle point if $\mu < 0$.

### Pitchfork bifurcation

The normal form of the supercritical pitchfork bifurcation in two dimensions is the system

$$
\begin{align*}
  \dot{x} &= \mu x - x^3 = x\left(x - \sqrt{\mu} \right) \left(x + \sqrt{\mu} \right) \\
  \dot{y} &= -y
\end{align*}
$$

with the Jacobian

$$
  J(x, y) = \begin{bmatrix} \mu - 3x^2 & 0 \\ 0 & -1 \end{bmatrix}
$$

For $\mu < 0$, the system has a single fixed point at the origin. The eigenvalues of $J(0,0)$ are $\lambda_1 = \mu$ and $\lambda_2 = -1$, showing that the fixed point origin is a stable node. While for $\mu > 0$, the system has three fixed points $(0, 0)$ and $\left(\pm\sqrt{\mu}, 0\right)$. In this case the eigenvalues of $J(0,0)$ are opposite in signs. The fixed point $(0, 0)$ is therefore a saddle point for $\mu > 0$. The eigenvalues for $J\left(\pm\sqrt{\mu}, 0 \right)$ are $\lambda_1 = -2\mu$ and $\lambda_2 = -1$, which shows that the fixed points $\left(\pm\sqrt{\mu}, 0\right)$ are stable nodes.

### Hopf bifurcation

An Andronov-Hopf bifurcation occurs when a periodic solution or limit cycle, surrounding an equilibrium point, arises or vanishes as the control parameter $\mu$ crosses a critical value. In a differential equation a Hopf bifurcation typically occurs when a complex conjugate pair of eigenvalues of the linearized flow at a fixed point becomes purely imaginary. 

Consider the planar system

$$
  \dot{\mathbf{x}} = \mathbf{f} (\mathbf{x}; \mu) \\
  \frac{\mathrm{d}}{\mathrm{d}t}\begin{bmatrix} x(t) \\ y(t) \end{bmatrix} = \begin{bmatrix} f(x, y; \mu) \\ g(x, y; \mu) \end{bmatrix}
$$

where $\mu$ is a control parameter. Let $\mathbf{x}_0(\mu)$ be a fixed point for any value of $\mu$. The Jacobian of $\mathbf{f}$ at $\mathbf{x}_0$ is

$$
  J\left[ \mathbf{f}\left( \mathbf{x}_0 \right)\right] \equiv D\mathbf{f}\left( \mathbf{x}_0 \right) = \begin{bmatrix} f_x (\mathbf{x}_0) & f_y (\mathbf{x}_0) \\ g_x (\mathbf{x}_0) & g_y (\mathbf{x}_0) \end{bmatrix}
$$

The eigenvalues of the Jacobian are

$$
  \lambda_\pm (\mu) = \frac{\mathrm{tr}\left[ D\mathbf{f}\left( \mathbf{x}_0 \right) \right] - \sqrt{\mathrm{tr}\left[ D\mathbf{f}\left( \mathbf{x}_0 \right) \right]^2 - 4\det\left[ D\mathbf{f}\left( \mathbf{x}_0 \right) \right]}}{2}
$$

The eigenvalues become purely imaginary when

$$
\begin{aligned}
  \mathrm{tr}\left[ D\mathbf{f}\left( \mathbf{x}_0 \right) \right] = 0
\end{aligned}\quad
\begin{aligned}
  \det\left[ D\mathbf{f}\left( \mathbf{x}_0 \right) \right] > 0
\end{aligned}\quad
$$

A Hopf bifurcation occurs when the eigenvalues are purely imaginary, $\lambda_\pm = \pm i\sqrt{\det\left[ D\mathbf{f}\left( \mathbf{x}_0 \right)\right]} \equiv \pm i\omega$ and also satisfy the transversality condition

$$
  \left.\frac{\mathrm{d}}{\mathrm{d}\mu} \Re\left[ \lambda_+ (\mu) \right] \right|_{\mu = \mu_0} \neq 0
$$

for a given $\mu_0$. The linearized system about $\mathbf{x}\left(\mu_0\right)$ (first order Taylor expansion) 

$$
  \frac{\mathrm{d}\mathbf{z}}{\mathrm{d}t} = D\mathbf{f}\left(\mathbf{x}_0 \right)\mathbf{z}
$$

has a center at $\mathbf{z} = \mathbf{0}$ with linearized solution

$$
  \mathbf{z}(t) = c_1 \cos(\omega t) \mathbf{e}_x + c_1 \sin(\omega t) \mathbf{e}_y
$$

By Hopf theory, for all $\mu$ with $|\mu - \mu_0|$ sufficiently small, there exists a $T$-periodic limit cycle $\mathbf{x}_p (t; \mu)$ satisfying the planar system equation. The period $T$ is given by

$$
  \lim_{|\mu - \mu_0| \to 0} T(\mu) = \frac{2\pi}{\omega}
$$

If the Jacobian has the special form

$$
  D\mathbf{f}\left( \mathbf{x}_0 \right) = \begin{bmatrix} \mu & -\omega \\ \omega & \mu \end{bmatrix}
$$

then a third-order Taylor expansion about $\mathbf{x}_0$ yields the normal form of the Hopf bifurcation

$$
\begin{align*}
  \frac{\mathrm{d}z_1}{\mathrm{d}t} &= \left[d\mu + a\left(z_1^2 + z_2^2 \right) \right]z_1 - \left[\omega + c\mu + b\left(z_1^2 + z_2^2 \right) \right]z_2 \\
  \frac{\mathrm{d}z_2}{\mathrm{d}t} &= \left[\omega + c\mu + b\left(z_1^2 + z_2^2 \right) \right]z_1 + \left[d\mu + a\left(z_1^2 + z_2^2 \right) \right]z_2 
\end{align*}
$$

where $a-d$ are constants. Expressed in polar coordinates, the system takes the form

$$
\begin{align*}
  \frac{\mathrm{d}r}{\mathrm{d}t} &= \left(d\mu + ar^2 \right)r \\
  \frac{\mathrm{d}\theta}{\mathrm{d}t} &= \left(\omega + c\mu + br^2 \right)
\end{align*}
$$

Depending on the signs of the constants $a$ and $d$, this system posesses periodic orbits along the locus

$$
  \mu = -ar^2/d
$$

and it can be shown that 

$$
  d = \left.\frac{\mathrm{d}}{\mathrm{d}\mu} \Re\left[ \lambda_+ (\mu) \right] \right|_{\mu = \mu_0}
$$

The constant $a$ is given by

$$
\begin{align*}
  a &= \frac{1}{16}\left(f_{xxx} + f_{xyy} + g_{xxy} + g_{yyy} \right) + \frac{1}{16\omega}\left[f_{xy}\left(f_{xx} + f_{yy} \right) \right] \\
  &\quad - \frac{1}{16\omega} \left[ g_{xy}\left(g_{xx} + g_{yy} \right) + f_{xx}g_{xx} - f_{yy}g_{yy} \right]
\end{align*}
$$

where the partial derivatives are evaluated at $\mathbf{x}_0$. Collectively, the signs of $a$ and $b$ determine wheter the limit cycles are stable, causing a supercritical Hopf bifurcation, or unstable, causing a subcritical Hopf bifurcation.

#### Normal form
The normal form of a Hopf bifurcation is

$$
    \frac{\mathrm{d}z}{\mathrm{d}t} = z\left[(\mu + i) + b|z|^2 \right] \quad z, b \in \mathbb{C}
$$

where $b = \alpha + i\beta$. A supercritical Hopf bifurcation occurs if $\alpha < 0$ creating a stable limit cycle for $\mu > 0$ given by

$$
    z(t) = re^{i\omega t}
$$

where $r = \sqrt{-\mu/\alpha}$ and $\omega = 1 + \beta r^2$. 

#### Supercritical Hopf bifurcation
In Cartesian coordinates, the normal form of supercritical Hopf bifurcation reads

$$
\begin{align*}
  \dot{x} &= \mu x - y - x\left(x^2 + y^2 \right) \\
  \dot{y} &= x + \mu y - y\left(x^2 + y^2 \right)
\end{align*}
$$

In polar coordinates, the system can be written as

$$
\begin{align*}
  \dot{r} &= \mu r - r^3 \\
  \dot{\theta} &= 1
\end{align*}
$$

### Homoclinic and heteroclinic bifurcations (separatrix)

A separatrix is a phase path which separates distinct regions in the phase plane. When a separatrix joins an equilibrium point, the phase path is known as a homoclinic path. While any separatrix which joins an equilibrium point to another is called heteroclinic path. In $\R^2$ homoclinic paths are associated with saddles while heteroclinic paths are connected with hyperbolic equilibrium points such as saddle-saddle, node-saddle and spiral-saddle. Phase paths which join one saddle to itself or two distinct saddles are known as saddle connections.

# Oscillations

## Linear oscillations

Consider a second-order linear homogenous differential equation of the form

$$
  \ddot{x} + a(t)\dot{x} + b(t)x = 0
$$

where $a(t)$ and $b(t)$ are real-valued functions that are continuous on an interval $I \subset \R$, ie. $a(t), \, b(t) \in C(I)$. A solution $x(t)$ of the differential equation is said to be oscillating on $I$, if it vanishes there at least two tiems, ie. $x(t)$ has at least to roots $I$. Otherwise, $x(t)$ is called non-oscillating on $I$.

The equation can be simplified with the transformation 

$$
  x(t) = z(t) \exp{\left(-\frac{1}{2}\int_{t_0}^t a(\tau) \mathrm{d}\tau \right)} \equiv ze^\alpha(t), \, t_0, t \in I
$$

with derivatives of the transformation is

$$
\begin{align*}
  \dot{x} &= \dot{z}e^\alpha - \frac{1}{2}az e^\alpha \\
  \ddot{x} &= \ddot{z}e^\alpha - \frac{1}{2}a\dot{z} e^\alpha - \frac{1}{2}\dot{a}z e^\alpha - \frac{1}{2}a\dot{z}e^\alpha + \frac{1}{4}a^2 ze^\alpha \\
  &= \ddot{z}e^\alpha - a\dot{z} e^\alpha - \frac{1}{2}\dot{a}z e^\alpha + \frac{1}{4}a^2 ze^\alpha
\end{align*}
$$

Inserting into the differential equation gives

$$
\begin{align*}
  0 &= \ddot{z} - a\dot{z} - \frac{1}{2}\dot{a}z + \frac{1}{4}a^2 z + a \dot{z} - \frac{1}{2}a^2 z + bz \\
  &= \ddot{z} + \left(-\frac{a^2}{4} - \frac{\dot{a}}{2} + b \right)z \\
  &= \ddot{z} + p(t)z = 0, \, p \in C(I)
\end{align*}
$$

Suppose $p(t)$ is constant. If $p > 0$, the general solution is of the form

$$
  z(t) = a\cos\left(\sqrt{p}t \right) + b\sin\left(\sqrt{p}t \right) = A\sin\left(\sqrt{p}t + \delta \right)
$$

which has inifinite number of zeroes each separated by $\frac{\pi}{\sqrt{p}}$. Thus, the solution is oscillating. If $p > 0$, the general solution is given by

$$
  z(t) = ae^{\sqrt{p}t} + be^{\sqrt{p}t}
$$

which is non-oscillating.

**Theorem:** If $p(t) \leq 0 \, \forall t \in I$, then all non-trivial solutions of $\ddot{z} + p(t)z = 0$ are non-oscillating on $I$. If $p(t) > 0 \, \forall t \in I$, then all non-trivial solutions are oscillating.

Let a solution $Z(t) \neq 0$ have at least two zeroes on $I$ and let $t_0$ and $t_1$ with $t_0 < t_1$ be two of them. Suppose $Z(t)$ has no zeroes in the interval $(t_0, t_1)$. Since $Z(t)$ is continuous, it has the same sign in $(t_0, t_1)$. Without loss of generality, we assume that $Z(t) > 0$ in $(t_0, t_1)$. Thus, $Z(t)$ has a maxiumum at some $c \in (t_0, t_1)$, and consequently $\ddot{Z}(t) < 0$ in some neighbourhood of $c$. However, if $p(t) \leq 0$, then it follows that $\ddot{Z}(t) \geq 0$ in the neighbourhood of $c$, which gives a contradiction. Therefore $Z(t)$ cannot have two or more zeroes on $I$, making it non-oscillating.

### Separation of successive roots

**Sturm's separation theorem:** Let $t_0$ and $t_1$ be two successive zeroes of a of a nontrivial solution $x_1(t)$ and let $x_2(t)$ be another linearly independent solution of that equation. Then there exists exactly one zero $x_2(t)$ between $t_0$ and $t_1$, that is, the zeroes of two linearly independent solutions separate each other. It at least on solution has more than two zeroes on $I$, then all the solution are oscillating on $I$.
 
Without loss of generality, we assume that $t_0 < t_1$. Suppose that $x_2 (t)$ has no zeroes in the interval $(t_0, t_1)$. Since $x_1(t)$ and $x_2 (t)$ are linearly independent, and $x_1(t)$ has two zeroes $t_0$ and $t_1$ in $I$, then $x_2 (t)$ does not vanish at $t_0, t_1$. Then the Wronskian

$$
  W(x_1, x_2; t) = \begin{vmatrix} x_1(t) & x_2(t) \\ \dot{x}_1(t) & \dot{x}_2(t) \end{vmatrix} = x_1(t)\dot{x}_2(t) - x_2(t)\dot{x}_1(t)
$$

does not vanish on $\left[t_0, t_1 \right]$. Assuming $W(x_1, x_2; t) > 0$ on $\left[t_0, t_1 \right]$, and dividing the Wronskian by $x_2^2 (t)$

$$
  \frac{W(x_1, x_2; t)}{x_2^2 (t)} = \frac{x_1(t)\dot{x}_2(t) - x_2(t)\dot{x}_1(t)}{x_2^2 (t)} = -\frac{\mathrm{d}}{\mathrm{d}t}\left[ \frac{x_1(t)}{x_2(t)} \right]
$$

Integrating wrt $t$ from $t_0$ to $t_1$ gives

$$
\begin{align*}
  \int_{t_0}^{t_1} \frac{W(x_1, x_2; t)}{x_2^2 (t)} &= -\int_{t_0}^{t_1} \mathrm{d}\left[ \frac{x_1(t)}{x_2(t)} \right] \\
  &= -\left.\left[ \frac{x_1(t)}{x_2(t)} \right]\right|_{t_0}^{t_1} \\
  &= \frac{x_1(t_0)}{x_2(t_0)} - \frac{x_1(t_1)}{x_2(t_1)} \\
  &= 0
\end{align*}
$$

This gives a contradiction because $\frac{W(x_1, x_2; t)}{x_2^2 (t)} > 0 \, \forall t \in \left[t_0, t_1 \right]$. Therefore, $x_2(t)$ has at least one zero in the interval $(t_0, t_1)$. To prove uniqueness, let $t_2, t_3$ be two distinct zeroes of $x_2(t)$ in $(t_0, t_1)$ with $t_2 < t_3$. Since $x_1$ and $x_2$ are linearly independent, $x_1(t)$ must have at least one zero in $(t_2, t_3)$. This is a contradiction, which ensures that $x_2(t)$ has exactly one zero in $(t_0, t_1)$. 

**Sturm's comparison theorem:** Consider two equations $\ddot{y} + p(t)y = 0$ and $\ddot{z} + q(t)z = 0$ where $p(t), q(t) \in C(I)$ and $q(t) \geq p(t), \, t\in I$. Let a pair $t_0, t_1$ ($t_0 < t_1$) of successive zeroes of a nontrivial solution be such that there exists $t \in (t_0, t_1)$ such that $q(t) > p(t)$. Then any solution $z(t)$ has at least on zero between $t_0$ and $t_1$.

Let $y(t)$ be a solution of the first equation such that $y(t_0) = y(t_1) = 0$ and $y(t) > 0 \, t \in (t_0, t_1)$. Suppose there exists a solution $z(t)$ such that $z(t) > 0 \, \forall t \in (t_0, t_1)$. Multiplying the first equation by $z(t)$ and the second by $y(t)$ and then subtracting gives

$$
  \ddot{y}z - \ddot{z}y = (q - p)yz \\
  \implies \frac{\mathrm{d}}{\mathrm{d}t}\left[\dot{y}(t)z(t) - \dot{z}(t)y(t)  \right] = \left[q(t) - p(t) \right]y(t)z(t)
$$

Integrating wrt $t$ from $t_0$ to $t_1$ gives

$$
  \dot{y}(t)z(t) - \dot{z}(t)y(t) = \int_{t_0}^{t_1} \left[ q(t) - p(t) \right]y(t)z(t) \mathrm{d}t
$$

The right-hand side is positive since $y(t)$ and $z(t)$ are positive on $(t_0, t_1)$ and $q(t) \geq p(t) \, \forall t\in (t_0, t_1)$. However, the left-hand side is negative, because $\dot{y}(t_0) > 0$, $\dot{y}(t_1) < 0$ and $z(t_1) \geq 0$. This gives a contradiction.

### Distance between successive roots

Sturm's comparison theorem has importance in finding the distance between two successive zeroes of any nontrivial solution.

**Theorem:** Consider two equations $\ddot{y} + my = 0$ and $\ddot{z} + Mz = 0$ where $m = \min_{t \in \left[t_0, t_1 \right] p(t)}$ and $m = \max_{t \in \left[t_0, t_1 \right] q(t)}$, and $m < M$. Suppose $0 < m^2 \leq p(t) \leq M^2$ on $t\in [t_0, t_1] \subset I$. Then the distance $d$ between successive zeroes of any nontrivial solution is estimated as

$$
  \frac{\pi}{M} \leq d \leq \frac{\pi}{m}
$$

**Theorem of de la Vallée Poussian:** Let the coefficients $a(t)$ and $b(t)$ of $\ddot{x} + a(t)\dot{x} + b(t) = 0$ be such that 

$$
  \left| a(t) \right| \leq M_1\, \quad \left| b(t) \right| \leq M_2, \, t \in I
$$

Then the distance $d$ between two successive zereos of any nontrivial solution satisfies

$$
  d \geq \frac{\sqrt{4M_1^2 + 8M_2} - 2M_1}{M_2}
$$

## Nonlinear oscillations

### Naïve perturbation

Consider a nonlinear oscillator of the form

$$
  \ddot{x} + \Omega_0^2 x = \epsilon h(x, \dot{x})
$$

where $\epsilon$ is a parameter and $h$ is a nonlinear function. Assuming that $\epsilon$ is small the solution $x(t)$ can be expanded in a power series

$$
  x(t) = \sum_{i=0}^\infty \epsilon^i x_i = x_0 + \sum_{i=1}^\infty \equiv x_0 + \eta
$$

By inserting this perturbed solution into the oscillator equation, we can expand $h$ into its Taylor series

$$
  h(x_0 + \eta) = \sum_{i=0}^\infty \frac{h^{(n)}(x_0)}{i!}\eta^i
$$

Rearranging in powers of $\epsilon$ gives

$$
  h(x) = h(x_0) + \epsilon h'(x_0)x_1 + \epsilon^2 \left[ h'(x_0)x_2 + \frac{1}{2}h''(x_0) x_1^2 \right] + \dots
$$

Equating terms of the order in $\epsilon$, we obtain a hierachical set of equations

$$
\begin{align*}
  \ddot{x}_0 + \Omega_0^2 x_0 &= 0 \\
  \ddot{x}_1 + \Omega_0^2 x_1 &= h(x_0) \\
  \ddot{x}_2 + \Omega_0^2 x_2 &= h'(x_0)x_1 \\
  \ddot{x}_3 + \Omega_0^2 x_3 &= h'(x_0)x_2 + \frac{1}{2}h''(x_0) x_1^2 \\
  &\vdots
\end{align*}
$$

Resonant forcing terms generally appear in the RHS of each equation of the hiearchy past the first. This can be shown by defining $\theta \equiv \Omega_0 t + \varphi$, which makes $x_0 (\theta)$ an even function of $\theta$ with period $2\pi$, hence also $h(x_0)$. This allows a Fourier series expansion of $h\left[x_0(\theta) \right]$

$$
  h(A\cos\theta) = \sum_{n=0}^\infty h_n (A) \cos n\theta
$$

The $n = 1$ term leads to resonant forcing. The solution for $x_1 (t)$ is

$$
  x_1 (t) = \frac{1}{\Omega_0^2}\sum_{\overset{n=0}{n\neq 1}}{\infty} \frac{h_n (A)}{1 - n^2} \cos\left( n\Omega t + n\varphi \right) + \frac{h_1 (A)}{2\Omega}t \sin\left(\Omega_0 t + \varphi \right)
$$

which increases linearly with time so that $x(t)$ grows without bound. Such secular terms make the approximate solution break in the long-term. This perturbation method only gives good approximation up to a given time provided that $\epsilon$ is sufficiently small.

### Poincaré-Lindstedt method

The Poincaré-Lindstedt method removes terms that grow without bound by assuming $\Omega = \Omega (\epsilon)$. This allows a power series expansion of $\Omega$

$$
  \Omega^2 = a_0 + \epsilon a_1 + \epsilon^2 a_2 + \dots 
$$

Defining a dimensionless time $s \equiv \Omega t$, the oscillator equation can be written as

$$
  \Omega^2 \frac{\mathrm{d}^2 x}{\mathrm{d}s^2} + \Omega_0^2 x = \epsilon h(x)
$$

Inserting the power series expansion for $x$ and $\Omega$ yields

$$
\begin{gather*}
  \left( a_0 + \epsilon a_1 + \epsilon^2 a_2 + \dots  \right) \left( \frac{\mathrm{d}^2 x_0}{\mathrm{d}s^2} + \epsilon \frac{\mathrm{d}^2 x_1}{\mathrm{d}s^2} + \epsilon^2 \frac{\mathrm{d}^2 x_2}{\mathrm{d}s^2} \right) \\
  + \Omega_0^2 \left(x_0 + \epsilon x_1) + \epsilon^2 x_2 + \dots \right) \\
  = \epsilon h(x_0) + \epsilon^2 h'(x_0)x_1 + \epsilon^3 \left[ h'(x_0)x_2 + \frac{1}{2}h''(x_0) x_1^2 \right] + \dots
\end{gather*}
$$

Equating terms by the order in $\epsilon$ results in the hierachical set of equations

$$
\begin{align*}
  a_0 \frac{\mathrm{d}^2 x_0}{\mathrm{d}s^2} + \Omega_0^2 x_0 &= 0 \\
  a_0 \frac{\mathrm{d}^2 x_1}{\mathrm{d}s^2} + \Omega_0^2 x_1 &= h(x_0) - a_1 \frac{\mathrm{d}^2 x_0}{\mathrm{d}s^2} \\
  a_0 \frac{\mathrm{d}^2 x_2}{\mathrm{d}s^2} + \Omega_0^2 x_2 &= h'(x_0)x_1 - a_2 \frac{\mathrm{d}^2 x_0}{\mathrm{d}s^2} - a_1 \frac{\mathrm{d}^2 x_1}{\mathrm{d}s^2} \\
  &\vdots
\end{align*}
$$

The first equation of the hierachy is immediately solved by

$$
  a_0 = \Omega_0^2 \quad x_0 (s) = A\cos(s + \varphi)
$$

Inserting into the $\mathcal{O}(\epsilon)$ equation gives

$$
  {\mathrm{d}^2 x_1}{\mathrm{d}s^2} + x_1 = \Omega_0^{-2} h\left[A\cos(s + \varphi) \right] + \Omega_0^{-2}a_1 A\cos(s + \varphi)
$$

Any resonant secular terms in $h(x_0)$ can now be canceled out by choosing

$$
  a_1 = -\frac{h_1 (A)}{A}
$$

The solution for $x_1 (s)$ is

$$
  x_1 (s) = \frac{1}{\Omega_0^2} \sum_{\overset{n=0}{n\neq 1}}^\infty \frac{h_n (A)}{1 - n^2}\cos \left( ns + n\varphi \right)
$$

which is periodic and does not increase without bound. The perturbed frequency is obtained from

$$
\begin{gather*}
  \Omega^2 = \Omega_0^2 - \frac{h_1 (A)}{A} + \mathcal{O}(\epsilon^2) \\
  \implies \Omega(\epsilon) = \Omega_0 - \frac{h_1 (A)}{2A\Omega_0}\epsilon + \mathcal{O}(\epsilon^2)
\end{gather*}
$$

### Multiple time scale method

The multiple time scale method also eliminates secular terms and is applicable beyond periodic motion alone. Consider the equation

$$
  \ddot{x} + x = \epsilon h(x, \dot{x})
$$

where $\epsilon$ is presumed small and $h$ is a nonlinear function. Defining a hierachy of time scales $T_n \equiv \epsilon^n t$ we can expand

$$
  \frac{\mathrm{d}}{\mathrm{d}t} = \sum_{n=0}^\infty \epsilon^n \frac{\partial}{\partial T_n}
$$

The solution can thus be expanded into

$$
  x(t) = \sum_{n=0}^\infty \epsilon^n x_n(T_0, T_1, \dots)
$$

Inserting the expansions into the equation yields

$$
  \left( \sum_{n=0}^\infty \epsilon^n \frac{\partial}{\partial T_n} \right)^2 \left( \sum_{k=0}^\infty \epsilon^k x_k \right) + \sum_{k=0}^\infty \epsilon^k x_k = \epsilon h\left[ \sum_{k=0}^\infty \epsilon^k x_k, \sum_{n=0}^\infty \left( \sum_{k=0}^\infty \epsilon^k x_k \right) \right] 
$$

Equating terms by the order in $\epsilon$ gives the set of hierachical equations

$$
\begin{align*}
  \left( \frac{\partial^2}{\partial T_0^2} + 1 \right)x_0 &= 0 \\
  \left( \frac{\partial^2}{\partial T_0^2} + 1 \right)x_1 &= -2\frac{\partial^2 x_0}{\partial T_0 \partial T_1} + h\left(x_0, \frac{\partial x_0}{\partial T_0} \right) \\
  \left( \frac{\partial^2}{\partial T_0^2} + 1 \right)x_2 &= -2\frac{\partial^2 x_1}{\partial T_0 \partial T_1} -2\frac{\partial^2 x_0}{\partial T_0 \partial T_2} - 2\frac{\partial^2 x_0}{\partial T_1^2} \\
  &\quad + \left.\frac{\partial h}{\partial x}\right|_{(x_0, \dot{x}_0)}x_1 + \left.\frac{\partial h}{\partial x}\right|_{(x_0, \dot{x}_0)} \left( \frac{\partial x_1}{\partial T_0} + \frac{\partial x_0}{\partial T_1} \right)
\end{align*}
$$

The first equation of the hierarchy is immediately solved by

$$
  x_0 = A\cos\left( T_0 + \phi \right) \equiv A\cos\theta
$$

where $A$ and $\phi$ are arbitrary functions of $T_1, T_2, \dots$. Inserting into the $\mathcal{O}(\epsilon)$ equation gives with $\frac{\partial}{\partial T_0} = \frac{\partial}{\partial \phi}$

$$
  \left(\frac{\partial^2}{\partial \theta^2} + 1 \right)x_1 = 2\frac{\partial A}{\partial T_1}\sin\theta + 2A\frac{\partial\phi}{\partial T_1} \cos\theta + h(A\cos\theta, -A\sin\theta)
$$

Since the arguments of $h$ are periodic, $h$ can be expanded in a Fourier series

$$
  h(A\cos\theta, -A\sin\theta) = \sum_{k=1}^\infty \alpha_k (A) \sin k\theta + \sum_{k=0}^\infty \beta_k (A) \cos k\theta
$$

where the coefficients are given by

$$
\begin{align*}
  \alpha_k (A) &= \frac{1}{\pi} \int_0^{2\pi} h(\theta)\sin (k\theta) \mathrm{d}\theta \\
  \beta_0 (A) &= \frac{1}{2\pi} \int_0^{2\pi} h(\theta) \mathrm{d}\theta \\
  \beta_k (A) &= \frac{1}{\pi} \int_0^{2\pi} h(\theta)\cos (k\theta) \mathrm{d}\theta
\end{align*}
$$

In order to eliminate the secular terms of $x_1(t)$, ie. those terms proportional to $\cos\theta$ and $\sin\theta$, we require

$$
\begin{align*}
  2\frac{\partial A}{\partial T_1} + \alpha_1 (A) &= 0 \\
  2A \frac{\partial\phi}{\partial T_1} + \beta_1 &= 0
\end{align*}
$$

These two equations require two initial conditions. With the secular terms eliminated, we may solve for $x_1$

$$
  x_1(t) = \sum_{k\neq 1}^\infty \left[ \frac{\alpha_k (A)}{1 - k^2}\sin(k\theta) + \frac{\beta_k (A)}{1 - k^2}\cos(k\theta) \right] + C_0 \cos\theta + D_0 \sin\theta
$$

### Liénard system

The Liénard equation models nonlinear oscillating circuits with the second-order differential equation 

$$
  \ddot{x} + f(x)\dot{x} + g(x) = 0
$$

The equation can be transformed into a two-dimensional system of first-order differential equations by defining

$$
\begin{gather*}
  F(x) := \int_0^x f(\xi) \mathrm{d}\xi \\
  x := x \\
  y := \dot{x} + F(x)
\end{gather*}
$$

resulting in

$$
\begin{align*}
  \dot{x} &= y - F(x) \\
  \dot{y} &= -g(x)
\end{align*}
$$

Liénard proved the existence and uniqueness of a stable limit cycle of the system under certain conditions on $f$ and $g$

**Liénard theorem:** Under the assumption that 
- $F, g \in C^1(\R)$
- $F$ and $g$ are odd functions of $x$ (hence $f$ is even)
- $xg(x) > 0$ for $x \neq 0$
- $F(0) = 0$, $F'(0) < 0$ and $F$ has a single positive zero at $x = \alpha$ and increases monotonically, ie $\lim_{x\to\infty} F(x) = \lim_{x\to\infty} \int_0^x f(\xi)\mathrm{d}\xi = \infty$

it follows that the Liénard system has a unique stable limit cycle about the origin.

### Van der Pol oscillator

The van der Pol oscillator is a non-conservative oscillator with non-linear damping expressed by

$$
    \ddot{x} - \mu(1 - x^2) \dot{x} + x = 0
$$

The van der Pol equation is a special case of the Liénard equation with $f(x) = \mu(x^2 - 1)$ and $g(x) = x$, which satisfy the conditions of the Liénard theorem. Checking the conditions for $F$

$$
\begin{align*}
  F(x) &= \int_0^x f(u)\mathrm{d}u = \int_0^x \mu (u^2 - 1) \mathrm{d}u \\
  &= \mu \left[ \frac{u^3}{3} - u \right]_0^x = \mu \left(\frac{x^3}{3} - x \right) \\
  &= \frac{1}{3}\mu x(x^2 - 3)
\end{align*}
$$

The function $F$ is odd and has exactly one positive zero at $x = \sqrt{3}$. Moreover, $F(x) < 0$ for $0 < x < \sqrt{3}$ and $F(x)$ is positive and monotonically increasing for $x > \sqrt{3}$. By Liénard's theorem, the van der Pol equation has a unique stable limit provided $\mu > 0$. The van der Pol equation can be rewritten as

$$
  \ddot{x} - \mu(1 - x^2) \dot{x} + x = 0 = \frac{\mathrm{d}}{\mathrm{d}x}\left[ \dot{x} + \mu\left( \frac{x^3}{3} - x \right) \right]
$$

applying the Liénard transformation $y = \frac{\dot{x}}{\mu} + F(x)$ with $F(x) = \frac{x^3}{3} - x$, the van der Pol equation can be expressed as a two dimensional system

$$
\begin{align*}
  \dot{x} &= \mu\left[y - F(x) \right] \\
  \dot{y} &= \frac{x}{\mu}
\end{align*}
$$

Dividing the $\dot{y}$ equation by the $\dot{x}$ one yields

$$
\begin{gather*}
  \frac{\dot{y}}{\dot{x}} = \frac{\mathrm{d}y}{\mathrm{d}x} = \frac{x^2}{\mu\left[y - F(x) \right]} \\
  \left[y - F(x) \right] \frac{\mathrm{d}y}{\mathrm{d}x} = -\frac{x}{\mu^2}
\end{gather*}
$$

Alternatively, applying the transformation $y = \dot{x}$ leads to

$$
\begin{align*}
  \dot{x} &= y \\
  \dot{y} &= \mu\left(1 - x^2 \right) - x
\end{align*}
$$

#### Relaxation oscillations

Relaxation oscillations is a periodic phenomenon in which a slow build-up is followed by a fast discharge. For large $\mu$, the van der Pol oscillator exhibits such behaviour. In the regime $\mu \gg 1$, the van der Pol system evolves fast in the $x$ direction and slow in the $y$ direction, until rapidly achieving $y \approx f(x)$. Starting from an arbitrary initial condition, the $y$ dynamics are slow so that the system will flow rapidly in $x$ direction until it approaches the curve $y = F(x)$. Assuming $y \approx F(x)$ then

$$
  \dot{y} = -\frac{x}{\mu} \approx F'(x)\dot{x} \implies \dot{x} \approx -\frac{x}{\mu F'(x)}
$$

so that in the region $x \in [-b, a)$ the system flows slowly along $y = F(x)$. After reaching a point where $F'(x) = 0$, the motion can no longer follow the curve $y = F(x)$ as $\dot{y} > 0$. The motion thus proceeds quickly to $x = b$, after which it once agains slows along $y = F(x)$ in the region $x \in (a, b)$. The relaxation period $T$ along the limit cycle $C$ is given by.

$$
  T = -\mu \int_{C} \frac{\mathrm{d}y}{x} = -2\mu \int_b^a \frac{\mathrm{d}y}{x} + 2\mu \int_{-a}^b \frac{\mathrm{d}y}{x}
$$

In the slow region $y = F(x)$ so that the first integral is

$$
  2\mu \int_a^b \frac{\mathrm{d}y}{x} = 2\mu \int_a^b \frac{f(x)}{x}\mathrm{d}x =  2\mu \int_2^{-1} \frac{-1 + x^2}{x^2} \mathrm{d}x  = \mu(3 - 2\ln 2)
$$

With $\left[y - F(x) \right] \frac{\mathrm{d}y}{\mathrm{d}x} = -\frac{x}{\mu^2}$, the second integral becomes

$$
  2\mu \int_{-a}^b \frac{\mathrm{d}y}{x} = \frac{2}{\mu}\int_{-a}^b \frac{x}{y - F(x)} \mathrm{d}x
$$

It can be found $y \sim \mathcal{O}(\mu^{2/3}) \implies y - F(x) \sim \mathcal{O}(\mu^{-1/3})$. Hence, the integral must be of order $\mathcal{O}(\mu^{-1/3})$. The relaxation period for the van der Pol oscillator is thus

$$
  T = \mu(3 - 2\ln 2) + \mathcal{O}(\mu^{-1/3}), \quad \mu \gg 1
$$

#### Multiple time scale approximation

The van der Pol equation can be expressed as

$$
  \ddot{x} + \mu \dot{x} + x = \ddot{x} + \mu h(x, \dot{x})) + x = 0
$$

Applying the multi-time scale method on the van der Pol equation, and plugging the zeroth order solution $x_0 = A\cos(t + \phi)$

$$
\begin{align*}
  h\left( x_0, \frac{\partial x_0}{\partial T_0} \right) &= \left(1 - A^2 \cos^2 \theta \right)\left(- A \sin \theta \right) \\
  &= \left(-A + \frac{A^3}{4} \right)\sin\theta + \frac{A^3}{4}\sin 3\theta
\end{align*}
$$

with $\theta \equiv t + \phi$. Thus $\alpha_1 = -A + \frac{A^3}{4}$ and $\beta_1 = 0$ which gives $\phi = \phi_0$ and

$$
  2\frac{\partial A}{\partial T_1} = A - \frac{A^3}{4}
$$

which is easily integrated

$$
\begin{align*}
  \mathrm{d}T_1 = -\frac{8 \mathrm{d}A}{A(A^2 - 4)} = \left(\frac{2}{A} - \frac{1}{A - 1} - \frac{1}{A + 2} \right)\mathrm{d}A = d\ln\left( \frac{A}{A^2 - 4} \right) \\
  \implies A(T_1) = \frac{2}{\sqrt{1 - \left( 1 - \frac{4}{A_0^2} \right)e^{-\mu t}}}
\end{align*}
$$

thus

$$
  x_0 (t) = \frac{\cos\left(t + \phi_0 \right)}{\sqrt{1 - \left( 1 - \frac{4}{A_0^2} \right)e^{-\mu t}}}
$$

This behaviour describes the approach to the limit cycle $2\cos(t + \phi_0)$. Eliminating the secular terms gives

$$
  x_1 (t) = -\frac{1}{32}A^3 \sin 3\theta = -\frac{\frac{1}{4}\sin\left(3t + 3\phi_0 \right)}{\left[ 1 - \left(1 - \frac{4}{A_0^2}e^{-\mu t} \right) \right]^{3/2}}
$$

# Discrete systems (maps)

## Logistic map


The logistic map is a polynomial difference equation of the form

$$
  x_{n+1} = rx_n (1 - x_n)
$$

## Newton-Raphson method

Newton's method is an algorithm for approximating the roots of a differentiable function $f:\R\to\R$. For an initial guess $x_0$, the intersection at $xi$ of the tangent at $f(x_0)$ and $x$-axis is

$$
  x_1 = x_0 - \frac{f\left(x_0 \right)}{f'\left(x_0 \right)}
$$

The algorithm gives rise to a recurrence equation

$$
  x_{n+1} = x_n - \frac{f\left(x_n \right)}{f'\left(x_n \right)}
$$

In terms of the Newton function

$$
  N_{f}(x) = x - \frac{f(x)}{f'(x)}
$$

we may write

$$
  x_n = N_f^n (x_0)
$$

Newton's fixed point theorem states that a $r$ is a root of multiplicity $k > 0$ for $f$ iff $r$ is a fixed point of $N_f$. Such a fixed point is always attracting. This ensures that the Newton-Raphson method converges to any root of $f$.

Suppose that $f(r) = 0$, but $f'(r) \neq 0$ meaning that the root $r$ has multiplicity $1$ (simple root). Then we have $N_f (r) = r$ so that $r$ is a fixed point of $N$. Conversely, if $N(r) = 0$ we must also have $f(r) 0$.

To see that $r$ is an attracting fixed point, we find the derivative $N_f$ using the quotient rule

$$
  N'(x) = \frac{f(x)f''(x)}{\left[f'(x) \right]^2}
$$

Evidently, $N'(r) = 0 < 1$ so that $r$ is a attracting point by definition.

If $f'(r) = 0$, suppose that $r$ has multiplicity $k > 1$. Thus we may write

$$
  f(x) = (x - r)^k G(x)
$$

Taking the first and second order derivatives

$$
\begin{gather*}
  f'(x) = k(x - r)^{k-1} G(x) + (x - r)^k G'(x) \\
  f''(x) = k(k-1)(x - 1)^{k-2} G(x) + 2k(x - r)^{k-1} G'(x) + (x-r)^k G''(x)
\end{gather*}
$$

The Newton function $N_f$ can thus be written as

$$
  N_f(x) = x - \frac{(x - r)G(x)}{kG(x) + (x - r)G'(x)}
$$

Hence $N_f(r) = r$, so that $r$ is a fixed point of $N_f$. The derivative of $N_f$ becomes

$$
  N_f'(x) = \frac{k(k-1)G(x)^2 + 2k(x - r)G(x)G'(x) + (x - r)^2 G(x) G''(x)}{k^2 G(x)^2 + 2k(x - r)G(x)G'(x) + (x - r)^2 + (x - r)^2 G'(x)^2}
$$

Since $G(r) \neq 0$ we have $N_f'(r) = \frac{k - 1}{k} < 1$. Thus, $r$ is an attracting fixed point of $N_f$.

If $N_f'(r) = 0$, then Newton's method will converge quadratically. This can be shown by taking the Taylor series expansion with remainder of $N_f$ about $r$

$$
\begin{gather*}
  N_f(x_n) = N_f(r) + N_f'(r)(x_n - r) + N_f''(c) \frac{\left(x_n - r \right)^2}{2!} \\
  N_f (x_n) - N_f(r) = N_f'' (c) \frac{\left( x_n - r \right)^2}{2} \\
  x_{n+1} - r = \frac{N_f''(c)}{2} \left( x_n - r \right)^2
\end{gather*}
$$

where $c$ is in between $x$ and $r$. Taking the limit as $n \to \infty$ we get

$$
  \lim_{n\to\infty} = \frac{\left| x_{n+1} - r  \right|}{\left| x_n - r \right|^2} = \frac{\left| N_f''(c) \right|}{2}
$$

which shows that $x_n$ converges quadratically to $r$.

If $N_f'(r) \neq 0$, then Newton's method converges linearly. If $f$ has a root $r$ with multiplicity $k > 1$, then we have by Taylor series expansion

$$
\begin{gather*}
  N_f(x_n) = N_f (r) + N'(c) (x_n - r) \\
  x_{n+1} - r = N'(c) (x_n - r)
\end{gather*}
$$

Taking the limit as $n \to \infty$ we get

$$
  \lim_{n \to \infty} \frac{\left| x_{n+1} - r \right|}{\left| x_n - r \right|} = \left| N_f'(c) \right|
$$

which shows that $x_n$ converges to $r$ linearly.

### Modified Newton's method

For functions with roots $r$ of multiplicity $k > 1$, quadratic convergence can be achieved by multiplying the second term of the Newton function by $k$

$$
  N_f(x) = x - k\frac{f(x)}{f'(x)}
$$

Since $f(x) = (x - r)^k G(x)$, the Newton function can be rewritten as

$$
  N_f(x) = x - \frac{k(x - r)G(x)}{kG(x) + (x - r)G'(x)}
$$

Taking the derivative

$$
  N_f' (x) = 1 - \frac{k\left[kG + (x-r)G' \right]\left[G 0 (x-r)G' \right] - \left[ k(x-r)G \right]\left[ kG' + G' + (x-r)G'' \right]}{\left[ kG + (x - r)G' \right]^2}
$$

we get

$$
  N'(r) = 1 - \frac{k^2 G(r)^2}{k^2 G(r)^2} = 0
$$

which shows the modified Newton's method converges quadratically.

### Basins of attraction

For functions with multiple roots, the choise of initial value $x_0$ determines which roots Newton's method converges to. Each root has a corresponding basin of attraction. If $r$ is a root $f(x)$, the basin of attraction of $r$, is the set of all initial numbers $x_0$ such that Newton's method converges to $r$

$$
  B(r) = \Set{ x_0 | x_n = N_f^n (x_0) \textrm{ converges to } r }
$$

### Complex basins of attraction

Arthur Cayley discovered the basins of attraction for Newton's method applied to complex quadratic polynomials.

**Cayley's theorem:** Let the complex quadratic polynomial $f(z) = az^2 + bz + c$ have roots $\alpha$ and $\beta$ in the complex plane. Let $L$ be the perpendicular bisector of the line segment from $\alpha$ to $\beta$. When Newton's method is applied to $f(z)$, the half-planes into which $L$ divides the complex plane, are exactly $B(\alpha)$ and $B(\beta)$.

### Root multiplicity

A root $r$ of the equation $f(x) = 0$ has multiplicity $k$ if $f(r) = f'(r) \dots = f^{(k-1)'} = 0$ while $f^{(k)'} \neq 0$. If $r$ is a root of multiplicity $k$ for $f(x)$, then $f$ can be written as

$$
  f(x) = (x - r)^k G(x), \quad G(r) \neq 0
$$

This can be shown by considering the Taylor series expansion of $f$ about $r$:

$$
\begin{align*}
  f(x) &= \sum_{n=0}^\infty \frac{f^{(n)'}(r)}{n!}(x - r)^n \\
  &= \sum_{n=k}^\infty \frac{f^{(n)'}(r)}{n!}(x - r)^n \\
  &= (x - r)^k \sum_{n=k}^\infty \frac{f^{(n)'}(r)}{n!}(x - r)^{n-k} \\ 
  &= (x - r)^k G(x)
\end{align*}
$$

### Power convergence

If a sequence $p_n$ converges to $p$ with $p_n \neq p$, and if there are positive constants $\lambda$ and $\alpha$ such that

$$
  \lim_{n\to\infty} \frac{\left| p_{n+1} - p \right|}{\left| p_n - p \right|^\alpha} = \lambda
$$

then $p_n$ converges to $p$ on the order of $\alpha$.

# Fractals

Fractal geometry describe shapes retaining a roughness at different scales due to fractal dimensions. The term was coined by the Polish-French-American mathematician Benoit Mandelbrot (1924-2010) in 1975.

## Self-similar fractals

Construction of self-similar fractals can be related with the affine transformation $T(x) = a_i x + b_i: \R^n \to\R^n$. The self-similarity is analogous to a real geometric series $\sum_{i=0}^\infty a^i$ with $|a| < 1$. The sum of a finite number of $n \in \N$ successive terms, $S_n = \sum_{i=0}^n a^i$ is given by

$$
   S_n - aS_n = 1 - a^{n+1} \\
   \implies S_n = \frac{1 - a^{n+1}}{1 - a} \overset{n\to\infty}{\longrightarrow} \frac{1}{1 - a}
$$

The geometric series can be expressed recursively as

$$
\begin{align*}
    \sum_{i=0}^\infty a^i &= 1 + a + a^2 + \dots \\
    &= 1 + a\left( 1 + a + a^2 + \dots \right) \\ 
    &= 1 + a\sum_{i=0}^\infty a^i
\end{align*}
$$

Which represents self-similarity of the geometric series. This self-similarity relation does not hold for finite terms as

$$
    1 + aS_n = 1 + a\sum_{i=0}^n a^i = 1 + \sum_{i=0}^n a^{i+1} = S_{n+1} \neq S_n
$$

### Cantor set

The Cantor set is constructed as follows
1. start with the unit interval $[0, 1]$
2. remove its open middle third, ie. remove $(\frac{1}{3}, \frac{2}{3})$
3. repeat the process for the remaining closed intervals

Each stage in the construction of the Cantor set creates $m = 2$ copies scaled down by a factor of $r = 3$. Hence the self-similar dimension is

$$
    d = \frac{\ln{m}}{\ln{r}} = \frac{\ln{2}}{\ln{3}} \approx 0.63
$$

The $n$th stage of the Cantor set has length $\left( \frac{2}{3} \right)^n$. The Cantor set can alternatively be constructed by the affine transformations

$$
\begin{aligned}
    T_1: x \mapsto \frac{x}{3}
\end{aligned}\quad
\begin{aligned}
    T_2: x \mapsto \frac{x}{3} + \frac{2}{3}
\end{aligned}
$$

### Cantor dust

### von Koch curve

The Koch curve is constructed iteratively as follows
1. start with a line segment of length $L_0$
2. divide the line segment into three equal line segments of length $\frac{L_0}{3}$
3. replace the middle line segment with an equilateral triangle without base

Each stage of in the construction the Koch curve creates $m = 4$ copies scaled down by a factor of $r = 3$. Hence the self-similar dimension is

$$
  d = \frac{\ln{m}}{\ln{r}} = \frac{\ln{4}}{\ln{3}} \approx 1.26
$$

At the $n$th stage, the Koch curve $S_n$ contains $4^n$ line segments, each of length $\frac{L_0}{3^n}$. The length of $S_n$ is given by

$$
  L_n = L_0 \left( \frac{4}{3} \right)^n \overset{n \to \infty}{\longrightarrow} \infty
$$

Hence the length of the Koch curve is infinite.

### Sierpińsky triangle

The Sierpińsky triangle is constructed iteratively as follows
1. start with a solid equilateral triangle of unit side length
2. divide the triangle into four smaller equal equilateral triangles using the midpoints the triangle vertices
3. remove the interior of the middle triangle
4. repeat this process in each of the three remaining solid equilateral triangles

Each stage in the construction of the Sierpińsky triangle creates $m = 3$ scaled down by a factor of $r = 2$. Hence the self-similar dimension is

$$
    d = \frac{\ln{m}}{\ln{r}} = \frac{\ln{3}}{\ln{2}} \approx 1.58
$$

At the $n$th stage, a Sierpińsky triangle $S_n$ is covered by $3^n$ equilateral triangles with each side of length $\frac{1}{2^n}$. The area of $S_n$ is given by

$$
    3^n\frac{\sqrt{3}}{4}\left( \frac{1}{2} \right)^{2n} = \frac{\sqrt{3}}{4} \left( \frac{3}{4} \right)^n \overset{n \to \infty}{\longrightarrow} 0
$$

Hence the area of the Sierpińsky triangle vanishes.

### Sierpińsky carpet

The Sierpińsky carpet is constructed as follows
1. start with a solid square of unit side length
2. divide the square into nine smaller equal squares
3. repeat this process in each of the remaining eight squares

At the $n$th stage, the Sierpińsky carpet $S_n$ is covered by $8^n$ squares of length $\frac{1}{3^n}$.

### Fatou and Julia sets

Let $f(z) = \frac{p(z)}{q(z)}: \hat{\mathbb{C}}\to\hat{\mathbb{C}}$ be a rational map for some complex polynomials $p$ and $q$. Here $\hat{\mathbb{C}}:= \mathbb{C} \cup \infty$ is the Riemann sphere. Let $\mathcal{F}$ be the family consisting of $f$ and all its iterates, ie. $\mathcal{F}:= \Set{ f_n : n \in \N }$. The Fatou set $F(f)$ is the maximal open subset of $\hat{\mathbb{C}}$ on which $\mathcal{F}$ is normal, meaning that every sequence $f_n \in \mathcal{F}$ contains a subsequence that converges uniformly on compact subsets of $F(f)$. The Julia set $J(f)$ is the complement of the Fatou set, $J(f) := \hat{\mathbb{C}}\backslash F(f)$.

Thus, $z \in F(f)$ iff $\mathcal{F}$ is normal in som neighbourhood of $z$. Equivalently, $z \in J(f)$ iff no infinite subsequence of $f_n$ converges uniformly on $\bar{U}$ for any neighbourhood $U$ of $z$.

Properties of the Fatou and Julia sets
- $F(f)$ and $J(f)$ are completely invariant
- $J(f)$ is an uncountable compact set containing no isolated points
- $J(f)$ is the minimal closed completely invariant subset with at least 3 points
- $F(f)$ is the closure of attracting and superattracting periodic orbits, 
    - every attracting and superattracting periodic orbit is in $F(f)$
- $J(f)$ is the closure of the set of repelling periodic orbits, ie. $J(f)$ is a dynamic repeller
    - every repelling periodic orbit is in $J(f)$
- $J(f)$ is either totally connected or disconnected

A set $X \subset \hat{\mathbb{C}}$ is completely invariant if $f(X) = X$ and $f^{-1}(X) = X$. Such sets can be formed by grand orbits. The grand orbit of $x \in \hat{\mathbb{C}}$, denoted $[x]$ is the set $y \in \hat{\mathbb{C}}$ for which there are integers $n$ and $m$ with $f_n (x) = f_m (y)$. Equivalently, if we let $\sim$ be the smallest equivalent relation for which $z \sim f(z) \, \forall z\in\hat{\mathbb{C}}$, then the grand orbit of $x$ is the equivalence class of $x$ under $\sim$. Evidently $X \subset \hat{\mathbb{C}}$ is completely invariant iff it is a union of grand orbits. Moreover, if $X$ is completely invariant, then so is its closure $\bar{X}$ and its derived set $X'$.

A periodic orbit of order $n$ is a finite set of distinct points $z_i$ so that $f(z_i) = z_{i+1}$ for all $i = 0, \dots, n-1$ taken mod $n$. For each $i$ the chain rule gives

$$
  f_n'(z_i) = \prod_{j=0}^{n-1} f'(z_j) \equiv \mu
$$

A periodic orbit is called
1. superattracting if $\mu = 0$, equivalently if some $z_i$ in the orbit is critical for $f$.
2. attracting if $0 < |\mu| < 1$
3. indifferent if $|\mu| = 1$; these are further distinguished into
    a. rationally indifferent if $\mu$ is a root of unity
    b. irrationally indifferent if $\mu$ is not a root of unity
4. repelling if $|\mu| > 1$

#### Quadratic polynomials

Julia sets have been extensively studied for complex quadratic polynomials of the form $f(z) = z^2 + c$ where $c$ is complex parameter. Iterations $f_n$ form bounded or unbounded seqences. The filled Julia set (prisoner set) is the set of all initial values $z_0$, for which iterations always remain in a bounded region, ie. $K_c (f) = \Set{ z_0 \in \mathbb{C} : \, \forall n \in \N, \, |f_n(z_0)| < \infty }$. More rigorously, for a fixed $R > 0$ large enough that $R^2 - R \geq |c|$, the filled Julia set for $f$ is the subset of the complex plane given by

$$
    K_c (f ) = \Set{ z \in \mathbb{C}: \, \forall n \in \N, \, |f_n (z) | \leq R }
$$

The Julia set $J(f)$ is the boundary of $K_c (f)$, ie. $J(f) = \partial K_c (f)$. The parameter plane of $f$, that is, the plane of $c$ values forming Julia sets, gives rise to the Mandelbrot set, which is defined as the set of all $c$ such that $J(f)$ is connected. For all parameters outside the Mandelbrot set, the Julia set is a Cantor space, commonly referred to as Fatou dust. 

### Mandelbrot set

The Mandelbrot set formed from the quadratic polynomial $f_c(z) = z^2 + c$ is the set of complex numbers $c$ for which the map

$$
  z_{n + 1} = z_n^2 + c \quad n \in \N
$$

remains bounded as $n \to \infty$. Equivalently, the Mandelbrot set is the set of $c$ for which the corresponding Julia set $J(f_c)$ is connected, ie. $M = \Set{ c \in \mathbb{C}: J(f_c) \textrm{ is connected} }$

consists of those values of $c \in \mathbb{C}$ for w

## Fractal dimension

Fractal dimension describes the space-filling property of an object and gives a quantitative measure of self-similarity of an object. It gives the number of smaller pieces obtained when an object is looked at a smaller length scale or finer resolution. Moreover, it measures correlations between small and large pieces and also among the smaller pieces themselves.

### Similar dimension

Self-similar fractals are made up of scale-down copies of themselves. For a self-similar fractal with dimension $d$, the number of copies $m$ obtained from down-scaling by a factor $r$ is $m = r^d$. Thus, the self-similar dimension is given by

$$
  d = \frac{\ln{m}}{\ln{r}}
$$

### Box dimension

The box dimension useful for finding an approximate dimension of non-similar fractals. It is obtained by covering the object in boxes of length $\epsilon$. The number of boxes $N(\epsilon)$ depends on the scale $\epsilon$ and the dimension of the fractal $d$ through the relation $N(\epsilon) \propto \frac{1}{\epsilon^d}$. Assuming, the fractal is made up of shapes of unitary length, then $N(\epsilon) = \frac{1}{\epsilon^d}$. Taking the logarithm on both sides gives

$$
\begin{gather*}
  \ln N(\epsilon) = -d \ln\epsilon \\
  \implies d = - \frac{\ln N(\epsilon)}{\ln\epsilon} = \frac{\ln N(\epsilon)}{\ln(1/\epsilon)}
\end{gather*}
$$

The box dimension is defined as the limit of $d$ as $\epsilon \to 0$ (if it exists)

$$
  d = \lim_{\epsilon\to 0} \frac{\ln N(\epsilon)}{\ln(1/\epsilon)}
$$

### Hausdorff dimension

Hausdorff dimension generalizes the concept of box dimension in that the Fractal sets are covered in arbitrary shapes. Given a fractal set $F \subset \R^n$, then for any $\delta > 0$ we can define the Hausdorff measure of $F$

$$
  H_\delta^s (F) = \inf\Set{ \sum_{i=1}^\infty |N_i|^s: \bigcup_{i=1}^\infty N_i \subset F, \, |N_i| < \delta  }
$$

where $|N_i|$ is the diameter of the non-empty set $N_i \subset \R^n$. The diameter for any non-empty subset $N \subset \R^n$ is defined as

$$
  |N| = \sup\Set{ |x - y|: x,y \in N }
$$

As $\delta \to 0$, the class of possible covers of $F$ shrinks while the Hausdorff measure increases and approaches a limiting value defining the $s$-dimensional Hausdorff measure of $F$

$$
    H^s(F) = \lim_{\delta \to 0} H_\delta^s (F)
$$

Given a mapping $f: F \to\R^n$ such that $|f(x) - f(y)| \leq c|x - y|^\alpha\, c, \alpha > 0$, then for each $s$ Hölder's inequality gives $H^{s/\alpha}\left[f(F) \right] \leq c^{s/\alpha}H^{s/\alpha}$.

The Hausdorff dimension $d_H (F)$ is defined as the critical value of $s$ below which the Hausdorff measure vanishes and above which it is infinity

$$
  d_H (F) = \inf\Set{ s \geq 0: H^s (F) = 0 } \equiv \sup\Set{s \geq 0 | H^s (F) = \infty}
$$

Properties of Hausdorff dimension
1. If $F \subset \R^n$ is open then $d_H (F) = n$, since $F$ contains a ball of positive $n$-dimensional volume.
2. If $F$ is smooth, that is a continuously differentiable $m$-dimensional surface of $\R^n$, then $d_H (F) = m$.
3. If $F \subset G$, then $d_H (F) < d_H (G)$.
4. If $F$ is countable, then $d_H (F) = 0$.
5. If $d_H (F) < 1$, then $F$ is totally disconnected.

### Topological dimension

Topological dimension describes the way points of an object are connecte to each other. The topological dimension always assumes an integer value. In this regard, the topological dimension for some common structures are

- edges: 1
- surfaces: 2
- volumes: 3

Fractals can be defined as an object with a fractal dimension exceeding its topological dimension. Thus, two topologically same structures may be fractally different. When the fractal dimension exceeds the topological dimension, the fractal object covers more space than the underlying topological space.

#### Covering dimension

The covering dimension of an object is obtained by determining the least number of sets, which may overlap, required to cover all parts of the object. If each point of the object is covered by $ \leq n$ sets, then the covering dimension is $d = n - 1$

#### Iterative dimension

An $n$ dimensional space has borders of dimension $n - 1$. The iterative dimension is the number of times the border of a space can be iterated down a dimension untill reaching a point with zero dimension. 

### Embedding dimension

The embedding dimension describes the dimension of the space that contains the fractal object.

## Strange attractor

Let $A \subset X$ be a closed, invariant set. Then the action of $f: X \to X$ on $A$ is called a chaotic set or an attractor under $f$ iff it is sensitively dependent on initial conditions, topologically transitive, and the set of all periodic points is dense in $A$. A dynamical system has an attractor if there exists $A \subset \R^n$ such that for almost all initial state and for infinitely large time $t$, the orbit is close to som point of $A$. A strange attractor is locally the product of a two-dimensional manifold by a Cantor set inside which the trajectories of a dynamical moves erratically and are highly sensitive to the initial condition.

The geometry of a strange attractor resembles the infinite complex surface and cannot be represented in Euclidean geometry with integral dimensions. Geometrically, strange attractors are fractals and dynamically they are chaotic. Strange attractors are generated by iteration of a map or the solution of a system of initial-value differential equations that exhibit chaos.

### Baker's transformation

The dissipative baker's transformation is the simplest planar dynamical system and acts as a model of strange attractors. The attractor generated by the baker map has Cantor set structure and has therefore fractal properties. The baker map is defined as

$$
  y_{n + 1} = \begin{cases} \epsilon y_n, \quad & 0 \leq x_n < \frac{1}{2} \\ \epsilon y_n + \frac{1}{2}, \quad & \frac{1}{2} \leq x_n \leq 1 \end{cases}
$$

where $\epsilon > 0$ with the Bernoulli shift $x_{n + 1}= 2x_n\mod 1$. The map is called baker's tranformation as it resembles the process of constantly stretching and doubling the dough. The attractor for the baker's transformation is a unit square $[0, 1] \times [0, 1]$. For each iteration, the total area of the attractor is reduced by a factor $2\epsilon$. In the $n$th iteration the unit square is contracted onto $2^n$ rectangles each of side length $l_n = \epsilon^n$ with a gap of at least $\left( \frac{1}{2} - \epsilon \right)\epsilon^{1 - n}$. The fractal dimension is the sum of $d_x$ and $d_y$ where $d_x = 1$ follows from the Bernoulli shift when $x_0$ is irrational, while $d_y = \frac{\ln 2}{|\ln\epsilon|}$. The fractal dimension is therefore $1 < d \leq 2$.

Baker's transformation has sensitive dependence on initial conditions and the periodic points of the transformation are dense in the generated fractal. The generated fractal is the product of $[0, 1]$ and the uniform Cantor set, which is formed by continuously replacing intervals $I$ by a pair of subintervals of lengths $\epsilon|I|$.

### Hénon map

$$
  H_{a,b} : (x, y) \mapsto (1 - ax^2 + y, bx)
$$

### Lorenz attractor

The Lorenz system is a simplified three-dimensional model of convenction rolls in atmospheric flow.

$$
\begin{align*}
  \dot{x} &= \sigma(y - x) \\
  \dot{y} &= x(r - z) - y \\
  \dot{z} &= xy - b z
\end{align*}
$$

The Lorenz model describes the motion of incompressible fluid in a planar horizontal cell that is heated from below. The temperature difference between the boundary layers of the fluid causes Rayleigh-Bérnard convections. Because of the planar geometry, the fluid flow can be considered two dimensional represented in the $xz$-plane. 

To determine the criteria for Rayleigh-Bérnard convection, we examine the stability conditions nonconvecting flow. In stable conditions, a small packet of fluid that is displaced upwards $\Delta z$ is subject to linear temperature difference

$$
  \Delta T = \frac{T_{\mathrm{w}} - T_{\mathrm{c}}}{h}\Delta z = \frac{\delta T}{h}\Delta z
$$

where $h$ is the height of the fluid cell and $T_{\mathrm{w}}, T_{\mathrm{c}}$ are the temperatures of the lower (warm) and upper (cold) boundaries, respectively. The rate of temperature change is described by the heat equation $\frac{\partial T}{\partial t} = \kappa \nabla^2 T$ where $\kappa$ is a diffusion constant. For small displacements the Laplacian can be approximated as

$$
  \nabla^2 T \approx \frac{\delta T}{h^2}\frac{\Delta z}{h}
$$

We may define a thermal relaxation time $\tau_{\Delta T}$ such that

$$
  \tau_{\Delta T} \frac{\partial T}{\partial t} = \tau_{\Delta T} \kappa \nabla^2 T = \Delta T
$$

Using the approximation for the Laplacian we get $\tau_{\Delta T} = \frac{h^2}{\kappa}$. 

The small packet of fluid gets subject to a bouyant force, which is proportional to the difference in density between the packet and its surroundings. This difference itself is proportional to the thermal expansion coefficient $\alpha$, which gives the relative change in density per unit temperature change, and the temperature difference $\Delta T$. The bouyant force can thus be expressed as

$$
  \mathbf{F}_{\mathrm{b}} = \alpha\rho g\Delta T\hat{\mathbf{z}} =  \alpha\rho g \frac{\delta T}{h}\hat{\mathbf{z}}
$$

where $g$ is the gravitational constant. In stable conditions, the boyant force is less than the viscous fluid force, which is given by the product of the viscosity $\mu$ and the Laplacian of the vertical velocity component $v_z$

$$
  \mathbf{F}_{\mathrm{b}} = -\mu \nabla^2 v_z \hat{\mathbf{z}} \approx -\mu\frac{v_z}{h^2}\hat{\mathbf{z}}
$$
In equilbrium the vertical speed $v_z$ is given by

$$
  v_z = \frac{\alpha \rho_0 g h \delta T}{\mu}
$$

and the displacement time $\tau_{\Delta z} = \frac{\Delta z}{v_z}$ becomes

$$
  \tau_{\Delta z} = \frac{\mu}{\alpha\rho_0 g h\delta T}
$$

The nonconvecting state is stable if the thermal diffusion time $\tau_{\Delta T}$ is less than the corresponding displacement time $\tau_{\Delta z}$. If the thermal diffusion time is longer, the bouyant force dominates and the fluid packet will keep accelerating upwards causing convection. This stability condition is governed by the Rayleigh number

$$
  R = \frac{\tau_{\Delta T}}{\tau_{\Delta z}} = \frac{\alpha\rho_0 g h^3\delta T}{\kappa\mu}
$$

The fluid flow is described by the Navier-Stokes equations

$$
  \frac{\partial \mathbf{v}}{\partial t} + \mathbf{v} \cdot \nabla \mathbf{v} = -\frac{1}{\rho_0}\nabla p + \mu \nabla^2 \mathbf{v} - \frac{1}{\rho}\mathbf{F}
$$

where
- $\mathbf{v} = v_x\hat{\mathbf{x}} + v_z\hat{\mathbf{z}}$ is the fluid velocity
- $p$ is the pressure of the fluid
- $\nu$ is the kinematic viscosity
- $\rho$ is the fluid density
- $\mathbf{F}$ are external forces

The temperature is given by the thermal diffusion equation

$$
  \frac{\partial T}{\partial t} + \mathbf{v}\cdot\nabla T = \kappa\nabla^2 T
$$

where $\kappa$ is the diffusion constant. In a convecting state the temperature deviates from the linear behaviour in a nonconvecting state

$$
  T(x, z, t) = T_{\mathrm{nc}} - T_{\mathrm{w}} + \frac{z}{h}\delta T
$$

where $T_{\mathrm{nc}} = T_w - \frac{z}{h}\delta T$ is the linear temperature behaviour. The variation in density with temperature, $\rho(T)$ can be approximated with a first order Taylor series expansion about $T_w$:

$$
  \rho(T) \approx \rho_0 + \frac{\mathrm{d}\rho}{\mathrm{d}T}(T - T_w)
$$

In terms of the temperature deviation function and the thermal expansion coefficient defined as $\alpha = -\frac{1}{\rho_0}\frac{\mathrm{d}\rho}{\mathrm{d}T}$, the density temperature variation takes the form

$$
  \rho(T) = \rho_0 - \alpha\rho_0\left[-\frac{z}{h}\delta T + T(x, z, t) \right]
$$

The Boussinesq approximation of the Navier-Stokes equations ignores density variation in all terms except the ones that involve gravitational forces. This approximation reduces the $z$ equation to

$$
\begin{align*}
  \rho\frac{\partial v_z}{\partial t} + \rho_0\mathbf{v}\nabla v_z &= -\rho_0 g - \alpha\rho_0\frac{z}{h}\delta T - \frac{\partial p}{\partial z} \\
  &\quad + \alpha g\rho_0 T(x, z, t) + \mu\nabla^2 v_z
\end{align*}
$$

In nonconvecting state, the first three terms add to $0$ and we may introduce an effective pressure gradient

$$
\begin{align*}
  p' &= p + \rho_0 g z + \alpha\rho_0 \frac{z^2}{2}\frac{\delta T}{h} \\
  \frac{\partial p'}{\partial z} &= \frac{\partial p}{\partial z} + \rho_0 g + \alpha\rho_0\frac{z}{h}\delta T
\end{align*}
$$

which equals $0$ when fluid motion is present. In terms of $p'$, the Navier stokes equations become

$$
\begin{align*}
  \frac{\partial v_x}{\partial t} + \mathbf{v}\cdot\nabla v_x &= -\frac{1}{\rho_0}\frac{\partial p'}{\partial z} + \nu\nabla^2 v_z\\
  \frac{\partial v_z}{\partial t} + \mathbf{v}\cdot\nabla v_z &= -\frac{1}{\rho_0}\frac{\partial p'}{\partial z} + \alpha g T + \nu\nabla^2 v_z
\end{align*}
$$

where $\nu = \frac{\mu}{\rho_0}$ is the kinematic viscosity. The Navier-Stokes equations can be made dimensionless by introducing the dimensionless variables

$$
\begin{align*}
  t' &= \frac{\kappa}{h^2}t \\
  x' &= \frac{x}{h} \quad v_x' \frac{\mathrm{d}x'}{\mathrm{d}t'} = \frac{\kappa}{h}v_x \\
  z' &= \frac{z}{h} \quad v_z' \frac{\mathrm{d}z'}{\mathrm{d}t'} = \frac{\kappa}{h}v_z \\
  T' &= \frac{T}{\delta T}
\end{align*}
$$

and the Laplacian $\nabla'^2 = h^2\nabla^2$. Substituting into the Navier-Stokes equations and multiplying with $\frac{h^3}{\nu\kappa}$ gives (dropping the ' for clarity)

$$
\begin{align*}
  \frac{1}{\sigma}\left[ \frac{\partial v_x}{\partial t} + \mathbf{v}\cdot\nabla v_x \right] &= -\frac{\partial \pi}{\partial x} + \nabla^2 v_x \\
  \frac{1}{\sigma}\left[ \frac{\partial v_z}{\partial t} + \mathbf{v}\cdot\nabla v_z \right] &= -\frac{\partial \pi}{\partial z} + RT + \nabla^2 v_z
\end{align*}
$$

where
- $\sigma = \frac{\nu}{\kappa}$ is the Prandtl number
- $R = \frac{\alpha g h^3}{\nu\kappa}\delta T$ is the Rayleigh number
- $\pi = \frac{p' h^2}{\nu\rho_0 \kappa}$ is a dimensionless pressure variable

Two-dimensional fluid flows can be represented by a stream function $\psi(x, y, t)$ satisfying

$$
  v_x = -\frac{\partial\psi}{\partial z} \quad v_z = \frac{\partial\psi}{\partial x}
$$

Inserting into the dimensionles Navier-Stokes equations gives

$$
\begin{align*}
  \frac{1}{\sigma}\left[\frac{\partial^2\psi}{\partial t\partial z} - \frac{\partial\psi}{\partial z}\frac{\partial^2\psi}{\partial x \partial z} + \frac{\partial\psi}{\partial x}\frac{\partial^2\psi}{\partial z \partial x} \right] &= -\frac{\partial\pi}{\partial x} + \nabla^2\frac{\partial\psi}{\partial z} \\
  \frac{1}{\sigma}\left[\frac{\partial^2\psi}{\partial t\partial x} + \frac{\partial\psi}{\partial z}\frac{\partial^2\psi}{\partial x^2} - \frac{\partial\psi}{\partial x}\frac{\partial^2\psi}{\partial z \partial x} \right] &= -\frac{\partial\pi}{\partial z} + RT + \nabla^2\frac{\partial\psi}{\partial x}
\end{align*}
$$

To eleminate the pressure flow we take $\partial/\partial x$ of the $z$-equation and substract $\partial/\partial z$ of the $x$-equation

$$
\begin{align*}
  R\frac{\partial T}{\partial x} + \nabla^4\psi &= \frac{1}{\sigma}\left[\frac{\partial}{\partial t}(\nabla^2\psi)\right. \\
  &\quad - \frac{\partial}{\partial z}\left(\frac{\partial\psi}{\partial z}\frac{\partial^2\psi}{\partial x\partial z} - \frac{\partial\psi}{\partial x}\frac{\partial^2 \psi}{\partial z^2} \right) \\
  &\quad - \left.\frac{\partial}{\partial x}\left(\frac{\partial\psi}{\partial z}\frac{\partial^2\psi}{\partial x^2} - \frac{\partial\psi}{\partial x}\frac{\partial^2 \psi}{\partial z^2}  \right) \right]
\end{align*}
$$

In terms of $\psi$, the thermal diffusion equation becomes

$$
  \frac{\partial T}{\partial T} - \frac{\partial\psi}{\partial z}\frac{\partial T}{\partial x} + \frac{\partial\psi}{\partial x}\frac{\partial T}{\partial z} - \frac{\partial\psi}{\partial x} = \nabla^2 T
$$

The planar geometry suggests that $\psi$ and $T$ take the form of a Fourier series

$$
\begin{align*}
  \psi(x, z, t) &= \sum_{m,n\in\Z} a_{n, m}\exp(in\pi z)\exp(in\pi x) \\
  T(x, z, t) &= \sum_{m,n\in\Z} b_{n, m}\exp(in\pi z)\exp(in\pi x) 
\end{align*}
$$

The Fourier series can be truncated to finite series by applying appropriate boundary condition, known as the Galerkin procedure. The temperature deviation function must vanish at the lower and upper boundary surfaces, i.e. 
$T = 0$ for $z=0$ and $z=1$. For the streamfunction, we assume that the vertical velocity vanishes at the boundary surfaces. Additionally, we assume that we can neglect any shear forces at the boundaries. These forces are proportional to the gradient of the tangential velocity component. This condiction translates into having $\frac{\partial v_x}{\partial z} = 0$ at $z=0$ and $z=1$. These boundary conditions are satisfied by a single term expansion for $\psi$ and a two term expansion for $T$

$$
\begin{align*}
  \psi(x, z, t) &= \psi(t)\sin(\pi z)\cos(ax) \\
  T(x, z, t) &= T_1 (t)\sin(\psi z)\cos(ax) - T_2(t)\sin(2\pi z)
\end{align*}
$$

Inserting $\psi$ into the dimensionless Navier-Stokes equation and simplifying leaves

$$
\begin{align*}
  -\frac{\mathrm{d}\psi T}{\mathrm{d}t} (a^2 + \pi^2)\sin(\pi z)\sin(ax) &= -\sigma R T_1 (t)\sin(\pi z)\sin(ax) \\
  &\quad + \sigma(a^2 + \pi^2)^2\psi(t)\sin(\pi z)\sin(a x)
\end{align*}
$$

This equation only holds for all values of $x$ and $z$ if

$$
  \frac{\mathrm{d}\psi(t)}{\mathrm{d}t} = \frac{\sigma R}{\pi^2 + a^2} T_1 (t) - \sigma(\pi^2 + a^2)\psi(t)
$$

Inserting $T$ into the thermal diffusion equation gives

$$
\begin{align*}
  \dot{T}_1\sin(\pi z)\cos(a x) &- \left(4\pi^2 T_2 + \dot{T}_2\right) \sin(2\pi x) \\
  &+ \left[\left( \pi^2 + a^2 \right)T_1 - a\psi\right] \sin(\pi z)\cos(ax) \\
  &= -\left[\pi\psi\cos(\pi z)\sin(ax)\right]\left[aT_1\sin(\pi z)\sin(ax)\right] \\
  &\quad - \left[a \psi\sin(\pi z)\cos(ax)\right]\left[\pi T_1\cos(\pi z)\cos(ax)\right] \\
  &\quad + \left[\psi\sin(\pi z)\cos(ax) \right]\left[2\pi T_2 \cos(2\pi z) \right]
\end{align*}
$$

Factoring out and equating coefficients of the term results in

$$
\begin{align*}
  \dot{T_1} &= a\psi - \left(\psi^2 + a^2\right)T_1 - \pi a\psi T_2 \\
  \dot{T_2} &= \frac{\pi a}{2}\psi T_1 - 4\pi^2 T_2
\end{align*}
$$

To arrive at the standard form of the Lorenz equations, we introduce the variables $\tilde{t} = (\pi^2 + a^2)t'$ and $b = \frac{4\pi^2}{a^2 + \pi^2}$, and make the following substitution

$$
\begin{align*}
  X(t) &= \frac{a\pi}{(pi^2 + a^2)\sqrt{2}}\psi(t) \\
  Y(t) &= \frac{r\pi}{\sqrt{2}}T_1 (t) \\
  Z(t) &= \pi t T_2 (t)
\end{align*}
$$

where $r = \frac{a^2}{(a^2+\pi^2)^3}R$ is the reduced Rayleigh number. Applying all the substitutions we finally arrive at the Lorenz equations

$$
\begin{align*}
  \dot{X} &= \sigma(Y - X) \\
  \dot{Y} &= rX - XZ - Y \\
  \dot{Z} &= XY - bZ
\end{align*}
$$

### Rössler attractor

$$
\begin{align*}
  \dot{x} &= -y - z \\
  \dot{y} &= x + ay \\
  \dot{z} &= b + xz - cz
\end{align*}
$$