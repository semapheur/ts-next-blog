---
title: 'Geometric Algebra'
subject: 'Mathematics'
showToc: true
references:
  - book_vaz_roldao_2016
---

# Alternators and exterior algebras

## Permutations and alternators
A permutation of the set of $p$ elements $A = \set{1,\dots,p}$ is a bijection $\sigma: A\to A$ represented by the cycle

$$
\begin{matrix}
  1 & \cdots & p \\
  \sigma(1) & \cdots & \sigma(p)
\end{matrix}
$$

The set of all permutations is group called the symmetric group, denoted $S_p$. The number of elements in $S_p$ is $p!$.

A permutation $\sigma$ of the set $\set{1,\dots,p}$ such that $\sigma(k) = k$ for all $k \neq i$, and $k \neq j$, and moreover $\sigma(i) = j$ and $\sigma(j) = i$, is called a transposition. A permutation of $n$ elements is even or odd if the permutation is obtained respectively by an even or odd number of transpositions. The sign $\varepsilon(\sigma)$ of the permutation $\sigma$ is defined to be $\varepsilon(\sigma) = 1$ if the permutation is even, and $\varepsilon(\sigma) = -1$ if the permutation is odd.

Let us consider a tensor that is either contravariant or covariant of the form $\bigotimes_{i=1}^p X_i$, where $X$ denotes respectively either a vector or a coverctor, and the indices enumerate such elements. The operator $\operatorname{alt}$ called alternator is defined in the following way

$$
  \operatorname{alt}(X_1 \otimes\cdots\otimes X_p) = \frac{1}{p!} \sum_{\sigma\in S_p} \varepsilon(\sigma) X_{\sigma(1)} \otimes\cdots\otimes X_{\sigma(p)}
$$

The alternator is a projection operator, i.e. $\operatorname{alt}^2 = \operatorname{alt}$. Considers covariant tensors of the form $\bigotimes_{i=1}^p \alpha_i$ given by

$$
  (\alpha_1 \otimes\cdots\alpha_p)(\mathbf{v}_1,\dots,\mathbf{v}_p) = \alpha_1 (\mathbf{v}_1)\cdots\alpha_p (\mathbf{v}_p)
$$

The alternator of a contravariant tensor is then defined by writing the resulting tensor acting on vectors as

$$
  \operatorname{alt}(\alpha_1 \otimes\cdots\otimes\alpha_p)(\mathbf{v}_1,\dots,\mathbf{v}_p) = \frac{1}{p!}\begin{vmatrix}
    \alpha_1 (\mathbf{v}_1) & \cdots & \alpha_1 (\mathbf{v}_p) \\
    \vdots & \ddots & \vdots \\
    \alpha_p (\mathbf{v}_1) & \cdots & \alpha_p (\mathbf{v}_p)
  \end{vmatrix}
$$

The right-hand side is the determinant of the associated matrix. If $\mathbf{A}$ is the matrix of order $p$ with entries $\mathbf{A}_{ij}$, then the determinant of $\mathbf{A}$ is defined as

$$
  \det(\mathbf{A}) = \sum_{\sigma\in S_p} \epsilon(\sigma) \mathbf{A}_{1\sigma(1)}\cdots \mathbf{A}_{p\sigma(p)}
$$

### Alternating tensors

<MathBox title='Alternating vectors and covectors' boxType='definition'>
A $p$-vector is an alternating contravariant tensor of order $p$. A $p$-vector is denoted by $\mathbf{A}_{[p]}$ and is characterized by

$$
  \mathbf{A}_{[p]} = \operatorname{alt}(\mathbf{A}_{[p]})
$$

In particular, if $\mathbf{A}_p \in \mathrm{T}_p (V)$, the space of contravariant tensors of order $p$, then $\operatorname{alt}(\mathbf{A}_p)$ is a $p$-vector, since $\operatorname{alt}(\operatorname{alt}(\mathbf{A}_p)) = \operatorname{alt}(\mathbf{A}_p)$.

A $p$-covector is an alternating covariant tensor of order $p$. It is denoted by $\Psi^{[p]}$, and is characterized by

$$
  \Psi^{[p]} = \operatorname{alt}(\Psi^{[p]})
$$

The spaces of $p$-vectors and $p$-covectors are denoted $\Lambda_p (V)$ and $\Lambda^p (V)$, respectively. In particular,
- $\Lambda^0 (V) = \Lambda_0 (V) = \R$
- $\Lambda^1 (V) = V^*$
- $\Lambda_1 (V) = V$
</MathBox>

It is common to use the term bivector for a $2$-vector, trivector for a $3$-vector, and so on (analogously for the $p$-covectors.)

<MathBox title='Bicovectors and trivectors' boxType='example'>
Let $\Psi = \Psi_{ij} \varepsilon^i \otimes \varepsilon^j \in \mathrm{T}^2 (V)$ be a covariant tensor of rank $2$. A $2$-covector $\Psi^{[2]} \in\Lambda^2 (V)$ is alternating by definitions

$$
\begin{align*}
  \Psi^{[2]} =& \frac{1}{2} \Psi_{ij} (\varepsilon^i \otimes \varepsilon^j - \varepsilon^j \otimes \varepsilon^i) \\
  =& \frac{1}{2} (\Psi_{ij} - \Psi_{ji}) \varepsilon^i \otimes \varepsilon^j
\end{align*}
$$

In this case, the inequality $\dim(V)\geq 2$ must hold since, if $\dim(V) = 1$ it follows that

$$
  \Psi^{[2]} = \frac{1}{2} \Psi_{11} (\mathbf{e}^1 \otimes \mathbf{e}^1 - \mathbf{e}^1 \otimes \mathbf{e}^1) = 0
$$

Let now $\mathbf{A} = A^{ijk} \mathbf{e}_i \otimes \mathbf{e}_j \otimes \mathbf{e}_k \in \mathrm{T}_3 (V)$ be a contravariant tensor of rank $3$. A $3$-vector $\mathbf{A}_{[3]} \in\Lambda_3 (V)$ can be written as

$$
\begin{align*}
  \mathbf{A}_{[3]} =& \frac{1}{6} A^{ijk} (\mathbf{e}_i \otimes \mathbf{e}_j \otimes \mathbf{e}_k + \mathbf{e}_j \otimes \mathbf{e}_k \otimes \mathbf{e}_i + \mathbf{e}_k \otimes \mathbf{e}_i \otimes \mathbf{e}_j \\
  & - \mathbf{e}_k \otimes \mathbf{e}_j \otimes \mathbf{e}_i - \mathbf{e}_j \otimes \mathbf{e}_i \otimes \mathbf{e}_k \mathbf{e}_i \otimes \mathbf{e}_k \otimes \mathbf{e}_j) \\
  =& \frac{1}{6} (A^{ijk} + A^{jki} + A^{kij} - A^{kji} - A^{jik} - A^{ikj}) \mathbf{e}_i \otimes \mathbf{e}_j \otimes \mathbf{e}_k
\end{align*}
$$

In this case, $\dim(V)\geq 3$, otherwise $\mathbf{A}_{[3]} = 0$.
</MathBox>

Consider the space $\mathrm{T}^p (V)$ of covariant tensors of rank $r$ with basis

$$
  B^p = \Set{\bigotimes_{k=1}^p \varepsilon^{i_k} | i_k = 1,\dots,n}
$$

where $\dim(V) = n$. A $p$-covector $\Psi^{[p]} \in\Lambda^p (V)$ can be written as $\Psi^{[p]} = \operatorname{alt}(\Psi^p)$, where $\Psi^p \in \mathrm{T}^p (V)$. To determine $\dim[\Lambda^p (V)]$, we count the basis elements of $\mathrm{T}^p (V)$ that contribute to $\Lambda^p (V)$.

The alternator $\operatorname{alt}$ annihilates all elements in $B^p$ that contain repeated indices. Thus, we must select index tuples $(i_1,\dots,i_p)$ with distinct values. There a $n$ choices for $i_1$, then $n - 1$ choices for $i_2$, continuing until $i_p$, which has $n - p + 1$ choices. This results in $n(n - 1)\cdots(n - p + 1)$ basis elements. However, these elements are not all independent in $\Lambda^p (V)$ because the alternator symmetrizes over index permutations, treating any permutation of a given set $\set{i_1,\dots,i_p}$ as the same. Since there are $p!$ such permutations, we by divide by $p!$, yielding

$$
  \dim[\Lambda^p (V)] = \dim[\Lambda_p (V)] = \frac{n(n-1)\cdots(n - p + 1)}{p!} = \frac{n!}{(n-p)! p!} = \binom{n}{p}
$$

Using the symmetry of binomial coefficients $\binom{n}{p} = \binom{n}{n - p}$, it follows that

$$
  \dim[\Lambda^p (V)] = \dim[\Lambda^{n-p} (V)]
$$

Although the spaces $\Lambda^p (V)$ and $\Lambda^{n-p} (V)$ have the same dimension are isomorphic, there is no natural isomorphism between them.

## Exterior product

<MathBox title='Exterior product (wedge product)' boxType='definition'>
Let $\mathbf{A}_{[p]} \in\Lambda_p (V)$ be a $p$-vector and let $\mathbf{B}_{[q]} \in\Lambda_q (V)$ be a $q$-vector. The exterior product (or wedge product) $\wedge: \Lambda_p (V) \to \Lambda_q (V)$ is defined as

$$
  \mathbf{A}_{[p]} \wedge \mathbf{B}_{[q]} = \operatorname{alt}(\mathbf{A}_{[p]} \otimes \mathbf{B}_{[q]}) = \frac{p!q!}{(Ã¥ + q)!} \sum_{\sigma\in S_{p,q}} (\mathbf{A}_{[p]} \otimes \mathbf{A}_{[q]})^\sigma
$$

where $S_{p,q}$ is the subset of $S_{p + q}$ containing all permutations $\sigma$ such that $\sigma(i) < \sigma(i + 1)$ if $0 < i < p$, or $p < i < p + 1$.

In particular, for vectors $\mathbf{u}, \mathbf{v}\in \Lambda_1 (V)$ we have


$$
  \mathbf{u}\wedge\mathbf{v} = \frac{1}{2} (\mathbf{u}\otimes\mathbf{v} - \mathbf{v}\otimes\mathbf{u})
$$

from which it follows that

$$
\begin{equation*}
  \mathbf{u}\wedge\mathbf{v} = -\mathbf{v}\wedge\mathbf{u}
\tag{\label{equation-1}}
\end{equation*}
$$
</MathBox>

<MathBox title='Properties of exterior product' boxType='proposition'>
Let $\mathbf{A}_{[p]} \in \Lambda_p (V)$, $\mathbf{B}_{[q]} \in \Lambda_q (V)$,  $\mathbf{C}_{[r]} \in \Lambda_r (V)$ and $a\in\Lambda_0 (V) = \R$. Then:
1. **Associativity:** $(\mathbf{A}_{[p]} \wedge \mathbf{B}_{[q]}) \wedge \mathbf{C}_{[r]} = \mathbf{A}_{[p]} \wedge (\mathbf{B}_{[q]} \wedge \mathbf{C}_{[r]})$
2. **Bilinearity:** $a\wedge\mathbf{A}_{[p]} = a\mathbf{A}_{[p]}$

In particular,

$$
  \mathbf{A}_{[p]} \wedge \mathbf{B}_{[q]} \wedge \mathbf{C}_{[r]} = \frac{p!q!r!}{(p + q + r)!} \sum_{\sigma\in S_{p,q,r}} (\mathbf{A}_{[p]} \otimes \mathbf{B}_{[q]} \otimes \mathbf{C}_{[r]})
$$

where $S_{p,q,r}\subset S_{p+q+r}$ contains all $\sigma$ such that $\sigma(i) < \sigma(i + 1)$ if
- $0 < i < p$ or $p < i < p + q$, and
- $0 < i < p + q$ or $p + q < i < p + q + r$
</MathBox>

From the associativity and bilinearity of the exterior product, we can generalize $\eqref{equation-1}$ as follows: Let $\mathbf{A}_{[p]} \in\Lambda_p (V)$ and $\mathbf{B}_{[q]} \in\Lambda_q (V)$ have the form

$$
\begin{equation}
\begin{split}
  \mathbf{A}_{[p]} =& \bigwedge_{i=1}^p \mathbf{u}_i \\
  \mathbf{B}_{[q]} =& \bigwedge_{j=1}^q \mathbf{v}_j
\end{split}
\tag{\label{equation-2}}
\end{equation}
$$

The exterior product $\mathbf{A}_{[p]} \wedge \mathbf{B}_{[q]}$ can thus be written

$$
  \mathbf{A}_{[p]} \wedge \mathbf{B}_{[q]} = \mathbf{u}_1 \wedge\cdots\wedge \mathbf{u}_p \wedge \mathbf{v}_1 \wedge\cdots\wedge \mathbf{v}_q
$$

Applying $\eqref{equation-1}$, we can reorder the right-hands side as

$$
  \mathbf{u}_1 \wedge\cdots\wedge \mathbf{u}_p \wedge \mathbf{v}_1 \wedge\cdots\wedge \mathbf{v}_q = (-1)^{pq} \mathbf{v}_1 \wedge\cdots\wedge \mathbf{v}_q \wedge \mathbf{u}_1 \wedge\cdots\wedge \mathbf{u}_p
$$

or 

$$
  \mathbf{A}_{[p]} \wedge \mathbf{B}_{[q]} = (-1)^{pq} \mathbf{B}_{[q]} \wedge \mathbf{A}_{[p]}
$$

A $p$-vector that can be written as the exterior product of a $p$ numbr of $1$-vectors, as in $\eqref{equation-2}$, is called a simple $p$-vector. In vector spaces $V$ with $\dim(V) \leq 3$, every $p$-vector is simples. For $\dim(V) \geq 4$, not all $p$-vectors are simples. For instance, let $V$ be a vector space of dimension $4$, with basis $\set{\mathbf{e}_1, \mathbf{e}_2, \mathbf{e}_3, \mathbf{e}_4}$. Let $\mathbf{A}_{[2]}$ be the $2$-vector given by $\mathbf{A}_{[2]} = \mathbf{e}_1 \wedge \mathbf{e}_2 + \mathbf{e}_3 \wedge \mathbf{e}_4$. There is no linear combination of the basis vectors $set{\mathbf{e}_i}_{i=1}^4$ that allows us to write $\mathbf{A}_{[2]} = \mathbf{v}_1 \wedge \mathbf{v}_2$.

<details>
<summary>Proof</summary>

Let $0 \neq \boldsymbol{\psi}\in\Lambda_2 (V)$. Then, $\boldsymbol{\psi}$ is simple if and only if $\boldsymbol{\psi}\wedge\boldsymbol{\psi} = 0\in\Lambda_4 (V)$. 

**Proof ($\impliedby$):**
If $\boldsymbol{\psi} = \mathbf{u}\wedge\mathbf{v}$, for $\mathbf{u},\mathbf{v}\in V$, then

$$
\begin{align*}
  \boldsymbol{\psi}\wedge\boldsymbol{\psi} =& \mathbf{u}\wedge\mathbf{v} \wedge \mathbf{u} \wedge \mathbf{v} \\
  =& \mathbf{u}\wedge\mathbf{v} \wedge(- \mathbf{v} \wedge \mathbf{u}) \\
  =& -\mathbf{u} \wedge \underbrace{(\mathbf{v} \wedge \mathbf{v})}_{=0} \wedge \mathbf{u} \\
  =& 0
\end{align*}
$$

**Proof ($\implies$):**
The converse can be shown by induction on $\dim(V)$. In the base cases $\dim(V) = 0$ or $1$, then $\Lambda_2 (V) = \set{0}$, and therefore the first case to be considered is when $\dim(V) = 2$. In this case, $\dim[\Lambda_2 (V)] = 1$, and $\mathbf{v}_1 \wedge \mathbf{v}_2$ is a non-trivial element if $\set{\mathbf{v}_1, \mathbf{v}_2}$ is a basis of $V$ and $\boldsymbol{\psi}$ is simple.

Now let us consider the case $\dim(V) = 3$. Given $0\neq \boldsymbol{\psi}\in\Lambda_2 (V)$, let us define a mapping $\mathrm{A}:V \to\Lambda_2 (V)$ by $\operatorname{A}(\mathbf{v}) = \boldsymbol{\psi}\wedge\mathbf{v}$. Since $\dim[\Lambda_3 (V)] = 1$, therefore $\dim[\ker(A)] \geq 2$. Now let $\mathbf{u}_1, \mathbf{u}_2 \in\ker(A)$ be linearly independent vectors which can be extended to a basis $\set{\mathbf{u}_1,\mathbf{u}_2,\mathbf{u}_3}$ of $V$, so we can write

$$
  \boldsymbol{\psi} = a\mathbf{u}_1 \wedge \mathbf{u}_2 + b\mathbf{u}_1 \wedge \mathbf{u}_3 + c\mathbf{u}_2 \wedge \mathbf{u}_3
$$

By definition, $\mathrm{A}(\mathbf{u}_1) = 0$, and therefore $0 = \boldsymbol{\psi}\wedge\mathbf{u}_1 = c\mathbf{u}_1 \wedge\mathbf{u}_2 \wedge\mathbf{u}_3$, which implies that $c = 0$. Similarly, $\operatorname{A}(\mathbf{u}_2) = 0$, and so $0 = \boldsymbol{\psi}\wedge\mathbf{u}_2 = -b\mathbf{u}_1 \wedge \mathbf{u}_2 \wedge \mathbf{u}_3$, which means that $b = 0$. It follows that $\boldsymbol{\psi} = a \mathbf{u}_1 \wedge \mathbf{u}_2$, which is simple.

Suppose now by induction that the assumption holds for $\dim(V) \leq n - 1$, and consider the case where $\dim(V) = n$. In terms of the basis $\set{\mathbf{v}_i}_{i=1}^n$, we can expand $\boldsymbol{\psi}$ as

$$
\begin{align*}
  \operatorname{\psi} =& \sum_{1\leq i < j}^n a_{ij} \mathbf{v}_i \wedge \mathbf{v}_j = \left( \sum_{i=1}^{n-1} a_{in} \mathbf{v}_i \right) \wedge \mathbf{v}_n 0 \sum_{1 \leq i < j}^{n-1} a_{ij} \mathbf{v}_i \wedge \mathbf{v}_j \\
  =& \mathbf{u}\wedge\mathbf{v}_n + \boldsymbol{\psi}'
\end{align*}
$$

where $U$ is the subspace generated by $\set{\mathbf{v}_i}_{i=1}^{n-1}$, $\mathbf{u}\in U$, and $\boldsymbol{\psi}' \in\Lambda_2 (U)$. Now,

$$
\begin{align*}
  0 =& \boldsymbol{\psi} \wedge\boldsymbol{\psi} = (\mathbf{u}\wedge\mathbf{v}_n + \boldsymbol{\psi}') \wedge (\mathbf{u}\wdege\mathbf{v}_n + \boldsymbol{\psi}') \\
  =& 2\mathbf{u}\wedge\boldsymbol{\psi}' \wedge \mathbf{v}_n + \boldsymbol{\psi}' \wedge \boldsymbol{\psi}'
\end{align*}
$$

but $\mathbf{v}_n$ appears neither in the expansion of $\mathbf{u}\wedge\boldsymbol{\psi}'$ nor in the expansion of $\boldsymbol{\psi}' \wedge\boldsymbol{\psi}'$, and separately we obtain $\mathbf{u}\wedge\boldsymbol{\psi}' = 0 = \boldsymbol{\psi}' \wedge \boldsymbol{\psi}'$. By induction, $0 = \boldsymbol{\psi}' \wedge \boldsymbol{\psi}'$ implies that $\boldsymbol{\psi}' = \mathbf{u}_1 \wedge \mathbf{u}_2$, and so $\mathbf{u} \wedge \mathbf{u}_1 \wedge \mathbf{u}_2 = 0$. Hence, there exists $\mu, \lambda_1, \lambda_2 \in \mathbb{F}$ such that $\mu\mathbf{u} + \lambda_2 \mathbf{u}_2 + \lambda_1 \mathbf{u}_1 = 0$. If $\mu = 0$, then $\mathbf{u}_1$ and $\mathbf{u}_2$ are linearly dependent, and therefore $\boldsymbol{\psi}' = \mathbf{u}_1 \wedge \mathbf{u}_2 = 0$, which means that $\boldsymbol{\psi} = \mathbf{u}\wedge\mathbf{v}_n$, and therefore $\boldsymbol{\psi}$ is simple. If $\mu\neq 0$, then $\mathbf{u} = -\frac{\lambda_2}{\mu}\mathbf{u}_2 - \frac{\lambda_1}{\mu}\mathbf{u}_1$ and

$$
  \boldsymbol{\psi} = -\frac{\lambda_1}{\mu}\mathbf{u}_1 \wedge \mathbf{v}_n - \frac{\lambda_2}{\mu} \mathbf{u}_2 \wedge\mathbf{v}_n + \mathbf{u}_1 \wedge \mathbf{u}_2
$$

corresponding to the tree-dimensional case, which was shown to be always simple.
</details>

## Bases

Let $V$ be an $n$-dimensional vector space with basis $B = \set{\mathbf{e}_i}_{i=1}^n$. From this basis, we can construct a basis for each one of the spaces $\Lambda_p (V)$.

Consider at first the space $\Lambda_2 (V)$ and the exterior product of the form $\mathbf{e}_i \wedge \mathbf{e}_j$. Because of the anit-commutativity of the exterior product between basis vectors, the linearly independent set of $2$-vectors is provided by

$$
\begin{align*}
  &\mathbf{e}_1 \wedge \mathbf{e}_2,\; \mathbf{e}_1 \wedge \mathbf{e}_3,\;\dots,\mathbf{e}_1 \wedge \mathbf{e}_n, \\
  &\mathbf{e}_2 \wedge \mathbf{e}_3,\; \mathbf{e}_2 \wedge \mathbf{e}_4,\;\dots,\mathbf{e}_2 \wedge \mathbf{e}_n, \\
  &\vdots \\
  &\mathbf{e}_{n-1} \wedge \mathbf{e}_n
\end{align*}
$$

Thus, there are $(n-1) + (n - 2) + \dots + 1 = n(n - 1)/2$ elemets, which is precisely $\dim[\Lambda_2 (V)]$. This set of $2$-vectors forms a basis for $\Lambda_2 (V)$. An arbitrary $2$-vector $\mathbf{A}_{[2]}$ can therefore by written as

$$
  \mathbf{A}_{[2]} = \frac{1}{2} \sum_{i,j} A^{ij} \mathbf{e}_i \wedge \mathbf{e}_j = \sum_{i < j} A^{ij} \mathbf{e}_i \wedge \mathbf{e}_j
$$

This result can be generalized for $\Lambda_p (V)$. A basis for this space consists of elements of the form $\mathbf{e}_{\mu_1} \wedge\cdots\wedge \mathbf{e}_{\mu_p}$, and the number of distinct elements is the number of $p$-combination of $n$ elements, denoted $\binom{n}{p}$. An arbitrary $p$-vector $\mathbf{A}_{[p]} \in\Lambda_p (V)$ can be written as

$$
\begin{align*}
  \mathbf{A}_{[p]} =& \frac{1}{p!} \sum_{\mu_1,\dots,\mu_p} A^{\mu_1 \cdots \mu_p} \bigwedge_{i=1}^p \mathbf{e}_{\mu_i} \\
  =& \sum_{\mu_1 <\cdots < \mu_p} A^{\mu_1 \cdots \mu_p} \bigwedge_{i=1}^p \mathbf{e}_{\mu_i}
\end{align*}
$$

## Pseudoscalars

Let $V$ be an $n$-dimensional vector space. The exterior product of $m$ vectors is $0$ whenever $m > n = \dim(V)$. To see this, consider the exterior product of $n + 1$ vectors $\bigwedge_{i=1}^{n+1} \mathbf{v}_i$. If $\dim(V) = n$, there are at most $n$ linearly independent vectors. Therefore, the $n + 1$ given vectors are necessarily linearly dependent and we can write one of those vectors as a linear combination of the others. Without loss of generality, it is possible to choose such a vector as being $\mathbf{v}_{n+1}$. If follows that $\mathbf{v}_{n+1} = \sum_{n=1}^n a^i \mathbf{v}_i$. Substituting this into the exterior product and using multilinearity and anit-commutation, we obtain

$$
\begin{align*}
  \bigwedge_{i=1}^{n+1} \mathbf{v}_i =& \left(\bigwedge_{i=1}^n \right) \wedge \left(\sum_{j=1}^n a^j \mathbf{v}_j \right) \\
  =& (-1)^{n-1} a^1 \mathbf{v}_1 \wedge \mathbf{v}_1 \wedge \mathbf{v}_2 \wedge\cdots\wedge \mathbf{v}_n \\
  &+ (-1)^{n-2} a^2 \mathbf{v}_1 \wedge \mathbf{v}_2 \wedge \mathbf{v}_2 \wedge \mathbf{v}_3 \wedge\cdots\wedge \mathbf{w}_n \\
  +\cdots+ a^n \mathbf{v}_1 \wedge\cdots\wedge \mathbf{v}_{n-1} \wedge \mathbf{v}_n \wedge \mathbf{v}_n = 0
\end{align*}
$$

Consequently, we have

$$
  \bigwedge_{i=1}^m \mathbf{v}_i = 0,\; m > n
$$

In fact, a stronger result holds:

$$
  \bigwedge_{i=1}^p \mathbf{v}_i = 0 \iff \set{\mathbf{v}_i}_{i=1}^p \text{ is linearly dependent}
$$

This implies that $\Lambda_p (V) = \set{0}$ for $p > n$. Thus, the nontrivial vector spaces that can be constructed are

$$
  \Lambda_0 (V), \Lambda_1 (V),\dots,\Lambda_{n-1} (V), \Lambda_n (V)
$$

satisfying $\Lambda_p (V) = \dim[\Lambda_{n-p} (V)]$. In particular, the space $\Lambda_n (V)$ has dimension $\binom{n}{n} = 1$. A basis for this space consists of the element $\bigwedge_{i=1}^n \mathbf{v}_i$, where $\set{\mathbf{v}_i}_{i=1}^n$ is a set of linearly independent vectors. If $B = \set{\mathbf{e}_i}_{i=1}^n$ is a basis of $V$, a natural basis for $\Lambda_n (V)$ is $\bigwedge_{i=1}^n \mathbf{e}_i$. Due to the anti-commutativity of the exterior product, any permutation of the basis vectors results in the same element up to a sign:

$$
  \bigwedge_{i=1}^n \mathbf{e}_{\sigma(i)} = \operatorname{sgn}(\sigma) \bigwedge_{i=1}^n \mathbf{e}_i
$$

Thus, the exterior product of $n$ linearly independent vectors can be written as a scalar multiple of this basis element:

$$
  \bigwedge_{i=1}^n \mathbf{v}_i = a \bigwedge_{i=1}^n \mathbf{e}_i,\; a\in\R
$$

The elements of $\Lambda_n (V)$ are called $n$-vectors, or more commonly, *pseudoscalars*.

## The exterior algebra $\Lambda(V)$

<MathBox title='Exterior algebra' boxType='definition'>
Let $V$ be an $n$-dimensional vector space. Consider the vector space $\Lambda(V)$ defined by the direct sum of the vectors spaces $\Lambda_p (V)$:

$$
  \Lambda (V) = \bigoplus_{p=0}^n \Lambda_p (V)
$$

The pair $(\Lambda(V), \wedge)$ is called the exterior algebra associated with the vector space $V$. An arbitary element $\mathbf{A}\in \Lambda(V)$ is called a multivector, and is written

$$
\begin{align*}
  \mathbf{A} =& \underbrace{a}_{\text{scalar}} + \underbrace{v^i e_i}_{\text{vector}} + \underbrace{F^{ij} e_i \Lambda e_j}_{2\text{-vector}} \\
  &+ \underbrace{T^{ijk} e_i \Lambda e_j \Lambda e_k}_{3 \text{-vector}} + \cdots + \underbrace{p e_1 \Lambda\cdots\Lambda e_n}_{n \text{-vector}} \in \Lambda(V)
\end{align*}
$$

The dimension of $\Lambda (V)$ is given by

$$
  \dim[\Lambda (V)] = \sum_{p=0}^n \dim[\Lambda_P (V)] = \sum_{p=0}^n \binom{n}{p} = 2^n
$$

We define the projection $\braket{\cdot}_p : \Lambda (V) \to \Lambda_p (V)$ such that

$$
  \braket{A}_p = \mathbf{A}_{[p]}
$$

where $\mathbf{A}_{[p]} \in \Lambda_p (V)$ is the $p$-vector component of the multivector $\mathbf{A}\in\Lambda (V)$.
</MathBox>

The operations grade involution, reversion, and conjugation, as dened in the tensor algebra, descend to the exterior algebra, since they leave the ideal invariant. The grade involution is given by

$$
  #(\mathbf{A}_{[p]}) = \widehat{\mathbf{A}}_{[p]} = (-1)^p \mathbf{A}_{[p]}
$$

For the reversion, it follows that

$$
  \widehat{\left(\bigwedge_{i=1}^p \mathbf{v}_i \right)} = \bigwedge_{i=1}^p \mathbf{v}_i
$$

which implies that

$$
  \widehat{\mathbf{A}}_{[p]} = (-1)^{p(p-1)/2} \mathbf{A}_{[p]}
$$

The conjugation is known to the be composition of the two operations

$$
  \widebar{\mathbf{A}}_{[p]} = \widetilde{\wildehat{\mathbf{A}}}_{[p]} = \widehat{\widetilde{\mathbf{A}}}_{[p]}
$$

# Clifford algebra

# Plane geometry $G(\R^2)$

Bivector basis
$$
\begin{align*}
  (e_1 e_2)^2 &= e_1 e_2 e_1 e_2 = -e_2 e_1 e_1 e_2 \\ 
  &= -e_2 e_1^2 e_2 = -e_2 e_2 = -e_2^2 \\
  &= -1
\end{align*}
$$

Geometric product
$$
\begin{align*}
  uv &= u\cdot v + u\wedge v \\ 
  &= |u|\cdot|v|\cos(\theta_{uv}) + |u|\cdot|v|\sin(\theta_{uv})I \\
  &= |u|\cdot|v|\left[\cos(\theta_{uv}) + I\sin(\theta_{uv})\right]
  &= |u|\cdot|v|e^{\theta_{uv}I}
\end{align*}
$$

## Rotations

Two reflections is a rotation 
$$
  u'' = (vw)^{-1}u(vw) = \frac{1}{|u|\cdot|v|}e^{-\theta I} u |v|\cdot|w|e^{\theta I} = e^{-\theta I}u e^{\theta I}
$$