---
title: 'Real Analysis'
subject: 'Mathematics'
showToc: true
---

# Real number line

## Neighbourhoods

A set $I \subseteq \R$ is called a neighbourhood of $x \in \R$ if there is an $\varepsilon > 0$ such that $I \supset B_\varepsilon (x)$ where $B_\varepsilon (x)$ is the $\varepsilon$-neighbourhood of $x$

$$
  B_\varepsilon (x) := (x - \varepsilon, x + \varepsilon)
$$

- $I \subseteq\R$ is called open in $\R$ if, for all $x \in I$, $I$ is a neighbourhood of $x$.
- $I \subseteq\R$ is called closed in $\R$ if $I^c := \R \backslash I$ is open
- $I \subseteq\R$ is called compact if for all sequences $\left( a_n \right)_{n \in\N}$ with $a_n \in I$ for all $n \in \N$, there is a convergent subsequence $\left( a_{n_k} \right)_{k \in \N}$ with $\lim_{k\to\infty} a_{n_k} \in M$

## Heine-Borel theorem
<MathBox title='Heine-Borel theorem' boxType='theorem'>
A set $A \subseteq \R$ is compact if and only if ($\iff$) it is closed and bounded.
<details>
<summary>Proof</summary>

Assume $A = [a, b] \subseteq \R$ is closed and bounded and let $\left( c_n \right)_{n \in \N} \subseteq [a, b]$. This implies that $\left( c_n \right)_{n \in \N}$ is bounded and by the Bolzano-Weierstrass theorem $\left( c_n \right)_{n \in \N}$ has an accumulation value $c \in \R$. Since $A = [a, b]$ is closed it follows that $c \in A$. Thus, $A$ is compact.

Assume $A$ is compact and let $\left( c_n \right)_{n \in \N} \subseteq A$ be a convergent sequence with limit $\tilde{c}\in\R$. Because $A$ is compact, then $\left( c_n \right)_{n \in \N}$ has an accumulation value $c \in A$. Due to convergence, the accumulation value must be unique, hence $c = \tilde{c} \in \R$ implying that $A$ is closed.

The boundedness of $A$ can be shown by contraposition. Assume $A$ is not bounded. Then there is a sequence $\left( c_n \right)_{n \in \N} \subseteq A$ with $\left| c_n \right| > n$ for all $n \in \N$. Such a sequence has no accumulation values, thus $A$ is not compact.
</details>
</MathBox>

# Sequences and limits

<MathBox title='Sequence' boxType='definition'>
A sequence of real numbers is a map $a: \N \rightarrow \R$ or $a: \Z^+ \rightarrow \R$ producing an infinite list of real numbers. In component form, a sequence is denoted $(a_n)_{n\in \N} \equiv (a_n)_{n=1}^\infty$.
</MathBox>

<MathBox title='Subsequence' boxType='definition'>
Let $\left(n_k \right)_{k\in\N}$ be a sequence of natural numbers that is stricty monotonically increasing, i.e. $\forall k \in \N: n_{k+1} > n_k$, then $\left( a_{n_k} \right)_{k \in\N}$ is called a subsequence of $(a_n)_{n\in \N}$.
</MathBox>

## Convergence
A sequence $(a_n)_{n\in \N}$ is called convergent to $a \in \R$ if for any $\varepsilon > 0$ there exists $N \in \N$ such that $\left| a_n - a \right| < \varepsilon$ for all $n \geq N$. If there is no such $a \in \R$, the sequence is divergent.

The convergence limit $a \in \R$ for a convergent sequence $(a_n)_{n\in \N}$ is unique and is denoted as

$$
\begin{gather*}
  \lim_{n\to\infty} a_n = a \\
  a_n \xrightarrow{n\to\infty} a
\end{gather*}
$$

<details>
<summary>Proof</summary>

Assume there are two limits $a \neq \tilde{a}$ and set $\varepsilon := \frac{1}{2}\left| a - \tilde{a} \right| > 0$, then

$$
\begin{gather*}
  \exists N \in \N \; \forall n \geq N: \; \left| a_n - a \right| < \varepsilon \\
  \exists \tilde{N} \in \N \; \forall n \geq \tilde{N}: \; \left| a_n - \tilde{a} \right| < \varepsilon
\end{gather*}
$$

For $n \geq \max(N, \tilde{N})$:

$$
  \left| a - \tilde{a} \right| = \left| a - a_n + a_n - \tilde{a} \right| \leq \left| a_n - a \right| + \left| a_n - \tilde{a} \right| <  \left| a - \tilde{a} \right|
$$

which is obviously a contradiction, hence $a = \tilde{a}$.

If $(a_n)_{n\in \N}$ is convergent with $\lim_{n\to\infty} a_n = a$, then every subsequence $\left( a_{n_k} \right)_{k\in\N}$ is convergent with $\lim_{n\to\infty} a_{n_k} = a$.
</details>

<MathBox title='Convergent sequence' boxType='example'>
The sequence $(a_n)_{n\in \N} = (\frac{1}{n})_{n\in \N}$ converges to $0$. For $\varepsilon > 0$ we choose $N \in \N$ such that $N\varepsilon > 1$. Then for $n \geq N$ we have

$$
  \left| a_n - 0 \right| = \left| a_n \right| = \frac{1}{n} \leq \frac{1}{N} < \varepsilon
$$
</MathBox>

<MathBox title='Divergent sequence' boxType='example'>
The sequence $(a_n)_{n\in \N} = ((-1)^n)_{n\in \N}$ is divergent. Assume the sequence is convergent to $a \in \R$. Then for any $\varepsilon > 0$ there is an $N \in \N$ such that $\left| a_n - a \right| < \varepsilon$. Choosing $\varepsilon = 1$ we have

$$
\begin{gather*}
  \left| a_N - a \right| = |1 - a| < \varepsilon \\
  \left| a_{N + 1} - a \right| = |(-1) - a | < \varepsilon
\end{gather*}
$$

Because $d\left(a_{n}, a_{n+1}\right) = |1 - (-1)| = 2$ we should have $|1 - a| + |(-1) - a| < 2\varepsilon < 2$. However this contradicts the triangle inequality since

$$
\begin{align*}
  2 = |1 - (-1)| &= |1 - a + a - (-1)| \\
  &\leq |1 - a| + |a - (-1)| \\
  &= |1 - a| + |(-1) - a| < 2
\end{align*}
$$
</MathBox>

### Accumulation value (partial limit)

<MathBox title='Accumulation value' boxType='definition'>
Given a sequence $\left(a_n\right)_{n\in\N}$, then a point $a \in \R$ is called an accumulation value of the sequence if there is a subsequence $\left(a_{n_k} \right)_{k\in\N}$ with $\lim_{n\to\infty} a_{n_k} = a$.
</MathBox>

## Boundedness

<MathBox title='Bounded sequence' boxType='definition'>
A sequence $(a_n)_{n\in \N}$ is called bounded if there exists a $C \in \R$ such that $\left| a_n \right| \leq C$ for all $n \in \N$. Otherwise the sequence is called unbounded. A convergent sequence is also bounded.
</MathBox>

### Bounds
For a subset $M \subseteq \R$ an element $b\in\R$ is called an upper bound for $M$ if $\forall x\in M: x \leq b$. If $b$ is an upper bound for $M$ and $b \in M$, then $b$ is called a maximal element of $M$, i.e. $\max M = b$.

Similarly an element $a\in\R$ is called a lower bound for $M$ if $\forall x\in M: x \geq a$. If $a$ is a lower bound for $M$ and $a \in M$, then $b$ is called a minimal element of $M$, i.e. $\min M = b$.

### Supremum and infimum

<MathBox title='Supremum' boxType='definition'>
For a subset $M \subseteq \R$ an element $s\in\R$ is called the supremum (lowest upper bound) of $M$ if
- $s$ is an upper bound for $M$, i.e. $\forall x \in M: x \leq s$
- $\forall \varepsilon \geq 0$ there is an $\tilde{x}\in M$ such that $s - \varepsilon < \tilde{x}$

we then write $\sup M := s$. If $M$ is not bounded from above, then $\sup M := \infty$.
</MathBox>

<MathBox title='Infimum' boxType='definition'>
An element $l\in\R$ is called the infimum of $M$ if
- $l$ is a lower bound for $M$, i.e. $\forall x \in M: x \leq s$
- $\forall \varepsilon \geq 0$ there is an $\tilde{x}\in M$ such that $l + \varepsilon > \tilde{x}$

we then write $\inf M := l$. If $M$ is not bounded from above, then $\inf M := -\infty$. 
</MathBox>

Let $\left( a_n \right)_{n\in\N}$ be a sequence of real numbers. An element $a \in \R \cup \Set{ -\infty, \infty } $ is called
- limit superior of $\left( a_n \right)_{n\in\N}$ if $a$ is the largest (improper) accumulation value of $a_n$
$$
  a = \limsup_{n\to\infty} a_n = \lim_{n \to \infty} \sup\Set{a_k | k \geq n}
$$
- limit inferior of $\left( a_n \right)_{n\in\N}$ if $a$ is the smallest (improper) accumulation value of $a_n$
$$
  a = \liminf_{n\to\infty} a_n = \lim_{n \to \infty} \inf\Set{a_k | k \geq n}
$$

### Bolzano-Weierstrass theorem

If a sequence $\left( a_n \right)_{n\in\N}$ is bounded, then it has an accumulation value (with a convergent subsequence).

<details>
<summary>Proof</summary>

The Bolzano-Weierstrass theorem can be proved using nested intervals. Let $\left( a_n \right)_{n\in\N}$ be a bounded sequence. Because $\left( a_n \right)_{n\in\N}$ is bounded, its sequence members are confined within an interval $[c_0, d_0]$. We split the interval at the mid into equally sized subintervals and take the subinterval containing infinitely many sequence members as the second interval of the sequence of nested intervals. Repeating the process we get
$$
\begin{gather*}
  [c_0, d_0] \supset [c_1, d_1] \supset [c_2, d_2] \supset \dots \\
  d_n - c_n = \frac{1}{2^n} (c_0 - d_0) \xrightarrow{n \to \infty} 0
\end{gather*}
$$

Since $\left( c_n \right)_{n\in\N}$ is monotonically increasing and bounded, while $\left( d_n \right)_{n\in\N}$ is monotonically decreasing and bounded, the two sequences are convergent and we get
$$
  \lim_{n\to \infty} \left( d_n - c_n \right) = \lim_{n\to \infty} d_n - \lim_{n \to \infty} c_n
$$

We then define a subseqence $\left( a_{n_k} \right)_{k\in\N}$ by chossing $a_{n_k} \in \left[ c_k, d_k \right]$. This implies
$$
  c_k \leq a_{n_k} \leq d_k
$$

and by the sandwich theorem $\left( a_{n_k} \right)_{k\in\N}$ is thus convergent.
</details>

## Completeness

If $\forall \varepsilon > 0$ there is an $N \in\N$ such that $\forall n, m \geq N: \left| a_n - a_m \right| < \varepsilon$, then $\left( a_n \right)_{n\in\N}$ is called a Cauchy sequence. A Cauchy sequence is equivalent to a convergent sequence.

<details>
<summary>Proof</summary>

Let $(a_n)_{n\in\N}$ be a convergent sequence with $\lim_{n\to \infty} a_n = a$. Consider $\varepsilon > 0$ and set $\varepsilon' := \frac{\varepsilon}{2}$. Since $(a_n)_{n\in\N}$ is convergent, there is $N \in \N$ such that
$$
  \forall n \geq N : \left| a_n - a \right| < \varepsilon'
$$

Therefore, for all $n, m \geq N$
$$
  \left| x_n - x_m \right| \leq \underbrace{\left| x_n - a \right|}_{<\varepsilon'} + \underbrace{\left| a - x_m \right|}_{<\varepsilon'} < 2\varepsilon' = \varepsilon
$$

Hence, $(a_n)_{n\in\N}$ is also a Cauchy sequence. The opposite implication is true by the completeness axiom stating that every Cauchy sequence is a convergent sequence.
</details>

### Dedeking completeness

If $M \subseteq \R$ is bounded from above, then $\sup M \in \R$ exists. Likewise, if $M$ is bounded from below, then $\inf M \in \R$ exists.

## Theorem on limits

Let $(a_n)_{n\in \N}$ and $(b_n)_{n\in \N}$ be convergent sequences, then the following properties hold

**Compound properties**
1. $\lim_{n\to\infty}\left( a_n + b_n \right) = \lim_{n\to\infty} a_n + \lim_{n\to\infty} b_n$
2. $\lim_{n\to\infty}\left( a_n \cdot b_n \right) = \lim_{n\to\infty} a_n \cdot \lim_{n\to\infty} b_n$
3. $\lim_{n\to\infty}\left( \frac{a_n}{b_n} \right) = \frac{\lim_{n\to\infty} a_n}{\lim_{n\to\infty} b_n}$ if $\lim_{n\to\infty} b_n \neq 0$

**Monotonicity:**
If $a_n \leq b_n$ for all $n \in\N \implies \lim_{n\to\infty} a_n \leq \lim_{n\to\infty} b_n$

<MathBox title='Sandwich theorem' boxType='theorem'>
If $a_n \leq c_n \leq b_n$ for all $n \in\N$ and $\lim_{n\to\infty} a_n = \lim_{n\to\infty} b_n \implies \left( c_n \right)_{n\in\N}$ is convergent with $\lim_{n\to\infty} c_n = \lim_{n\to\infty} a_n = \lim_{n\to\infty} b_n$
<details>
<summary>Proof</summary>

We know that 
$$
  \lim_{n\to\infty} \left(b_n - a_n \right) = \lim_{n\to\infty} b_n - \lim_{n\to\infty} a_n = 0
$$

Define
$$
  d_n := c_n - a_n \implies 0 \leq d_n \leq b_n - a_n
$$

Let $\varepsilon > 0$. Then there is an $N \in \N$ with $\forall n\geq N: \left| b_n - a_n \right| \geq \left| d_n - 0 \right| < 0$. This implies that $\left( d_n \right)_{n\in\N}$ is convergent with $\lim_{n\to\infty} d_n = 0$. Thus $\left( c_n \right)_{n\in\N} = \left( d_n + a_n \right)_{n\in\N}$ is convergent with $\lim_{n\to\infty} c_n = \lim_{n\to\infty} a_n$.
</details>
</MathBox>

# Series

Let $\left( a_k \right)_{k \in \N}$ be a sequence. The sequence $\left( S_n \right)_{n \in \N}$ given by
$$
  S_n := \sum_{k=1}^n a_k
$$

is called a series, i.e. a sequnce of partial sums. If $\left( S_n \right)_{n \in \N}$ is convergent we write
$$
  \sum_{k=1}^\infty a_k := \lim_{n\to\infty} S_n = \lim_{n\to\infty}\sum_{k=1}^n a_k
$$

If $\sum_{k=1}^\infty a_k$ and $\sum_{k=1}^\infty b_k$ are convergent then

1. $\sum_{k=1}^\infty \left(a_k + b_k \right)$ is convergent with limit $\sum_{k=1}^\infty \left(a_k + b_k \right) = \sum_{k=1}^\infty a_k + \sum_{k=1}^\infty b_k$
2. $\sum_{k=1}^\infty \lambda a_k$ is convergent for $\lambda \in \R$ with limit $\sum_{k=1}^\infty \lambda a_k = \lambda \sum_{k=1}^\infty a_k$

## Examples

<MathBox title='Example: Geometric series' boxType='example'>
A geometric sum is of the form
$$
  S_n = \sum_{k=0}^n q^k, \quad q\in\R
$$

For $q \neq 1$ we have
$$
\begin{gather*}
  (1 - q) \cdot \sum_{k=0}^n q^k = \sum_{k=0}^n q^k - \sum_{k=0}^n q^{k+1} = \sum_{k=0}^n q^k - \sum_{k=1}^{n+1} q^k = 1 - q^{n+1} = (1 - q)S_n \\
  \implies S_n = \frac{1 - q^{n+1}}{1 - q}
\end{gather*}
$$

Evidently $\left( S_n \right)_{n \in \N}$ converges if and only if $\left( q_n \right)_{n \in \N}$ converges to $0$, which is the case for $|q| < 1$:
$$
  \sum_{k=0}^\infty = \lim_{n\to\infty} S_n = \frac{1}{1 - q}, \quad |q| < 1
$$
</MathBox>

<MathBox title='Example: Harmonic series' boxType='example'>
A harmonic series is of the form
$$
  \sum_{k=1}^\infty \frac{1}{k}
$$

which diverges to infinity.
<details>
<summary>Proof</summary>

Consider the sequence $S_n = \sum_{k=1}^n \frac{1}{k}$, which is monotonically increasing. To show that $\left( S_n \right)_{n\in\N}$ is not bounded from above, we consider the subsequence
$$
  S_{2^m} = S_1 + \sum_{j=1}^m \left( S_{2^j} - S_{2^{j-1}} \right)
$$

where
$$
  S_{2^j} - S_{2^{j-1}} = \sum_{k=2^{j-1} + 1}^{2j} \frac{1}{k} > \sum_{k=2^{2-1} + 1}^{2j} \frac{1}{2^j} = 2^{j-1} \frac{1}{2^j} = \frac{1}{2}
$$

Hence
$$
  S_{2^m} > S_1 + \frac{m}{2} \xrightarrow{m\to\infty} \infty
$$
</details>
</MathBox>

## Convergence of series

<MathBox title='Cauchy criterion' boxType='proposition'>
A series $\sum_{k=1}^\infty a_k$ is convergent if and only if for all $\varepsilon > 0$ there is $N \in \N$ such that for all $n \geq m \geq N$
$$
  \left| \sum_{k=m}^n a_k \right| < \varepsilon
$$
<details>
<summary>Proof</summary>

Consider the sequence of partial sums $S_n := \sum_{k=1}^n a_k$. We know that $\left( S_n \right)_{n \in \N}$ is convergent if and only if it is a Cauchy sequence, from which it follows that $\forall \varepsilon > 0 \; \exists N \in \N \; \forall \tilde{n} \geq \tilde{m} \geq N$ we have $\left| s_{\tilde{n}} - s_{\tilde{m}} \right| < \varepsilon$. This is equivalent to
$$
  \left| S_n - S_{m-1} \right| = \left| \sum_{k=m}^n a_k \right| < \varepsilon, \quad \forall n \geq m \geq N
$$

Generally, this implies that a series $\sum_{k=1}^\infty a_k$ is convergent if the sequence $\left( a_k \right)_{k\in\N}$ converges to zero, i.e. $\lim_{k\to\infty} a_k = 0$. 

A series $\sum_{k=1}^\infty a_k$ is called absolutely convergent if $\sum_{k=1}^\infty \left| a_k \right|$ is convergent. Applying the triangle inequality and the Cauchy criterion it can be shown that absolute convergence implies ordinary convergence. Let $\sum_{k=1}^\infty \left| a_k \right|$ be convergent, i.e. $\sum_{k=1}^\infty a_k$ is absolutely convergent, then $\forall \varepsilon > 0 \; \exists N \in \N \; \forall n \geq m \geq N$ we get
$$
  \left| \sum_{k=m}^n a_k \right| \overset{\triangle}{\leq} \sum_{k=m}^n \left| a_k \right| < \varepsilon
$$

The Cauchy criterion implies that $\sum_{k=1}^\infty a_k$ is convergent.
</details>
</MathBox>

<MathBox title='Leibniz criterion' boxType='proposition'>
Let $\left( a_k \right)_{k\in\N}$ be convergent with $\lim_{k\to\infty} a_k = 0$ and monotonically decreasing. Then the series $\sum_{k=1}^\infty (-1)^k a_k$ is convergent.
<details>
<summary>Proof</summary>

Consider the sequence of partial sums $s_n := \sum_{k=1}^n (-1)^k a_k$ with $a_k \geq 0$. For even and odd $n$ we have 
$$
\begin{gather*}
  s_{2\ell + 2} - s_{2\ell} = -a_{2\ell + 1} + a_{2\ell + 2} \leq 0 \quad \textrm{(monotonically decreasing)} \\
  s_{2\ell + 3} - s_{2\ell + 1} = a_{2\ell + 2} - a_{2\ell + 3} \geq 0 \quad \textrm{(monotonically increasing)}
\end{gather*}
$$

The difference between the odd and even subsequences becomes
$$
\begin{gather*}
  s_{2\ell + 1} - s_{2\ell} = -a_{2\ell + 1} \leq 0 \\
  \implies s_3 \leq s_{2\ell + 1} \leq s_{2\ell} \leq s_2
\end{gather*}
$$

showing that the subsequences are bounded. By the monotone convergence criteria we have
$$
  \lim_{\ell\to\infty} \left( s_{2\ell + 1} - s_{2\ell} \right) = \lim_{\ell\to\infty}\left( - a_{2\ell + 1} \right) = 0
$$

Defining $s:= \lim_{\ell\to\infty} s_{2\ell + 1} = \lim_{\ell\to\infty} s_{2\ell}$ it can be shown that $s_n$ is convergent with $\lim_{n\to\infty} s_n = s$.
</details>
</MathBox>

### Comparison criterions

Let $\sum_{k=1}^\infty a_k$ be a series.

<MathBox title='Majorant criterion (convergence)' boxType='proposition'>
If there is $n_0 \in \N$ and a convergent series $\sum_{k=1}^\infty b_k$ with $b_k \geq 0$ and with $ \left| a_k \right| \leq b_k$ for all $k \geq n_0$, then $\sum_{k=1}^\infty a_k$ is absolutely convergent.
<details>
<summary>Proof</summary>

Applying the Cauchy criterion on $\sum_{k=1}^\infty b_k$ we have $\forall \varepsilon \; \exists N\geq n_0 \; \forall n\geq m \geq N$ such that
$$
  \sum_{k=m}^n  \left| a_k \right| \leq \sum_{k=m}^n b_k = \left| \sum_{k=m}^n b_k \right| < \varepsilon 
$$

The geometric series $\sum_{k=0}^\infty q^k$ with $|q| < 1$ can be used as a majorant convergent. If there is $n_0 \in \N$ and $C, q \in \R$ with $|q| < 1$ such that $\left| a_k \right| \leq Cq^k$ for all $k \geq n_0$, then $\sum_{k=1}^\infty a_k$ is absolute convergent.
</details>
</MathBox>

Let $\sum_{k=1}^\infty a_k$ be a series.

<MathBox title='Majorant criterion (divergence)' boxType='proposition'>
If there is $n_0 \in \N$ and a divergent series $\sum_{k=1}^\infty b_k$ with $b_k \geq 0$ and with $ a_k \geq b_k$ for all $k \geq n_0$, then $\sum_{k=1}^\infty a_k$ is divergent.
</MathBox>

### Ratio and root test

<MathBox title='Ratio test' boxType='proposition'>
If there is $n_0 \in \N$ and $q \in [0, 1)$ such that $a_k \neq 0$ and $\left| \frac{a_{k+1}}{a_k} \right| \leq q$ for all $k \geq n_0$, then $\sum_{k=1}^\infty a_k$ is absolutely convergent.
<details>
<summary>Proof</summary>

By induction we have 
$$
  \left| a_{k+1} \right| \leq q^{k+1-n_0} \left| a_0 \right| = \underbrace{\frac{\left| a_0 \right|}{q^{n_0}}}_{C} q^{k+1}
$$

from which it follows that $\sum_{k=1}^\infty a_k$ is absolutely convergent by the majorant criterion.
</details>
</MathBox>

<MathBox title='Ratio test' boxType='proposition'>
If there is $n_0 \in \N$ and $q \in [0, 1)$ such that $\sqrt[k]{\left| a_k \right|} \leq q$ for all $k \geq n_0$, then $\sum_{k=1}^\infty a_k$ is absolute convergent. The same also holds if $\limsup_{k\to\infty} \sqrt[k]{\left| a_k \right|} < 1$.
<details>
<summary>Proof</summary>

Immediately we have
$$
  \sqrt[k]{\left| a_k \right|} \leq q \iff \left| a_k \right| \leq q^k
$$

from which it follows that $\sum_{k=1}^\infty a_k$ is absolutely convergent by the majorant criterion.
</details>
</MathBox>

## Reordering

Let $\sum_{k=1}^\infty a_k$ be a series and $\tau: \N \to \N$ be a bijective map. Then $\sum_{k=1}^\infty a_{\tau(k)}$ is called a reordering of $\sum_{k=1}^\infty a_k$.

<MathBox title='Theorem' boxType='theorem'>
If $\sum_{k=1}^\infty a_k$ is absolutely convergent then so is $\sum_{k=1}^\infty a_{\tau(k)}$ and $\sum_{k=1}^\infty a_{\tau(k)} = \sum_{k=1}^\infty a_k$.
<details>
<summary>Proof</summary>

Applying the Cauchy criterion we have $\forall \varepsilon > 0 \; \exists N_1 \in \N \; \forall n \geq m \geq N_1$ that
$$
  \sum_{k=m}^n \left| a_k \right| < \varepsilon
$$

To show that the reordered series is absolutely convergent we note that
$$
\begin{align*}
  \left| \sum_{k=1}^\infty a_k - \sum_{k=1}^n a_{\tau(k)} \right| &= \left| \sum_{k=1}^\infty a_k - \sum_{k=1}^{N_1 - 1} a_k + \sum_{k=1}^{N_1 - 1} a_k - \sum_{k=1}^n a_{\tau(k)} \right| \\
  &\overset{\triangle}{\leq} \left| \sum_{k=1}^\infty a_k - \sum_{k=1}^{N_1 - 1} a_k \right| + \left| \sum_{k=1}^{N_1 - 1} a_k - \sum_{k=1}^n a_{\tau(k)} \right|
\end{align*}
$$

The first absolute term is easily shown to converge
$$
  \left| \sum_{k=1}^\infty a_k - \sum_{k=1}^{N_1 - 1} a_k \right| = \left| \sum_{k=N_1}^\infty a_k \right| = \lim_{n\to\infty} \left| \sum_{k=N_1}^n a_k \right| \leq \lim_{n\to\infty}  \sum_{k=N_1}^n \left| a_k \right| < \varepsilon
$$

For the second absolute term we note for some $n \geq N \in \N$ that $\Set{t(i)}_{i=1}^n \supset \Set{j}_{j=1}^{N_1 - 1}$. Thus, the series $\sum_{k=1}^{N_1 - 1} a_k$ vanishes altogether leaving
$$
  \left| \sum_{k=1}^{N_1 - 1} a_k - \sum_{k=1}^n a_{\tau(k)} \right| \leq \sum a_{\tau(k)} \leq \sum_{j=N_1}^\infty a_j < \varepsilon
$$

In summary we have $\forall \varepsilon' = 2\varepsilon > 0 \; \exists N \in \N \; \forall n \geq N$ that
$$
  \left| \sum_{k=1}^\infty a_k - \sum_{k=1}^n a_{\tau(k)} \right| < \varepsilon'
$$

showing that the reorderd series converges to the original series.
</details>
</MathBox>

## Cauchy product

The Cauchy product of two series $\sum_{k=1}^\infty a_k$ and $\sum_{k=1}^\infty b_k$ is the series $\sum_{k=1}^\infty c_k$ with 
$$
  c_k = \sum_{l=0}^k a_l b_{k-l}
$$

<MathBox title='Theorem' boxType='theorem'>
If $\sum_{k=1}^\infty a_k$ is absolutely convergent and $\sum_{k=1}^\infty b_k$ convergent, then the Cauchy product $\sum_{k=1}^\infty c_k = \left( \sum_{k=1}^\infty a_k \right)\left( \sum_{k=1}^\infty b_k \right)$ is absolutely convergent.
</MathBox>

# Continuous functions

## Sequence of functions

A real function is a map $f: I \subseteq \R \to \R$. The function is bounded if its range $f[I] = \mathrm{ran}(f) = \Set{f(x) | x \in I}$ is bounded in $\R$. Equivalently, this holds if the supremum of the functions absolute value is finite
$$
  \sup_{x\in I} \left| f(x) \right| < \infty
$$

<MathBox title='Sequence of functions' boxType='definition'>
A sequence of functions is an infinite list of functions $\left( f_n : I \to \R \right)_{n\in\N}$. For any fixed $\tilde{x}$, we get an ordinary sequence of real numbers $\left( f_n \left(\tilde{x}\right) \right)_{n\in\N}$.
</MathBox>

## Convergence for sequence of functions

<MathBox title='Pointwise converge' boxType='definition'>
A sequence of functions $\left( f_n : I \to \R \right)_{n\in\N}$ is pointwisely convergent to a function $f: I \to \R$ if 
$$
  \forall \tilde{x} \in I \; \forall \varepsilon > 0 \; \exists N \in \N \; \forall n \geq N : \left| f_n \left(\tilde{x} \right) - f\left(\tilde{x}\right) \right| < \varepsilon
$$

Meaning that  $\left( f_n \left(\tilde{x}\right) \right)_{n\in\N}$ converges to $f\left( \tilde{x} \right)$ for all $\tilde{x} \in I$.
</MathBox>

<MathBox title='Uniform converge' boxType='definition'>
A sequence of functions $\left( f_n : I \to \R \right)_{n\in\N}$ is uniformly convergent to a function $f: I \to \R$ if 
$$
  \forall \varepsilon > 0 \; \exists N \in \N \; \forall n \geq N \; \forall \tilde{x} \in I : \left| f_n \left(\tilde{x} \right) - f\left(\tilde{x}\right) \right| < \varepsilon
$$

Uniform convergence means $\lim_{n\to \infty} \| f_n - f \|_\infty = \sup_{x\in I} \left| f_n (x) - f(x) \right| = 0$. The condition for uniform convergence is more rigorous than for pointwise convergence. Thus uniform convergence guarantees pointwise convergence, and not vice versa.
</MathBox>

## Limit for functions

Let $f: I \to \R$ and consider a fixed point $x_0 \in I$. If there is $c \in \R$ and all sequences $\left( x_n \right)_{n\in\N} \subset I \backslash \Set{ x_0 }$ with $\lim_{n\to\infty} x_n = x_0$, then we have $\left( f\left(x_n\right) \right)_{n\in\N}$ is also convergent with $\lim_{n\to \infty} f\left( x_n\right) = c$. If this holds then we write
$$
\begin{align*}
  \lim_{x \to x_0} f(x) &= c & \\
  \lim_{x \nearrow x_0} f(x) &= c & \textrm{if }x_n < x_0 \;\forall n \in \N \\
  \lim_{x \searrow x_0} f(x) &= c & \textrm{if }x_n > x_0 \;\forall n \in \N
\end{align*}
$$

## Continuity

<MathBox title='Continuous function' boxType='definition'>
A function $f: I \to \R$ with $I \subseteq \R$ is called continuous at $x_0 \in I$ if
$$
  \lim_{x\to x_0} f(x) = f(x_0)
$$

or if $x_0$ is isolated in $I$, i.e. there is no sequence $\left(x_n \right)_{n \in \N} \subseteq I \backslash \Set{ x_0 }$ with $\lim_{n\to\infty} x_n = x_0$. If $f$ is continuous at all $x_0 \in I$, then $f$ is called a continuous function (on $I$).
</MathBox>

### Epsilon-delta criterion

<MathBox title='Epsilon-delta criterion for continuity' boxType='proposition'>
A function $f: I \to \R$ with $I \subseteq \R$ is called continuous at $x_0 \in I$ if and only if ($\iff$)
$$
  \exists \varepsilon > 0 \; \exists \delta > 0 \; \forall x \in I: \left| x - x_0 \right| < \delta \implies \left| f(x) - f\left(x_0\right) \right| < \varepsilon
$$
<details>
<summary>Proof</summary>

($\implies$): The right implication can be proved by contra position, assuming 
$$
  \exists \varepsilon > 0 \; \exists \delta > 0 \; \forall x \in I: \left| x - x_0 \right| < \delta \land \left| f(x) - f\left(x_0\right) \right| \geq \varepsilon
$$

Taking $\delta = \frac{1}{n}$, then for all $n\in\N$ we find $x_n \in I \backslash \Set{ x_0 }$ with $\left| x - x_0 \right| < \frac{1}{n}$ land $\left| f(x) - f\left(x_0\right) \right| \geq \varepsilon$. This shows that $f(x_n)$ does not converge to $f(x_0)$ while $\lim_{n\to \infty} x_n = x_0$. Therefore $f$ is not continuous at $x_0 \in I$, proving the implication.

($\Leftarrow$): To prove the left implication, choose a sequence $\left( x_n \right)_{n\in\N} \subseteq I \backslash \Set{ x_0 }$ with limit $x_0$. Let $\varepsilon >0$ and take $\delta > 0$, then there is $N \in\N$ such that for all $n \geq N$ we have $\left| x_n - x_0 \right| < 0$. Since by assumption, we have $\left| f\left(x_n\right) - f\left(x_0 \right) \right| < \varepsilon$ it follows that $f$ is continuous at $x_0 \in I$.
</details>
</MathBox>

### Properties of continuous functions

Continuity is preserved under addition, multiplication and division. Let $f: I \to \R$ and $g: I \to \R$ be continuous at $x_0 \in I$, then the following combinations are also continuous at $x_0$

- $f + g: I \to \R$
- $f \cdot g: I \to \R$
- $\frac{f}{g}$ if $g(x_0) \neq 0$

Continuity is also preserved under composition. Let $f: I \to \R$ and $g: J \to \R$ with $g[J] \subseteq I \subseteq \R$. If $g$ is continuous at $x_0 \in J$, i.e. $\lim_{n\to\infty} g\left(x_n \right) = x_0$, and $f$ is continuous at $g(x_0) \in I$, then the composition $f \circ g: J \to \R$ is continuous at $x_0 \in J$.

<details>
<summary>Proof</summary>

Choose a sequence $\left(x_n \right)_{n\in\N} \subseteq J\backslash \Set{ x_0 }$ with limit $x_0$, then
$$
  \lim_{n\to\infty}(f\circ g) \left(x_n\right) = \lim_{n\to\infty} f\left( g\left(x_n\right) \right) = f\left( \lim_{n\to\infty} g\left(x_n \right) \right) = f\left( g\left( \lim_{n\to\infty} x_n \right) \right) = (f\circ g) \left(x_0\right)
$$
</details>

<MathBox title='Theorem' boxType='theorem'>
Let $f: I \to \R$ be continuous and $I\subseteq \R$ compact (bounded and closed), then $f[I] \subseteq \R$ is compact, and there are $x^+, x^- \in I$ with

$$
\begin{gather*}
    f\left(x^+\right) = \sup\Set{ f(x) | x\in I } \\
    f\left(x^-\right) = \inf\Set{ f(x) | x\in I }
\end{gather*}
$$
<details>
<summary>Proof</summary>

Let $\left(y_n \right)_{n\in\N} \subseteq f[I]$ be a sequence. For each $y_n$ there is $x_n \in I$ with $f(x_n) = y_n$, producing a sequence $\left(x_n \right)_{n\in\N} \subseteq I$. Because $I$ is compact, there is a subsequence $\left(x_{n_k} \right)_{k\in\N}$ that is convergent with $x:= \lim_{k\to\infty} x_{n_k} \in I$, and we get
$$
  \lim_{k\to\infty} y_{n_k} = \lim_{k\to\infty} f\left(x_{n_k} \right) = f\left(\lim_{k\to\infty} x_{n_k}\right) = f(x) =: y
$$

showing that $\left( y_{n_k} \right)_{k\in\N}$ is convergent with limit $y \in f[I]$. Thus, $f[I]$ is also compact.
</details>
</MathBox>

<MathBox title='Theorem' boxType='theorem'>
Let $\left(f_n \right)_{n\in\N}$ be a sequence of continuous functions $f_n : I \to \R$. If the sequence uniformly converges to $f: I \to \R$, then $f$ is also continuous.
<details>
<summary>Proof</summary>

Let $\varepsilon > 0$ and $x_0 \in I$, and set $\varepsilon' := \frac{\varepsilon}{3}$. The uniform convergence of $\left(f_n \right)_{n\in\N}$ gives that
$$
  \forall \varepsilon' > 0 \; \exists N \in\N \; \forall n \geq N \; \forall \tilde{x} \in I : \left| f_n \left( \tilde{x} \right) - f \left( \tilde{x} \right) \right| < \varepsilon'
$$

The continuity of $f_N$ guarantees there is $\delta > 0$ with $\left| x - x_0 \right| < \delta \implies \left| f_N (x) - f_N (x_0) \right| < \varepsilon'$. Applying the triangle inequality gives
$$
\begin{align*}
  \left| f(x) - f(x_0) \right| &= \left| f(x) - f_N (x) + f_N (x) - f_N (x_0) + f_N (x_0) - f(x_0) \right| \\
  &\leq \underbrace{\left| f(x) - f_N (x) \right|}_{< \varepsilon'} + \underbrace{\left| f_N(x) - f_N (x_0) \right|}_{< \varepsilon'} + \underbrace{\left| f_N(x) - f (x_0) \right|}_{< \varepsilon'} \\
  &< 3\varepsilon' = \varepsilon
\end{align*}
$$

which shows that $f$ is continuous at $x_0$. Since $x_0$ was arbitrarily choosen, $f$ is a continuous function (on $I$).
</details>
</MathBox>

## Intermediate value theorem

<MathBox title='Intermediate value theorem' boxType='theorem'>
Let $f: [a, b] \to \R$ be continuous and $y \in \left[ f(a), f(b) \right]$ or $y \in \left[ f(b), f(a) \right]$. Then there is $\tilde{x} \in [a, b]$ with $f\left( \tilde{x} \right) = y$.
<details>
<summary>Proof</summary>

Define a new function $g:= f - y$ and 
$$
  \tilde{f} := \begin{cases} -g &\textrm{if } g(a) > 0 \\ g &\textrm{if } g(a) \leq 0 \end{cases}
$$

Then $\tilde{f}$ is continuous, $\tilde{y} := 0$, and $\tilde{f}(a) \leq 0$, $\tilde{f}(b) \geq 0$. Splitting the interval $[a, b]$ in nested half-subintervals produces two Cauchy sequences $\left( a_n \right)_{n\in\N}$ and $\left( b_n \right)_{n\in\N}$ and $b_n - a_n \xrightarrow{n\to\infty} 0$. By the completeness axiom and limit theorems we get
$$
  \tilde{x} := \lim_{n\to\infty} a_n = \lim_{n\to\infty} b_n \in [a, b]
$$

We also know that
$$
\begin{gather*}
  \lim_{n\to\infty} \tilde{f}\left(a_n\right) \leq 0 \implies \tilde{f}\left( \lim_{n\to\infty} a_n \right) \leq 0 \implies \tilde{f}\left(\tilde{x}\right) \leq 0 \\
  \lim_{n\to\infty} \tilde{f}\left(b_n\right) \geq 0 \implies \tilde{f}\left( \lim_{n\to\infty} g_n \right) \geq 0 \implies \tilde{f}\left(\tilde{x}\right) \geq 0 
\end{gather*}
$$

Resulting in
$$
  \tilde{f}\left(\tilde{x}\right) = 0 \implies f\left(\tilde{x}\right) - y = g\left(\tilde{x}\right) = 0 \implies f\left( \tilde{x} \right) = y
$$
</details>
</MathBox>

# Differential calculus

A function $f: I\subseteq\R\to\R$ can be linearly approximated at a point $a\in I$ by a secant $\ell: \R\to\R$ of the form

$$
  \ell(t) = \frac{f(x)-f(a)}{x - a} (t - a) + f(a) 
$$

In the limit $x \to a$, the secant converges to the tangent of $f$ at $a$, given by the difference quotient of $f$ at $a$. If this limit exists, it defines the derivative of $f$ at $a$,  which is equal to the slope of $\ell$

$$
  f'(a) \equiv \frac{\mathrm{d}f}{\mathrm{d}x}(a) := \lim_{x\to a} \frac{f(x)-f(a)}{x - a}
$$

<MathBox title='Differentiable function' boxType='definition'>
A function $f: I \subseteq\R\to\R$ is differentiable at $a \in I$ if there is an affine linear function $\Delta_{f, a}: I\to\R$ with
$$
  f(x) = f(a) + (x - a)\Delta_{f, a}(x), \, \forall x \in I
$$

and $\Delta_{f, a}$ is continuous at $a$. The function $\Delta_{f, a}$ gives the slope of $f$ at $a$, and is called the derivative (differential quotient) of $f$ at $a$
$$
  \Delta_{f, a} \equiv f'(a) \equiv \frac{\mathrm{d}f(a)}{\mathrm{d}x} := \lim_{x \to a} \frac{f(x) - f(a)}{x - a}
$$

This can alternatively be formulated as 
$$
  \frac{\mathrm{d}f(x)}{\mathrm{d}x} \equiv f'(x) := \lim_{h \to 0}{\frac{f(x + h) - f(x)}{h}}
$$

A function $f: I \subseteq \R\to\R$ is called differentiable (on $I$) if $f$ is differentiable at all $a \in I$. In this case, $f': I \to\R$ defined by $x \mapsto f'(x)$ is called the derivative of $f$.
</MathBox>

<MathBox title='Proposition' boxType='proposition'>
If $f: I \subseteq\R\to\R$ is differentiable at $a\in I$ then $f$ is also continuous at $a$
</MathBox>

## Higher derivatives

Let $f: I \subseteq \R \to \R$ and define inductively $f^{(n)} := \left( f^{(n-1)} \right)' = \frac{\mathrm{d}}{\mathrm{d}x}\left(\frac{\mathrm{d}^{n-1}f}{\mathrm{d}x^{n-1}}\right)$ for $n \in \N$,  where $f^{(0)} := f$.

- $f$ is called $n$-times differentiable if $f^{(n)}$ exists
- $f$ is called $n$-times continuously differentiable if $f^{(n)}$ exists and is continuous
- $f$ is called $\infty$-times differentiable if $f^{(n)}$ exists for all $n\in \N$

The set of $n$-times continuously differentiable functions is denoted by 

$$
  C^n (I) := \Set{ f: I \to \R | f\text{ n-times continuously differentiable} }
$$

**Properties:** Let $f: I \subseteq \R \to \R$ and $g: I \subseteq \R \to \R$ be differentiable at $x_0$, then
- $f: I \subseteq \R \to \R$ differentiable at $x_0$ with $(f + g)'\left(x_0\right) = f'\left(x_0\right) + g'\left(x_0\right)$ 

## Differentiation rules

### Sum rule

Let $f, g: I \subseteq \R \to \R$ be differentiable at $x_0 \in I$, and let $\alpha, \beta \in \R$ be scalars. Then 

$$
  (\alpha f + \beta g)'(x_0) = \alpha f'(x_0) + \beta g'(x_0)
$$

This shows that differentiation is linear.

### Product rule

Let $f, g: I \subseteq \R \to \R$ be differentiable at $x_0 \in I$. Then $f \cdot g: I \subseteq \R \to \R$ is differentiable at $x_0$ with

$$
  (f \cdot g)'\left(x_0\right) = f\left(x_0\right) g'\left(x_0\right) + f'\left(x_0\right) g\left(x_0\right)
$$ 

<details>
<summary>Proof</summary>

The linearization of $f \cdot g$ at $x_0$ is
$$
\begin{align*}
  (f\cdot g) (x) &= f(x)g(x) = \left[ f(x_0) + (x - x_0) \Delta_{f, x_0}(x) \right] \left[ g(x_0) + (x - x_0) \Delta_{g, x_0}(x) \right] \\
  &= f(x_0) g(x_0) + (x - x_0) \left[ f(x_0) \Delta_{g, x_0} (x) + \Delta_{f, x_0} (x) g(x_0) + (x - x_0) \Delta_{f, x_0}(x) \cdot \Delta_{g, x_0} (x) \right] \\
  &= f(x_0) g(x_0) + (x - x_0) \Delta_{f\cdot g, x_0} (x)
\end{align*}
$$

Differentiating gives
$$
  (f\cdot g)'(x) = \lim_{x \to x_0} (f\cdot g)(x) = f(x_0) g'(x_0) + f'(x_0) g(x_0)
$$
</details>

### Quotient rule

$$
  \frac{\mathrm{d}}{\mathrm{d}x}\left(\frac{f}{g} \right) = \frac{g \frac{\mathrm{d}f}{\mathrm{d}x} - f \frac{\mathrm{d}g}{\mathrm{d}x} }{g^2}
$$

### Chain rule

Let $I, J \subseteq \R$ be two intervals and $g: I \to J$, $f: J \to \R$ two functions. If $g$ is differentiable at $x_0$ and $f$ is differentiable at $y_0 = g(x_0)$, then $f\circ g: I \to \R$ is differentiable at $x_0$ and
$$
\begin{gather*}
  (f \circ g)' = (f' \circ  g) \cdot g' = f'\left(g(x_0) \right) \cdot g'(x_0) \\
  \left. \frac{\mathrm{d}f\left[g(x)\right]}{\mathrm{d}x} \right|_{x_0} = \left. \frac{\mathrm{d}f(g)}{\mathrm{d}g} \right|_{g(x_0)} \cdot \left. \frac{\mathrm{d}g(x)}{\mathrm{d}x} \right|_{x_0}
\end{gather*}
$$

<details>
<summary>Proof</summary>

The linearization of $f \circ g$ at $y_0 = g(x_0)$ is
$$
\begin{align*}
  (f \circ g)(x) = f\left( g(x) \right) &= f(y_0) + \left( g(x) - y_0 \right)\Delta_{f, y_0}\left( g(x) \right) \\
  &= f(y_0) + \left( g(x_0) + (x - x_0) \Delta_{g, x_0} (x) - y_0 \right)\Delta_{f, y_0}\left( g(x) \right) \\
  &= f(y_0) + (x - x_0) \Delta_{g, x_0} \cdot \Delta_{f, y_0} \left( g(x) \right) \\
  &= (f \circ g) (x_0) + (x - x_0) \Delta_{f\circ g, x_0} (x)
\end{align*}
$$

Differentiating gives
$$
  (f \circ g)' (x_0) = g'(x_0) \cdot f'\left( g(x_0) \right)
$$
</details>

### Derivative of inverse functions

<MathBox title='Derivative of inverse functions' boxType='proposition'>
Let $f: I \to J$ be bijective with $I, J \subseteq \R$. If $f$ is differentiable at $x_0$ with $f'(x_0) \neq 0$ and $f^{-1}$ is continuous at $y_0 := f(x_0)$, then $f^{-1}$ is differentiable at $y_0$ with
$$
  \left( f^{-1} \right)'(y_0) = \frac{1}{f'\left(f^{-1}(y_0) \right)}
$$
<details>
<summary>Proof</summary>

Choose a sequence $\left( y_n \right)_{n\in\N} \subseteq J \backslash \Set{ y_0 }$ with $f(x_n) = y_n$ and $\lim_{n\to\infty} y_n = y_0$, then
$$
\begin{align*}
  \frac{f^{-1}(y_n) - f^{-1}(y_n)}{y_n - y_0} &= \frac{f^{-1}\left( f(x_n)\right) - f^{-1}\left( f(x_n) \right)}{f(x_n) - f(x_0)} = \frac{x_n - x_0}{f(x_n) - f(x_0)} \\
  &= \left( \frac{f(x_n) - f(x_0)}{x_n - x_0} \right)^{-1}
\end{align*}
$$

Since $f^{-1}$ is continuous at $y_0$, then $f^{-1}(y_n) \xrightarrow{n\to\infty} f^{-1}(y_0) \iff x_n \xrightarrow{n\to\infty} x_0$. The derivative of $f^{-1}$ at $y_0$ thus becomes
$$
  \left( f^{-1} \right)'(y_0) = \lim_{n\to\infty} \frac{f^{-1}(y_n) - f^{-1}(y_0)}{y_n - y_0} = \left( \lim_{n\to\infty} \frac{f(x_n) - f(x_0)}{x_n - x_0} \right)^{-1} = \frac{1}{\left( f'(x_0) \right)} = \frac{1}{f'\left(f^{-1}(y_0) \right)}
$$
</details>
</MathBox>

## Uniform convergence

<MathBox title='Theorem' boxType='theorem'>
Let $\left(f_n\right)_{n\in\N}$ be a sequence of functions $f_n: I \to \R$ and assume
- $\left(f_n\right)_{n\in\N}$ is pointwisely convergent to a function $f: I \to \R$
- $f_n: I \to \R$ is differentiable for all $n\in\N$
- There is $g: I \to \R$ with $\| f_n' - g \|_\infty \xrightarrow{n\to\infty} 0$

Then $\| f_n' - f \|_\infty \xrightarrow{n\to\infty}{\longrightarrow} 0$ and $f$ is differentiable with $f' = g$.
<details>
<summary>Proof</summary>

$$
\begin{align*}
  \left| \frac{f(x) - f(x_0)}{x - x_0} - g(x_0) \right| \leq \left| \frac{f(x) - f(x_0)}{x - x_0} - \frac{f_n(x) - f_n(x_0)}{x - x_0} \right| + \left| \frac{f_n(x) - f_n(x_0)}{x - x_0} - f_n'(x_0) \right| + \left| f_n' (x_0) - g(x_0) \right|
\end{align*}
$$

For any $\varepsilon > 0$ we get
$$
  \left| \lim_{x\to x_0} \frac{f(x) - f(x_0)}{x - x_0} - g(x_0) \right| \leq \varepsilon
$$
</details>
</MathBox>

## Local extrema

<MathBox title='Local extrema' boxType='definition'>
Let $f: I \subseteq \R \to \R$ be a differentiable function, then $f$ has a local extrema $x_0 \in I$ if $x_0$ is a local maximum or minimum of $f$, defined respectively as follows:
- $f$ has a local maximum at $x_0$ if there is a neighbourhood $U \subseteq \R$ of $x_0$ with $f(x_0) = \max\Set{ f(x) | x \in U \cap I }$
- $f$ has a local minimum at $x_0$ if there is a neighbourhood $U \subseteq \R$ of $x_0$ with $f(x_0) = \min\Set{ f(x) | x \in U \cap I }$
</MathBox>

<MathBox title='Proposition' boxType='proposition'>
Let $f: (a, b) \to \R$ be differentiable at $x_0 \in (a, b)$. Then $f$ has a local extremum at $x_0$ if $f'(x_0) = 0$.
<details>
<summary>Proof</summary>

Consider the case where $f$ has a local maximum at $x_0$. Then there is a neighbourhood $U \subseteq \R$ of $x_0$ with $f(x_0) = \max\Set{ f(x) | x \in U \cap I }$. Because $f$ is differentiable at $x_0$, we have $f(x) = f(x_0) + (x - x_0)\Delta_{f, x_0}(x)$. 

To show that $f'(x_0) = 0$, we construct a contradiction. Assume $f'(x_0) > 0$, then there exist a neighbourhood $V \subseteq U$ such that $\Delta_{f, x_0} (x) > 0\; \forall x\in V$. Evidently, for $x > x_0$, we get 
$$
  f(x) = f(x_0) + \underbrace{x - x_0}_{> 0}\underbrace{\Delta_{f, x_0}(x)}_{> 0} > f(x_0)
$$

implying that $f(x_0)$ is not a local maximum. Similarly, assuming $f'(x_0) < 0$, there exists a neighbourhood $V \subseteq U$ such that $\Delta_{f, x_0} (x) < 0\; \forall x\in V$. Then for $x < x_0$, we get
$$
  f(x) = f(x_0) + \underbrace{x - x_0}_{< 0}\underbrace{\Delta_{f, x_0}(x)}_{< 0} < f(x_0)
$$

implying that $f(x_0)$ is not a local minimum. This leaves $f'(x_0) = 0$ as the only possibility. The result follows similarly for the case where $f$ has a local minimum at $x_0$.
</details>
</MathBox>

<MathBox title='Proposition' boxType='proposition'>
Let $f: [a, b] \to \R$ be differentiable with $f'(x_0) = 0$ for $x_0 \in [a, b]$ and let $f'$ be differentiable at $x_0$. The function $f$ is called

1. convex at $x_0$ if $f''(x_0) > 0$, implying that $f$ has a local minimum at $x_0$
2. concave at $x_0$ if $f''(x_0) < 0$, implying that $f$ has a local maximum at $x_0$
<details>
<summary>Proof</summary>

For the first case, assume $0 < f''(x_0) = \lim_{x\to x_0} \frac{f'(x) - f'(x_0)}{x - x_0} = \lim_{x\to x_0} \Delta_{f', x_0} (x)$. Then there is a neighbourhood of $x_0$ called $U \subseteq [a, b]$ with $\Delta_{f', x_0}(x) > 0$. This leaves $0 < \frac{f'(x)}{x - x_0}$ for $x \in U \backslash \Set{ x_0 }$ such that for

- $x < x_0 \implies f'(x) < 0$, i.e. $f$ is decreasing
- $x > x_0 \implies f'(x) > 0$, i.e. $f$ is increasing

This implies that $x_0$ is a local minimum of $f$.
</details>
</MathBox>

<MathBox title="Rolle's theorem" boxType='proposition'>
Let $f: [a, b] \to \R$ be differentiable with $f(a) = f(b)$. Then there is $\hat{x} \in (a, b)$ with $f'(\hat{x}) = 0$.
<details>
<summary>Proof</summary>

Consider the non-trivial case where $f$ is not constant. Since $f$ is continuous with a compact domain, its image is also compact. Therefore, there are $x^-, x^+ \in [a, b]$ with
$$
\begin{gather*}
  f(x^+) = \sup\Set{ f(x) | x \in [a, b] } \\
  f(x^-) = \inf\Set{ f(x) | x \in [a, b] }
\end{gather*}
$$

Because $f$ is not constant, there is a global maximum, $x^+ \in (a, b)$, or minimum, $x^- \in (a, b)$, which we call $\hat{x}$. Since a global extremum is also local, by definition, the proposition above ensures that $f'(\hat{x}) = 0$.
</details>
</MathBox>

## Mean value theorem

<MathBox title='Mean value theorem' boxType='theorem'>
Let $f: [a, b] \to \R$ be differentiable. Then there is $\hat{x} \in (a, b)$ with 
$$
  f'(\hat{x}) = \frac{f(b) - f(a)}{b - a}
$$

geometrically, this means that there exists some middle point, whose tangent slope equals the secant slope.
<details>
<summary>Proof</summary>

The case where $f(a) = f(b)$ is proved by Rolle's theorem. For the case where $f(a) \neq f(b)$, define $g: [a, b] \to \R$ by
$$
  g(x) := f(x) - \left( \frac{f(b) - f(a)}{b - a}(x - a) + f(a) \right)
$$

such that $g(a) = g(b)$. The function $g$ is differentiable with $g'(x) = f'(x) - \frac{f(b) - f(a)}{b - a}$. By Rolle's theorem, there is  $\hat{x} \in (a, b)$ with $g'(\hat{x}) = 0$ implying
$$
  f'(\hat{x}) = \frac{f(b) - f(a)}{b - a}
$$
</details>
</MathBox>

<MathBox title='Corollary' boxType='corollary'>
Let $f: [a, b] \to \R$ be differentiable. The monotonicity of $f$ is deduced from applying the mean value theorem
- if $f'(x) \geq 0$ for all $x \in [a, b]$, then $f$ is monotonically increasing
- if $f'(x) \leq 0$ for all $x \in [a, b]$, then $f$ is monotonically decreasing

If the unequalities are strict, then $f$ is strictly monotonically increasing/decreasing.
</MathBox>
 
<MathBox title='Extended mean value theorem' boxType='theorem'>
Let $f, g: [a, b] \to \R$ be differentiable with $g'(x) \neq 0$ for all $x \in (a, b)$. Then there exists $\hat{x} \in (a, b)$ with 
$$
  \frac{f'(\hat{x})}{g'(\hat{x})} = \frac{f(b) - f(a)}{g(b) - g(a)}
$$

The standard mean value theorem is retrieved for $g(x) = x$. 
<details>
<summary>Proof</summary>

Define $h: [a, b] \to \R$ by
$$
  h(x) := f(x) - \left( \frac{f(b) - f(a)}{g(b) - g(a)}(g(x) - g(a)) + f(a) \right)
$$

such that $h(a) = h(b)$. The function $h$ is differentiable with $h'(x) = f'(x) - \frac{f(b) - f(a)}{g(b) - g(a)}$. By Rolle's theorem, there is  $\hat{x} \in (a, b)$ with $g'(\hat{x}) = 0$ implying
$$
  \frac{f'(\hat{x})}{g'(\hat{x})} = \frac{f(b) - f(a)}{g(b) - g(a)}
$$
</details>
</MathBox>

## L'Hôpital's rule
<MathBox title="L'Hôpital's rule" boxType='theorem'>
Let $f, g: I \subseteq \R \to \R$ be differentiable and let $x_0 \in I$ with $g'(x) \neq 0$ for $x \neq x_0$. If either

1. $f(x_0) = g(x_0) = 0$
2. $\lim_{x\to x_0} f(x) = \lim_{x\to x_0} f(x) = \infty$

$$
  \lim_{x \to x_0} \frac{f(x)}{g(x)} = \lim_{x \to x_0} \frac{f'(x)}{g'(x)}
$$

The same holds if $I$ is not bounded, i.e. $x_0 = \infty$.
<details>
<summary>Proof</summary>

Choose a sequence $\left(x_n\right)_{n\in\N} \subseteq I \backslash \Set{ x_0 }$ with $x_n \xrightarrow{n\to\infty}{\longrightarrow} x_0$. The first case where $f(x_0) = g(x_0) = 0$ can be proved by applying the mean value theorem for $[a, b] = [x_n, x_0] \lor [x_0, x_n]$. Then there is a sequence $\left(\hat{x}_n\right)_{n\in\N}$ with $\hat{x}_n \in (x_n, x_0) \lor (x_0, x_n)$ and $\hat{x}_n \xrightarrow{n\to\infty}{\longrightarrow} x_0$ satisfying
$$
  \lim_{x\to x_0} \frac{f(x)}{g(x)} = \frac{f(x_n)}{g(x_n)} = \frac{f(x_n) - f(x_0)}{g(x_n) - g(x_0)} = \frac{f'(\hat{x}_n)}{g'(\hat{x}_n)} \xrightarrow{n\to\infty} \lim_{x\to x_0} \frac{f'(x)}{g'(x)}
$$

The second case where $\lim_{x\to x_0} f(x) = \lim_{x\to x_0} f(x) = \infty$ can be proved by noting that 
$$
  \lim_{x\to x_0} \frac{f(x)}{g(x)} = \lim_{x\to x_0} \frac{\frac{1}{g(x)}}{\frac{1}{f(x)}}
$$

and defining
$$
\begin{aligned}
  \tilde{f}(x) := \begin{cases} \frac{1}{f(x)} & x \in I \backslash\Set{x_0} \\ 0 & x = x_0 \end{cases}
\end{aligned}\quad
\begin{aligned}
  \tilde{g}(x) := \begin{cases} \frac{1}{g(x)} & x \in I \backslash\Set{x_0} \\ 0 & x = x_0 \end{cases}
\end{aligned}
$$

Applying the mean value theorem gives
$$
\begin{gather*}
  \lim_{x\to x_0} \frac{f(x)}{g(x)} = \lim_{x\to x_0} \frac{\frac{1}{g(x_n)}}{\frac{1}{f(x_n)}} = \frac{\tilde{f}(x_n) -\tilde{f}(x_0)}{\tilde{g}(x_n) - \tilde{g}(x_0)} = \frac{\tilde{f}'\left(\hat{x}_n\right)}{\tilde{g}'\left(\hat{x}_n\right)} = \frac{\frac{f'\left( \hat{x}_n \right)}{\left(f\left( \hat{x}_n \right)\right)^2}}{\frac{g'\left( \hat{x}_n \right)}{\left(g\left( \hat{x}_n \right)\right)^2}} \xrightarrow{n\to\infty} \lim_{x\to x_0} \frac{\frac{f'(x)}{\left(f(x)\right)^2}}{\frac{g'(x)}{\left(g(x)\right)^2}} \\
  \lim_{x\to x_0} \frac{\frac{1}{g(x_n)}}{\frac{1}{f(x_n)}} = \lim_{x\to x_0} \frac{\frac{f'(x)}{\left(f(x)\right)^2}}{\frac{g'(x)}{\left(g(x)\right)^2}} \implies \lim_{x \to x_0} \frac{f(x)}{g(x)} = \lim_{x \to x_0} \frac{f'(x)}{g'(x)}
\end{gather*}
$$

The case where $x_0 = \infty$ can be proved by defining
$$
  \tilde{f}(x) := \begin{cases} f\left( \frac{1}{x} \right) & x > 0,\; x^{-1} \in I \\ 0 & x = 0 \end{cases}
$$
</details>
</MathBox>

## Taylor series

<MathBox title="Taylor's theorem" boxType='theorem'>
Let $f: I \subseteq \R \mapsto \R$ be $(n+1)$-differentiable at $x_0$. If $h \in \R$ such that $x_0 + h \in I$ then
$$
  f(x_0 + h) = \sum_{k=0}^n \frac{f^{(k)}(x_0)}{k!}h^k + \frac{f^{(n+1)}(\xi)}{(n + 1)!}h^{n+1} = T_n (h) + R_n(h) \quad \xi \in (x_0, x_0 + h) \lor (x_0 + h, x_0)
$$

where $T_n (h)$ is the $n$-th order Taylor polynomial and $R_n$ is the remainder term. This allows $f$ to approximated by the Taylor polynomial around $x_0$. The remainder term is often denoted with the Landau symbol
$$
  R_n (h) \equiv \mathcal{O}\left(h^{n+1}\right)
$$

Setting $x = x_0 + h$, the Taylor polynomial can be reformulated as
$$
  f(x) = \sum_{k=0}^n \frac{f^{(k)}(x_0)}{k!}(x - x_0)^k + \mathcal{O}\left( (x - x_0)^{n+1} \right)
$$
<details>
<summary>Proof</summary>

To derived the Taylor polynomial, define the auxilliary function
$$
  F_{n, h} (t) := \sum_{k=0}^n \frac{f^{(k)}(t)}{k!}(h + x_0 - t)^k
$$

such that $F_{n, h} (x_0) = T_n (h)$ and $F_{x_0 + h} = f(x_0 + h)$. To derive the remainder function, define
$$
  g_{n, h} (T) := (h + x_0 - t)^{n + 1}
$$

To find $\xi \in (x_0, x_0 + h) \lor (x_0 + h, x_0)$, apply the generalised mean value theorem 
$$
\begin{gather*}
  \frac{F_{n, h} (x_0 + h) - F_{n, h}(x_0)}{g_{n, h}(x_0 + h) - g_{n, h}(x_0)} = \frac{f(x_0 + h) - T_n (h)}{g_{n, h}(x_0 + h) - g_{n, h}(x_0)} \frac{F_{n, h}' (\xi)}{g_{n, h}' (\xi)} \\
  \implies f(x_0 + h) - T_n (h) = \left( g_{n, h} (x_0 + h) - g_{n, h} (x_0) \right) \frac{F_{n, h}' (\xi)}{g_{n, h}' (\xi)} = \frac{h^{n+1} F_{n, h}' (\xi)}{(n + 1) (h + x_0 - \xi)^n}
\end{gather*}
$$

Apply the product rule to find the derivative of the auxilliary function
$$
\begin{align*}
  F_{n, h}' (t) &= \frac{\mathrm{d}}{\mathrm{d}t} \sum_{k=0}^n \frac{f^{(k)}(t)}{k!}(h + x_0 - t)^k \\
  &= \sum_{k=0}^n \frac{f^{(k+1)}(t)}{k!} (h + x_0 - t)^k - \sum_{k=1}^n \frac{f^{(k)}(t)}{(k - 1)!}(h + x_0 - t)^{k - 1} \\
  &= \frac{f^{(n+1)}(t)}{n!} (h + x_0 - t)^n
\end{align*}
$$

Plugging in the derivative yields the desired Taylor expansion
$$
\begin{gather*}
  f(x_0 + h) - T_n(h) = \frac{h^{n+1} }{(n + 1) (h + x_0 - \xi)^n} \frac{f^{(n+1)}(\xi)}{n!} (h + x_0 - \xi)^n = h^{n+1}\frac{f^{(n+1)}(\xi)}{(n + 1)!} \\
  \implies f(x_0 + h) = \sum_{k=0}^n \frac{f^{(k)}(x_0)}{k!}h^k + \frac{f^{(n+1)}(\xi)}{(n + 1)!}h^{n+1} 
\end{gather*}
$$
</details>
</MathBox>

# Integral calculus 

## Riemann integral

<MathBox title='Definiton: Riemann-integrable function' boxType='definition'>
A bounded function $f: [a, b] \to \R$ is Riemann-integrable if
$$
  \sup\Set{ \int_a^b \phi(x) \mathrm{d}x | \phi \in S([a, b]),\; \phi \leq f } = \inf\Set{ \int_a^b \phi(x) \mathrm{d}x | \phi \in S([a, b]),\; \phi \geq f}
$$

In this case $\int_a^b f(x) \mathrm{d}x$ is called the (Riemann) integral of $f$. The set of all Riemann-integrable functions on an interval $[a, b]$ is denoted
$$
  \mathcal{R}([a, b]) := \Set{f: [a, b] \to \R \textrm{ bounded } | f \text{ Riemann-integrable}}
$$
</MathBox>

Changing the order of the integration bounds, negates the integral
$$
  \int_b^a f(x) \mathrm{d}x := -\int_a^b f(x) \mathrm{d}x
$$

Integrating over a subinterval of $[a, b]$, is defined by restricting the function over the subinterval. That is, for $c, d \in [a, b]$ with $c < d$ we can define 
$$
  \int_c^d f(x) \mathrm{d}x := \int_c^d f|_{[c, d]} (x) \mathrm{d}x
$$

Splitting the interval, also splits the Riemann integral, i.e. for $c\in[a,b]$ we have
$$
  \int_a^b f(x) \mathrm{d}x = \int_a^c f(x) \mathrm{d}x + \int_c^b f(x) \mathrm{d}x
$$

<MathBox title='Proposition' boxType='proposition'>
The Riemann integral defines a linear and monotonic map $\int_a^b f(x) \mathrm{d}x : \mathcal{R}([a, b]) \to \R$.
</MathBox>

<MathBox title='Proposition' boxType='proposition'>
All continuous or monotonically increasing functions are also Riemann integrable, i.e. $f\in C([a, b]) \implies f \in \mathcal{R}([a, b])$. The opposite is not generally true.
</MathBox>

### Step functions

<MathBox title='partition' boxType='definition'>
The partition of a bounded interval $[a, b]$ is a finite sequence of the form
$$
  a < x_0 < x_1 < \dots < x_{n-1} < x_n = b
$$
</MathBox>

<MathBox title='step function' boxType='definition'>
A map $\phi: [a, b] \to \R$ is called a step function if it is piecewisely constant. This means that there is a partition of $[a, b]$, and there are numbers $c1, \dots, c_n \in \R$ such that $\phi|_{(x_{j-1}, x_j)} = c_j$ for all $j \in \Set{1, \dots, n}$. The set of all step functions on an interval $[a, b]$ is commonly denoted $S([a, b])$.
</MathBox>

<MathBox title='Proposition' boxType='proposition'>
The Riemann integral of a step function $\phi: [a, b] \to \R$ is well-defined by
$$
  \int_a^b \phi(x) \mathrm{d}x := \sum_{j=1}^n c_j \left( x_j - x_{j-1} \right)
$$

and describes a map $S([a, b]) \to \R$ given by $\phi \mapsto \int_a^b \phi(x) \mathrm{d}x$, which is linear and monotonic.
<details>
<summary>Proof</summary>

Let $P_1 = \Set{ x_j }_{j=1}^n$ and $P_2 = \Set{ \tilde{x}_j }_{j=1}^m$ be two partitions of $[a, b]$ with
$$
  \phi|_{(x_{j-1}, x_j)} = c_j \quad \phi|_{(\tilde{x}_{j-1}, \tilde{x}_j)} = d_j
$$

The case where $P_2 \supset P_1$, i.e. $P_2$ is finer than $P_1$, is trivial and it follows that
$$
  \sum_{j=1}^n c_j (x_j - x_{j-1}) = \sum_{j=1}^n d_j (\tilde{x}_j - \tilde{x}_{j-1})
$$

For the case $P_2 \not\supset P_1 \land P_1 \not\supset P_2$, we can define $P_3 := P_1 \cup P_2$ such that $P_3 \supset P_1$ and $P_3 \supset P_1$. This reduces to the trivial case and it follows that $\sum_{P_1} = \sum_{P_3}$ and $\sum_{P_2} = \sum_{P_3}$.
</details>
</MathBox>

<MathBox title='Proposition' boxType='proposition'>
The Riemann integral of step functions $\int_a^b \phi(x) \mathrm{d}x : S([a, b]) \to \R$ defines a linear and monotonic map satisfying:
- Homogeneous: for $\lambda \in \R$: $\int_a^b \lambda \phi(x) \mathrm{d}x = \lambda\int_a^b \phi(x) \mathrm{d}x$
- Additive: for $\phi, \psi \in S([a, b])$: $\int_a^b (\phi + \psi)(x) \mathrm{d}x = \int_a^b \phi(x) \mathrm{d}x + \int_a^b \psi(x) \mathrm{d}x$
- Monotonic: for $\phi, \psi \in S([a, b])$ with $\phi \leq \psi$, then $\int_a^b \phi(x) \mathrm{d}x \leq \int_a^b \psi(x) \mathrm{d}x$
<details>
<summary>Proof</summary>

To prove the additive property, consider two step functions $\phi, \psi \in S([a, b])$ with respective partitions $P_\phi = \Set{ x_j }_{j=1}^n$ and $P_\psi = \Set{ \tilde{x}_j }_{j=1}^m$. Define $P = P_\phi \cup P_\psi = \Set{ \hat{x}_j }_{j=1}^N$, then
$$
\begin{align*}
  \int_a^b \phi(x) \mathrm{d}x + \int_a^b \psi(x) \mathrm{d}x &= \sum_{j=1}^N c_j \left(\hat{x}_{j} - \hat{x}_{j-1}\right) + \sum_{j=1}^N d_j \left( \hat{x}_{j} - \hat{x}_{j-1} \right) \\
  &= \sum_{j=1}^N \left( c_j + d_j \right) \left( \hat{x}_{j} - \hat{x}_{j-1} \right) \\
  &= \int_a^b (\phi + \psi)(x) \mathrm{d}x
\end{align*}
$$
</details>
</MathBox>

## Integrals on unbounded domains

Let $f: \R \to \R$ be a function with the property $f|_{[a, b]} \in \mathcal{R}([a, b])$ for all $b \geq a$. If

1. $\lim_{b\to\infty}\int_a^b$ exists, then $\int_a^\infty f(x)\mathrm{d}x$ converges 
2. $\lim_{a\to -\infty} f(x) \mathrm{d}x$ exists, then $\int_{-\infty}^b f(x)\mathrm{d}x$ converges

If there is a $c \in \R$ such that $\int_{-\infty}^c f(x)\mathrm{d}x$ and $\int_c^\infty f(x)\mathrm{d}x$ converge, we can define

$$
  \int_{-\infty}^\infty f(x)\mathrm{d}x := \int_{-\infty}^c f(x)\mathrm{d}x + \int_c^\infty f(x)\mathrm{d}x
$$

## Improper integrals

Let $f: (a, b] \to \R$ be a function with the property that $f|_{[a+\varepsilon, b]} \in \mathcal{R}([a, b])$ for all $\varepsilon > 0$. The integral $\int_a^b f(x) \mathrm{d}x$ converges if the following limit exists
$$
  \lim_{\varepsilon \to 0} \int_{a+\varepsilon}^b f(x) \mathrm{d}x
$$

For a function $f:[a, b]\backslash \Set{p} \to \R$ with a pole at $p$, the improper Riemann integral is defined as
$$
  \int_a^b f(x) \mathrm{d}x := \lim_{\varepsilon_1 \to 0} \int_a^{p-\varepsilon_1} f(x)\mathrm{d}x + \lim_{\varepsilon_2 \to 0} \int_{p-\varepsilon_2}^b f(x)\mathrm{d}x
$$

### Cauchy principal value

The Cauchy principal value is a method for integrating functions with singularities.

<MathBox title='Cauchy principal value' boxType='definition'>
Let $f: \R \to \R$ be continuous. If 
1. $f$ has a singularity at $c\in [a, b]$ such that $\int_a^c f(x) \mathrm{d}x = \pm \infty$ and $\int_c^b f(x)\mathrm{d}x = \mp \infty$, then the Cauchy principal value is defined as
$$
  \mathrm{p.v.}\int_a^b f(x)\mathrm{d}x := \lim_{\varepsilon\to 0} \left( \int_a^{c-\varepsilon} f(x)\mathrm{d}x + \int_{c+\varepsilon}^b f(x)\mathrm{d}x \right)
$$

2. $f$ has singularities at infinity such that $\int_{-\infty}^0 f(x)\mathrm{d}x = \pm\infty$ and $\int_0^\infty f(x)\mathrm{d}x = \mp\infty$, then the Cauchy principal value is defined as
$$
  \mathrm{p.v.}\int_{-\infty}^\infty f(x)\mathrm{d}x := \lim_{a\to \infty} \int_{-a}^a f(x)\mathrm{d}x
$$
</MathBox>

## Mean value theorem of integration

<MathBox title='Mean value theorem of integration' boxType='theorem'>
Let $f, g: [a, b] \to \R$ be continuous with $g \geq 0$. Then there is $\hat{x}\in[a, b]$ such that
$$
  \int_a^b f(x)g(x) \mathrm{d}x = f\left(\hat{x}\right) \int_a^b g(x) \mathrm{d}x
$$

In the special case where $g = 1$, this reduces to
$$
  \int_a^b f(x) \mathrm{d}x = f\left(\hat{x}\right) (b - a)
$$
<details>
<summary>Proof</summary>

Because $f$ is continuous on a closed interval there is a lower and upper bound for $f$, i.e. $m \leq f(x) \leq M$ for all $x \in [a, b]$. Since $g\geq 0$, then $m g(x) \leq f(x) g(x) \leq Mg(x)$, and by the monotonicity property of integrals we get
$$
  m\int_a^b g(x) \mathrm{d}x \leq \int_a^b f(x) g(x) \mathrm{d}x \leq M \int_a^b g(x) \mathrm{d}x
$$

This implies there is $\mu \in [m, M]$ such that
$$
  \mu \int_a^b g(x) \mathrm{d}x = \int_a^b f(x) g(x) \mathrm{d}x
$$

By the intermediate value theorem there is a$\hat{x} \in [a, b]$ with $f\left(\hat{x}\right) = \mu$
</details>
</MathBox>

## Fundamental theorems of calculus

<MathBox title='First fundamental theorem of calculus' boxType='theorem'>
Let $f: I \subseteq{I} \to \R$ be a continuous function. A differentiable function $F: I \to \R$ is called an anti-derivative of $f$ if $F' = f$. In this case we can define
$$
  F(x) := \int_a^x f(t) \mathrm{d}t
$$
<details>
<summary>Proof</summary>

Applying the mean value theorem of integration we get
$$
\begin{gather*}
  F(x + h) - F(x) = \int_x^{x+h} f(t) \mathrm{d}t = f\left(\hat{x}\right),\; \hat{x} \in [x, x + h] \\
  \implies \frac{F(x + h) - F(x)}{h} = f\left(\hat{x} \right)
\end{gather*}
$$

In the limit of vanishing $h$ we get
$$
  \lim_{h\to 0} \frac{F(x + h) - F(x)}{h} = \lim_{h\to 0} f\left(\hat{x}\right) = f(x) \implies F' = f
$$
</details>
</MathBox>

<MathBox title='Proposition' boxType='proposition'>
Let $f: I \subseteq{I} \to \R$ be a continuous function, and $F: I \to \R$ is an antiderivative of $f$. Antother differentiable function $G: I \to \R$ is also an antiderivative of $f$ if and only if $F - G$ is constant.
<details>
<summary>Proof</summary>

$(\implies)$: Let $F, G: I \to \R$ be two antiderivatives of $f$
$$
  (F - G)' = F' - G' = f - f = 0 \implies F - G \textrm{ is constant}
$$

where the final implication is ensured by the mean value theorem.

$(\Leftarrow)$: Let $F(x) - G(x) = c$ for a constant $c \in \R$, then
$$
  G = F - c \implies G' = F' = f
$$

which shows that $G$ is antiderivative of $f$
</details>
</MathBox>

<MathBox title='Second fundamental theorem of calculus' boxType='theorem'>
Let $f: I \subseteq{I} \to \R$ be a continuous function, and $F: I \implies \R$ an anti-derivative of $f$. Then
$$
  \int_a^b f(x) \mathrm{d}x = F(b) - F(a) := \left. F(x) \right|_a^b \, , \quad F'(x) = f(x)
$$
<details>
<summary>Proof</summary>

By definition $F(x) := \int_a^x f(t) \mathrm{d}t$ is an anti-derivative of $f$ with $F(a) = 0$. Choosing an arbitrary antiderivative of $f$ by $\tilde{F} = F + c$ for some constant $c \in \R$ we get
$$
  \tilde{F}(b) - \tilde{F}(a) = F(b) - F(a) = \int_a^b f(t)\mathrm{d}t
$$
</details>
</MathBox>

## Integration rules

### Integration by substitution

Let $f: I \subseteq{I} \to \R$ be continuous, and $\phi: [a, b] \to I$ continuously differentiable. Substituting $x = \phi(t)$ with $\frac{\mathrm{d}x}{\mathrm{d}t} = \phi'(t)$, then 
$$
  \int_a^b f\left( \phi(t) \right) \underbrace{\phi'(t)}_{\mathrm{d}x\mathrm{d}t} = \int_{\phi(a)}^{\phi(b)} f(x) \mathrm{d}x
$$

The subsitution rule can be inversely formulated by letting $f:[a, b] \subseteq I \to \R$ be continuous and $\phi: J \to I$ be continuously differentiable and bijective. Then
$$
  \int_a^b f(x) \mathrm{d}x = \int_{\phi^{-1}(a)}^{\phi^{-1}(b)} f\left( \phi(t) \right)\phi'(t) \mathrm{d}t
$$

<details>
<summary>Proof</summary>

Let $F: I \to \R$ be an antiderivative of $f$. Applying the chain rule on the composition $F \circ \phi$ gives
$$
  \left(F\circ\phi\right)' (t) = F'\left( \phi(t) \right)\phi'(t) = f\left(\phi(t)\right)\phi'(t)
$$

Integrating and applying the second fundamental theorem of calculus gives
$$
\begin{align*}
  \int_a^b f\left(\phi(t)\right)\phi'(t) \mathrm{d}t &= \int_a^b \left(F\circ\phi\right)'(t)\mathrm{d}t = \left.\left(F\circ\phi\right)(t)\right|_{t=a}^{t=b} \\
  &= \left. F(x) \right|_{x=\phi(a)}^{x=\phi(b)} = \int_{\phi(a)}^{\phi(b)}f(x) \mathrm{d}x
\end{align*}
$$
</details>

### Integration by parts (partial integration)

Let $f, g: [a, b] \to \R$ be continuously differentiable functions. Then
$$
  \int_a^b f'(x) g(x) \mathrm{d}x = \left.f(x)g(x) \right|_{x=a}^{x=b} - \int_a^b f(x) g'(x) \mathrm{d}x
$$

<details>
<summary>Proof</summary>

Applying the product rule $(f \cdot g)' (x) = f'(x) g(x) + f(x) g'(x)$ and integrating gives
$$
  \int_a^b (f\cdot g)'(x)\mathrm{d}x = \int_a^b f'(x)g(x)\mathrm{d}x + \int_a^b f(x)g(x) \mathrm{d}x
$$

By the second fundamental theorem of calculus
$$
  \int_a^b (f\cdot g)'(x)\mathrm{d}x = \left. f(x)g(x) \right|_a^b
$$

resulting in the formula for integration by parts
$$
  \int_a^b f'(x) g(x) \mathrm{d}x = \left.f(x)g(x) \right|_{x=a}^{x=b} - \int_a^b f(x) g'(x) \mathrm{d}x
$$
</details>

### Integration by partial fraction decomposition

Let $f: I\subseteq\R\to \R$ be a rational function $f(x) = \frac{p(x)}{q(x)}$ with $\deg (p) < \deg(q) := n$. The function $f$ can be decomposed into partial fractions by finding the roots of $q$

1. If $q$ has $n$ different real zeroes, i.e. $q(x) = \prod_{i=1}^n (x - x_i)$, then
$$
  \frac{p(x)}{q(x)} = \sum_{i=1}^n \frac{a_i}{x - x_i},\; a_i \in \R
$$

2. If $q$ has $k < n$ different real zeroes $x_1, x_2, \dots, x_k$ with corresponding multiplicities $\alpha_1, \alpha_2, \dots \alpha_k$ such that $\sum_{j=1}^k \alpha_j = n$, i.e. $q(x) = \prod_{i=1}^k \prod_{j=1}^{\alpha_i} \left(x - x_i\right)^j$, then
$$
  \frac{p(x)}{q(x)} = \sum_{i=1}^k \sum_{j=1}^{\alpha_i} \left(\frac{a_i}{x - x_i}\right)^j,\; a_i \in \R
$$

3. If $q$ has complex zeroes, then $\frac{p(x)}{q(x)}$ is calculated as above with $x_1, x_2, \dots, x_k \in \mathbb{C}$ and $a_i^1, \dots a_k^{\alpha_k} \in \mathbb{C}$.

By the linearity of the Riemann integral, each partial fraction of $f$ can then be integrated independentantly
$$
  \int_a^b \frac{p(x)}{q(x)} \mathrm{d}x = \int_a^b \left[ \sum_{i=1}^k \sum_{j=1}^{\alpha_i} \left(\frac{a_i}{x - x_i}\right)^j \right] \mathrm{d}x  = \sum_{i=1}^k \sum_{j=1}^{\alpha_i} \left[ \int_a^b \left(\frac{a_i}{x - x_i}\right)^j\mathrm{d}x \right]
$$

## Comparison test for integrals

Let $f, g: [a, \infty) \to \R$ with $g(x) \geq 0$ for all $x\in[a, \infty)$ and $g|_{[a,b]}, f|_{[a,b]} \in \mathcal{R}([a, b])$. If

1. $|f(x)| \leq g(x) \; \forall x\in[a, \infty)$, then if $\int_a^\infty g(x) \mathrm{d}x$ converges so does $\int_a^\infty f(x) \mathrm{d}x$.
2. $g(x) \leq f(x) \; \forall x\in[a, \infty)$, then if $\int_a^\infty g(x) \mathrm{d}x$ diverges so does $\int_a^\infty f(x) \mathrm{d}x$.

## Integral test for series

Let $f: [0, \infty) \to [0, \infty)$ be monotonically decreasing. Then $\sum_{k=0}^\infty$ is convergent if and only if $(\iff)$ $\int_0^\infty f(x)\mathrm{d}x$ is convergent. In this case
$$
  0 \leq \sum_{k=0}^\infty f(k) - \int_0^\infty f(x)\mathrm{d}x \leq f(0)
$$

<details>
<summary>Proof</summary>

From the monotonicity we get
$$
  f(k) = \int_{k-1}^k f(k) \mathrm{d}x \leq \int_{k-1}^k f(x)\mathrm{d}x \leq \int_{k-1}^k f(k - 1) \mathrm{d}x = f(k - 1)
$$

Applying a sum preserves the inequalities
$$
  \sum_{k=1}^n f(k) \leq \sum_{k=1}^n \int_{k-1}^k f(x)\mathrm{d}x \leq \sum_{k=1}^n f(k-1) = \sum_{k=0}^{n-1} f(k) 
$$

In the limit $n\to \infty$ we get
$$
  \sum_{k=1}^\infty \leq \int_0^\infty f(x)\mathrm{d}x \leq \sum_{k=0}^\infty f(k)
$$
</details>

## Repeated integration

<MathBox title="Cauchy's formula for repeated integration" boxType='theorem'>
Let $f:\R\to\R$ be a continous function. The $n$-th repeated integral of $f$ with basepoint $a\in\R$, i.e.

$$
  I_a^n f(x) := \int_a^x \int_a^{x_{n-1}}\cdots\int_a^{x_1} f(x_0)\;\mathrm{d}x_0\dots\mathrm{d}x_{x-1},\quad n\in\N
$$

is given by

$$
  I_a^n f(x) = \frac{1}{(n - 1)!}\int_a^x (x - t)^{n-1}f(t)\;\mathrm{d}t
$$

<details>
<summary>Proof</summary>

The repeated integral formula can be proved by induction. The base case $n=1$ reduces expectedly to single Riemann integral

$$
  I_a^1 f(x) = \frac{1}{0!}\int_a^x (x - t)^0 f(t)\;\mathrm{d}t = \int_a^x f(t)\;\mathrm{d}t
$$

Suppose that the formula is true for $n\in\N$. Note that by the Leibniz integral rule

$$
  \frac{\mathrm{d}}{\mathrm{d}x}\left[ \frac{1}{n!}\right] \int_a^x (x-t)^n f(t)\,mathrm{d}t = \frac{1}{(n - 1)!}\int_a^x (x-t)^{n-1} f(t)\;\mathrm{d}t
$$

Applying the induction hypothesis for $n+1$

$$
\begin{align*}
  I_a^{n+1} f(x) &= \int_a^x \int_a^{x_1}\cdots\int_a^{x_n} f(x_n)\;\mathrm{d}x_{n+1}\mathrm{d}x_n\dots\mathrm{d}x_1 \\
  &= \int_a^x \frac{1}{(n - 1)!}\int_a^{x_1} (x_1 - t)^{n-1} f(t)\;\mathrm{d}t\,mathrm{d}x_1 \\
  &= \int_a^x \frac{\mathrm{d}}{\mathrm{d}x_1} \left[\frac{1}{n!} \int_a^{x_1} (x_1 - t)^n f(t)\;\mathrm{d}t \right]\,mathrm{d}x_1 \\
  &= \frac{1}{n!} \int_a^x (x - t)^n f(t)\;\mathrm{d}t
\end{align*}
$$
</details>
</MathBox>

$$
  I^n f(x) = \frac{1}{(n - 1)!}\int_a^x (x - t)^{n-1} f(t) \mathrm{d}t
$$

## Convolution

$$
  (g * f)(x) = \int_0^x g(x - u)f(u) \mathrm{d}u 
$$

## Gaussian integral

The integral of the Gaussian function $f(x) = e^{-x^2}$ over the entire real line can be found with a double integral converted to polar coordinates
$$
\begin{align*}
  \left( \int_{-\infty}^\infty e^{-x^2}\mathrm{d}x \right)^2 &= \int_{-\infty}^\infty e^{-x^2}\mathrm{d}x \, \int_{-\infty}^\infty e^{-y^2}\mathrm{d}y = \int_{-\infty}^\infty \int_{-\infty}^\infty e^{-(x + y)^2}\mathrm{d}x\mathrm{d}y \\
  &= \int_0^{2\pi} \int_0^\infty e^{-r^2}r\mathrm{d}r \mathrm{d}\theta \\
  &= 2\pi \int_0^\infty r e^{-r^2} \mathrm{d}r \\
  &= 2\pi \int_{-\infty}^0 \frac{1}{2} e^s \mathrm{d}s\, , \quad s = -r^2\, , \quad \mathrm{d}r = -\frac{\mathrm{d}s}{2r} \\
  &= \pi \int_{-\infty}^0 e^s \mathrm{d}s \\
  &= \pi
\end{align*}
$$

# Elementary functions

## Exponential function

The exponential function $\exp: \R\to\R$ is defined by
$$
  \exp(x) = e^x := \sum_{i=0}^\infty \frac{x^i}{i!}
$$

where $e$ is Euler's number defined by
$$
  e := \exp(1) = \lim_{n\to\infty} \left(1 + \frac{1}{n}\right)^n
$$

**Properties**
- The exponential function is continuous and strictly monotonically increasing
- $\lim_{x\to\infty} \exp(x) = \infty$ and $\lim_{x\to-\infty} \exp(x) = 0$
- $\exp: \R \to (0, \infty)$ is bijective
- $\exp(x + y) = \exp(x) \cdot \exp(y)$

<details>
<summary>Derivation</summary>

The Cauchy product of $\exp(x)$ and $\exp(y)$ has coefficients
$$
  c_k = \sum_{l=0}^k \frac{x^l}{l!} \cdot \frac{y^{k-l}}{(k - l)!} = \frac{1}{k!} \sum_{l=0}^k \binom{k}{l} x^l y^{k - l} = \frac{1}{k!} (x + y)^k
$$

where the last transition follows from the binomial theorem. Evidentently
$$
  \exp(x + y) = \sum_{k=0}^\infty \frac{1}{k!} (x + y)^k = \left( \sum_{k=0}^\infty  \frac{x^k}{k!} \right) \left( \sum_{k=0}^\infty  \frac{y^k}{k!} \right) = \exp(x) \exp(y)
$$
</details> 

### Euler's formula

Euler's formula $e^{ix} = \cos x + i\sin x$ where $i = \sqrt{-1}$ is the imaginary unit, can be derived from the series representation of $e$ (noting $i^{2k} = (-1)^{k}$ and $i^{2k + 1} = i\cdot i^2k = i(-1)^k $)

$$
\begin{align*}
  e^{ix} &= \sum_{k=0}^\infty \frac{(ix)^k}{k!} \\
  &= \sum_{k=0}^\infty (-1)^k \frac{x^{2k}}{2k!} + i(-1)^{k}\frac{x^{2k + 1}}{(2k + 1)!} \\
  &= \sum_{k=0}^\infty (-1)^k \frac{x^{2k}}{2k!} + i(-1)^{k}\frac{x^{2k + 1}}{(2k + 1)!} \\
  &= \cos x + i\sin x
\end{align*}
$$

## Logarithm funtion

The logarithm function $\log: (0, \infty)\to\R$ is defined by the inverse of $\exp: \R \to (0, \infty)$.

**Properties**
- The logarithm function is continuous and strictly monotonically increasing
- $\log\left( \exp(x) \right) = x$
- $\log(x \cdot y) = \log(x) + \log(y)$

## Polynomials

A polynomial function $p: \R \to \R$ of degree $m$ is defined as finite power series
$$
  p(x) = \sum_{i=0}^m a_i x^i
$$