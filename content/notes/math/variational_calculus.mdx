---
title: 'Variational Calculus'
subject: 'Mathematics'
showToc: true
---

Calculus of variations provides a theoretical framework for finding extrema of functionals, which are mappings $F:X\to\mathbb{F}$ from a function space $X$ to a field $\mathbb{F}$. This framework extends the concept of optimization from finite-dimensional spaces to infinite-dimensional spaces, where the variables are functions rather than finite-dimensional vectors.

# Functional differentiation

<MathBox title='Functional' boxType='definition'>
A *functional* is a function $f:X\to\mathbb{F}$ mapping from a function space $X$ to a field $\mathbb{F}$. A functional is linear if it satisfies
1. **Homogeneity:** $f(\alpha x) = \alpha f(x)$ for $x\in X$ and $\alpha\in\mathbb{F}$
2. **Additivity:** $f(x_1 + x_2) = f(x_1) + f(x_1)$ for $x_1, x_2 \in X$

A functional $K:X^m \to\mathbb{F}$ is *m*-linear if it is linear in each argument, in which case we write $K\in\mathcal{L}^m (X,\mathbb{F})$. A functional is $f:X\to\mathbb{F}$ is quadratic if there exists a bilinear functional $K:X^2 \to\mathbb{F}$ such that $f(x) = K(x, x)$ for every $x\in X$. Any quadratic functional satisfies

$$
  f(\alpha x) = K(\alpha x, \alpha x) = \alpha K (x, \alpha x) = \alpha^2 K(x, x) = \alpha^2 f(x)
$$

A functional is continuous if $\lim_{x\to x_0} f(x) = f(x_0)$.
</MathBox>

<MathBox title='' boxType='proposition'>
Let $X$ be a normed vector space and $\mathbb{F}$ a field. If $f:X\to\mathbb{F}$ is a linear functional continuous at some point $x_0$, then $f$ is continuous on $X$.

<details>
<summary>Proof</summary>

Suppose $f:X\to\mathbb{F}$ is linear and continuous at $x_0 \in X$. Let $y_0 \in X$ and define $z = y - y_0 + x_0$. Then

$$
  \lim_{y\to y_0} \lVert z - x_0 \rVert = \lVert y - y_0 \rVert = 0 \implies z \xrightarrow{y\to y_0} x_0 
$$

Thus,

$$
  f(y) - f(y_0) = f(y - y_0) = f(z - x_0) = f(z) - f(x_0) \xrightarrow{y\to y_0} 0
$$

showing that $f$ is continuous at $y_0$. Since $y_0$ was arbitrary, $f$ is continuous on $X$.
</details>
</MathBox>

<MathBox title='Boundedness of continuous quadratic functionals' boxType='proposition' tag='proposition-11'>
If a functional $f:X\to\mathbb{F}$ is quadratic and continuous, there exists $M < \infty$ such that $|f(x)| < M$ for every $x\in B(0,1) \subset X$.

<details>
<summary>Proof</summary>

Since $f$ is continuous, the pre-image $f^{-1}[B(0,1)]$ is open. Thus, there is $\epsilon > 0$ such that $|f(x)| < 1$ for every $x \in B(0,\epsilon)\subset X$. Set $M = \epsilon^{-2}$, then for $x = 0$ we have $|f(x)| = 0 < M$. For $x\in B(0,1) \setminus\Set{0}$, let $y = \frac{\epsilon}{\lVert x \rVert}x$. Then $\lVert y \rVert = \epsilon$, giving

$$
  |f(x)| = \frac{\lVert x \rVert^2}{\epsilon^2} |f(y)| < \frac{\lVert x \rVert^2}{\epsilon^2} = M
$$
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $f:X\to\mathbb{F}$ be a quadratic functional and $K:X^2 \to\mathbb{F}$ be a bilinear functional with $f(x) = K(x,x)$
1. $\delta f(x)h = K(x,h) +  K(h,x)$ for every $x\in X$
2. If $K$ is continuous, then $f$ is Fréchet differentiable

<details>
<summary>Proof</summary>

**(1):** Evaluating the Gâteaux derivative of $f$ at $x$ using $f(x) = K(x,x)$ gives

$$
\begin{align*}
  \delta f(x) h =& \lim_{\tau\to 0} \frac{f(x + \tau h) - f(x)}{\tau} \\
  =& \lim_{\tau\to 0} \frac{K(x + \tau h, x + \tau h) - K(x,x)}{\tau} \\
  =& \lim_{\tau\to 0} \frac{\tau K(x,h) + \tau K(h,x) + \tau^2 K(h, h)}{\tau} \\
  =& \lim_{\tau\to 0} K(x,h) + K(h, x) + \tau K(h, h) \\
  =& K(x, h) + K(h, x)
\end{align*}
$$

**(2):** Since $\delta f(x) h = K(x, h) + K(h, x)$, then $\delta f(x)$ is clearly linear. If $K$ is continuous, then $\delta f(x)$ is continuous for every $x$. By Proposition $\ref{proposition-11}$

$$
\begin{align*}
  & \frac{f(x + h) - f(x) - \delta f(x)h}{\lVert h \rVert} \\
  &= \frac{K(x + h, x + h) - K(x,x) - (K(x, h) + K(h, x))}{\lVert h \rVert} \\
  &= \frac{K(h, h)}{\lVert h \rVert} \\
  &= \lVert h \rVert K\left(\frac{h}{\lVert h \rVert}, \frac{h}{\lVert h \rVert} \right) \\
  &= \lVert h \rVert f\left(\frac{h}{\lVert h \rVert}\right) \\
  &< \lVert h \rVert M \xrightarrow{h\to 0} 0
\end{align*}
$$

Hence $f'(x) = \delta f(x)$.
</details>
</MathBox>

## Differentiable functionals

<MathBox title='Fréchet differentiable functional' boxType='definition'>
Let $X$ be a normed space. A functional $J:X\to\mathbb{F}$ is Fréchet differentiable at $x\in X$ if there is a continuous linear functional $J'(x): X\to\mathbb{F}$ such that

$$
  \lim_{h\to 0} \frac{J(x + h) - J(x) - J'(x)h}{\lVert h \rVert_X}
$$

The functional $J'(x)$ is the Fréchet derivative of $J$ at $x$. 
</MathBox>

<MathBox title='Gâteaux differentiable functional' boxType='definition'>
Let $X$ be a normed vector space. A functional $J:X\to\mathbb{F}$ is Gâteaux differentiable at $x\in X$ if there is a functional $\delta J(x): X\to\mathbb{F}$ such that

$$
  \lim_{\tau \to 0} \frac{J(x + \tau h) - J(x)}{\tau} = \left.\frac{\d}{\d\tau} J(x + \alpha h)\right|_{\tau=0} = \delta J(x; h)
$$

The functional $\delta J(x)$ is the Gâteaux derivative of $J$ at $x$. The differential $\delta J(x; h)$ is called the *first variation* of $J$.
</MathBox>

<MathBox title='Differential properties of linear functionals' boxType='proposition'>
A linear functional $f:X\to\mathbb{F}$ satisifies
1. $\delta f(x) = f$ for all $x\in X$
2. $\delta^2 f(x) = 0$ for all $x\in X$
3. If $f$ is continuous, then it is Fréchet differentiable for every $x\in X$

<details>
<summary>Proof</summary>

**(1):** By linearity of $f$ we have for $\tau\in\mathbb{F}$

$$
  \lim_{\tau\to 0} \frac{f(x + \tau h) - f(x)}{\tau} = \lim_{\tau\to 0} \frac{f(x) + \tau f(h) - f(x)}{\tau} = f(h)
$$

**(2):** From **(1)** we have $\delta f (x) = f$ such that

By linearity of $f$ we have for $\tau\in\mathbb{F}$

$$
  \lim_{\tau\to 0} \frac{f(x + \tau h) - f(x) - \tau \delta f(h)}{\tau^2} = \lim_{\tau\to 0} \frac{f(x) + \tau f(h) - f(x) - \tau f(h)}{\tau^2} = 0
$$

**(3):** By linearity of $f$ we have

$$
  \frac{f(x + h) - f(x) - f(h)}{\lVert h \rVert} = \frac{f(x) + f(h) - f(x) - f(h)}{\lVert h \rVert} = 0
$$
</details>
</MathBox>

<MathBox title='Differential properties of quadratic functionals' boxType='proposition'>
Suppose that $K:X^2 \to\mathbb{F}$ is bilinear and $f(x) = K(x,x)$ so that $f$ is quadratic.
1. $\delta^2 f(x) = 0$ for every $x\in X$
2. If $f$ is continuous, then it is twice Fréchet differentiable for every $x\in X$

<details>
<summary>Proof</summary>

**(1):** By the bilinearity of $K$

$$
\begin{align*}
  &\lim_{\tau\to 0} \frac{f(x + \tau h) - f(x) - \tau\delta f(x; h)}{\tau^2} \\
  =& \lim_{\tau\to 0} \frac{K(x + \tau h, x + \tau h) - K(x,x) - \tau (K(x, h) + K(x, h))}{\tau^2} \\
  =& \lim_{\tau\to 0} \frac{\tau^2 K(h, h)}{\tau^2} \\
  =& f(h)
\end{align*}
$$

**(2):** Clearly, $\delta^2 f(x)$ is quadratic. If $K$ is continuous, then $\delta^2 f(x)$ is continuous for every $x$. The result follows from

$$
  \frac{f(x + h) - f(x) - \delta f(x)h - \frac{1}{2}\delta^2 f(x)h}{\lVert h \rVert^2} = \frac{K(h,h) - f(h)}{\lVert h \rVert^2} = 0
$$


</details>
</MathBox>

## Definiteness of quadratic functionals

<MathBox title='Definiteness of quadratic functionals' boxType='definition'>
A quadratic functional $J:X\to\mathbb{F}$ is 
- *positive definite*, written $J > 0$, if there is $\epsilon > 0$ such that
$$
  J(x) > \epsilon\lVert x \rVert^2,\; \forall x\neq 0
$$
- *negative definite*, written $J > 0$, if there is $\epsilon > 0$ such that
$$
  J(x) < -\epsilon\lVert x \rVert^2,\; \forall x\neq 0
$$
</MathBox>

<MathBox title='Semi-definiteness of quadratic functionals' boxType='definition'>
A quadratic functional $J:X\to\mathbb{F}$ is 
- *positive semidefinite*, written $J \geq 0$, if $J(x) \geq 0$ for every $x\in X$
- *negative semidefinite*, written $J \leq 0$, if $J(x) \leq 0$ for every $x\in X$
</MathBox>

From the definition, positive (negative) definiteness of a functional implies positive (negative) semidefiniteness. Note that definiteness depends on the choice of norm.

<MathBox title='' boxType='proposition'>
A quadratic functional $J:\R^n \to\R$ by $J(\mathbf{x}) = \mathbf{x}^\top \mathbf{Px}$ is positive (negative) definite if and only if $\mathbf{P}$ is positive (negative) definite.

<details>
<summary>Proof</summary>

For convenience, choose the Euclidean norm $\lVert\mathbf{x}\rVert = \sqrt{\mathbf{x}^\top \mathbf{x}}$. Setting $\mathbf{Q} = \frac{1}{2}(\mathbf{P} + \mathbf{P}^\top)$, we can write $J(\mathbf{x}) = \mathbf{x}^\top \mathbf{Qx}$. Note that $J(\mathbf{x}) > \varepsilon\mathbf{x}^\top\mathbf{x}$ if and only if 

$$
  \mathbf{x}^\top (\mathbf{Q} - \varepsilon\mathbf{I}_n)\mathbf{x} > 0
$$

Hence, $J$ is positive definite if and only if $\mathbf{Q} - \varepsilon \mathbf{I}_n > 0$ for some $\varepsilon > 0$. The eigenvalues of $\mathbf{Q} - \varepsilon\mathbf{I}$ are $\lambda - \varepsilon$, where $\lambda$ ranges of the eigenvalues of $\mathbf{Q}$. By symmetry of $\mathbf{Q}$, it follows that $\mathbf{Q} - \varepsilon \mathbf{I}_n > 0$ if and only if each $\lambda$ satisfies $\lambda > \varepsilon$. Since we are free to choose $\varepsilon$, positive definiteness of $J$ is equivalent to $\lambda > 0$ for every eigenvalue of $\mathbf{Q}$. This implies $\mathbf{Q} > 0$, which is equivalent to $\mathbf{P} > 0$. Negative definiteness is handled analogously.
</details>
</MathBox>

## Extremas

<MathBox title='Critical point' boxType='definition'>
Let $J:X\to\mathbb{F}$ be a functional. Then $x^* \in X$ is a critical point of $J$ if $\delta J(x^*)$ exists and $\delta J(x^*) = 0$ for every $h\in X$, i.e. $\delta J (x^*) = 0$.
</MathBox>

<MathBox title='Local extrema are critical points' boxType='proposition' tag='proposition-5'>
Let $J:X\to\mathbb{F}$ be a functional. If $J$ achieves a local extremum at $x^* \in X$ and $\delta J(x^*)$ exists, then $x^*$ is a critical point. Hence, a necessary condition for an extremum is that the Gâteaux derivative vanishes in every direction.

<details>
<summary>Proof</summary>

Let $h\in J$. If $J$ achieves a local extremum at $x^*$, then $\tau = 0$ is a local extremum of $f(\tau) := J(x^* + \tau h)$ for $\tau\in\mathbb{F}$. Hence

$$
  \delta J(x^*; h) = \left.\frac{\d}{\d\tau} J(x^* + \tau h) \right|_{\tau = 0} = \left.\frac{\d}{\d\tau} f(\tau)\right|_{\tau=0} = 0
$$
</details>
</MathBox>

<MathBox title='Gâteaux conditions for local minima' boxType='proposition' tag='proposition-8'>
Suppose the functional $J:X\to\mathbb{F}$ is twice Gâteaux differentiable at $x\in X$. A necessary condition for $x\in X$ to be a local minimum is that $\delta J (x; h) = 0$ and $\delta^2 F(x; h) \geq 0$. A sufficient condition for a strict local minimum is if in addition $\delta^2 J(x; h) \geq c > 0$ for all unit vectors $h\in\partial B(0, 1)\subseteq X$ and $\delta^2 J$ is continuous at $x$ uniformly with respect to $h\in\partial B(0, 1)$.


<details>
<summary>Proof</summary>

The necessary condition for local extrema was established in Proposition $\ref{proposition-5}$. If $J$ has a local mimimum at $x\in X$, there is $\epsilon > 0$ such that $J(y) \geq J(x)$  for $y \in B(x, \epsilon)$, then

$$
  J(x) \leq J(y + \tau h),\; \tau \in\left(-\frac{\epsilon}{\lVert h \rVert}, \frac{\epsilon}{\lVert h \rVert} \right)
$$

By Proposition $\ref{proposition-5}$ $\delta J(x; h) = 0$ because $J$ has a local extremum at $x$. Since $J$ is twice Gâteaux differentiable at $x$ we have

$$
  0 \leq \frac{J(x - \tau h) - J(x) - \tau\delta J(x; h)}{\tau^2} \xrightarrow{\tau\to 0} \frac{1}{2}\delta^2 J(x; h)
$$

Since $h$ is arbitrary, it follows that $\delta^2 J(x) \geq 0$.

To see the sufficient conditions note that the assumption on $\delta^2 J$ imply that there is $\epsion > 0$ such that $\delta^2 J (y, h) \geq c/2$ for all $y\in B(0,\epsilon)$ and all $h\in\partial B(0,1)$. Equivalently, $\partial^2 J (y, h) \geq\frac{c}{2}|h|^2$ for all $y\in B(0,\epsilon)$ and all $h\in X$. Hence, applying Taylor's theorem to $f(t)$ using $\ddot{f}(t) = \delta^2 J(x + th, h)$ gives

$$
  J(x + h) = f(1) = f(0) + \int_0^1 (1 - s)\ddot{f}(s)\;\d t \geq F(x) + \frac{c}{4}|h|^2
$$

for $h\in B(0,1)$.
</details>
</MathBox>

<MathBox title='Fréchet conditions for local exrema' boxType='proposition'>
Suppose the functional $J:X\to\mathbb{F}$ has a critical point $x\in X$ and a second Fréchet derivative $J''(x)$. If $J''(x) > 0$, then $J$ has a strict local minimum at $x$.

<details>
<summary>Proof</summary>

From the definition of positive definiteness, there is $\varepsilon > 0$ such that

$$
  J''(x) h > \varepsilon\lVert h \rVert^2
$$

for every $h\in X$. Then

$$
  \lim_{h\to 0} \frac{J(x + h) - J(x) - \frac{1}{2}J''(x)h}{\lVert h \rVert^2} = 0 
$$

so there is $\delta > 0$

$$
  \frac{J(x + h) - J(x) - \frac{1}{2}J''(x)h}{\lVert h \rVert^2} > -\frac{\varepsilon}{2}
$$

for $h\in B(0,\delta)$. Hence

$$
  J(x + h) - J(x) > \frac{1}{2}(J''(x) h - \varepsilon\lVert h \rVert^2) > 0
$$

for $h \in B(0,\delta)$.
</details> 
</MathBox>

<MathBox title='Sufficient condition for twice continuously differentiable functionals' boxType='corollary'>
Suppose $J\in C^2 (U, \R)$ is twice continuously differentiable on a normed subspace $U\subseteq X$. A sufficient condition for $x \in U$ to be a strict local minimum is $J'(x) = 0$ and $\d^2 J(x)h^2 \geq c|h|^2$ for all $h\in X$.

<details>
<summary>Proof</summary>

Note that since

$$
  |\delta^2 J(x; h) - \delta^2 J(y; h)| \leq\lVert \d^2 J(x) - \d^2 J(y) \rVert\cdot|h|^2
$$

continuity requirement from Proposition $\ref{proposition-8}$ is satisfied.
</details>
</MathBox>

<MathBox title='Existence of global minima' boxType='proposition'>
Suppose $C \subseteq X$ is a convex subspace and $J:C\to\mathbb{F}$ is a convex functional. Then every local minimum is a global minimum. Moreover, if $J$ is strictly convex, then the minimum is unique.

<details>
<summary>Proof</summary>

Suppose $x$ is a local minimum and $J(y) < J(x)$. Then $J(\lambda y + (1 - \lambda) x) \leq\lambda J(y) + (1 - \lambda)J(x) < J(x)$ for $\lambda\in (0,1)$ contradicts the fact that $x$ is a local minimum. If $x, y$ are two global minima, then $J(\lambda y + (1 - \lambda)x) < J(y) = J(x)$ yielding a contradiction unless $x = y$. 
</details>
</MathBox>

<MathBox title='Condition for global minima' boxType='proposition'>
uppose $C \subseteq X$ is a convex subspace and $J:C\to\mathbb{F}$ is a convex functional. If the Gâteaux derivative exists at an interior point $x\in C$ and satisfies $\delta J(x;h) = 0$ for all $h\in X$, then $x$ is a global minimum.

<details>
<summary>Proof</summary>

By assumption $f(t) := J(x + th)$ is a convex function defined on an interval containing $0$ with $f'(0) = 0$. If $y$ is another point we can choose $h = y - x$, such that $J(y) = f(1) \geq f(0) = J(x)$.
</details>
</MathBox>

<MathBox title='Convex functionals have positive second Gâteaux derivative' boxType='proposition'>
uppose $C \subseteq X$ is a convex subspace and $J:C\to\mathbb{F}$ is twice Gâteaux differentible on $C$. Then $J$ is convex if and only if $\delta^2 J(x; h)\geq 0$ for all $x\in C$ and $h\in X$. Moreover, $J$ is strictly convex if and only if $\delta^2 F(x; h) > 0$ for all $x\in C$ and $h \in X\setminus\Set{0}$.

<details>
<summary>Proof</summary>

Define $f(t) := J(x + th)$ with $f'(t) = \delta J(x + th; h)$ and $f''(t) = \delta^2 J(x + th; h)$. Moreover, note that $f$ is (strictly) convex for all $x\in C$ and $u\in X\setminus\Set{0}$ if and only if $J$ is (strictly) convex. If $J$ is (strictly) convex, so is $f$. To see the converse note that

$$
\begin{align*}
  J(\lambda y + (1-\lambda)x) =& f(\lambda) \\
  \leq& \lambda f(1) - (1 - \lambda)f(0) \\
  =& \lambda J(y) - (1 - \lambda) J(x)
\end{align*}
$$

with strict inequality if $f$ is strictly convex.
</details>
</MathBox>

<MathBox title='Convex functionals have monotone Gâteaux derivative' boxType='proposition'>
Suppose $C\subseteq X$ is open and convex and $J:C\to\R$ has Gâteaux derivatives $\delta J(x)\in X^*$ for every $x\in C$. Then $J$ is (strictly) convex if and only if $\delta J$ is (strictly) monotone.

<details>
<summary>Proof</summary>

Define $f(t) := J(x + th)$. Note that by assumption $\delta J: C\to X^*$ and the claim follows since $f'(t) = \delta J(x + th; h)$, which shows that $\delta J$ is (strictly) monotone if and only if $f'$ is (strictly) increasing.
</details>
</MathBox>

<MathBox title='Fundamental lemma of variational calculus' boxType='lemma'>
Let $f:[a,b]\to\R$ be a continuous function. If

$$
  \int_a^b f(x)g(x)\;\d x = 0
$$

for all twice differentiable functions $g:[a,b]\to\R$ with $g(a) = g(b) = 0$ and compact support in $(a,b)$, then $f$ is identically zero.

<details>
<summary>Proof</summary>

Suppose by contradiction that $f$ is nonzero in some subinterval $(c,d) \subseteq (a, b)$. Without loss of generality, assume $f:(c,d) > 0$. Since $g$ is arbitrary, we are free to set it at our choice, say

$$
  g(x) = \begin{cases} (x - c)(d - x),\quad x\in [c,d] \\ 0,\quad x < c \lor x > d \end{cases}
$$

Noting that $(x - c) > 0$ and $(d - x) > 0$ for $x\in [c,d]$, consider the integral

$$
  \int_a^b f(x)g(x)\;\d x = \int_c^d f(x) (x - c)(d - x)\;\d x > 0
$$

which is positive because the integrand is positive in $(c,d)$. This gives a contradiction, proving that $f$ must be identically zero in $[a,b]$.
</details>
</MathBox>

# Lagrange problem

Let $(X, L)$ be a real dynamical system of dimension $n$ with configuration space $X\subseteq\R^n$ and evolution function $L\in C^2 (\R\times\R^n \times\R^n, \R)$ called the *Lagrangian*. A Lagrange problems seek to find the path $\mathbf{q} \in C^2 ([t_0, t_1]\subseteq\R, X)$ with fixed endpoints that minimizes the integral functional 

$$
  S(\mathbf{q}) := \int_{t_0}^{t_1} L(t, \mathbf{q}(t), \dot{\mathbf{q}(t)})\;\d t
$$

on $D = \Set{\mathbf{q} \in C^2 ([t_0, t_1], X) | \mathbf{q}(t_0) = \mathbf{x}_0, \mathbf{q}(t_1) = \mathbf{x}_1}$.

<MathBox title='Euler-Lagrange equations' boxType='theorem'>
Let $(X, L)$ be a real dynamical system of dimension $n$ with configuration space $X\subseteq\R^n$ and *Lagrangian* $L\in C^2 (\R\times\R^n \times\R^n, \R)$. Let 

$$
  D = \Set{\mathbf{q} \in C^2 ([t_0, t_1]\subseteq\R, X) | \mathbf{q}(t_0) = \mathbf{x}_0, \mathbf{q}(t_1) = \mathbf{x}_1}
$$

be the set of twice continuously differentiable paths in $X$ with fixed endpoints. The action functional $S:D\to\R$ given by

$$
  S(\mathbf{q}) := \int_{t_0}^{t_1} L(t, \mathbf{q}(t), \dot{\mathbf{q}(t)})\;\d t
$$

is stationary on a path $\mathbf{q} \in X$ if and only if $L$ satisfies the Euler-Lagrange equations

$$
\begin{align*}
  \nabla_{\mathbf{q}} L(t,\mathbf{q}(t),\dot{\mathbf{q}}(t)) =& \frac{\d}{\d t} \nabla_{\dot{\mathbf{q}}} L(t,\mathbf{q}(t),\dot{\mathbf{q}}(t)) \\
  \frac{\partial L}{\partial q_i}(t,\mathbf{q}(t),\dot{\mathbf{q}}(t)) =& \frac{\d}{\d t} \frac{\partial L}{\partial \dot{q}_i}(t,\mathbf{q}(t),\dot{\mathbf{q}}(t)),\; i=1,\dots,n
\end{align*}
$$

<details>
<summary>Proof</summary>

The first Gâteaux variation of $S$ at $\mathbf{q}+in D$ in the direction $\mathbf{h} \in C^2 ([t_0, t_1], X)$ for $s\in\R$ is given by

$$
\begin{align*}
  \delta S(\mathbf{q}(t); \mathbf{h}(t)) =& \left.\frac{\d}{\d s} S(\mathbf{q}(t) + t\mathbf{h}(t))\right|_{s=0} \\
  =& \left.\frac{\d}{\d s} \right|_{s=0} \int_{t_0}^{t_1} L(t, \mathbf{q}(t) + s\mathbf{h}(t), \dot{\mathbf{q}}(t) + s\dot{\mathbf{h}}(t))\;\d t \\
  =& \int_{t_0}^{t_1} \left.\frac{\d}{\d s} L(t, \mathbf{q} + s\mathbf{h}, \dot{\mathbf{q}} + s\dot{\mathbf{h}})\right|_{s=0} \;\d t
\end{align*}
$$

Applying the chain rule gives

$$
\begin{align*}
  \delta S(\mathbf{q}; \mathbf{h}) =& \int_{t_0}^{t_1} \left.\frac{\d}{\d s} L(t, \mathbf{q} + s\mathbf{h}, \dot{\mathbf{q}} + s\dot{\mathbf{h}})\right|_{s=0} \;\d t \\
  =& \int_{t_0}^{t_1} \left.\mathbf{h}\cdot\nabla_{\mathbf{q} + s\mathbf{h}} L(t, \mathbf{q} + s\mathbf{h}, \dot{\mathbf{q}} + s\dot{\mathbf{h}})\right|_{s=0} \\
  &+ \int_{t_0}^{t_1} \left.\dot{\mathbf{h}}\cdot\nabla_{\dot{\mathbf{q}} + s\dot{\mathbf{h}}} L(t, \mathbf{q} + s\mathbf{h}, \dot{\mathbf{q}} + s\dot{\mathbf{h}})\right|_{s=0} \;\d t \\
  =& \int_{t_0}^{t_1} \left[\mathbf{h}\cdot\nabla_\mathbf{q} L(t,\mathbf{q},\dot{\mathbf{q}}) + \dot{\mathbf{q}}\cdot\nabla_{\dot{\mathbf{q}}} L(t,\mathbf{q},\dot{\mathbf{q}})\right]\;\d t
\end{align*}
$$

Integrating the second term by parts we get

$$
\begin{align*}
  \int_{t_0}^{t_1} \dot{\mathbf{q}}\cdot\nabla_{\dot{\mathbf{q}}} L(t,\mathbf{q},\dot{\mathbf{q}})\;\d t =& \underbrace{\left.\mathbf{h} L(t,\mathbf{q},\dot{\mathbf{q}})\right|_{t_0}^{t_1}}_{=0} \\
  &- \int_{t_0}^{t_1} \mathbf{v} \frac{\d}{\d t}\nabla_{\dot{\mathbf{q}}} L(t,\mathbf{q},\dot{\mathbf{q}})\;\d t
\end{align*}
$$

Since $\mathbf{q} + s\mathbf{h}$ and $\mathbf{q}$ satisfy the same boundary conditions, we must have $\mathbf{h}(t_0) = \mathbf{h}(t_1) = 0$. Substituting back gives

$$
  \delta S(\mathbf{q}; \mathbf{h}) = \int_{t_0}^{t_1} \mathbf{h}\cdot\left(\nabla_\mathbf{q} L(t,\mathbf{q},\dot{\mathbf{q}}) - \frac{\d}{\d t} \nabla_{\dot{\mathbf{q}}} L(t, \mathbf{q}, \dot{\mathbf{q}}) \right)\;\d t
$$

In order the be a critical point, a path $\mathbf{q}$ must satisfy $\delta J(\mathbf{q}; \mathbf{h}) = 0$ for all $\mathbf{h}$. By the fundamental lemma of variational calculus, we must have

$$
  \nabla_{\mathbf{q}} L(t,\mathbf{q}(t),\dot{\mathbf{q}}(t)) = \frac{\d}{\d t} \nabla_{\dot{\mathbf{q}}} L(t,\mathbf{q}(t),\dot{\mathbf{q}}(t))
$$
</details>
</MathBox>

Calculus of variations seeks to find extremas of functionals in the form of definite integrals

$$
  I[y] = \int_{x_1}^{x_2} L\left(x, y, \frac{\d y}{\d x} \right) \d x
$$

The extremal function $y = f(x)$ of $I[y]$ can be found by introducing a variation in $f$

$$
\begin{align*}
  \bar{f}(x) &= f(x) + \varepsilon \eta(x) \, , \quad \eta(x_1) = \eta(x_2) = 0 \\
  \bar{f}'(x) &= f'(x) + \varepsilon\eta'(x)
\end{align*}
$$

Substituting $\bar{f}$ into $I[y]$ results in a function of $\varepsilon$, $\Phi(\varepsilon) \equiv I[\bar{f}]$ and since $y = f(x)$ is a minimum of $I[y]$, the function $\Phi(\varepsilon)$ is stationary at $\varepsilon = 0$ 

$$
\begin{align*}
  0 &= \left. \frac{\d\Phi}{\d\varepsilon} \right|_{\varepsilon = 0} \\
  &= \left. \frac{\d}{\d\varepsilon} \right|_{\varepsilon = 0} \int_{x_1}^{x_2} L\left(x, \bar{f}, \bar{f}' \right) \d x \\
  &= \left. \int_{x_1}^{x_2} \frac{\d}{\d\varepsilon} L\left(x, \bar{f}, \bar{f}' \right) \d x \right|_{\varepsilon = 0} \\
  &= \left. \int_{x_1}^{x_2} \left( \frac{\partial L}{\partial \bar{f}} \frac{\partial \bar{f}}{\partial \varepsilon} + \frac{\partial L}{\partial \bar{f}'}\frac{\partial \bar{f}'}{\partial \varepsilon} \right) \right|_{\varepsilon = 0} \d x \\
  &= \left. \int_{x_1}^{x_2} \left( \frac{\partial L}{\partial \bar{f}} \eta + \frac{\partial L}{\partial \bar{f}'}\eta' \right) \right|_{\varepsilon = 0} \d x
\end{align*}
$$

Integrating the second term by parts

$$
  \int_{x_1}^{x^2} \frac{\partial L}{\partial \bar{f}'}\eta' \d x = \frac{\partial L}{\partial \bar{f}'} \underbrace{\left[ \eta \right]_{x_1}^{x_2}}_{=0} - \int_{x_1}^{x^2} \eta \frac{\d}{\d x} \left( \frac{\partial L}{\partial \bar{f}'} \right) \d x
$$

and inserting back

$$
\begin{align*}
  0 &= \left. \int_{x_1}^{x_2} \left[ \frac{\partial L}{\partial \bar{f}} \eta - \frac{\d}{\d x} \left( \frac{\partial L}{\partial \bar{f}'} \right) \eta \right] \right|_{\varepsilon = 0} \d x \\
  &= \int_{x_1}^{x_2} \left[ \frac{\partial L}{\partial f} - \frac{\d}{\d x} \left( \frac{\partial L}{\partial f'} \right) \right] \eta \d x 
\end{align*}
$$

By the fundamental lemma of calculus of variations, the integrand part in the paranthesis vanishes. Thus, if $f(x)$ is an extremal of $I[y]$, then $f(x)$ satisfy the Euler-Lagrange equation

$$
  \frac{\partial L}{\partial f} - \frac{\d}{\d x} \left( \frac{\partial L}{\partial f'} \right) = 0
$$

## Beltrami identity

The Beltrami identity is a special case of the Euler-Lagrange equation when $L$ does not depend on $x$, i.e. $\frac{\partial L}{\partial x} = 0$

$$
\begin{align*}
  \frac{\partial L}{\partial f} - \frac{\d}{\d x} \left( \frac{\partial L}{\partial f'} \right) = 0 \\
  y' \frac{\partial L}{\partial f} - y' \frac{\d}{\d x} \left( \frac{\partial L}{\partial f'} \right)
\end{align*}
$$

The term $y' \frac{\partial L}{\partial y}$ is also found in the total derivative $\frac{\d L(x, y, y')}{\d x}$

$$
\begin{gather*}
  \frac{\d L}{\d x} = \frac{\partial L}{\partial x} + \frac{\partial L}{\partial y}y' + \frac{\partial L}{\partial y'}y'' \\
  y' \frac{\partial L}{\partial y} = \frac{\d L}{\d x} - \frac{\partial L}{\partial x} - \frac{\partial L}{\partial y'}y''
\end{gather*}
$$

Inserting back to the modified Euler-Lagrange equation gives

$$
\begin{gather*}
  \frac{\d L}{\d x} - \frac{\partial L}{\partial x} - \left[ \frac{\partial L}{\partial y'}y'' - y' \frac{\d}{\d x} \left( \frac{\partial L}{\partial f'} \right) \right] = 0 \\
  \frac{\d L}{\d x} - \frac{\partial L}{\partial x} - \frac{\d}{\d x} \left( y' \frac{\partial L}{\partial y'} \right) = 0 \\
  \frac{\d}{\d x} \left( L - y' \frac{\partial L}{\partial y'} \right) = \underbrace{\frac{\partial L}{\partial x}}_{=0} \\
  \frac{\d}{\d x} \left( L - y' \frac{\partial L}{\partial y'} \right) = 0
\end{gather*}
$$

Integrating on both sides results in the Beltrami identity

$$
  L - y' \frac{\partial L}{\partial y'} = C
$$

where $C$ is a constant dependent on boundary conditions.

### Brachistochrone problem

The brachistochrone problem seeks to find the path $y = f(x)$ which minimized the time travelled from $a = (0, h)$ to $b = (L, 0)$ in a gravitational field:

$$
  T = \int_a^b \d t = \int_a^b \frac{\d S}{v(x, y)}
$$

where
- the infinitisemal distance $\d S$ is retrieved from the Pythagorean theorem: $\sqrt{1 + \left(\frac{\d y}{\d x}\right)^2}\d x$
- the velocity $v(x, y)$ is retrieved from conservation of energy
$$
\begin{gather*}
  mgh = mgy + \frac{1}{2}mv^2 \\
  v = \sqrt{2g(h-y)}
\end{gather*}
$$

Substituting back into $T$ yields

$$
  T = \int_a^b \sqrt{\frac{1 + (y')^2}{2g(h-y)}} \d x
$$

The minimal function is found with the Euler-Lagrange equation. Since the integrand is not dependent on $x$, the Beltrami identity applies

$$
\begin{gather*}
  \sqrt{\frac{1 + (y')^2}{2g(h-y)}} - \frac{(y')^2}{\sqrt{\left[1 + (y')^2 \right]\left[2g (h - y) \right]}} = C \\
  1 + (y')^2 - (y')^2 = C \sqrt{\left[1 + (y')^2 \right]\left[2g (h - y) \right]} \\
  1 = C^2 \left[1 + (y')^2 \right]\left[2g (h - y) \right] \\
  \left[1 + (y')^2 \right] (h - y) = \underbrace{C_1}_{=\left(2gC^2\right)^{-1}} \\
  \frac{\d y}{\d x} = \sqrt{\frac{C_1 - (h - y)}{h - y}}
\end{gather*}
$$

The differential equation can be solved by separation of variables

$$
\begin{align*}
  \d x &= \sqrt{\frac{C_1 - (h - y)}{h - y}} \d y \\
  x &= \int \sqrt{\frac{C_1 - (h - y)}{h - y}} \d y
\end{align*}
$$

The right hand side integral can be solved by substituting 

$$
\begin{gather*}
  y = h - C_1 \sin^2 \left( \frac{\theta}{2} \right) = h - \frac{C_1}{2}(1 - \cos\theta) \\
  \d y = - C_1 \sin\left( \frac{\theta}{2} \right)\cos\left( \frac{\theta}{2} \right) \d\theta
\end{gather*}
$$

Inserting back into the differential equation gives

$$
\begin{align*}
  x &= -C_1 \int \sqrt{\frac{C_1 \sin^2 \left( \frac{\theta}{2} \right)}{C_1 \left[1 - \sin^2 \left( \frac{\theta}{2} \right) \right]}}  \sin\left( \frac{\theta}{2} \right)\cos\left( \frac{\theta}{2} \right) \d\theta \\
  &= -C_1 \int \sin^2 \left( \frac{\theta}{2} \right) \d\theta \\
  &= -\frac{C_1}{2} \int (1 - \cos\theta)\d\theta \\
  &= \frac{K_1}{2}\left( \theta - \sin\theta \right) + K_2
\end{align*}
$$

This results in the following parametric equations

$$
\begin{align*}
  x &= \frac{K_1}{2}\left( \theta - \sin\theta \right) + K_2 \\
  y &= h + \frac{K_1}{2}(1 - \cos\theta)
\end{align*}
$$

The boundary condition at $a = (0, h)$ corresponding to $\theta = 0$ implies $K_2 = 0$, while the boundary condition at $b = (L, 0)$ gives an equation set that is not easily solved analytically. Hence, the following parametric equations describe the brachistochrone curve

$$
\begin{align*}
  x &= \frac{K_1}{2}\left( \theta - \sin\theta \right) \\
  y &= h + \frac{K_1}{2}(1 - \cos\theta)
\end{align*}
$$

which are the equations of a cycloid.

# Lagrange multiplier method

The Langrange multiplier method finds extrema of a function $f: \R^n \to \R$ subject to $k$ equality constraints $g_i(\mathbf{x}) = c_i$ where $g_i : \R^n \to \R$.

The extrema of $f$ are points whose gradients are linear combinations of the constraint gradients

$$
  \nabla f(\mathbf{x}) = \sum_i^k \lambda_i \left[c_i - \nabla g_i \left(\mathbf{x}\right)\right]
$$

where $\lambda_i$ are the Langrange multipliers. The problem can be converted to an unconstrained optimization by defining the Lagrange function 

$$
  L(\mathbf{x}, \lambda) = f(\mathbf{x}) - \sum_i^k \lambda_i \left[c_i - g_i\left( \mathbf{x} \right)\right]
$$

whoose extrames are given by the Euler-Lagrange equations.