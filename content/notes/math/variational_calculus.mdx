---
title: 'Variational Calculus'
subject: 'Mathematics'
showToc: true
---

Calculus of variations provides a theoretical framework for finding extrema of functionals, which are mappings $F:X\to\mathbb{F}$ from a function space $X$ to a field $\mathbb{F}$. This framework extends the concept of optimization from finite-dimensional spaces to infinite-dimensional spaces, where the variables are functions rather than finite-dimensional vectors.

# Functional differentiation

<MathBox title='Functional' boxType='definition'>
A *functional* is a function $f:X\to\mathbb{F}$ mapping from a function space $X$ to a field $\mathbb{F}$. A functional is linear if it satisfies
1. **Homogeneity:** $f(\alpha x) = \alpha f(x)$ for $x\in X$ and $\alpha\in\mathbb{F}$
2. **Additivity:** $f(x_1 + x_2) = f(x_1) + f(x_1)$ for $x_1, x_2 \in X$

A functional $K:X^m \to\mathbb{F}$ is *m*-linear if it is linear in each argument, in which case we write $K\in\mathcal{L}^m (X,\mathbb{F})$. A functional is $f:X\to\mathbb{F}$ is quadratic if there exists a bilinear functional $K:X^2 \to\mathbb{F}$ such that $f(x) = K(x, x)$ for every $x\in X$. Any quadratic functional satisfies

$$
  f(\alpha x) = K(\alpha x, \alpha x) = \alpha K (x, \alpha x) = \alpha^2 K(x, x) = \alpha^2 f(x)
$$

A functional is continuous if $\lim_{x\to x_0} f(x) = f(x_0)$.
</MathBox>

<MathBox title='' boxType='proposition'>
Let $X$ be a normed vector space and $\mathbb{F}$ a field. If $f:X\to\mathbb{F}$ is a linear functional continuous at some point $x_0$, then $f$ is continuous on $X$.

<details>
<summary>Proof</summary>

Suppose $f:X\to\mathbb{F}$ is linear and continuous at $x_0 \in X$. Let $y_0 \in X$ and define $z = y - y_0 + x_0$. Then

$$
  \lim_{y\to y_0} \lVert z - x_0 \rVert = \lVert y - y_0 \rVert = 0 \implies z \xrightarrow{y\to y_0} x_0 
$$

Thus,

$$
  f(y) - f(y_0) = f(y - y_0) = f(z - x_0) = f(z) - f(x_0) \xrightarrow{y\to y_0} 0
$$

showing that $f$ is continuous at $y_0$. Since $y_0$ was arbitrary, $f$ is continuous on $X$.
</details>
</MathBox>

<MathBox title='Fréchet differentiable functional' boxType='definition'>
Let $X$ be a normed space. A functional $J:X\to\mathbb{F}$ is Fréchet differentiable at $x\in X$ if there is a continuous linear functional $J'(x): X\to\mathbb{F}$ such that

$$
  \lim_{h\to 0} \frac{J(x + h) - J(x) - J'(x)h}{\lVert h \rVert_X}
$$

The functional $J'(x)$ is the Fréchet derivative of $J$ at $x$. 
</MathBox>

<MathBox title='Gâteaux differentiable functional' boxType='definition'>
Let $X$ be a normed vector space. A functional $J:X\to\mathbb{F}$ is Gâteaux differentiable at $x\in X$ if there is a functional $\delta J(x): X\to\mathbb{F}$ such that

$$
  \lim_{\tau \to 0} \frac{J(x + \tau h) - J(x)}{\tau} = \left.\frac{\d}{\d\tau} J(x + \alpha h)\right|_{\tau=0} = \delta J(x) h
$$

The functional $\delta J(x)$ is the Gâteaux derivative of $J$ at $x$. 
</MathBox>

<MathBox title='Differential properties of linear functionals' boxType='proposition'>
A linear functional $f:X\to\mathbb{F}$ satisifies
1. $\delta f(x) = f$ for all $x\in X$
2. If $f$ is continuous, then it is Fréchet differentiable for every $x\in X$

<details>
<summary>Proof</summary>

**(1):** By linearity of $f$ we have for $\tau\in\mathbb{F}$

$$
  \lim_{\tau\to 0} \frac{f(x + \tau h) - f(x)}{\tau} = \lim_{\tau\to 0} \frac{f(x) + \tau f(h) - f(x)}{\tau} = f(h)
$$

**(2):** By linearity of $f$ we have

$$
  \frac{f(x + h) - f(x) - f(h)}{\lVert h \rVert} = \frac{f(x) + f(h) - f(x) - f(h)}{\lVert h \rVert} = 0
$$
</details>
</MathBox>

<MathBox title='Boundedness of continuous quadratic functionals' boxType='proposition' tag='proposition-11'>
If a functional $f:X\to\mathbb{F}$ is quadratic and continuous, there exists $M < \infty$ such that $|f(x)| < M$ for every $x\in B(0,1) \subset X$.

<details>
<summary>Proof</summary>

Since $f$ is continuous, the pre-image $f^{-1}[B(0,1)]$ is open. Thus, there is $\epsilon > 0$ such that $|f(x)| < 1$ for every $x \in B(0,\epsilon)\subset X$. Set $M = \epsilon^{-2}$, then for $x = 0$ we have $|f(x)| = 0 < M$. For $x\in B(0,1) \setminus\Set{0}$, let $y = \frac{\epsilon}{\lVert x \rVert}x$. Then $\lVert y \rVert = \epsilon$, giving

$$
  |f(x)| = \frac{\lVert x \rVert^2}{\epsilon^2} |f(y)| < \frac{\lVert x \rVert^2}{\epsilon^2} = M
$$
</details>
</MathBox>

<MathBox title='' boxType='proposition'>
Let $f:X\to\mathbb{F}$ be a quadratic functional and $K:X^2 \to\mathbb{F}$ be a bilinear functional with $f(x) = K(x,x)$
1. $\delta f(x)h = K(x,h) +  K(h,x)$ for every $x\in X$
2. If $K$ is continuous, then $f$ is Fréchet differentiable

<details>
<summary>Proof</summary>

**(1):** Evaluating the Gâteaux derivative of $f$ at $x$ using $f(x) = K(x,x)$ gives

$$
\begin{align*}
  \delta f(x) h =& \lim_{\tau\to 0} \frac{f(x + \tau h) - f(x)}{\tau} \\
  =& \lim_{\tau\to 0} \frac{K(x + \tau h, x + \tau h) - K(x,x)}{\tau} \\
  =& \lim_{\tau\to 0} \frac{\tau K(x,h) + \tau K(h,x) + \tau^2 K(h, h)}{\tau} \\
  =& \lim_{\tau\to 0} K(x,h) + K(h, x) + \tau K(h, h) \\
  =& K(x, h) + K(h, x)
\end{align*}
$$

**(2):** Since $\delta f(x) h = K(x, h) + K(h, x)$, then $\delta f(x)$ is clearly linear. If $K$ is continuous, then $\delta f(x)$ is continuous for every $x$. By Proposition $\ref{proposition-11}$

$$
\begin{align*}
  & \frac{f(x + h) - f(x) - \delta f(x)h}{\lVert h \rVert} \\
  &= \frac{K(x + h, x + h) - K(x,x) - (K(x, h) + K(h, x))}{\lVert h \rVert} \\
  &= \frac{K(h, h)}{\lVert h \rVert} \\
  &= \lVert h \rVert K\left(\frac{h}{\lVert h \rVert}, \frac{h}{\lVert h \rVert} \right) \\
  &= \lVert h \rVert f\left(\frac{h}{\lVert h \rVert}\right) \\
  &< \lVert h \rVert M \xrightarrow{h\to 0} 0
\end{align*}
$$

Hence $f'(x) = \delta f(x)$.
</details>
</MathBox>

<MathBox title='' boxType='proposition'>

</MathBox>



<MathBox title='Differentiable functional' boxType='definition'>
A functional $S[]:\mathcal{F}\to\R$ is *differentiable*, if for any smooth curve $\mathbf{r}(t)\in\mathcal{F}$ and $\delta\mathbf{r}(t) \in\mathcal{F}_\varepsilon$, where

$$
  \mathcal{F}_\varepsilon = \Set{\delta\mathbf{r}:I\to\R^n : \lvert\delta\mathbf{r}(t)\rvert < \varepsilon,\; \left\lvert \frac{\d}{\d t} \delta\mathbf{r}(r) \right\rvert < \varepsilon}
$$

a functional $\delta S[\cdot,\cdot]: \mathcal{F}\times\mathcal{F}_\varepsilon \to\R$ exists with the properties
1. $S[\mathbf{r}(t) + \delta\mathbf{r}(t)] = S[\mathbf{r}(t)] + \delta S[\mathbf{r}(t), \delta\mathbf{r}(t)] + \mathcal{O}(\varepsilon^2)$
2. $\delta S[\mathbf{r}(t), \delta\mathbf{r}(t)]$ is linear in $\delta\mathbf{r}(t)$, i.e. for $\alpha_1, \alpha_2 \in \R$
$$
  \delta S[\mathbf{r}(t), \alpha_1 \delta\mathbf{r}_1(t) + \alpha_2 \delta\mathrm{r}_2(t)] = \alpha_1 S[\mathbf{r}(t), \delta\mathbf{r}_1 (t)] + \alpha_2 \delta S[\mathbf{r}(t), \delta\mathbf{r}_2 (t)] 
$$

The map $\delta S[\cdot,\cdot]$ is called the *first variation* of $S[]$.
</MathBox>





<MathBox title='Fundamental lemma of variational calculus' boxType='lemma'>
Let $f:[a,b]\to\R$ be a continuous function. If

$$
  \int_a^b f(x)g(x)\;\d x = 0
$$

for all twice differentiable functions $g:[a,b]\to\R$ with $g(a) = g(b) = 0$ and compact support in $(a,b)$, then $f$ is identically zero.

<details>
<summary>Proof</summary>

Suppose by contradiction that $f$ is nonzero in some subinterval $(c,d) \subseteq (a, b)$. Without loss of generality, assume $f:(c,d) > 0$. Since $g$ is arbitrary, we are free to set it at our choice, say

$$
  g(x) = \begin{cases} (x - c)(d - x),\quad x\in [c,d] \\ 0,\quad x < c \lor x > d \end{cases}
$$

Noting that $(x - c) > 0$ and $(d - x) > 0$ for $x\in [c,d]$, consider the integral

$$
  \int_a^b f(x)g(x)\;\d x = \int_c^d f(x) (x - c)(d - x)\;\d x > 0
$$

which is positive because the integrand is positive in $(c,d)$. This gives a contradiction, proving that $f$ must be identically zero in $[a,b]$.
</details>
</MathBox>

# Lagrange problems

# Euler-Lagrange equations

<MathBox title='Objective function' boxType='definition'>
Let $\mathbf{u}$
</MathBox>


<MathBox title='Action functional differential' boxType='theorem'>
Let $(X,L)$ be a real dynamic system with $n$ degrees of freedom, where $X$ is a smooth manifold representing the configuration space and $L: I\subseteq \R \times TX\to\R$ as $L(\mathbf{q}(t), \dot{\mathbf{q}}(t), t)$ is the Lagrangian of the system. Let $\mathcal{F}(a, b, \mathbf{x}_a, \mathbf{x}_b)$ be the set of smooth curves $\mathbf{q}: [a,b]\to X$ for which $\mathbf{q}(a) = \mathbf{x}_a$ and $\mathbf{q}(b) = \mathbf{x}_b$. Define the action functional $S:\mathcal{F}(a,b,\mathbf{x}_a, \mathbf{x}_b)\to\R$ by

$$
  S[\mathbf{q}] = \int_a^b L(\mathbf{q}(t), \dot{\mathbf{q}}(t), t) \,\d t
$$

The differential of $S[]$ is given by

$$
  \delta S[\mathbf{q}(t), \delta\mathbf{q}(t)] = \int_a^b \d t \left( \sum_{i=1}^n \left[\frac{\partial L}{\partial q_i} - \frac{\d}{\d t}\left(\frac{\partial L}{\partial \dot{q}_i} \right)\right] \delta q_i (t) \right) + \sum_{i=1}^n \left.\frac{\partial L}{\partial \dot{q}_j} \partial q_j (t) \right|_a^b
$$

<details>
<summary>Proof</summary>

By variation in $\mathbf{q}$, we have

$$
  S[\mathbf{q}(t) + \delta\mathbf{q}(t)] = \int_a^b L(t, \mathbf{q}(t) + \delta\mathbf{q}(t), \dot{\mathbf{q}}(t) + \partial\dot{\mathbf{q}}(t), t) \;\d t
$$

Introducing a perturbation $\varepsilon$ and applying Taylor series expansion of $L$ about $t$ to first order we get

$$
\begin{align*}
  L(\mathbf{q}(t) + \delta\mathbf{q}(t), \dot{\mathbf{q}}(t) + \partial\dot{\mathbf{q}}(t), t) =& L(\mathbf{q}(t), \dot{\mathbf{q}}(t), t) \\
  &+ \sum_{i=1}^n \left(\frac{\partial L}{\partial q_i} \delta q_i + \frac{\partial L}{\partial\dot{q}_i} \delta q_i \right) + \mathcal{O}(\varepsilon^2)
\end{align*}
$$

By the product rule, we have

$$
  \frac{\partial L}{\partial \dot{q}_i} \delta\dot{q}_i = \frac{\d}{\d t}\left(\frac{\partial L}{\partial\dot{q}_i} \delta q_i \right) - \left(\frac{\d}{\d t} \frac{\partial L}{\partial\dot{q}_i} \right)\delta q_i
$$

Since $S[\mathbf{q}(t) + \delta\mathbf{q}(t)] = S[\mathbf{q}(t)] + \delta S[\mathbf{q}(t), \delta(\mathbf{q})(t)] + \mathcal{O}(\varepsilon^2)$ we get

$$
\begin{align*}
  \delta S[\mathbf{q}(t), \delta(\mathbf{q})(t)] =& S[\mathbf{q}(t) + \delta\mathbf{q}(t)] - \delta
\end{align*}
$$
</details>
</MathBox>




Calculus of variations seeks to find extremas of functionals in the form of definite integrals

$$
  I[y] = \int_{x_1}^{x_2} L\left(x, y, \frac{\d y}{\d x} \right) \d x
$$

The extremal function $y = f(x)$ of $I[y]$ can be found by introducing a variation in $f$

$$
\begin{align*}
  \bar{f}(x) &= f(x) + \varepsilon \eta(x) \, , \quad \eta(x_1) = \eta(x_2) = 0 \\
  \bar{f}'(x) &= f'(x) + \varepsilon\eta'(x)
\end{align*}
$$

Substituting $\bar{f}$ into $I[y]$ results in a function of $\varepsilon$, $\Phi(\varepsilon) \equiv I[\bar{f}]$ and since $y = f(x)$ is a minimum of $I[y]$, the function $\Phi(\varepsilon)$ is stationary at $\varepsilon = 0$ 

$$
\begin{align*}
  0 &= \left. \frac{\d\Phi}{\d\varepsilon} \right|_{\varepsilon = 0} \\
  &= \left. \frac{\d}{\d\varepsilon} \right|_{\varepsilon = 0} \int_{x_1}^{x_2} L\left(x, \bar{f}, \bar{f}' \right) \d x \\
  &= \left. \int_{x_1}^{x_2} \frac{\d}{\d\varepsilon} L\left(x, \bar{f}, \bar{f}' \right) \d x \right|_{\varepsilon = 0} \\
  &= \left. \int_{x_1}^{x_2} \left( \frac{\partial L}{\partial \bar{f}} \frac{\partial \bar{f}}{\partial \varepsilon} + \frac{\partial L}{\partial \bar{f}'}\frac{\partial \bar{f}'}{\partial \varepsilon} \right) \right|_{\varepsilon = 0} \d x \\
  &= \left. \int_{x_1}^{x_2} \left( \frac{\partial L}{\partial \bar{f}} \eta + \frac{\partial L}{\partial \bar{f}'}\eta' \right) \right|_{\varepsilon = 0} \d x
\end{align*}
$$

Integrating the second term by parts

$$
  \int_{x_1}^{x^2} \frac{\partial L}{\partial \bar{f}'}\eta' \d x = \frac{\partial L}{\partial \bar{f}'} \underbrace{\left[ \eta \right]_{x_1}^{x_2}}_{=0} - \int_{x_1}^{x^2} \eta \frac{\d}{\d x} \left( \frac{\partial L}{\partial \bar{f}'} \right) \d x
$$

and inserting back

$$
\begin{align*}
  0 &= \left. \int_{x_1}^{x_2} \left[ \frac{\partial L}{\partial \bar{f}} \eta - \frac{\d}{\d x} \left( \frac{\partial L}{\partial \bar{f}'} \right) \eta \right] \right|_{\varepsilon = 0} \d x \\
  &= \int_{x_1}^{x_2} \left[ \frac{\partial L}{\partial f} - \frac{\d}{\d x} \left( \frac{\partial L}{\partial f'} \right) \right] \eta \d x 
\end{align*}
$$

By the fundamental lemma of calculus of variations, the integrand part in the paranthesis vanishes. Thus, if $f(x)$ is an extremal of $I[y]$, then $f(x)$ satisfy the Euler-Lagrange equation

$$
  \frac{\partial L}{\partial f} - \frac{\d}{\d x} \left( \frac{\partial L}{\partial f'} \right) = 0
$$

## Beltrami identity

The Beltrami identity is a special case of the Euler-Lagrange equation when $L$ does not depend on $x$, i.e. $\frac{\partial L}{\partial x} = 0$

$$
\begin{align*}
  \frac{\partial L}{\partial f} - \frac{\d}{\d x} \left( \frac{\partial L}{\partial f'} \right) = 0 \\
  y' \frac{\partial L}{\partial f} - y' \frac{\d}{\d x} \left( \frac{\partial L}{\partial f'} \right)
\end{align*}
$$

The term $y' \frac{\partial L}{\partial y}$ is also found in the total derivative $\frac{\d L(x, y, y')}{\d x}$

$$
\begin{gather*}
  \frac{\d L}{\d x} = \frac{\partial L}{\partial x} + \frac{\partial L}{\partial y}y' + \frac{\partial L}{\partial y'}y'' \\
  y' \frac{\partial L}{\partial y} = \frac{\d L}{\d x} - \frac{\partial L}{\partial x} - \frac{\partial L}{\partial y'}y''
\end{gather*}
$$

Inserting back to the modified Euler-Lagrange equation gives

$$
\begin{gather*}
  \frac{\d L}{\d x} - \frac{\partial L}{\partial x} - \left[ \frac{\partial L}{\partial y'}y'' - y' \frac{\d}{\d x} \left( \frac{\partial L}{\partial f'} \right) \right] = 0 \\
  \frac{\d L}{\d x} - \frac{\partial L}{\partial x} - \frac{\d}{\d x} \left( y' \frac{\partial L}{\partial y'} \right) = 0 \\
  \frac{\d}{\d x} \left( L - y' \frac{\partial L}{\partial y'} \right) = \underbrace{\frac{\partial L}{\partial x}}_{=0} \\
  \frac{\d}{\d x} \left( L - y' \frac{\partial L}{\partial y'} \right) = 0
\end{gather*}
$$

Integrating on both sides results in the Beltrami identity

$$
  L - y' \frac{\partial L}{\partial y'} = C
$$

where $C$ is a constant dependent on boundary conditions.

### Brachistochrone problem

The brachistochrone problem seeks to find the path $y = f(x)$ which minimized the time travelled from $a = (0, h)$ to $b = (L, 0)$ in a gravitational field:

$$
  T = \int_a^b \d t = \int_a^b \frac{\d S}{v(x, y)}
$$

where
- the infinitisemal distance $\d S$ is retrieved from the Pythagorean theorem: $\sqrt{1 + \left(\frac{\d y}{\d x}\right)^2}\d x$
- the velocity $v(x, y)$ is retrieved from conservation of energy
$$
\begin{gather*}
  mgh = mgy + \frac{1}{2}mv^2 \\
  v = \sqrt{2g(h-y)}
\end{gather*}
$$

Substituting back into $T$ yields

$$
  T = \int_a^b \sqrt{\frac{1 + (y')^2}{2g(h-y)}} \d x
$$

The minimal function is found with the Euler-Lagrange equation. Since the integrand is not dependent on $x$, the Beltrami identity applies

$$
\begin{gather*}
  \sqrt{\frac{1 + (y')^2}{2g(h-y)}} - \frac{(y')^2}{\sqrt{\left[1 + (y')^2 \right]\left[2g (h - y) \right]}} = C \\
  1 + (y')^2 - (y')^2 = C \sqrt{\left[1 + (y')^2 \right]\left[2g (h - y) \right]} \\
  1 = C^2 \left[1 + (y')^2 \right]\left[2g (h - y) \right] \\
  \left[1 + (y')^2 \right] (h - y) = \underbrace{C_1}_{=\left(2gC^2\right)^{-1}} \\
  \frac{\d y}{\d x} = \sqrt{\frac{C_1 - (h - y)}{h - y}}
\end{gather*}
$$

The differential equation can be solved by separation of variables

$$
\begin{align*}
  \d x &= \sqrt{\frac{C_1 - (h - y)}{h - y}} \d y \\
  x &= \int \sqrt{\frac{C_1 - (h - y)}{h - y}} \d y
\end{align*}
$$

The right hand side integral can be solved by substituting 

$$
\begin{gather*}
  y = h - C_1 \sin^2 \left( \frac{\theta}{2} \right) = h - \frac{C_1}{2}(1 - \cos\theta) \\
  \d y = - C_1 \sin\left( \frac{\theta}{2} \right)\cos\left( \frac{\theta}{2} \right) \d\theta
\end{gather*}
$$

Inserting back into the differential equation gives

$$
\begin{align*}
  x &= -C_1 \int \sqrt{\frac{C_1 \sin^2 \left( \frac{\theta}{2} \right)}{C_1 \left[1 - \sin^2 \left( \frac{\theta}{2} \right) \right]}}  \sin\left( \frac{\theta}{2} \right)\cos\left( \frac{\theta}{2} \right) \d\theta \\
  &= -C_1 \int \sin^2 \left( \frac{\theta}{2} \right) \d\theta \\
  &= -\frac{C_1}{2} \int (1 - \cos\theta)\d\theta \\
  &= \frac{K_1}{2}\left( \theta - \sin\theta \right) + K_2
\end{align*}
$$

This results in the following parametric equations

$$
\begin{align*}
  x &= \frac{K_1}{2}\left( \theta - \sin\theta \right) + K_2 \\
  y &= h + \frac{K_1}{2}(1 - \cos\theta)
\end{align*}
$$

The boundary condition at $a = (0, h)$ corresponding to $\theta = 0$ implies $K_2 = 0$, while the boundary condition at $b = (L, 0)$ gives an equation set that is not easily solved analytically. Hence, the following parametric equations describe the brachistochrone curve

$$
\begin{align*}
  x &= \frac{K_1}{2}\left( \theta - \sin\theta \right) \\
  y &= h + \frac{K_1}{2}(1 - \cos\theta)
\end{align*}
$$

which are the equations of a cycloid.

# Lagrange multiplier method

The Langrange multiplier method finds extrema of a function $f: \R^n \to \R$ subject to $k$ equality constraints $g_i(\mathbf{x}) = c_i$ where $g_i : \R^n \to \R$.

The extrema of $f$ are points whose gradients are linear combinations of the constraint gradients

$$
  \nabla f(\mathbf{x}) = \sum_i^k \lambda_i \left[c_i - \nabla g_i \left(\mathbf{x}\right)\right]
$$

where $\lambda_i$ are the Langrange multipliers. The problem can be converted to an unconstrained optimization by defining the Lagrange function 

$$
  L(\mathbf{x}, \lambda) = f(\mathbf{x}) - \sum_i^k \lambda_i \left[c_i - g_i\left( \mathbf{x} \right)\right]
$$

whoose extrames are given by the Euler-Lagrange equations.