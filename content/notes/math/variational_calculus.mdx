---
title: 'Variational Calculus'
subject: 'Mathematics'
showToc: true
---

Calculus of variations provides a theoretical framework for finding extrema of functionals, which are mappings $F:X\to\mathbb{F}$ from a function space $X$ to a field $\mathbb{F}$. This framework extends the concept of optimization from finite-dimensional spaces to infinite-dimensional spaces, where the variables are functions rather than finite-dimensional vectors.

# Fréchet and Gâteaux derivatives
<MathBox title='Fréchet derivative' boxType='definition'>
Let $(X, \lVert\cdot\rVert_X)$ and $(Y, \lVert\cdot\rVert_Y)$ be normed spaces. A function $f:X\to Y$ is *Fréchet differentiable* at $x\in X$ if there exists a bounded linear operator $T\in\mathcal{B}(X, Y)$ such that

$$
  \lim_{\lVert h \rVert_X \to 0} \frac{\lVert f(x + h) - f(x) - Th \rVert_Y}{\lVert h \rVert_X} = 0
$$

The operator $T$ is called the *Fréchet derivative* of $F$ at $x$, denoted $T = \d f(x)$. The Fréchet differential $Th = \d f(x) h$ is called the *first order variation* of $f$ at $x$.

If $f$ is $m$ times differentiable at $x$, the $m$-th order Fréchet derivative at $x$ is the $m$-linear map $\d^m f : X^m \to Y$ defined recursively as 

$$
  \d^m f (x) := \d(\d^{m-1} f(x))
$$

The $m$-th order Fréchet differential $\d^m f(x) (h_1,\dots,h_m)$ is called the *m*-th order variation of $f$ at $x$.

The function $f$ is $m$ times *Fréchet differentiable* for any point of $X$ if the function $\d^m f : X^m \to \mathcal{L}^m(X, Y)$ is continuous, where $\mathcal{L}^m(X, Y)$ is the space of $m$-linear functions from $X$ to $Y$. In this case, we write $f \in C^m (X, Y)$.
</MathBox>

<MathBox title='The Fréchet derivative is unique' boxType='proposition'>
Let $X$ and $Y$ be normed spaces. If a function $f:X\to Y$ is Fréchet differentiable at $x\in X$, then the Fréchet derivative $\d f(x): X\to Y$ is unique.

<details>
<summary>Proof</summary>

Assume $T_1, T_2 \in\mathcal{B}(X,Y)$ are Fréchet derivatives of $f:X\to Y$ at $x\in X$ satisfying

$$
  \lim_{h\to 0} \frac{\lVert f(x + h) - f(x) - T_i h \rVert_Y}{\lVert h\rVert_X} = 0,\; i=1,2
$$

For nonzero $v\in X$ and $\tau\in\mathbb{F}$ we have

$$
\begin{align*}
  \frac{\lVert T_1 v - T_2 v \rVert_Y}{\lVert v \rVert_X} =& \frac{|\tau|}{|\tau|}\frac{\lVert T_1 v - T_2 v\rVert_Y}{\lVert v \rVert_X} \\
  =& \frac{\lVert T_1 (\tau v) - T_2 (\tau v) \rVert_Y}{\lVert \tau v \rVert_X} \\
  =& \frac{\lVert (f(x + \tau v) - f(x) - T_2 (\tau v)) - (f(x + \tau v) - f(x) - T_1 (\tau v)) \rVert_Y}{\lVert \tau v\rVert_X} \\
  \overset{\triangle}{\leq} \frac{\lVert f(x + \tau v) - f(x) - T_2 (\tau v) \rVert_Y}{\lVert \tau v \rVert_X} + \frac{\lVert f(x + \tau v) - f(x) - T_1 (\tau v) \rVert_Y}{\lVert \tau v \rVert_X} \xrightarrow{\tau\to 0} \to 0
\end{align*}
$$

This shows that $T_1 v = T_2 v$ for all $v\in X$, implying that $T_1 = T_2$.
</details>
</MathBox>

<MathBox title='Fréchet differentiability criterion' boxType='proposition'>
Let $X$ and $Y$ be normed spaces. Then the following are equivalent
1. A function $f:X\to Y$ is Fréchet differentiable at $x$ with derivative $T \in\mathcal{B}(X,Y)$
2. For every $\epsilon > 0$ there exists $\delta > 0$ with the open ball $B(x, \delta) \subset X$ such that for all $h\in B(x, \delta)$
$$
  \lVert f(x + h) - f(x) - Th \rVert_Y \leq \epsilon \lVert h \rVert_X
$$
</MathBox>

<MathBox title='Fréchet differentiable functions are Lipschitz' boxType='proposition' tag='proposition-3'>
Let $X$ and $Y$ be normed spaces. If $f:X\to Y$ is Fréchet differentiable at $x_0$, then $f$ is locally Lipschitz at $x_0$. That is, there exists $\delta > 0$ and $L > 0$ such that $B(x_0, \delta) \subset X$ and for all $x \in B(x,\delta)$

$$
  \lVert f(x) - f(x_0) \rVert_Y \leq L\lvert x - x_0 \rVert_X
$$

<details>
<summary>Proof</summary>

Since $f:X\to Y$ is Fréchet differentiable at $x_0 \in X$, then for $\epsilon = 1$ there is $\delta > 0$ such that for all $x\in X$ with $\lVert x - x_0 \rVert_X < \delta$ we have

$$
  \frac{\lVert f(x) - f(x_0) - \d f(x_0) (x - x_0) \rVert_Y}{\lVert x - x_0 \rVert_X} < 0
$$

This can be rewritten as

$$
  \lVert f(x) - f(x_0) - \D f(x_0)(x - x_0) \rVert_Y < \lVert x - x_0 \rVert_X
$$

By the triangle inequality, we have

$$
\begin{align*}
  \lVert f(x) - f(x_0) \rVert_Y =& \lVert f(x) - f(x_0) - \d f(x_0) (x - x_0) + \d f(x_0)(x - x_0) \rVert_Y \\
  \leq& \lVert f(x) - f(x_0) - \d f(x_0) (x - x_0) \rVert_Y + \lVert \d f(x_0)(x - x_0) \rVert_Y \\
  \leq& \lVert x - x_0 \rVert_X + \lVert \d f(x_0)(x - x_0) \rVert_Y 
\end{align*}
$$

Since $\d f(x_0)$ is a bounded linear operator, it follows that

$$
  \lVert \d f(x_0)(x - x_0) \rVert_Y \leq \lVert \d f(x_0) \rVert \cdot \lVert \rVert_X
$$

Thus, we obtain

$$
  \lVert f(x) - f(x_0) \rVert_Y \leq (1 + \lVert \d f(x_0)\rVert)\lVert x - x_0 \rVert_X
$$

Taking $L = 1 + \lVert \d f(x_0) \rVert$ shows that $f$ is locally Lipschitz at $x_0$.
</details>
</MathBox>

<MathBox title='Fréchet linearity' boxType='proposition'>
Let $X$ and $Y$ be normed spaces. If $f,g:X\to Y$ are differentiable at $x \in X$, then for any scalars $\alpha,\beta\in\mathbb{F}$, the linear combination $\alpha f + \beta g$ is differentiable at $x$ with derivative

$$
  \d (\alpha f + \beta g)(x) = \alpha \d f(x) + \beta \d g(x)
$$

<details>
<summary>Proof</summary>

By differentiability of $f$ and $g$ at $x$, for any $\epsilon > 0$ there is $\delta > 0$ such that for all $h \in B(x, \delta) \subset X$ we have

$$
\begin{align*}
  \lVert f(x + h) - f(x) - \d f(x)h \rVert_Y \leq& \frac{\epsilon \lVert h \rVert_X}{2(|\alpha| + 1)} \\
  \lVert g(x + h) - g(x) - \d g(x)h \rVert_Y \leq& \frac{\epsilon \lVert h \rVert_X}{2(|\beta| + 1)}
\end{align*}
$$

Thus

$$
\begin{align*}
  & \lVert \alpha f(x + h) + \beta g(x + h) - \alpha f(x) - \beta g(x) - \alpha\d f(x)h - \beta\d g(x) h \rVert_Y \\
  &\leq |\alpha|\cdot\lVert f(x + h) - f(x) - \d f(x)h \rVert_Y + |\beta|\cdot\lVert g(x + h) - g(x) - \d g(x)h \rVert_Y \\
  &\leq \frac{\epsilon|\alpha|\cdot\lVert h \rVert_X}{2(|\alpha| + 1)} + \frac{\epsilon|\beta|\cdot\lVert h \rVert_X}{2(|\beta| + 1)} = \epsilon \lVert h \rVert_X \left(\frac{|\alpha|}{2(|\alpha| + 1)} + \frac{|\beta|}{2(|\beta| + 1)} \right) \\
  <& \epsilon \lVert h \rVert_X
\end{align*}
$$

Since $\d f(x), \d g(x)\in\mathcal{B}(X,Y)$ it follows that $\alpha \d f(x) + \beta g(x) \in \mathcal{B}(X,Y)$. Hence, $\alpha f + \beta g$ is differentiable at $x$ with Fréchet derivative $\alpha \d f(x) + \beta g (x)$.
</details>
</MathBox>

<MathBox title='Fréchet product rule' boxType='proposition'>
Let $X$ be a normed space and $\mathbb{F}$ a field. If $f,g: X\to\mathbb{F}$ are Fréchet differentiable at $x\in X$, then the product $fg$ is differentiable at $x$ with derivative

$$
  \d (fg)(x) = g(x) \d f(x) + f(x) \d g(x)
$$

<details>
<summary>Proof</summary>

By differentiability of $f$ and $g$ at $x$, for each $\epsilon > 0$ there is $\delta_x > 0$ with $B(x,\delta_x)\subset X$, and a constant $L > 0$ (by Proposition $\ref{proposition-3}$) such that for all $0 < \lVert h\rVert < \delta_x$

$$
  |f(x + h) - f(x)| \leq L \lVert h \rVert_X
$$

and

$$
\begin{align*}
  |f(x + h) - f(x) - \d f(x)h| \leq& \frac{\epsilon \lVert h \rVert_X}{3(|g(x)| + 1)} \\
  |g(x + h) - f(x) - \d f(x)h| \leq& \frac{\epsilon \lVert h \rVert_X}{3(|f(x)| + L)}
\end{align*}
$$

For $\epsilon > 0$ choose

$$
  \delta = \min\Set{1, \delta_x, \frac{\epsilon}{3L(\lVert \d g(x) \rVert + 1)}}
$$

When $0 \leq \lVert h \rVert_X < 0$, we get

$$
\begin{align*}
  & |f(x + h)g(x + h) - f(x)g(x) - g(x) \d f(x) h - f(x) \d g(x) h| \\
  =& |f(x + h)g(x + h) - f(x + h)g(x) + f(x + h)g(x) - f(x)g(x) \\
  &+ f(x + h)\d g(x) h - f(x + h) \d g(x) h - g(x) \d f(x) h - f(x) \d g(x) h| \\
  \leq& |f(x + h)|\cdot |g(x + h) - g(x) - \d g(x)h| \\
  &+ |g(x)|\cdot|f(x + h) - f(x) - \d f(x) h| \\
  &+ |f(x + h) - f(x)|\cdot\lVert \d g(x) \rVert\cdot\lVert h \rVert_X \\
  \leq& (|f(x)| + L)\frac{\epsilon\lVert h \rVert_X}{3(|f(x) + L|)} \\
  &+ |g(x)|\frac{\epsilon\lVert h \rVert_X}{3(|g(x)| + 1)} + \delta L \lVert \d g(x) \rVert\cdot\lVert h \rVert_X \\
  <& \epsilon \lVert h \rVert_X
\end{align*}
$$

Where we have used the Lipschitz implication

$$
  |f(x + h) - f(x)| \leq L\lVert h \rVert_X \implies |f(x + h)| \leq |f(x)| + L\lVert h \rVert_X
$$

Hence, $fg$ is differentiable at $x$ with Fréchet derivative $\d (fg)(x) = g(x) \d f(x) + f(x) \d g(x)$.
</details>
</MathBox>

<MathBox title='Fréchet chain rule' boxType='proposition'>
Let $X, Y, Z$ be normed spaces. If $f:X\to Y$ is Fréchet differentiable at $x\in X$ and $g:Y\to Z$ is Fréchet differentiable at $y = f(x) \in Y$, the composition $g\circ f$ is Fréchet differentiable at $x$ with derivative

$$
  \d (g\circ f)(x) = \d g(f(x)) \d f(x)
$$

<details>
<summary>Proof</summary>

Choose $\epsilon > 0$. By differentiability of $f:X\to Y$ at $x\in X$ and of $g:Y\to Z$ at $y = f(x) \in Y$, there is $\delta_1 > 0$ such that $B(x,\delta_1)\subset X$ and for all $\xi \in X$ satisfying $0 \lVert\xi\rVert_X < \delta_1$ we have

$$
  \lVert f(x + \xi) - f(x) - \d f(x)\xi \rVert_Y \leq \frac{\epsilon\lVert\xi\rVert_X}{2(\lVert \d g(y) \rVert + 1)}
$$

By Proposition $\ref{proposition-3}$, $f$ is locally Lipschitz at $x$, i.e. there is $\delta_2 > 0$ and $L > 0$ such that $B(x,\delta_2)\subset X$ and for all $\xi\in X$ satisfying $0 < \lVert\xi\rVert_X < \delta_2$ we have

$$
  \lVert f(x + \xi) - f(x) \rVert_X \leq L\lVert\xi\rVert_X
$$

Set $\delta_x = \min\Set{\delta_1, \delta_2}$. Since $g$ is differentiable at $y$, there is $\delta_y > 0$ such that $B(y, \delta_y) \subset Y$ and for all $0 \leq \lVert\eta\rVert_Y < \delta_y$ we have

$$
  \lVert g(y + \eta) - g(y) -\d g(y)\eta \rVert_Z \leq \frac{\epsilon\lVert\eta\rVert_Y}{2L}
$$

Set $\delta = \min\Set{\delta_x, \delta_y / L}$ such that $L\delta\leq\delta_y$. If we take $\eta(\xi) = f(x + \xi) - f(x) = f(x + \xi) - y$ then $h = g\circ f$ satisfies

$$
  h(x + \xi) - h(x) = g(f(x + \xi)) - g(f(x)) = g(y + \eta(\xi) - g(y))
$$

Thus for all $\xi\in X$ satisfying $\lVert\xi\rVert_X < \delta$ we have

$$
  \lVert \rVert_Y = \lVert f(x + \xi) - f(x) \rVert_Y \leq L\lVert\xi\rVert_X < L\delta \leq \delta_y
$$

so that

$$
\begin{align*}
  &\lVert h(x + \xi) - h(x) - \d g(y) \d f(x)\xi \rVert_Z \\
  =& \lVert g(y + \eta(\xi)) - g(y) - \d g(y)\eta(\xi) + \d g(y) \eta(\xi) - \d g(y) \d f(x)\xi \rVert_Z \\
  \overset{\triangle}{\leq}& \lVert g(y + \eta(\xi)) - g(y) - \d g(y)\eta(\xi) \rVert_Z \\
  &+ \lVert \d g(y)\eta(\xi) - \d g(y) \d f(x)\xi \rVert_Z \\
  \leq& \lVert g(y + \eta(\xi)) - g(y) - \d g(y)\eta(\xi) \rVert_Z \\
  &+ \lVert \d g(y) \rVert \cdot \lVert \eta(\xi) - \d f(x)\xi \rVert_Y \\
  =& \lVert g(y + \eta(\xi)) - g(y) - \d g(y)\eta(\xi) \rVert_Z \\
  &+ \lVert \d g(y) \rVert \cdot \lVert f(x + \xi) - f(x) - \d f(x)\xi \rVert_Y \\
  \leq& \frac{\epsilon\lVert\eta(\xi)\rVert_Y}{2L} + \lVert \d g(y) \rVert \frac{\epsilon\lVert\xi\rVert_X}{2(\lVert\d g(y)\rVert + 1)} \\
  <& \frac{\epsilon L\lVert\xi\rVert_X}{2 L} + \frac{\epsilon\lVert\xi\rVert_X}{2} = \epsilon\lVert\xi\rVert_X
\end{align*}
$$

Since $\d g(x) \in\mathcal{B}(X,Y)$ and $\d g(y) \in\mathcal{B}(Y,Z)$, it follows that $\d g(y) \d f(x) \in \mathcal{B}(X,Z)$. Hence $g\circ f$ is differentiable at $x$ with Fréchet derivative $\d (g\circ f)(x) = \d g(f(x)) \d f(x)$.
</details>
</MathBox>

<MathBox title='Matrix representation of Fréchet derivatives' boxType='proposition'>
Let $U\subseteq \R^n$ be open. If a vector function $f:U \to\R^m$ by $f = (f_i:U\to\R)_{i=1}^m$, in standard coordinates, is Fréchet differentiable at $\mathbf{x}\in U$ , then the partial derivatives $\d_j f_i \mathbf{x}$ exist for all $j = 1,\dots,n$ and $i=1,\dots,m$. The matrix representation of $\d f(\mathbf{x})$ in standard coordinates is the Jacobian matrix

$$
  J(\mathbf{x}) = \begin{bmaxtrix} 
    \d_1 f_1 (\mathbf{x}) & \cdots & \d_n f_1 (\mathbf{x}) \\
    \vdots & \ddots & \vdots \\
    \d_1 f_m (\mathbf{x}) & \cdots & \d_n f_m (\mathbf{x})
  \end{bmatrix}
$$

<details>
<summary>Proof</summary>

Let $J_j$ be the $j$-th column of $J(\mathbf{x})$, i.e. $J_j = \d f(\mathbf{x})\mathbf{e}_j$, and let $J_{ij}$ be the $i$-th entry of the $j$-column of $J(\mathbf{x})$. For $\mathbf{h} = r\mathbf{e}_j$ we have

$$
\begin{align*}
  0 =& \lim_{\mathbf{h}\to\mathbf{0}} \frac{\lVert f(\mathbf{x} + \mathbf{h}) - f(\mathbf{x}) - \d f(\mathbf{x}\mathbf{h}) \rVert}{\lVert \mathbf{h} \rVert} \\
  =& \lim_{r\to 0} \frac{\lVert f(\mathbf{x} + r\mathbf{e}_j) - f(\mathbf{x}) - r\d f(\mathbf{x}) \mathbf{e}_j \rVert}{|r|\cdot\lVert\mathbf{e}_j\rVert} \\
  =& \lim_{r\to 0} \frac{\lVert f(x_1,\dots,x_j + r,\dots,x_n) - f(x_1,\dots,x_n) - rJ_j \rVert}{|r|}
\end{align*}
$$

This implies that for each component of $f$

$$
  \lim_{r\to 0} \frac{|f(x_1,\dots,x_j + r,\dots,x_n) - f(x_1,\dots,x_n) - rJ_{ij} |}{|r|} = 0
$$

showing that the partial derivative $\d_j f_i (\mathbf{x}) = J_{ij}$ exists.
</details>
</MathBox>

<MathBox title='Gâteaux derivative' boxType='definition'>
Let $X$ and $Y$ be locally convex topological vector spaces. A function $f: X \to Y$ is Gâteaux differentiable at $x\in X$ in the direction $v\in X$ if the following limit exists

$$
  \D f(x; z) = \lim_{t\to 0} \frac{f(x + tv) - f(x)}{t} = \left.\frac{\d}{\d t} f(x + tv)\right|_{t=0}
$$

This limit is called the *Gâteaux differential* of $f$ at $x$ in the direction $v$. If the limit exists for all $v\in X$, then $f$ is Gâteaux differentiable at $x$. 

If the Gâteaux differential $\D f(x; \cdot): X\to Y$ is linear and continuous, i.e. there is a linear map $A\in\mathcal{L}(X,Y)$ with $\D f(x; v) = Av$ for all $v\in X$, then $A$ is called the *Gâteaux derivative* of $F$ at $x$, denoted $\D f(x) = A$.

If $f$ is $m$ times Gâteaux differentiable at $x$, the $m$-th order Gâteaux differential in iterated directions $v_1,\dots,v_m \in X$ is defined recursively as 

$$
  \D^m f(x; v_1,\dots,v_m) = \D(\D^{m-1} f)(x; v_1,\dots,v_{m-1};v_m)
$$ 
</MathBox>

<MathBox title='Fréchet differentiability implies Gâteaux differentiability' boxType='proposition'>
Let $X, Y$ be normed spaces. If $f:X\to Y$ is Fréchet differentiable at $x\in X$, then $f$ is Gâteaux differentiable at $x$. In this case $\d f(x) = \D f(x)$, where $\d f(x)$ is the Fréchet derivative of $f$ at $x$ and $\D f(x)$ is the Gâteaux derivative of $f$ at $x$.

<details>
<summary>Proof</summary>

Because $f: X\to Y$ is a Fréchet differentiable at $x\in X$, we have

$$
  f(x + h) = f(x) + \d f(x) h + \mathcal{O}(h)
$$

For any $v\in X$, the Gâteaux differential limit at $x$ is

$$
  \lim_{t\to 0} \frac{f(x + tv) - f(x)}{t}
$$

Inserting the Fréchet differentiability condition with $h = tv$, we get

$$
\begin{align*}
  \lim_{t\to 0} \frac{\d f(x)(tv) + \mathcal{O}(\lVert tv \rVert)}{t} =& \lim_{t\to 0} \frac{t \d f(x)(v) + \mathcal{O}(\lVert tv \rVert)}{t} \\
  =& \d f(x)(v) + \lim_{t\to 0} \frac{\mathcal{O}(|t|\cdot\lVert v \rVert)}{t} \\
  =& \d f(x)(v)
\end{align*}
$$

Hence, 

$$
  \lim_{t\to 0} \frac{f(x + tv) - f(x)}{t} = \d f(x)(v),\; \forall v\in X
$$

showing that the Gâteaux differential of $f$ at $x$ in the direction $v$ equals the Fréchet differential of $f$ at $x$. Since the Fréchet derivative $\d f(x) \in \mathcal{B}(X,Y)$ is a bounded linear operator, it follows that $f$ is Gâteaux differentiable at $x$ with Gâteaux derivative $\D f(x) = \d f(x)$.
</details>
</MathBox>

# Functional differentiation

<MathBox title='Functional' boxType='definition'>
A *functional* is a function $f:X\to\mathbb{F}$ mapping from a function space $X$ to a field $\mathbb{F}$. A functional is linear if it satisfies
1. **Homogeneity:** $f(\alpha x) = \alpha f(x)$ for $x\in X$ and $\alpha\in\mathbb{F}$
2. **Additivity:** $f(x_1 + x_2) = f(x_1) + f(x_1)$ for $x_1, x_2 \in X$

A functional $K:X^m \to\mathbb{F}$ is *m*-linear if it is linear in each argument, in which case we write $K\in\mathcal{L}^m (X,\mathbb{F})$. A functional is $f:X\to\mathbb{F}$ is quadratic if there exists a bilinear functional $K:X^2 \to\mathbb{F}$ such that $f(x) = K(x, x)$ for every $x\in X$. Any quadratic functional satisfies

$$
  f(\alpha x) = K(\alpha x, \alpha x) = \alpha K (x, \alpha x) = \alpha^2 K(x, x) = \alpha^2 f(x)
$$

A functional is continuous if $\lim_{x\to x_0} f(x) = f(x_0)$.
</MathBox>

<MathBox title='Fréchet differentiable functional' boxType='definition'>
Let $X$ be a normed space. A functional $J:X\to\mathbb{F}$ is Fréchet differentiable at $x\in X$ if there is a continuous linear functional $J'(x): X\to\mathbb{F}$ such that

$$
  \lim_{h\to 0} \frac{J(x + h) - J(x) - J'(x)h}{\lVert h \rVert_X}
$$

The functional $J'(x)$ is the Fréchet derivative of $J$ at $x$. 
</MathBox>

<MathBox title='Gâteaux differentiable functional' boxType='definition'>
Let $X$ be a normed space. A functional $J:X\to\mathbb{F}$ is Gâteaux differentiable at $x\in X$ if there is a functional $\delta J(x): X\to\mathbb{F}$ such that

$$
  \lim_{\tau \to 0} \frac{J(x + \tau h) - J(x)}{\tau} = \left.\frac{\d}{\d\tau} J(x + \alpha h)\right|_{\tau=0} = \delta J(x) h
$$

The functional $\delta J(x)$ is the Gâteaux derivative of $J$ at $x$. 
</MathBox>


<MathBox title='Differentiable functional' boxType='definition'>
A functional $S[]:\mathcal{F}\to\R$ is *differentiable*, if for any smooth curve $\mathbf{r}(t)\in\mathcal{F}$ and $\delta\mathbf{r}(t) \in\mathcal{F}_\varepsilon$, where

$$
  \mathcal{F}_\varepsilon = \Set{\delta\mathbf{r}:I\to\R^n : \lvert\delta\mathbf{r}(t)\rvert < \varepsilon,\; \left\lvert \frac{\d}{\d t} \delta\mathbf{r}(r) \right\rvert < \varepsilon}
$$

a functional $\delta S[\cdot,\cdot]: \mathcal{F}\times\mathcal{F}_\varepsilon \to\R$ exists with the properties
1. $S[\mathbf{r}(t) + \delta\mathbf{r}(t)] = S[\mathbf{r}(t)] + \delta S[\mathbf{r}(t), \delta\mathbf{r}(t)] + \mathcal{O}(\varepsilon^2)$
2. $\delta S[\mathbf{r}(t), \delta\mathbf{r}(t)]$ is linear in $\delta\mathbf{r}(t)$, i.e. for $\alpha_1, \alpha_2 \in \R$
$$
  \delta S[\mathbf{r}(t), \alpha_1 \delta\mathbf{r}_1(t) + \alpha_2 \delta\mathrm{r}_2(t)] = \alpha_1 S[\mathbf{r}(t), \delta\mathbf{r}_1 (t)] + \alpha_2 \delta S[\mathbf{r}(t), \delta\mathbf{r}_2 (t)] 
$$

The map $\delta S[\cdot,\cdot]$ is called the *first variation* of $S[]$.
</MathBox>

If $\mathbf{r}:I\subset\R\to\R^n$ is the trajectory of an $n$-dimensional system, then $\delta\mathbf{r}(t)$ describes around $\mathbf{r}(t)$.

<MathBox title='Fundamental lemma of variational calculus' boxType='lemma'>
Let $f:[a,b]\to\R$ be a continuous function. If

$$
  \int_a^b f(x)g(x)\;\d x = 0
$$

for all twice differentiable functions $g:[a,b]\to\R$ with $g(a) = g(b) = 0$ and compact support in $(a,b)$, then $f$ is identically zero.

<details>
<summary>Proof</summary>

Suppose by contradiction that $f$ is nonzero in some subinterval $(c,d) \subseteq (a, b)$. Without loss of generality, assume $f:(c,d) > 0$. Since $g$ is arbitrary, we are free to set it at our choice, say

$$
  g(x) = \begin{cases} (x - c)(d - x),\quad x\in [c,d] \\ 0,\quad x < c \lor x > d \end{cases}
$$

Noting that $(x - c) > 0$ and $(d - x) > 0$ for $x\in [c,d]$, consider the integral

$$
  \int_a^b f(x)g(x)\;\d x = \int_c^d f(x) (x - c)(d - x)\;\d x > 0
$$

which is positive because the integrand is positive in $(c,d)$. This gives a contradiction, proving that $f$ must be identically zero in $[a,b]$.
</details>
</MathBox>

# Lagrange problems

# Euler-Lagrange equations

<MathBox title='Objective function' boxType='definition'>
Let $\mathbf{u}$
</MathBox>


<MathBox title='Action functional differential' boxType='theorem'>
Let $(X,L)$ be a real dynamic system with $n$ degrees of freedom, where $X$ is a smooth manifold representing the configuration space and $L: I\subseteq \R \times TX\to\R$ as $L(\mathbf{q}(t), \dot{\mathbf{q}}(t), t)$ is the Lagrangian of the system. Let $\mathcal{F}(a, b, \mathbf{x}_a, \mathbf{x}_b)$ be the set of smooth curves $\mathbf{q}: [a,b]\to X$ for which $\mathbf{q}(a) = \mathbf{x}_a$ and $\mathbf{q}(b) = \mathbf{x}_b$. Define the action functional $S:\mathcal{F}(a,b,\mathbf{x}_a, \mathbf{x}_b)\to\R$ by

$$
  S[\mathbf{q}] = \int_a^b L(\mathbf{q}(t), \dot{\mathbf{q}}(t), t) \,\d t
$$

The differential of $S[]$ is given by

$$
  \delta S[\mathbf{q}(t), \delta\mathbf{q}(t)] = \int_a^b \d t \left( \sum_{i=1}^n \left[\frac{\partial L}{\partial q_i} - \frac{\d}{\d t}\left(\frac{\partial L}{\partial \dot{q}_i} \right)\right] \delta q_i (t) \right) + \sum_{i=1}^n \left.\frac{\partial L}{\partial \dot{q}_j} \partial q_j (t) \right|_a^b
$$

<details>
<summary>Proof</summary>

By variation in $\mathbf{q}$, we have

$$
  S[\mathbf{q}(t) + \delta\mathbf{q}(t)] = \int_a^b L(t, \mathbf{q}(t) + \delta\mathbf{q}(t), \dot{\mathbf{q}}(t) + \partial\dot{\mathbf{q}}(t), t) \;\d t
$$

Introducing a perturbation $\varepsilon$ and applying Taylor series expansion of $L$ about $t$ to first order we get

$$
\begin{align*}
  L(\mathbf{q}(t) + \delta\mathbf{q}(t), \dot{\mathbf{q}}(t) + \partial\dot{\mathbf{q}}(t), t) =& L(\mathbf{q}(t), \dot{\mathbf{q}}(t), t) \\
  &+ \sum_{i=1}^n \left(\frac{\partial L}{\partial q_i} \delta q_i + \frac{\partial L}{\partial\dot{q}_i} \delta q_i \right) + \mathcal{O}(\varepsilon^2)
\end{align*}
$$

By the product rule, we have

$$
  \frac{\partial L}{\partial \dot{q}_i} \delta\dot{q}_i = \frac{\d}{\d t}\left(\frac{\partial L}{\partial\dot{q}_i} \delta q_i \right) - \left(\frac{\d}{\d t} \frac{\partial L}{\partial\dot{q}_i} \right)\delta q_i
$$

Since $S[\mathbf{q}(t) + \delta\mathbf{q}(t)] = S[\mathbf{q}(t)] + \delta S[\mathbf{q}(t), \delta(\mathbf{q})(t)] + \mathcal{O}(\varepsilon^2)$ we get

$$
\begin{align*}
  \delta S[\mathbf{q}(t), \delta(\mathbf{q})(t)] =& S[\mathbf{q}(t) + \delta\mathbf{q}(t)] - \delta
\end{align*}
$$
</details>
</MathBox>




Calculus of variations seeks to find extremas of functionals in the form of definite integrals

$$
  I[y] = \int_{x_1}^{x_2} L\left(x, y, \frac{\d y}{\d x} \right) \d x
$$

The extremal function $y = f(x)$ of $I[y]$ can be found by introducing a variation in $f$

$$
\begin{align*}
  \bar{f}(x) &= f(x) + \varepsilon \eta(x) \, , \quad \eta(x_1) = \eta(x_2) = 0 \\
  \bar{f}'(x) &= f'(x) + \varepsilon\eta'(x)
\end{align*}
$$

Substituting $\bar{f}$ into $I[y]$ results in a function of $\varepsilon$, $\Phi(\varepsilon) \equiv I[\bar{f}]$ and since $y = f(x)$ is a minimum of $I[y]$, the function $\Phi(\varepsilon)$ is stationary at $\varepsilon = 0$ 

$$
\begin{align*}
  0 &= \left. \frac{\d\Phi}{\d\varepsilon} \right|_{\varepsilon = 0} \\
  &= \left. \frac{\d}{\d\varepsilon} \right|_{\varepsilon = 0} \int_{x_1}^{x_2} L\left(x, \bar{f}, \bar{f}' \right) \d x \\
  &= \left. \int_{x_1}^{x_2} \frac{\d}{\d\varepsilon} L\left(x, \bar{f}, \bar{f}' \right) \d x \right|_{\varepsilon = 0} \\
  &= \left. \int_{x_1}^{x_2} \left( \frac{\partial L}{\partial \bar{f}} \frac{\partial \bar{f}}{\partial \varepsilon} + \frac{\partial L}{\partial \bar{f}'}\frac{\partial \bar{f}'}{\partial \varepsilon} \right) \right|_{\varepsilon = 0} \d x \\
  &= \left. \int_{x_1}^{x_2} \left( \frac{\partial L}{\partial \bar{f}} \eta + \frac{\partial L}{\partial \bar{f}'}\eta' \right) \right|_{\varepsilon = 0} \d x
\end{align*}
$$

Integrating the second term by parts

$$
  \int_{x_1}^{x^2} \frac{\partial L}{\partial \bar{f}'}\eta' \d x = \frac{\partial L}{\partial \bar{f}'} \underbrace{\left[ \eta \right]_{x_1}^{x_2}}_{=0} - \int_{x_1}^{x^2} \eta \frac{\d}{\d x} \left( \frac{\partial L}{\partial \bar{f}'} \right) \d x
$$

and inserting back

$$
\begin{align*}
  0 &= \left. \int_{x_1}^{x_2} \left[ \frac{\partial L}{\partial \bar{f}} \eta - \frac{\d}{\d x} \left( \frac{\partial L}{\partial \bar{f}'} \right) \eta \right] \right|_{\varepsilon = 0} \d x \\
  &= \int_{x_1}^{x_2} \left[ \frac{\partial L}{\partial f} - \frac{\d}{\d x} \left( \frac{\partial L}{\partial f'} \right) \right] \eta \d x 
\end{align*}
$$

By the fundamental lemma of calculus of variations, the integrand part in the paranthesis vanishes. Thus, if $f(x)$ is an extremal of $I[y]$, then $f(x)$ satisfy the Euler-Lagrange equation

$$
  \frac{\partial L}{\partial f} - \frac{\d}{\d x} \left( \frac{\partial L}{\partial f'} \right) = 0
$$

## Beltrami identity

The Beltrami identity is a special case of the Euler-Lagrange equation when $L$ does not depend on $x$, i.e. $\frac{\partial L}{\partial x} = 0$

$$
\begin{align*}
  \frac{\partial L}{\partial f} - \frac{\d}{\d x} \left( \frac{\partial L}{\partial f'} \right) = 0 \\
  y' \frac{\partial L}{\partial f} - y' \frac{\d}{\d x} \left( \frac{\partial L}{\partial f'} \right)
\end{align*}
$$

The term $y' \frac{\partial L}{\partial y}$ is also found in the total derivative $\frac{\d L(x, y, y')}{\d x}$

$$
\begin{gather*}
  \frac{\d L}{\d x} = \frac{\partial L}{\partial x} + \frac{\partial L}{\partial y}y' + \frac{\partial L}{\partial y'}y'' \\
  y' \frac{\partial L}{\partial y} = \frac{\d L}{\d x} - \frac{\partial L}{\partial x} - \frac{\partial L}{\partial y'}y''
\end{gather*}
$$

Inserting back to the modified Euler-Lagrange equation gives

$$
\begin{gather*}
  \frac{\d L}{\d x} - \frac{\partial L}{\partial x} - \left[ \frac{\partial L}{\partial y'}y'' - y' \frac{\d}{\d x} \left( \frac{\partial L}{\partial f'} \right) \right] = 0 \\
  \frac{\d L}{\d x} - \frac{\partial L}{\partial x} - \frac{\d}{\d x} \left( y' \frac{\partial L}{\partial y'} \right) = 0 \\
  \frac{\d}{\d x} \left( L - y' \frac{\partial L}{\partial y'} \right) = \underbrace{\frac{\partial L}{\partial x}}_{=0} \\
  \frac{\d}{\d x} \left( L - y' \frac{\partial L}{\partial y'} \right) = 0
\end{gather*}
$$

Integrating on both sides results in the Beltrami identity

$$
  L - y' \frac{\partial L}{\partial y'} = C
$$

where $C$ is a constant dependent on boundary conditions.

### Brachistochrone problem

The brachistochrone problem seeks to find the path $y = f(x)$ which minimized the time travelled from $a = (0, h)$ to $b = (L, 0)$ in a gravitational field:

$$
  T = \int_a^b \d t = \int_a^b \frac{\d S}{v(x, y)}
$$

where
- the infinitisemal distance $\d S$ is retrieved from the Pythagorean theorem: $\sqrt{1 + \left(\frac{\d y}{\d x}\right)^2}\d x$
- the velocity $v(x, y)$ is retrieved from conservation of energy
$$
\begin{gather*}
  mgh = mgy + \frac{1}{2}mv^2 \\
  v = \sqrt{2g(h-y)}
\end{gather*}
$$

Substituting back into $T$ yields

$$
  T = \int_a^b \sqrt{\frac{1 + (y')^2}{2g(h-y)}} \d x
$$

The minimal function is found with the Euler-Lagrange equation. Since the integrand is not dependent on $x$, the Beltrami identity applies

$$
\begin{gather*}
  \sqrt{\frac{1 + (y')^2}{2g(h-y)}} - \frac{(y')^2}{\sqrt{\left[1 + (y')^2 \right]\left[2g (h - y) \right]}} = C \\
  1 + (y')^2 - (y')^2 = C \sqrt{\left[1 + (y')^2 \right]\left[2g (h - y) \right]} \\
  1 = C^2 \left[1 + (y')^2 \right]\left[2g (h - y) \right] \\
  \left[1 + (y')^2 \right] (h - y) = \underbrace{C_1}_{=\left(2gC^2\right)^{-1}} \\
  \frac{\d y}{\d x} = \sqrt{\frac{C_1 - (h - y)}{h - y}}
\end{gather*}
$$

The differential equation can be solved by separation of variables

$$
\begin{align*}
  \d x &= \sqrt{\frac{C_1 - (h - y)}{h - y}} \d y \\
  x &= \int \sqrt{\frac{C_1 - (h - y)}{h - y}} \d y
\end{align*}
$$

The right hand side integral can be solved by substituting 

$$
\begin{gather*}
  y = h - C_1 \sin^2 \left( \frac{\theta}{2} \right) = h - \frac{C_1}{2}(1 - \cos\theta) \\
  \d y = - C_1 \sin\left( \frac{\theta}{2} \right)\cos\left( \frac{\theta}{2} \right) \d\theta
\end{gather*}
$$

Inserting back into the differential equation gives

$$
\begin{align*}
  x &= -C_1 \int \sqrt{\frac{C_1 \sin^2 \left( \frac{\theta}{2} \right)}{C_1 \left[1 - \sin^2 \left( \frac{\theta}{2} \right) \right]}}  \sin\left( \frac{\theta}{2} \right)\cos\left( \frac{\theta}{2} \right) \d\theta \\
  &= -C_1 \int \sin^2 \left( \frac{\theta}{2} \right) \d\theta \\
  &= -\frac{C_1}{2} \int (1 - \cos\theta)\d\theta \\
  &= \frac{K_1}{2}\left( \theta - \sin\theta \right) + K_2
\end{align*}
$$

This results in the following parametric equations

$$
\begin{align*}
  x &= \frac{K_1}{2}\left( \theta - \sin\theta \right) + K_2 \\
  y &= h + \frac{K_1}{2}(1 - \cos\theta)
\end{align*}
$$

The boundary condition at $a = (0, h)$ corresponding to $\theta = 0$ implies $K_2 = 0$, while the boundary condition at $b = (L, 0)$ gives an equation set that is not easily solved analytically. Hence, the following parametric equations describe the brachistochrone curve

$$
\begin{align*}
  x &= \frac{K_1}{2}\left( \theta - \sin\theta \right) \\
  y &= h + \frac{K_1}{2}(1 - \cos\theta)
\end{align*}
$$

which are the equations of a cycloid.

# Lagrange multiplier method

The Langrange multiplier method finds extrema of a function $f: \R^n \to \R$ subject to $k$ equality constraints $g_i(\mathbf{x}) = c_i$ where $g_i : \R^n \to \R$.

The extrema of $f$ are points whose gradients are linear combinations of the constraint gradients

$$
  \nabla f(\mathbf{x}) = \sum_i^k \lambda_i \left[c_i - \nabla g_i \left(\mathbf{x}\right)\right]
$$

where $\lambda_i$ are the Langrange multipliers. The problem can be converted to an unconstrained optimization by defining the Lagrange function 

$$
  L(\mathbf{x}, \lambda) = f(\mathbf{x}) - \sum_i^k \lambda_i \left[c_i - g_i\left( \mathbf{x} \right)\right]
$$

whoose extrames are given by the Euler-Lagrange equations.