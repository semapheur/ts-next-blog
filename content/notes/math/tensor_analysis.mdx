---
title: 'Tensor Analysis'
subject: 'Mathematics'
showToc: true
---

# Notation
## Einstein summation convetion

- Indices can not occur more twice
- Twice-repeated indices denote summation
- Free indices have the same range as summation indices

## Levi-Civita symbol

<MathBox title='Levi-Civita symbol' boxType='definition'>
In $n$-dimension, the Levi-Civita symbol is defined as

$$
\begin{align*}
  \varepsilon_{a_1 a_2\dots a_n} &:= \begin{cases}
    +1 &\textrm{ if } (a_i)_{i=1}^n \textrm{ is an even permutation of } (1, 2,\dots, n) \\
    -1 &\textrm{ if } (a_i)_{i=1}^n \textrm{ is an odd permutation of } (1, 2,\dots, n) \\
    0 &\textrm{ otherwise}
  \end{cases} \\
  &= \prod_{1\leq i < j \leq n} \mathrm{sgn}(a_j - a_i)
\end{align*}
$$
</MathBox>

### Two dimension

In two dimensions, the Levi-Civita symbol is defined as

$$
  \varepsilon_{ij} = \begin{cases}
    +1, &\quad (i, j) = (1,2) \\
    -1, &\quad (i, j) = (2,1) \\
    0, &\quad i=j
  \end{cases}
$$

**Product identity**

$$
  \varepsilon_{ij}\varepsilon^{mn} = \delta_i^m \delta_j^n - \delta_i \delta^n \delta_j^m
$$

### Three dimensions

In three dimensions, the Levi-Civita symbol is defined as

$$
  \varepsilon_{ijk} = \begin{cases}
    +1, &\quad (i, j, k) \in \Set{(1, 2, 3), (2, 3, 1), (3, 1, 2)} \\
    -1, &\quad (i, j, k) \in \Set{(3, 2, 1), (1, 3, 2), (2, 1, 3)} \\
    0, &\quad i=j \lor j=k \lor k=i
  \end{cases}
$$

**Product identity**

$$
\begin{align*}
  \delta_{ijklmn} = \varepsilon_{ijk}\varepsilon_{lmn} &= \begin{vmatrix} 
    \delta_{il} & \delta_{im} & \delta_{in} \\
    \delta_{jl} & \delta_{jm} & \delta_{jn} \\
    \delta_{kl} & \delta_{km} & \delta_{kn}
  \end{vmatrix} \\ 
  &= \delta_{il}(\delta_{jm}\delta_{kn} - \delta_{jn}\delta_{km}) - \delta_{im}(\delta_{jl}\delta_{kn} - \delta_{jn}\delta_{kl}) + \delta_{in}(\delta_{jl}\delta_{km} - \delta_{jm}\delta_{kl})
\end{align*}
$$

A special case of the result is

$$
  \delta_{jkmn} = \varepsilon_{ijk}\varepsilon_{imn} = \delta_{jm}\delta_{kn} - \delta_{jn}\delta_{km}
$$

# Tensor product

## Universality for bilinearity

<MathBox title='Bilinear function' boxType='definition'>
Let $U$, $V$ and $W$ be vector spaces over $\mathbb{F}$. A function $F:U\times V\to W$, where $U\times V$ is the cartesian product of sets, is bilinear if it is linear in both variables separately

$$
\begin{align*}
  F(\alpha\mathbf{u}_1 + \beta\mathbf{u}_2, \mathbf{v}) =& \alpha F(\mathbf{u}_1,\mathbf{v}) + \beta F(\mathbf{u}_2, \mathbf{v}) \\
  F(\mathbf{u}, \alpha\mathbf{v}_1 + \beta\mathbf{v_2}) =& \alpha F(\mathbf{u},\mathbf{v}_1) + \beta F(\mathbf{u}, \mathbf{v}_2) 
\end{align*}
$$

The set of all bilinear functions from $U\times V$ to $W$ is denoted $\hom_{\mathbb{F}}(U\times V, W)$. A bilinear function $F:U\times V\to\mathbb{F}$ taking values in the base field $\mathbb{F}$ is called a *bilinear form* on $U\times V$.
</MathBox>

Note the difference between bilinear functions $\hom_{\mathbb{F}}(V\times V;W)$, mapping from cartesian products of vector spaces, and linear functions $\mathcal{L}(V\times V,W)$, mapping from direct products of vector spaces. Unlike direct products, cartesion products of sets are not associated with any algebraic structure.

Using category theory, the tensor product can be defined as a universal for bilinearity. Let $U$ and $V$ be vector spaces over $\mathbf{F}$, and consider the functor

$$
  \mathbf{Bilin}(U,V;-): \mathbf{Vector}_{\mathbb{F}}\to\mathbf{Set} \\
  W \mapsto \hom_{\mathbb{F}}(U\times V, W)
$$

mapping a vector space $W$ to the set of bilinear maps $U\times V\to W$. It can be shown that this functor is representable, i.e. there is a vector space $S$ such that $\mathbf{Bilin}(U,V;W)\cong\mathbf{Vect}_{\mathbb{F}}(T,W) = \mathcal{L}(T,W)$ naturally in $W$. 

In other words, there is a universal pair $(T, B:U\times V \to T)$ of a vector space $T$ and a bilinear map $B$ such that for any pair $(W, F:U\times V \to W)$ of a vector space $W$ and a bilinear map $F$ there is a unique linear map $\mathrm{L}:T\to W$ with $F = \mathrm{L} \circ B$ according to the diagram below.

<LatexFig width={50} src='/fig/universal_property_bilinearity.svg' alt=''
  caption='Universal property for bilinearity'
>
```tex
\documentclass[tikz]{standalone}
\usepackage{amssymb}
\usepackage{tikz}
\usetikzlibrary{cd}

\begin{document}

\begin{tikzcd}[every label/.append style={font=\scriptsize}]
  U\times V \arrow[r, "B"] \arrow[dr, "\forall F" swap] & 
  T \arrow[d, "\exists!\mathrm{L}", dashed] \\
  & \forall W
\end{tikzcd}

\end{document}
```
</LatexFig>

This property, called the universal property, uniquely determines $(T, B)$ up to isomorphism. The vector space $T$ is called the tensor product of $U$ and $V$, denoted $U \otimes V$ and the bilinear map $B$ is written

$$
\begin{gather*}
  \otimes: U\times V \to U\otimes V \\
  (\mathbf{u}, \mathbf{v}) \mapsto \mathbf{u}\otimes\mathbf{v}
\end{gather*}
$$

Since a bilinear map out of $U\times V$ is essentially the same as linear map out of $U\times V$, tensor products reduce the study of bilinear functions to the study of linear transformations. 

<MathBox title='Tensor product' boxType='definition'>
The tensor product of two vector spaces $U$ and $V$ is a vector space denoted $U\otimes V$ together with a bilinear map $\otimes:U\times V\to U\otimes V$ given by $(\mathbf{u},\mathbf{v})\mapsto \mathbf{u}\otimes \mathbf{v}$ satisfying the universal property. That is for every for every bilinear map $F:U\times V\to W$ there is a unique linear map $\mathcal{L}:U\otimes V\to W$ such that $F = \mathcal{L} \circ\otimes$, i.e. $f(\mathbf{u},\mathbf{v}) = \mathcal{L}(\mathbf{u}\otimes\mathbf{v})$ for every $\mathbf{u}\in U$ and $\mathbf{v}\in V$.

<LatexFig width={50} src='/fig/universal_property_tensor_product.svg' alt=''
  caption='Universal property for the tensor product'
>
```tex
\documentclass[tikz]{standalone}
\usepackage{amssymb}
\usepackage{tikz}
\usetikzlibrary{cd}

\begin{document}

\begin{tikzcd}[every label/.append style={font=\scriptsize}]
  U\times V \arrow[r, "\otimes"] \arrow[dr, "\forall F" swap] & 
  U\otimes V \arrow[d, "\exists!\mathrm{L}", dashed] \\
  & \forall W
\end{tikzcd}

\end{document}
```
</LatexFig>
</MathBox>

## Construction from bases

Let $U$ and $V$ be vector spaces with respective bases $B = \Set{\mathbf{c}_j }_{j\in J}$. The tensor product from $U\otimes V$ can be constructed as vector space $U\otimes V$ with basis $D = \Set{ \mathbf{b}_i \otimes \mathbf{c}_j | \mathbf{b}_i \in B, \mathbf{c}_j \in C }$ where $\otimes:U\times V \to U\otimes V$ maps $(\mathbf{b}_i, \mathbf{c}_j)\mapsto \mathbf{b}_i \otimes\mathbf{c}_i$.

For any bilinear function $F:U\times V \to W$, then the universal condition $F = \mathrm{L}\circ\otimes$ is equivalent to

$$
  \mathrm{L}(\mathbf{b}_i \otimes \mathbf{c}_j) = F(\mathbf{b}_i, \mathbf{c}_i)
$$

which uniquely defines a linear map $\mathbf{L}: U\otimes V\to W$. Hence, $(U\otimes V, \otimes)$ has the universal property for bilinearity.

If $U\ni \mathbf{u} = \sum_{i\in J}^n \alpha_i \mathbf{b}_i$ and $V\ni \mathbf{v} = \sum_{j\in J} \beta\mathbf{c}_i$ for $\alpha_i, \beta_j\in \mathbb{F}$ then the elements of $U\otimes V$, called tensors take the form

$$
\begin{align*}
  \mathbf{u}\otimes\mathbf{v} &= \left(\sum_{i\in I} \alpha_i\mathbf{b}_i \right) \otimes \left( \sum_{j\in J} \beta_j\mathbf{c}_j \right) \\
  &= \sum_{i\in I}\sum_{j\in J} \alpha_i \beta_j \left( \mathbf{b}_i \otimes \mathbf{c}_j \right)
\end{align*}
$$

## Construction from quotient space

Let $S_{U\times V}$ be the vector space over $\mathbb{F}$ with basis $U\times V$. Let $R$ be the subspace of $S_{U\times V}$ generated by all vectors of the form

$$
\begin{gather*}
  \alpha(\mathbf{u}, \mathbf{v}) + \beta(\mathbf{u}', \mathbf{w}) - (\alpha\mathbf{u} + \beta\mathbf{u}', \mathbf{v}),\; \mathbf{u},\mathbf{u}'\in U, \mathbf{v}\in V \\
  \alpha(\mathbf{u}, \mathbf{v}) + \beta(\mathbf{u}, \mathbf{w}') - (\alpha\mathbf{u}, \alpha\mathbf{v} + \beta\mathbf{v}'),\; \mathbf{u}\in U, \mathbf{v},\mathbf{v}'\in W
\end{gather*}
$$

where $\alpha, \beta\in\mathbb{F}$. The quotient space $U\otimes V = \mathbf{F}_{U\times V}/S$ is also called the *tensor product* of $U$ and $V$. The elements of the tensor product have the form

$$
  \left(\sum_{i\in I} \alpha_i(\mathbf{u}_i, \mathbf{v}_i) \right) + R = \sum_{i\in I} \alpha_i[(\mathbf{u}_i, \mathbf{v}_i) + R],\; \alpha_i\in\mathbb{F}, \mathbf{u}_i \in U, \mathbf{v}_i \in V
$$

Because $\alpha(\mathbf{u}, \mathbf{v}) - (\alpha\mathbf{u}, \mathbf{v})\in R$ and $\alpha(\mathbf{u},\mathbf{v}) - (\mathbf{u},\alpha\mathbf{v})\in R$, we can absorb the scalar in either coordinate, i.e.

$$
  \alpha[(\mathbf{u}, \mathbf{v}) + R] = (\alpha\mathbf{u},\mathbf{v}) + S = (\mathbf{u}, \alpha\mathbf{v}) + R
$$

such that the tensors of $U\otimes V$ can be written simply as

$$
  \sum_{i\in I} [(\mathbf{u}_i, \mathbf{v}_i) + R] = \sum_{i\in I} \mathbf{u}_i \otimes \mathbf{v}_i
$$

To show that the pair $(U\times V, \otimes: U\times V\to U\otimes V)$ has the universal property for bilinearity, consider the following diagram.

<LatexFig width={50} src='/fig/tensor_product_isomorphism.svg' alt=''
  caption='Tensor product diagram'
>
```tex
\begin{tikzcd}[every label/.append style={font=\scriptsize}]
  U\times V 
    \arrow[rr, bend left, "\otimes"] 
    \arrow[r, "G"] 
    \arrow[dr, "F" swap] 
  & L_{U\times V}
    \arrow[r, "\mathrm{R}"]
    \arrow[d, dashed, "\mathrm{S}"]
  & U\otimes V
    \arrow[dl, dashed, "\mathrm{T}"]
  \\ & W &
\end{tikzcd}
```
</LatexFig>

Since $\otimes(\mathbf{u},\mathbf{v}) = \mathbf{u}\times\mathbf{v} = \mathrm{R}\circ G(\mathbf{u}, \mathbf{v})$ it follows that $\otimes = \mathrm{R}\circ G$. The universal property of vector spaces implies that there is a unique linear transformation $\mathrm{S}: L_{U\times V}\to W$ for which $\mathrm{S}\circ G = F$.

Note that $\mathrm{S}$ sends any of the vectors $(1)$ and $(2)$ that generate $S$ to the zero vector, i.e. $S\subseteq \ker(\mathrm{S})$

$$
\begin{align*}
  &\mathrm{S}[\alpha(\mathbf{u}, \mathbf{v}) + \beta(\mathbf{u}',\mathbf{v}) - (\alpha\mathbf{u} + \beta\mathbf{u}', \mathbf{v})] \\
  =& \mathrm{S}[\alpha G(\mathbf{u}, \mathbf{v}) + \beta G(\mathbf{u}', \mathbf{v}) - G(\alpha\mathbf{u} + \beta\mathbf{u}',\mathbf{v})] \\
  =& \alpha\mathrm{S}h(\mathbf{u}, \mathrm{v}) + \beta\mathrm{S}h(\mathbf{u}',\mathbf{v}) - \mathrm{S}G(\alpha\mathbf{u} + \beta\mathbf{u}',\mathbf{v}) \\
  =& \alpha f(\mathbf{u},\mathbf{v}) + \beta F(\mathbf{u}', \mathbf{v}) - F(\alpha\mathbf{u} + \beta\mathbf{u}', \mathbf{v}) \\
  =& \alpha F(\mathbf{u},\mathbf{v}) + \beta F(\mathbf{u}', \mathbf{v}) - \alpha F(\mathbf{u}, \mathbf{v}) - \beta F(\mathbf{u}', \mathbf{v})
  =& \mathbf{0}
\end{align*}
$$

and similarly for the second coordinate. Hence by the universal property for linear transformations, there is a unique linear transformation $T:U\otimes V\to W$ such that $\mathrm{T}\circ \mathrm{R} = \mathrm{S}$. Thus

$$
  \mathrm{T}\circ \otimes = \mathrm{T} \circ \mathbf{R}\circ h = \mathrm{S}\circ h \circ f
$$

As to uniqueness, if $\mathrm{T}' \circ \otimes = F$, then $\mathrm{S} = \mathrm{T}' \circ\mathrm{R}$ satisfies

$$
  \mathrm{S}' \circ G = \mathrm{T}' \circ \mathrm{R} = \mathrm{T}' \circ \otimes = F
$$

The uniqueness of $\mathrm{S}$ then implies that $\mathrm{S}' = \mathrm{S}$, which in turn implies that 

$$
  \mathrm{T}' \circ \mathrm{R} = \mathrm{S}' = \mathrm{S} = \mathrm{T}\circ\mathrm{R}
$$

Finally, the uniqueness of $\mathrm{T}$ implies that $\mathrm{T}' = \mathrm{T}$.

## Basis

<MathBox title='Linear independence of tensor products' boxType='proposition'>
Let $U\otimes V$ be the tensor product of $n$-dimensional vector spaces $U$ and $V$. If $\Set{\mathbf{u}_i}_{i=1}^n$ are are linearly independent vectors in $U$ and $\Set{\mathbf{v}_i}_{i=1}^n$ arbitrary vectors in $V$, then

$$
  \sum_{i=1}^n \mathbf{u}_i \otimes \mathbf{v}_i = \mathbf{0} \implies \mathbf{v}_i = \mathbf{0}\; \forall i
$$

In particular $\mathbf{u}\otimes\mathbf{v}=\mathbf{0}$ if and only if $\mathbf{u}=\mathbf{0}$ or $\mathrm{v} = \mathbf{0}$.

<details>
<summary>Proof</summary>

By the bilinearity of the tensor product

$$
\begin{gather*}
  \mathbf{0}\otimes\mathbf{v} = (\mathbf{0} + \mathbf{0})\otimes\mathbf{v} = \mathbf{0}\otimes\mathbf{v} + \mathbf{0}\otimes\mathbf{0} \\
  \implies \mathbf{0}\otimes\mathbf{v} = \mathbf{0}
\end{gather*}
$$

and similarly $\mathbf{u}\otimes\mathbf{0} = \mathbf{0}$.

Suppose that $\sum_{i=1}^n \mathbf{u}_i \otimes \mathbf{v}_i = \mathbf{0}$ assuming none of $\mathbf{u}_i$ and $\mathbf{v}_i$ are zero. According to the universal property of the tensor product, for any bilinear function $F:U\times V\to W$, there is a unique linear transformation $\mathrm{L}: U\otimes V\to W$ for which $\mathrm{L}\circ \otimes = F$. Hence

$$
\begin{align*}
  \mathbf{0} =& \mathrm{T}\left(\sum_{i=1}^n \mathbf{u}_i \otimes \mathbf{v}_i \right) \\
  =& \sum_{i=1}^n (\mathrm{T}\circ t)(\mathbf{u}_i, \mathbf{v}_i) \\
  =& \sum_{i=1}^n F(\mathbf{u}_i, \mathbf{v}_i)
\end{align*}
$$

One possibility for $F$, which can take any form, is to mulitply two linear functionals $a\in U^*$ and $b\in V^*$, i.e. $f(\mathbf{u}, \mathbf{v}) = a(\mathbf{u})b(\mathbf{v})$. This is easily seen to be bilinear and gives

$$
  \sum_{i=1}^n u(\mathbf{u}_i)v(\mathbf{v}_i)
$$

where all $\mathbf{u}_i$ and $\mathbf{v}_i$ are nonzero. If all $\mathbf{u}_i$ are linearly independent, then we can find dual vectors $u_i$ such that $u_i(\mathbf{u}_j) = \delta_{i,j}$. Setting $a = u_j$ gives

$$
  \mathbf{0} = \sum_{i=1}^n u_j (\mathbf{u}_i)b(\mathbf{v}_i) = a(\mathbf{v}_j)
$$

for all linear functionals $b\in V^*$, implying that $\mathbf{v}_i = \mathbf{0}$ for all $i$.
</details>
</MathBox>

<MathBox title='Basis of tensor products' boxType='proposition'>
Let $B = \Set{\mathbf{b}_i}_{i\in I}$ be a basis for a vector space $U$ and $C = \Set{\mathbf{c}_i}_{j\in J}$ be a basis for a vector space $V$. Then the set

$$
  D = \Set{\mathbf{b}_i \otimes \mathbf{v}_j | i\in I, j\in J}
$$

is a basis for $U\otimes V$.

<details>
<summary>Proof</summary>

To show that $D$ is linearly independent, suppose that

$$
  sum_{i\in I, j\in J} \alpha_{i,j}(\mathbf{b}_i \otimes \mathbf{c}_j) = \mathbf{0},\; \alpha_{i,j}\in\mathbb{F}
$$

This can be written

$$
  \sum_{i\in I}\mathbf{b}_i \otimes \left(\sum_{j\in J} \alpha_{i,j}\mathbf{c}_j \right) = \mathbf{0}
$$

Since $\mathbf{b}_i$ are linearly independent, it follows that $\sum_{j\in J} \alpha_{i,j}\mathbf{v}_i = 0$ for all $i\in I$ and thus $\alpha_{i,j} = 0$ for all $i$ and $j$.

To show that $D$ spans $U\times V$ let $\mathbf{u}\otimes\mathbf{v}\in U\otimes V$. Since $\mathbf{u} = \sum_{i\in I}\alpha_i \mathbf{b}_i$ and $\mathbf{v} = \sum_{j\in J}\beta_j \mathbf{c}_j$ we have

$$
\begin{align*}
  \mathbf{u}\otimes\mathbf{v} =& \sum_{i\in I}\alpha_i \mathbf{b}_i \otimes \sum_{j\in J} \beta_j \mathbf{c}_j \\
  =& \sum_{i\in I} \alpha_i \left( \mathbf{b}_i \otimes \sum_{j\in J} \beta \mathbf{c}_j \right) \\
  =& \sum_{i\in I} r_i \left(\sum_{j\in J} (\mathbf{b}_i \otimes \mathbf{c}_j) \right) \\
  =& \sum_{i\in I, j\in J} \alpha_i \beta_j (\mathbf{b}_i \otimes \mathbf{c}_j)
\end{align*}
$$

Hence, any sum of elements of the form $\mathbf{u}\otimes\mathbf{v}$ is a linear combination of the vectors $\mathbf{b}_i \otimes \mathbf{c}_j$.
</details>
</MathBox>

<MathBox title='Tensor product dimension' boxType='proposition'>
For finite dimensional vector spaces

$$
  \dim(U\otimes V) = \dim(U)\cdot\dim(V)
$$
</MathBox>

## Coordinate matrices and rank

If $U$ and $V$ are vector spaces over $\mathbb{F}$ with respective bases $B = \Set{\mathbf{b}_i}_{i\in I}$ and $C = \Set{\mathbf{c}_j}_{j\in J}$, then any vector $\mathbf{w}\in U\otimes V$ can be expressed as a summand

$$
  \mathbf{w} = \sum_{i\in I, i\in J} \rho_{i,j}(\mathbf{b}_i\otimes \mathbf{c}_j)
$$

where only a finite number of the scalars $\rho_{i,j}\in\mathbf{F}$ are nonzero. In fact, for a fixed $\mathbf{w}\in U\otimes V$, we can reindex the bases so that

$$
  \mathbf{w} = \sum_{i=1}^a \sum_{j=1}^b \rho_{i,j}(\mathbf{b}_i\otimes \mathbf{c}_j)
$$

where none of the rows or columns of the matrix $\mathbf{R} = (\rho_{i,j})$ consists of only zeroes. The matrix $\mathbf{R}$ is called a *coordinate matrix* of $\mathbf{w}$ with respect to bases $B$ and $C$.

Suppose that $X = \Set{\mathbf{x}_i}_{i\in I}$ and $Y = \Set{\mathbf{y}_j}{j\in J}$ also are bases for $U$ and $V$, respectively and that

$$
  \mathbf{w} = \sum_{i=1}^c \sum_{j=1}^d \sigma_{i,j}(\mathbf{x}_i\otimes \mathbf{y}_j)
$$

where $\mathbf{S} = (\sigma_{i,j})$ is a coordinate matrix of $\mathbf{w}$ with respect to the bases. We claim that the coordinate matrices $\mathbf{R}$ and $\mathbf{S}$ have the same rank, which defines the rank of the tensor $\mathbf{w}\in U\otimes W$.

Each $\mathbf{x}_1,\dots,\mathbf{x}_c$ is a finite linear combination of basis vectors in $B$. We can reindex $B$ so that each $\mathbf{w}_k$ is a linear combination of the vectors $\Set{\mathbf{b}_i}_{i=1}^n$ where $a\leq n$ generating the subspace

$$
  U_n = \mathrm{span}\Set{\mathbf{b}_i}_{i=1}^n
$$

Next, extend $\Set{\mathbf{x}_i}_{i=1}^c$ to a basis $X' = \Set{\mathbf{x}_1,\dots,\mathbf{x}_c,\mathbf{x}_{c+1},\dots,\mathbf{x}_n}$ for $U_n$. Thus

$$
  \mathbf{x}_i = \sum_{h=1}^n \alpha_{i,h}\mathbf{u}_h,\; i=1,\dots,n
$$

where $\mathbf{A} = (\alpha_{i,h})$ is an invertible $n\times n$ matrix.

Repeating the same process for $V$, where we reindex $C$ so that the subspace $V_m = \mathrm{span}\Set{\mathbf{v}_j}_{j=1}^m$ contains $\mathbf{y}_1,\dots,\mathbf{y}_d$ and extend to a basis $Y' = \Set{\mathbf{y}_1,\dots,\mathbf{y}_d,\mathbf{y}_{d+1},\dots,\mathbf{y}_m}$ for $V_m$. Then

$$
  \mathbf{y}_j = \sum_{k=1}^m \beta_{j,k}\mathbf{v}_k,\; j=1,\dots,m
$$

where $\mathbf{B} = (\beta_{j,k})$ is an invertible $n\times n$ matrix. 

Next, write

$$
  \mathbf{w} = \sum_{i=1}^n \sum_{j=1}^m \rho_{i,j}(\mathbf{u}_i\otimes \mathbf{v}_j)
$$

by setting $\rho_{i,j} = 0$ for $i > a$ or $j > b$. Thus, the $n\times m$ matrix $\mathbf{R}_1 = (\rho_{i,j})$ comes from $\mathbf{R}$ by adding $n-a$ rows of zeroes to the bottom and the $m-b$ columns of zeroes. In particular $\mathbf{R}_1$ and $\mathbf{R}$ have the same rank.

The expression for $\mathbf{w}$ in terms of the bases $X$ and $Y$ can also be extended by zero scalars to

$$
  \mathbf{w} = \sum_{i=1}^n \sum_{j=1}^m \sigma_{i,j}(\mathbf{x}_i\otimes\mathbf{y}_j)
$$

where the $n\times m$ matrix $\mathbf{S}_1 = (\sigma_{i,j})$ has the same rank as $\mathbf{S}$.

Finally, we can compute

$$
\begin{align*}
  \sum_{i=1}^n \sum_{j=1}^m \mathbf{\sigma}(\mathbf{x}_i \otimes\mathbf{y}_j) =& \sum_{i=1}^n \sum_{j=1}^m \sigma_{i,j}\left(\sum_{h=1}^n \alpha_{i,h}\mathbf{b}_h \otimes \sum_{k=1}^m \beta_{j,k}\mathbf{c}_k \right) \\
  =& \sum_{i=1}^n \sum_{j=1}^m \sum_{h=1}^n \sum_{k=1}^m \alpha_{i,j}\sigma_{i,j}\beta_{j,k}(\mathbf{b}_h \otimes \mathbf{c}_k) \\
  =& \sum_{h=1}^n \sum_{k=1}^m \sum_{j=1}^m \sum_{i=1}^n \left(\alpha_{h,i}^T \sigma_{i,j}\right)\beta_{j,k}(\mathbf{b}_h \otimes \mathbf{c}_k) \\
  =& \sum_{h=1}^n \sum_{k=1}^m \sum_{j=1}^m [\mathbf{A}^T \mathbf{S}_1]_{h,j} \beta_{j,k} (\mathbf{b}_h \otimes \mathbf{c}_k) \\
  =& \sum_{h=1}^n \sum_{k=1}^m [\mathbf{A}^T \mathbf{S}_1 \mathbf{B}]_{h,k} (\mathbf{b}_h \otimes \mathbf{c}_k)
\end{align*}
$$

yielding

$$
  \sum_{i=1}^n \sum_{j=1}^m \rho_{i,j}(\mathbf{b}_i \otimes\mathbf{c}_j) = \sum_{h=1}^n \sum_{k=1}^m [\mathbf{A}^T \mathbf{S}_1 \mathbf{B}]_{h,k} (\mathbf{b}_h \otimes \mathbf{c}_k)
$$

It follows that $\mathbf{R}_1 = \mathbf{A}^T \mathbf{S_1} \mathbf{B}$. Since $A$ and $B$ are invertible, we deduce that

$$
  \mathrm{rank}(\mathbf{R}) = \mathrm{rank}(\mathbf{R}_1) = \mathrm{rank}(\mathbf{S}_1) = \mathrm{rank}(\mathbf{S})
$$

In block terms

$$
  \mathbf{R_1} = \begin{bmatrix} \mathbf{R} & 0 \\ 0 & 0 \end{bmatrix} \quad\text{and}\quad \mathbf{S_1} = \begin{bmatrix} \mathbf{S} & 0 \\ 0 & 0 \end{bmatrix}
$$

and

$$
  \mathbf{A}^T = \begin{bmatrix} \mathbf{A}_{a,c}^T & - \\ - & - \end{bmatrix} \quad\text{and}\quad \mathbf{B} = \begin{bmatrix} \mathbf{B}_{d,b} & - \\ - & - \end{bmatrix}
$$

Then $\mathbf{R}_1 = \mathbf{A}^T \mathbf{S_1} \mathbf{B}$ implies that

$$
  \mathbf{R} = \mathbf{A}_{a,c}^T \mathbf{S_1} \mathbf{B}_{d,b}
$$

where $\mathrm{rank}(\mathbf{A}_{c,d}^T)\geq\mathrm{rank}(\mathbf{R})$ and $\mathrm{rank}(\mathbf{B})\geq\mathrm{rank}(\mathbf{R})$.

In the special case

$$
  \mathbf{w} = \sum_{i=1}^r \mathbf{b}_i \otimes \mathbf{c}_j = \sum_{i=1}^r \mathbf{x}_i \otimes\mathbf{y}_i
$$

it follows that $a=b=c=d=r$ and $\mathbf{R} = \mathbf{S} = \mathbf{I}_r$ with

$$
  \mathbf{R_1} = \begin{bmatrix} \mathbf{I}_r & 0 \\ 0 & 0 \end{bmatrix} \quad\text{and}\quad \mathbf{S_1} = \begin{bmatrix} \mathbf{I}_r & 0 \\ 0 & 0 \end{bmatrix}
$$

Thus, the equation  $\mathbf{R} = \mathbf{A}_{a,c}^T \mathbf{S} \mathbf{B}_{d,b}$ becomes

$$
  \mathbf{I}_r = \mathbf{A}_{r,r}^t \mathbf{B}_{r,r}
$$

such that for $\mathbf{A}_{r,r} = (\alpha_{i,h})$

$$
  \mathbf{x}_i = \sum_{i=1}^r \alpha_{i,h}\mathbf{b}_h,\; i=1,\dots,r
$$

and for $\mathbf{B}_{r,r} = (\beta_{j,k})$

$$
  \mathbf{y}_j = \sum_{k=1}^r \beta_{j,k} \mathbf{c}_k,\; j=1,\dots,r
$$

### The rank of a decomposable tensor

A tensor of the form $\mathbf{u}\otimes\mathbf{v}\in U\otimes V$ for called decomposable. If $B = \Set{\mathbf{b}_i}_{i\in I}$ is a basis for $U$ and $C = \Set{\mathbf{c}_j}_{j\in J}$ is a basis for $V$, then any decomposable tensor has the form

$$
  \mathbf{u}\otimes\mathbf{v} = \sum_{i\in I, j\in J} \rho_i \sigma_j (\mathbf{b}_i \otimes\mathbf{c}_j)
$$

Hence, the rank of a decomposable tensor is $1$.

## Characterizing vectors in a tensor product

<MathBox title='Representations of a tensor product' boxType='proposition'>
Let $U$ and $V$ be vector spaces over $\mathbb{F}$ with respective bases $B = \Set{\mathbf{b}_i}_{i\in I}$ and $C = \Set{\mathbf{c}_j}_{j\in J}$. Then every $\mathbf{w} \in U\otimes V$ has a unique expression as a finite sum (up to order of zero terms) of the forms

$$
\begin{align}
  \sum_{i\in I,j\in J}& \rho_{i,j} \mathbf{b}_i \times \mathbf{c}_j,\; \rho_{i,j}\in\mathbb{F} \\
  \sum_{i\in I,j\in J}& \mathbf{b}_i \times \mathbf{v}_j,\; \mathbf{v}_j \in V \\
  \sum_{i\in I}& \mathbf{u}_i \otimes \mathbf{c}_i,\; \mathbf{u}_i \in U
\end{align}
$$

**(4)** Every nonzero $\mathbf{w}\in U\otimes V$ has an expression of the form

$$
  \sum_{i=1}^n \mathbf{u}_i \otimes\mathbf{v}_i
$$

where $\Set{\mathbf{u}_i}_{i\in I}\subseteq U$ and $\Set{\mathbf{v}_j}_{j\in J}\subseteq V$ are linearly independent sets. As to uniqueness, $n$ is the rank of $\mathbf{w}$ and so it is unique. Also, we have

$$
  \sum_{i=1}^r \mathbf{u}_i \otimes\mathbf{v}_i = \sum_{i=1}^r \mathbf{x}_i \otimes\mathbf{y}_i
$$

where $\Set{\mathbf{x}_i}_{i\in I}\subseteq U$ and $\Set{\mathbf{y}_j}_{j\in J}\subseteq V$, if and only if there exists $r \times r$ matrices $\mathbf{A} = (\alpha_{i,j})$ and $\mathbf{B} = (\beta_{i,j})$ for which $\mathbf{A}^T \mathbf{B} = \mathbf{I}_r$ such that for $j=1,\dots,r$

$$
\begin{align*}
  \mathbf{x}_i =& \sum_{j=1}^r \alpha_{i,j}\mathbf{u}_j \\
  \mathbf{y}_i =& \sum_{j=1}^r \alpha_{i,j}\mathbf{v}_j
\end{align*}
$$

<details>
<summary>Proof</summary>

**(1):** The first sum merely expresses the fact that $D = \Set{ \mathbf{b}_i \otimes\mathbf{c}_i | i\in I, j\in J }$ is a basis for $U\otimes V$

**(2):** From the first sum, we write

$$
\begin{align*}
  \sum_{i\in I, j\in J} \rho_{i,j}\mathbf{b}_i \otimes\mathbf{c}_j =& \sum_{i\in I} \left( \mathbf{b}_i \otimes \sum_{j\in J} \rho_{i, j} \mathbf{c}_j \right) \\
  =& \sum_{i\in I} \mathbf{b}_i \otimes \mathbf{v}_i
\end{align*}
$$

The uniqueness of the sum follows from the linear independence of $C$.

**(3):** Similarly, we write the first sum as

$$
\begin{align*}
  \sum_{i\in I, j\in J} \rho_{i,j}\mathbf{b}_i \otimes\mathbf{c}_j =& \sum_{j\in J} \left( \sum_{i\in I} \rho_{i,j} \mathbf{b}_i \right) \otimes \mathbf{c}_j \right \\
  =& \sum_{j\in J} \mathbf{u}_j \otimes \mathbf{c}_j
\end{align*}
$$

The uniqueness of the sum follows from the linear independence of $B$.


**(4)**: Assume from the second sum that none of $\mathbf{v}_i$ are zero. If the set $\Set{\mathbf{v}_i}$ is linearly independent, we are done. If not, then we may suppose (after reindexing if necessary) that

$$
  \mathbf{y}_n = \sum_{i=1}^{n-1}\rho_i \mathbf{v}_i
$$

Then

$$
\begin{align*}
  \sum_{i=1}^n \mathbf{b}_i \otimes\mathbf{v}_i =& \sum_{i=1}^{n-1}\mathbf{b}_i \otimes\mathbf{v}_i + \left( \mathbf{b}_n \otimes \sum_{i=1}^{n-1} \rho_i \mathbf{v}_i \right) \\
  =& \sum_{i=1}^{n-1} \mathbf{b}_i \otimes\mathbf{v}_i + \sum_{i=1}^{n-1} (\rho_i \mathbf{v}_i) \\
  =& \sum_{i=1}^{n-1} (\mathbf{v}_i + \rho_i \mathbf{v}_n) \otimes\mathbf{v}_i
\end{align*}
$$

The vectors $\Set{\mathbf{b}_i + \rho_i \mathbf{b}_n | 1\leq i \leq n-1}$ are linearly independent. This reduction can be repeated until the second coordinates are linearly independent. Also, the identity matrix $\mathbf{I}_n$ is a coordinate matrix for $\mathbf{w}$ giving $n = \mathrm{rank}(\mathbf{I}_n) = \mathrm{rank}(\mathbf{w})$.
</details>
</MathBox>

## The tensor product of linear transformations

<MathBox title='Dual tensor product isomorphism' boxType='proposition'>
If $U$ and $V$ are finite-dimensional vector spaces over $\mathbb{F}$, then

$$
  U^* \otimes V^* \cong (U \otimes V)^*
$$

via the isomorphism $\mathrm{T}:U^* \otimes V^* \to (U\otimes V)^*$ given by

$$
  \mathrm{T}(f\otimes g)(\mathbf{u}\otimes\mathbf{v}) = f(\mathbf{u})g(\mathbf{v})
$$

Since $(U \times V)^* \cong \hom(U\times V; \mathbb{F})$ it follows that

$$
  U^* \otimes V^* \cong (U\otimes V)^* \cong \hom(U\times V, \mathrm{F})
$$

<details>
<summary>Proof</summary>

For fixed $f$ and $g$, the map $F_{f,g}: U\times V \to \mathbb{F}$ defined by $F_{f,g} = f(\mathbf{u})g(\mathbf{v})$ is bilinear. By the universal property of tensor products, there exists a unique linear functional $\ell_{f, g}(\mathbf{u}\otimes\mathbf{v}) = F_{f,g} (\mathbf{u},\mathbf{v}) = f(\mathbf{u})g(\mathbf{v})$.

Next, the map $G: U^* \to V^* \to (U\otimes V)^*$ defined by $G(f,g) = \ell_{f,g}$ is bilinear since for $\alpha,\beta\in\mathbb{F}$ and $h\in V^*$

$$
\begin{align*}
  G(\alpha f + \beta, g, h)(\mathbf{u}\otimes\mathbf{v}) =& \ell_{\alpha f+ \beta g, h} (\mathbf{u}\otimes\mathbf{v}) \\
  =& (\alpha f + \beta g)(\mathbf{u})\cdot h(\mathbf{v}) \\
  =& \alpha f(\mathbf{u}) h(\mathbf{v}) + \beta g(\mathbf{u})h(\mathbf{v}) \\
  =& (\alpha\ell_{f,h} + \beta\ell_{g,h})(\mathbf{u}, \mathbf{v}) \\
  =& (\alpha G(f,h) + \beta G(g,h))(\mathbf{u}\otimes\mathbf{v})
\end{align*}
$$

showing that $G$ is linear in the first coordinate, and similar for the second. By the universal property, there exists a unique linear map $\mathrm{T}: U^* \otimes V^* \to (U\otimes V)^*$ for which

$$
  \mathrm{T}(f\otimes g) = G(f,g) = \ell_{f,g}
$$

that is,

$$
  \mathrm{T}(f\otimes g) = (\mathbf{u}\otimes\mathbf{v}) = \ell_{f,g}(\mathbf{u}\otimes\mathbf{v}) = f(\mathbf{u})g(\mathbf{v})
$$

It remains to show that $\mathrm{T}$ is bijective. Let $\Set{\mathbf{b}_i}_{i\in I}$ be a basis for $U$, with dual basis $\Set{b_i}_{i\in I}$ and let $\Set{\mathbf{c}_j}_{j\in J}$ be a basis for $V$ with dual basis $\Set{ c_j}_{j\in J}$. Then

$$
\begin{align*}
  \mathrm{T}(b_i \otimes c_i)(\mathbf{b}_u \otimes \mathbf{c}_v) = b_i(\mathbf{b}_u) c_j (\mathbf{c}_v) = \delta_{i,u}\delta_{j,v} = \delta_{(i,j),(u,v)}
\end{align*}
$$

showing that $\Set{\mathrm{T}(b_i \otimes c_i) }\subseteq (U\otimes V)^*$ is the dual basis to basis $\Set{\mathbf{b}_u \otimes \mathbf{c}_v}$ for $U\otimes V$. Thus, $\mathrm{T}$ takes the basis $\Set{ b_i \otimes c_j }$ for $U^* \otimes V^*$ to the basis $\Set{\mathrm{T}(b_i \otimes c_j}$ and is therefore bijective.
</details>
</MathBox>

<MathBox title='Linear transformation of tensor products' boxType='proposition'>
The linear transformation

$$
  \Theta: \mathcal{L}(U, U') \otimes \mathcal{L}(V, V') \to\mathcal{L}(U\otimes V, U' \otimes V') 
$$

defined by $\Theta(\mathrm{T}\otimes\mathrm{S}) = \mathrm{T}\odot\mathrm{S}$ where

$$
  (\mathrm{T}\odot\mathrm{S})(\mathbf{u}\otimes\mathbf{v}) = \mathrm{T}(\mathbf{u})\mathrm{S}(\mathbf{v})
$$

is an embedding (injective linear transformation), and is an isomorphism if all vector spaces are finite-dimensional. Thus, the tensor product $\mathrm{T}\otimes\mathrm{S}$ of all linear transformation is a linear transformation on tensor products.

<details>
<summary>Proof</summary>

For fixed $\mathrm{T}:U\to U'$ and $\mathrm{S}:V\to V'$ the tensor product defined by $f(\mathbf{u}, \mathbf{v}) = \mathrm{T}(\mathbf{u}\otimes\mathrm{S}(\mathbf{v})$ is bilinear. By the universal property for bilinear transformations, there is a unique linear map $(\mathrm{T}\odot\mathrm{S}):U\otimes V \to U' \otimes V'$ for which

$$
  (\mathrm{T}\odot\mathrm{S})(\mathbf{u}\otimes\mathrm{v}) = \mathrm{T}(\mathbf{u})\mathrm{S}(\mathbf{v})
$$

Since $\mathrm{T}\odot\mathrm{S}\in\mathcal{L}(U,U') \times \mathcal{L}(V,V') \to \mathcal{L}(U\otimes V, U' \otimes V')$, there is a function

$$
  F:\mathcal{L}(U, U')\times\mathcal{L}(V,V')\to \mathcal{L}(U\otimes V, U'\otimes V')
$$

defined by

$$
  F(\mathrm{T}, \mathrm{S}) = \mathrm{T}\odot\mathrm{S}
$$

which is linear since for $\alpha,\beta\in\mathbb{F}$ and $\mathrm{R}\in\mathcal{L}(U,U')$

$$
\begin{align*}
  ((\alpha\mathrm{T} + \beta\mathrm{R})\odot\mathrm{S})(\mathbf{u, \mathbf{v}}) =& (\alpha\mathrm{T} + \beta\mathrm{R})(\mathbf{u})\otimes\mathrm{S}(\mathbf{v}) \\
  =& (\alpha\mathrm{T}(\mathbf{u}) + \beta\mathrm{R}(\mathbf{u}))\otimes\mathrm{S}(\mathbf{v}) \\
  =& \alpha[\mathrm{T}(\mathbf{u})\otimes\mathrm{S}(\mathbf{v})] + \beta[\mathrm{R}(\mathbf{u})\otimes\mathbf{v}] \\
  =& \alpha(\mathrm{T}\odot\mathrm{S})(\mathbf{u},\mathbf{v}) + \beta(\mathrm{R},\mathrm{S})(\mathbf{u, \mathbf{v}}) \\
  =& (\alpha(\mathrm{T}\odot\mathrm{S})+ \beta(\mathrm{R}\odot\mathrm{S}))(\mathbf{u},\mathbf{v}) 
\end{align*}
$$

showing that $F$ is linear in the first coordinate, and similarly for the second. By the universal property for bilinear maps there is a unique linear transformation

$$
  \Theta: \mathcal{L}(U,U')\otimes\mathcal{L}(V,V') \to \mathcal{L}(U\otimes V, U'\otimes V')
$$

satisfying $\Theta(\mathrm{T}\otimes\mathrm{S}) = \mathrm{T}\odot\mathrm{S}$, i.e.

$$
  [\Theta(\mathrm{T}\odot\mathrm{S})](\mathbf{u}\odot\mathbf{v}) = \mathrm{T}(\mathbf{u})\mathrm{S}(\mathbf{v})
$$

To see that $\Theta$ is injective, if $\Theta(\mathrm{T}\odot\mathrm{S}) = 0$ then $\mathrm{T}(\mathbf{u})\otimes\mathrm{S}(\mathbf{v}) = 0$ for all $\mathbf{u}\in U$ and $\mathbf{v}\in V$. If $\mathrm{S} = 0$, then $\mathrm{T}\otimes\mathrm{S} = 0$. If $\mathrm{S}\neq 0$, then there is a $\mathbf{v}\in V$ for which $\mathrm{S}(\mathbf{v}) \neq\mathbf{0}$. However, $\mathrm{T}(\mathbf{u})\otimes\mathrm{T}(\mathbf{v}) = \mathbf{0}$ implies that one of the factors is $0$ and so $\mathrm{T}(\mathbf{u}) = \mathbf{0}$ for all $\mathbf{u}\in U$, i.e. $\mathrm{T} = 0$. Hence $\mathrm{T}\otimes\mathrm{S} = 0$. In either case, we see that $\Theta$ is injective.

Thus, $\Theta$ is an embedding and if all vector spaces are finite dimensional then

$$
  \dim(\mathcal{L}(U, U')\otimes\mathcal{L}(V,V')) = \dim(\mathcal{L}(U\otimes V, U'\otimes V'))
$$

showing that $\Theta$ is also subjective and thus an isomorphism.

The embedding of $\mathcal{L}(U,U')\otimes\mathcal{L}(V,V')$ into $\mathcal{L}(U\otimes V, U'\otimes V')$ means that each $\mathrm{T}\otimes\mathrm{S}$ can be thought of as the linear transformation $\mathrm{T}\odot\mathrm{S}$ from $U\otimes V$ to $U' otimes V'$ defined by

$$
  (\mathrm{T}\odot\mathrm{R})(\mathbf{u}\otimes\mathrm{v}) = \mathrm{T}(\mathbf{u}) \odot\mathrm{R}(\mathbf{v})
$$
</details>
</MathBox>

<MathBox title='' boxType='corollary'>
If $X$ an $Y$ are finite-dimensional vector spaces let $X\xhookrightarrow{\cong} Y$ denote that $X$ and $Y$ are isomorphic through an embedding. Consider the following special cases of the linear map for finite-dimensional vector spaces $U$ and $V$ over $\mathbb{F}$

$$
  \Theta: \mathcal{L}(U, U') \otimes \mathcal{L}(V, V') \to\mathcal{L}(U\otimes V, U' \otimes V') 
$$

1. Taking $U' = \mathbb{F}$ gives
$$
  U^* \otimes\mathcal{L}(V,V') \xhookrightarrow{\cong} \mathcal{L}(U\otimes V, V')
$$
where $(f \otimes \mathrm{S})(\mathbf{u}\otimes\mathbf{v}) = f(\mathbf{u})\mathrm{S}(\mathbf{v})$ for $f\in U^*$.<br/>
2. Taking $U' = \mathbb{F}$ and $V' = \mathbb{F}$ gives
$$
  U^* \otimes V^* \xhookrightarrow{\cong} (U\otimes V)^*
$$
where $(f\otimes g)(\mathbf{u}\otimes\mathbf{v}) = f(\mathbf{u})g(\mathbf{v})$.<br/>
3. Taking $V = \mathbb{F}$ and noting that $\mathcal{L}(\mathbb{F},V') \cong V'$ and $U\otimes\mathbb{F}\cong U$ gives
$$
  \mathcal{L}(U,U') \otimes W \xhookrightarrow{\cong} \mathcal{L}(U, U'\otimes W)
$$
where $(\mathrm{T}\otimes \mathbf{w})(\mathbf{u}) = \mathrm{T}(\mathbf{u})\otimes\mathbf{w}$.<br/>
4. Taking $U' = \mathbb{F}$ and $V = \mathbb{F}$ gives (letting $W = V'$)
$$
  U^* \otimes W \xhookrightarrow{\cong} \mathcal{L}(U,W)
$$
where $(f\otimes \mathbf{w})(\mathbf{u}) = f(\mathbf{u})\mathbf{w}$.
</MathBox>

## Change of base field

Let $V_{\mathbb{F}}$ be a vector space over $\mathbb{F}$ and suppose that $\mathbb{K}\supseteq\mathbb{F}$ is an extension field of $\mathbb{K}$. The vector space $V_{\mathbb{F}}$ can be extended over $\mathbb{K}$ via a tensor product of the form $W_{\mathbb{F}} = \mathbb{K}\otimes_{\mathbb{F}} V_{\mathbb{F}}$, where $\otimes_{\mathbb{F}}$ denotes that the tensor product is taken with respect to $\mathbb{F}$. In this case, the subscript can be dropped since the only tensor product that makes sense in $\mathbb{K}\otimes V_{\mathbb{F}}$ is the $\mathbb{F}$-tensor product as $V_{\mathbb{F}}$ is not a $\mathbb{K}$-space.

The vector space $W_{\mathbb{F}}$ is an $\mathbb{F}$-space by definition of the tensor product, but can be extended into a $\mathbb{K}$-space as follows. Intuitively, a scalar $\alpha\in\mathbb{K}$ should be absorbed into the first coordinate of the tensor product, i.e.

$$
  \alpha(\beta\otimes\mathbf{v}) = (\alpha\beta)\otimes\mathbf{v}
$$

and for this to be well-defined we require that

$$
  \beta\otimes\mathbf{v} = \gamma\otimes\mathbf{w} \implies (\alpha\beta)\otimes\mathbf{v} = (\alpha\gamma)\otimes\mathbf{w}
$$

This can be ensured with the universal property for bilinearity by considering the map $F_\alpha: (\mathbb{K}\times V_{\mathbb{F}})\to (\mathbb{K}\otimes V_{\mathbb{F}})$ defined by $F_\alpha (\beta, \mathbf{v}) = (\alpha\beta)\otimes\mathbf{v}$. This map is obviously well-defined and since it is also bilinear, the universal property of tensor products implies that there is a unique $\mathbb{F}$-linear map $\mathrm{T}_\alpha (\mathbb{K}\otimes V_{\mathbb{F}} \to (\mathbb{K}\otimes V_{\mathbb{F}})$ for which

$$
  \mathrm{T}_\alpha (\beta\otimes\mathbf{v}) = (\alpha\beta)\otimes\mathbf{v}
$$

Since $\mathrm{T}_\alpha$ is $\mathbb{F}$-linear, it is additive such that

$$
  \mathrm{T}_\alpha [(\beta\otimes\mathbf{v}) + (\gamma\otimes\mathbf{w})] = \mathrm{T}_\alpha (\beta\otimes\mathbf{v}) + \mathrm{T}_\alpha (\gamma\otimes\mathbf{w})
$$

that is

$$
  \alpha [(\beta\otimes\mathbf{v}) + (\gamma\otimes\mathbf{w})] = \alpha (\beta\otimes\mathbf{v}) + \alpha (\gamma\otimes\mathbf{w})
$$

which is one the properties required of a scalar multiplication. Since the other defining properties of scalar multiplication are satisfied, the set $\mathbb{K}\otimes V_{\mathbb{F}}$ is indeed a $\mathbb{K}$-space under this operation (and addition), which we denote by $W_{\mathbf{K}}$.

At this stage, we have have three distinct vector spaces: the $\mathbb{F}$-spaces $V_{\mathbb{F}}$ and $W_{\mathbb{F}} = \mathbb{K}\otimes V_{\mathbb{F}}$ and the $\mathbb{K}$-space $W_{\mathbb{K}} = \mathbb{K}\otimes V_{\mathbb{F}}$, where the tensor product in both cases is with respect to $\mathbb{F}$. The spaces $W_{\mathbb{F}}$ and $W_{\mathbb{K}}$ are identical as sets and abelian groups, and only differ in the base field for scalar multiplication. Accordingly, we can recover $W_{\mathbb{F}}$ from $W_{\mathbb{K}}$ simply by restricting scalar multiplication to scalars from $\mathbb{F}$.

This suggests that there exists $\mathbb{F}$-linear maps $\mathbf{T}:V_{\mathbb{F}}\to W_{\mathbb{K}}$ such that for $\alpha,\beta\in\mathbb{F}$

$$
  \mathrm{T}(\alpha\mathbf{u} + \beta\mathbf{v}) = \alpha\mathrm{T}(\mathbf{u}) + \beta\mathrm{T}(\mathbf{v})
$$

If the dimension of $\mathbb{K}$ as a vector space over $\mathbb{F}$ is $d$ then

$$
\begin{align*}
  \dim(W_{\mathbb{F}}) =& \dim(\mathbb{K}\otimes V_{\mathbb{F}}) \\
  =& \dim(\mathbb{K})\cdot\dim(V_{\mathbb{F}}) = d\cdot\dim(V_{\mathbb{F}})
\end{align*}
$$

As to the dimension of $W_{\mathbb{K}}$ it is not hard to so that if $\Set{\mathbf{b}_i}_{i\in I}$ is a basis for $V_{\mathbb{F}}$ then $\Set{\mathbf{1}\otimes\mathbf{b}_i}_{i\in I}$ is a basis for $W_{\mathbb{K}}$, hence

$$
  \dim(W_{\mathbb{K}}) = \dim(\mathbb{V}_{\mathbb{F}})
$$

even when $V_{\mathbb{F}}$ is finite-dimensional.

The map $\mathrm{F}: V_{\mathbb{F}}\to W_{\mathbb{F}}$ defined by $F(\mathbf{v}) = \mathbf{1}\otimes\mathbf{v}$ is easily seen to be injective and $\mathbb{F}$-linear, and so $W_{\mathbb{F}}$ contains an isomorphic copy of $V_{\mathbb{F}}$. We can also think of $\mathrm{F}$ as mapping $V_{\mathbb{F}}$ into $W_{\mathbb{K}}$, in which case $\mathrm{F}$ is called the $\mathbb{K}$-extension map of $V_{\mathbb{F}}$.

<MathBox title='Universal property for exension maps' boxType='proposition'>
The $\mathbb{K}$-extension map $\mathrm{F}: V_{\mathbb{F}} \to W_{\mathbb{K}}$ has the universal property for the collection of all $\mathbb{F}$-linear maps with domain $V_{\mathbb{F}}$ and codomain a $\mathbb{K}$-space, as measured by $\mathbb{K}$-linear maps. In particular, for any $\mathbb{F}$-linear map $\mathrm{T}: V_{\mathbb{F}} \to Y_{\mathbb{K}}$, there exists a unique $\mathbb{K}$-linear map $\mathrm{K}: W_{\mathbb{K}}\to Y_{\mathbb{K}}$ for which

$$
  \mathrm{K}\circ \mathrm{F} = \mathrm{T}
$$

<details>
<summary>Proof</summary>

If such a map $\mathrm{K}:\mathbb{K}\otimes V_{\mathbb{F}}\to  Y_{\mathbb{K}}$ exists, then it must satisfy

$$
\begin{align*}
  \mathrm{K}(\beta\otimes\mathbf{v}) =& \beta\mathrm{K}(\mathbf{1}\otimes\mathbf{v}) \\
  =& \beta\mathrm{K}\mathrm{F}(\mathbf{v}) = \beta\mathrm{T}(\mathbf{v})
\end{align*}
$$

This shows that if $\mathrm{K}$ exists, it is uniquely determined by $\mathrm{T}$. Let $G: (\mathbb{K}\times V_{\mathbb{F}})\to Y$ be defined by

$$
  G(\beta,\mathbf{v}) = \beta\mathrm{T}(\mathbf{v})
$$

Since this is bilinear, there exists a unique $\mathbb{F}$-linear map $\mathrm{K}$ for which $\mathrm{K}(\beta\otimes\mathbf{v}) = \beta\mathrm{T}(\mathbf{v})$. It is easy to se that $\mathrm{K}$ is also $\mathbb{K}$-linear, since if $\alpha\in\mathbb{K}$ then

$$
\begin{align*}
  \mathrm{K}[\alpha(\beta\otimes\mathbf{v})] =& \mathrm{K}(\alpha\beta\otimes\mathbf{v}) \\
  =& \alpha\beta\mathrm{T}(\mathbf{v}) \\
  =& \alpha\mathrm{K}(\beta\otimes\mathbf{v})
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Extension linear map' boxType='proposition'>
Let $V$ and $W$ be $\mathbb{F}$-spaces with $\mathbb{K}$-extension maps $\mathrm{F}_V$ and $\mathrm{F}_W$, respectively. Then for any $\mathbb{F}$-linear map $\mathrm{K}:V\to W$, the map $\bar{\mathrm{K}} = \mathrm{I}_\mathbb{K} \otimes\mathrm{K}$ is the unique linear map for which

$$
  \mathrm{F}_W \circ \mathrm{K} = \bar{\mathrm{K}} \circ \mathrm{F}_V
$$

according to the following diagram.

$$
\begin{CD}
  V @>\mathrm{K}>> W \\
  @A{\mathrm{F}_V}AA @AA{\mathrm{F}_W}A \\
  \mathbb{K}\otimes V_{\mathbb{F}} @>>\bar{\mathrm{K}}> \mathbb{K}\otimes W_{\mathbb{K}}
\end{CD}
$$

<details>
<summary>Proof</summary>

Consider the $\mathbb{F}$-linear map $\mathrm{L} = (\mathrm{F}_W \circ\mathrm{K}):V\to\mathbb{K}\otimes W_{\mathbb{K}}$. By the universal property of the $\mathbb{K}$-extension map, there is a unique $\mathbb{K}$-linear map $\bar{\mathrm{K}}:\mathbb{K}\otimes V_{\mathbb{K}}\to\mathbb{K}\otimes W_{\mathbb{K}}$ for which $\bar{\mathrm{K}} \circ\mathrm{F}_V = \mathrm{L} = \mathrm{F}_W \circ \mathrm{K}$, satisfying

$$
\begin{align*}
  \bar{\mathrm{K}}(\beta\otimes\mathbf{v}) =& \beta\bar{\mathrm{K}}(\mathbf{1}\otimes\mathbf{v}) \\
  =& \beta(\bar{\mathrm{K}}\circ\mathrm{F}_V)(\mathbf{v}) \\
  =& \beta(\mathrm{F}_W \circ\mathrm{K})(\mathbf{v}) \\
  =& \beta(\mathbf{1}\otimes\mathrm{K}(\mathbf{v})) \\
  =& \beta\otimes\mathrm{K}(\mathbf{v}) \\
  =& (I_{\mathbb{K}}\otimes\mathrm{K})(\beta\otimes\mathbf{v})
\end{align*}
$$
</details>
</MathBox>

## Multilinear maps and iterated tensor products

<MathBox title='Multilinear map' boxType='definition'>
If $V_1,\dots,V_n$ and $W$ are vector spaces over $\mathbb{F}$, a map $F:V_1 \times\cdots\times V_n \to W$ is multilinear if it is linear in each variable separately, i.e. for each $k=1,\dots,n$ and $\alpha,\beta\in\mathbb{F}$

$$
\begin{align*}
  &F(\mathbf{u}_1,\dots,\mathbf{u}_{k-1},\alpha\mathbf{v} + \beta\mathbf{v}',\mathbf{u}_{k+1},\dots,\mathbf{u}_n) \\
  =& \alpha F(\mathbf{u}_1,\dots,\mathbf{u}_{k-1},\mathbf{v},\mathbf{u}_{k+1},\dots,\mathbf{u}_n) \\
  &+ \beta F(\mathbf{u}_1,\dots,\mathbf{u}_{k-1},\mathbf{v}',\mathbf{u}_{k+1},\dots,\mathbf{u}_n)
\end{align*}
$$

A multilinear function of $n$ variable is called an $n$-linear function. The set of all $n$-linear functions is denoted $\hom\left(\prod_{i=1}^n V_i; W\right)$. A multilinear function from $V_1 \times\cdots\times V_n$ to the base field $\mathbb{F}$ is called a multilinear form or $n$-form.
</MathBox>

<MathBox title='Universal property for multilinear maps' boxType='proposition'>
Let $V_1,\dots,V_n$ be vector spaces over $\mathbb{F}$. The pair $(\bigotimes_{i=1}^n V_i, T)$ where

$$
  T: \prod_{i=1}^n V_n \to\bigotimes_{i=1}^n V_i
$$

is the multilinear map defined by

$$
  T(\mathbf{v}_1,\dots,\mathbf{v}_n) = \mathbf{v}_1 \otimes\cdots\otimes\mathbf{v}_n
$$

has the following property. If $W$ is an $\mathbb{F}$-space and $F: \prod_{i=1}^n V_i \to W$ is any multilinear function, then there is a unique linear transformation $\mathrm{L}:\bigotimes_{i=1}^n V_i \to W$ such that $\mathrm{L}\circ T = F$.
</MathBox>

The tensor product construction from bases can be extended to multilinear functions as follows. Let $V_1,\dots,V_n$ be vector spaces with corresponding bases $B_i = \Set{\mathbf{b}_{i,j}}_{j\in J_i}$. For each ordered $n$-tuple $(\mathbf{b}_{k,i_k})_{k=1}^n$, we invent a new formal symbol $\mathbf{b}_{1,i_1}\otimes\cdots\otimes\mathbf{b}_{n,i_n}$ and define a vector space $\bigotimes_{i=1}^n V_i$ with basis

$$
  D = \Set{ \mathbf{b}_{i, i_1} \otimes\cdots\otimes\mathbf{b}_{n,i_n} | \mathbf{b}_{k, i_k}\in B_k, k=1,\dots,n }
$$

Then we define a multilinear map $\otimes:\prod_{i=1}^n \to \bigotimes_{i=1}^n V_i$ by

$$
  \otimes(\mathbf{b}_{i, i_1},\dots,\mathbf{b}_{n,i_n}) = \mathbf{b}_{i, i_1} \otimes\cdots\otimes\mathbf{b}_{n,i_n} 
$$

and extend to multilinearity. This uniquely defines a multilinear map $\otimes$, that is universal for multilinear maps. Indeed, if $F:\prod_{i=1}^n V_i \to W$ is multilinear, the universal condition $F = \mathrm{L}\circ\otimes$ is equivalent To

$$
  \mathrm{L}(\mathbf{b}_{i, i_1} \otimes\cdots\otimes\mathbf{b}_{n,i_n}) = F(\mathbf{b}_{i, i_1},\dots,\mathbf{b}_{n,i_n})
$$

which uniquely defines a linear map $\mathrm{L}:\bigotimes_{i=1}^n V_i \to W$. Hence, the pair $(\bigotimes_{i=1}^n V_i, \otimes)$ has the universal property for multilinearity.

<MathBox title='Iterated tensor product' boxType='definition'>
Let $V_1,\dots,V_n$ be vector spaces over $\mathbb{F}$ and let $T$ be the subspace of the free space $F$ on $V_1 \times\cdots\times V_n$ generated by all vectors of the form

$$
\begin{align*}
  &\alpha(\mathbf{v}_1,\dots,\mathbf{v}_{k-1},\mathbf{u},\mathbf{v}_{k+1},\dots,\mathbf{v}) \\
  &+ \beta(\mathbf{v}_1,\dots,\mathbf{v}_{k-1},\mathbf{u}',\mathbf{v}_{k+1},\dots,\mathbf{v}) \\
  &- (\mathbf{v}_1,\dots,\mathbf{v}_{k-1},\alpha\mathbf{u} + \beta\mathbf{u}',\mathbf{v}_{k+1},\dots,\mathbf{v})
\end{align*}
$$

for all $\alpha,\beta\in\mathbb{F}$, $\mathbf{u},\mathbf{u}' \in U$ and $\mathbf{v}_1,\dots,\mathbf{v}_n \in V$. The quotient space $F/T$ is called the *tensor product* of $V_1,\dots,V_n$, denoted 
  
$$
  \bigotimes_{i=1}^n V_i = V_1 \otimes\cdots\otimes V_n 
$$

The coset $(\mathbf{v}_i)_{i=1}^n + T$ is denoted $\mathbf{v}_1 \otimes\cdots\otimes\mathbf{v}_n$ such that any element of $V_1 \otimes\cdots\otimes V_n$ is a sum of decomposable tensors, i.e.

$$
  \sum_{i} \mathbf{v}_{i_1} \otimes\cdots\otimes\mathbf{v}_{i_n}
$$
</MathBox>

<MathBox title='Properties of tensor products' boxType='proposition'>
The tensor product has the following properties (for vector spaces over $\mathbb{F}$):
1. **(Associativity):** There exists an isomorphism

$$
  \mathrm{T}:\left(\bigotimes_{i=1}^n V_i \right)\otimes\left(\bigotimes_{j=1}^m W_m \right) \to V_1\otimes\cdots\otimes V_n\otimes W_1\otimes\cdots\otimes W_m
$$

for which

$$
\begin{align*}
  &\mathrm{T}[(\mathbf{v}_1\otimes\cdots\otimes\mathbf{v}_n)\otimes(\mathbf{w}_1\otimes\cdots\otimes\mathbf{w}_n)] \\
  =& \mathbf{v}_1\otimes\cdots\otimes\mathbf{v}_n\otimes\mathbf{w}_1\otimes\cdots\otimes\mathbf{w}
\end{align*}
$$

In particular

$$
  (U\otimes V)\otimes W \cong U\otimes (V\otimes W) \cong U\otimes V\otimes W
$$
2. **(Commutativity):** Let $\pi$ be any permutation of the indices $\Set{1,\dots,n}$. Then there is an isomorphism

$$
  \mathrm{S}: \bigotimes_{i=1}^n V_i \to \bigotimes_{i=1}^n V_{\pi(i)}
$$

for which

$$
  \mathrm{S}(\mathbf{v}_1 \otimes\cdots\otimes\mathbf{v}_n) = \mathbf{v}_{\pi(1)}\otimes\cdots\otimes\mathbf{v}_{\pi(n)}
$$
3. There is an isomorphism $\mathrm{R}_1:\mathbb{F}\otimes V\to V$ for which $\mathrm{R}_1 (\alpha\otimes\mathbf{v} = \alpha\mathbf{v}$, and similarly there is an isomorphism $\mathrm{R}_2:V\otimes\mathbb{F} \to V$ for which $\mathrm{R}_2(\mathbf{v}\otimes\alpha) = \alpha\mathbf{v}$. Hence $\mathbb{F}\otimes V \cong V \cong V\otimes\mathbb{F}$.
</MathBox>

<MathBox title='Tensor product isomorphism' boxType='proposition'>
Let $V_1,\dots,V_n$ and $W$ be vector spaces over $\mathbb{F}$. Then there is an isomorphism $\phi: \hom\left(\prod_{i=1}^n V_i; W \right) \to \mathcal{L}\left(\bigotimes_{i=1}^n V_i, W \right)$ such that

$$
  \hom\left(\prod_{i=1}^n V_i; W \right) \cong \mathcal{L}\left(\bigotimes_{i=1}^n V_i, W \right)
$$

If all vector spaces are finite-dimensional, then

$$
  \dim\left[\hom\left(\prod_{i=1}^n V_i; W\right)\right] = \dim(W) \prod_{i=1}^n \dim(V_i)
$$
</MathBox>

<MathBox title='Linear transformation of tensor products' boxType='proposition'>
The linear transformation

$$
  \Theta: \bigotimes_{i=1}^n \mathcal{L}(U_i, U'_i) \to \mathcal{L}(U_1\otimes\cdots\otimes U_n, U'_1 \otimes\cdots\otimes U'_n)
$$

defined by

$$
  \Theta\left(\bigotimes_{i=1}^n \mathrm{T}_i \right)\left(\bigotimes_{i=1}^n \mathbf{u}_i \right) = \bigotimes_{i=1}^n \mathrm{T}_i (\mathbf{u}_i)
$$

is an embedding (injective linear transformation), and is an isomorphism if all vector spaces are finite-dimensional. Thus, the tensor product $\bigotimes_{i=1}^n \mathrm{T}_i$ of linear transformations is a linear transformation on tensor products.

In particular,

$$
\begin{gather*}
  \bigotimes_{i=1}^n U_i^* \xhookrightarrow{\cong} \left(\bigotimes_{i=1}^n U_i \right)^* \\
  \left(f_i \otimes\cdots\otimes f_n \right)\left(\mathbf{u}_1 \otimes\cdots\otimes\mathbf{u}_n \right) = f_1(\mathbf{u}_1)\cdots f_n(\mathbf{u}_n)
\end{gather*}
$$

and

$$
\begin{gather*}
  \left(\bigotimes_{i=1}^n U_i^* \right)\otimes V \xhookrightarrow{\cong} \mathcal{L}\left(\bigotimes_{i=1}^n U_i, V \right)^* \\
  \left(f_i \otimes\cdots\otimes f_n \otimes\mathbf{v}\right)\left(\mathbf{u}_1 \otimes\cdots\otimes\mathbf{u}_n \right) = f_1(\mathbf{u}_1)\cdots f_n(\mathbf{u}_n)\mathbf{v}
\end{gather*}
$$
</MathBox>

## Tensor spaces

Let $V$ be a finite-dimensional vector space over $\mathbb{F}$. For $p, q\in\mathbb{N}$, the tensor product

$$
  T_q^p (V) = \underbrace{V\otimes\cdots\otimes V}_{p\text{ factors}}\otimes\underbrace{V^* \otimes\cdots\otimes V^*}_{q\text{ factors}} = V^{\otimes p} \otimes (V^*)^{\otimes q}
$$

is called the space of tensors of rank $(p,q)$, where $p$ is the contravariant type and $q$ is the covariant type. In particaluar
- $(0,0)$-rank tensors are scalars, i.e. $T_0^0 (V) = \mathbb{F}$
- $(p, 0)$-rank tensors are *contravariant tensors*, i.e. $T^p (V) = T_0^p (V) = V^{\otimes p}$
- $(0,q)$-rank tensors are *covariant tensors*, i.e. $T_q (V) = T_q^0 (V) = (V^*)^{\otimes q}$
- tensors with both contravariant and covariant indices are called *mixed tensors*

Since $V$ is finite-dimensional, it follows that $V\cong V^{**}$ such that

$$
\begin{align*}
  T_q^p (V) =& V^{\otimes p}\otimes (V^*)^{\otimes q} \\
  \cong& ((V^*)^{\otimes p} \otimes V^{\otimes q})^* \\
  \cong& \hom((V^*)^{\times p}\times V^{\times q}, \mathbb{F})
\end{align*}
$$

This is the space of all multilinear functionals on $(V^*)^{\times p} \times V^{\times q}$ and gives an alternative way of defining tensors of rank $(p,q)$.

Note that $\dim[T_q^p(V)] = \dim(V)^{p+q}$. Since tensor products are associative and commutative it follows that

$$
  T_q^p (V) \otimes T_s^r (V) = T_{q+s}^{p+r} (V)
$$

Generally, tensors can be interpreted in a variety of ways as multilinear maps on a cartesian product, or a linear map on a tensor product. The form of the tensor depends on how many of the contra- and covariant factors are "active", and how many are "passive". More specifically, consider a $(p,q)$-rank tensor written

$$
  \mathbf{v}_1 \otimes\cdots\otimes\mathbf{v}_m\otimes\cdots\otimes\mathbf{v}_p \otimes f_1 \otimes\cdots\otimes f_n \otimes\cdots\otimes f_q \in T_q^p (V)
$$

where $m\leq p$ and $n\leq q$ denote the active vectors and linear functionals, respectively. The map from the cartesian product $(V^*)^{\times m} \times V^{\otimes n}$ $V^{\otimes p-m} \times (V^*)^{\otimes q-n}$ of the remaining factors is defined by

$$
\begin{align*}
  &(\mathbf{v}_1 \otimes\cdots\otimes \mathbf{v}_p \otimes f_1 \otimes\cdots\otimes f_q)(h_1,\dots,h_m,\mathbf{x}_1,\dots,\mathbf{x}_n) \\
  =& h_1(\mathbf{v}_1)\cdots h_m(\mathbf{v}_m) f_1 (\mathbf{x}_1) \cdots f_n (\mathbf{x}_n) \mathbf{v}_{m+1} \otimes\cdots \\
  &\otimes\mathbf{v}_p \otimes f_{n+1} \otimes\cdots\otimes f_q
\end{align*}
$$

The first group $\mathbf{v}_1\otimes\cdots\otimes\mathbf{v}_m$ of (active) vectors interact with the first set $h_1,\cdots,h_m$ of arguments to produce the scalar $h_1(\mathbf{v}_1)\cdots h_m(\mathbf{v}_m)$. The first group $f_1 \otimes\cdots\otimes f_n$ of (active) functionals interact with the second group $\mathbf{x}_1,\dots,\mathbf{x}_n$ of arguments to produce the scalar $f_1(\mathbf{x}_1)\cdots f_n(\mathbf{x}_n)$. The remaining (passive) vectors $\mathbf{v}_{m+1}\otimes\cdots\otimes\mathbf{v}_p$ and functionals $f_{n+1}\otimes\cdots\otimes f_q$ are just "copied" to the image vector.

It is straightforward to show that this map is multilinear. Thus, by the universal property there is a unique linear map from the tensor product $(V^*)^{\otimes m} \otimes V^{\otimes n}$ to the tensor product $V^{\otimes p-m}\otimes (V^*)^{\otimes q-n}$ defined by

$$
\begin{align*}
  &(\mathbf{v}_1 \otimes\cdots\otimes \mathbf{v}_p \otimes f_1 \otimes\cdots\otimes f_q)(h_1 \otimes\cdots\otimes h_m \otimes\mathbf{x}_1 \otimes\cdots\otimes \mathbf{x}_n) \\
  =& h_1(\mathbf{v}_1)\cdots h_m(\mathbf{v}_m) f_1(\mathbf{x}_1)\mathbf{v}_{m+1} \otimes\cdots\otimes\mathbf{v}_p \otimes f_{n+1}\otimes\cdots\otimes f_q
\end{align*}
$$

In particular
- for a contravariant tensor of rank $(p,0)$, i.e. $\mathbf{v}_1\otimes\cdots\otimes\mathbf{v}_p \in T_0^p (V)$ we get a linear map $(V^*)^{\otimes m} \to V^{\otimes p-m}$ defined by 

$$
  (\mathbf{v}_1 \otimes\cdots\otimes\mathbf{v}_p)(h_1 \otimes\cdots\otimes h_m) = h_1(\mathbf{v}_1)\cdots h_m(\mathbf{v}_m)\mathbf{v}_{m+1}\otimes\cdots\mathbf{v}_p
$$
- for a covariant tensor of type $(0,q)$, i.e. $f_1 \otimes\cdots\otimes f_q \in T_q^0 (V)$ we get a linear map $V^{\otimes n}\to (V^*)^{\otimes q-n}$ defined By

$$
  (f_1\otimes\cdots\otimes f_q)(\mathbf{x}_1 \otimes\cdots\otimes\mathbf{x}_n) = f_1(\mathbf{x}_1)\cdots f_n(\mathbf{x}_n) f_{n+1}\otimes\cdots\otimes f_q
$$

The special case $n=q$ gives a linear functional on $V^{\otimes q}$, i.e. each element of $(V^*)^{\otimes q}$ is a distinct member of $(V^{\otimes q})^*$, producing the embedding $(V^*)^{\otimes q} \xhookrightarrow{\cong} (V^{\otimes q})^*$.

### Contraction

Covarant and contravariant factor can be combined in the following way. Consider the map

$$
  h: V^{\times p} \times (V^*)^{\times q} \to T_{q-1}^{p-1}(V)
$$

defined By

$$
  H(\mathbf{v}_1,\dots,\mathbf{v}_p,f_1,\dots,f_q) = f_1(\mathbf{v}_1)(\mathbf{v}_2 \otimes\cdots\otimes\mathbf{v}_p \otimes f_1\otimes\cdots\otimes f_q)
$$

This is easily seen to be multilinear; by the universal property there is a unique linear map

$$
  \Theta: T_{q}^{p} (V) \to T_{q-1}^{p-1} (V)
$$

defined by

$$
  \Theta(\mathbf{v}_1 \otimes\cdots\otimes\mathbf{v}_p \otimes f_1 \otimes\cdots\otimes f_q) = f_1(\mathbf{v}_1)(\mathbf{v}_1\otimes\cdots\otimes\mathbf{v}_p \otimes f_1\otimes\cdots\otimes f_q)
$$

This is called the contraction in the contravariant index $1$ and covariant index $1$. Contractions in other indices can be defined similarly.

For a vector space $V$, the outer product of its basis $B = \Set{e_i}_{i \in I \subset \N}$ and its dual basis $B^* = \Set{ e^i }_{i \in I \subset \N}$ spans all endomorphic linear transforms. Every linear transform $L : V \to V$ can be writen as a linear combination of vector-covector pairs 

$$
  L = L_j^i \left(e_i \otimes e^j \right)
$$

The outer product itself forms a linear map

$$
\begin{align*}
  w &= L(v) = L_j^i \left(e_i \otimes e^j \right)(v^k e_k) \\
  &= L_j^i v^k e_i e^j (e_k) \\
  &= L_j^i v^k e_i \delta_k^j \\
  &= L_j^i v^j e_i
\end{align*}
$$

### Tensor algebra

<MathBox title='Graded algebra' boxType='definition'>
An algebra $A$ over a field $\mathbb{F}$ is a *graded algebra* if, as a vector space over $\mathbb{F}$, $A$ takes the form

$$
  A = \bigoplus_{i=0}^\infty A_i,\; A_i \subseteq A
$$

and where multiplication behaves nicely, that is

$$
  A_i A_j \subseteq A_{i+j}
$$

The elements of $A_i$ are called *homogeneous of degree* $i$. If $a\in A$ is written

$$
  a = a_{i_1} +\cdots+ a_{i_n} 
$$

for $a_{i_k}\in A_{i_k}$, with $i_k \neq i_j$, then $a_{i_k}$ is called the *homogeneous component* of $a$ of degree $i_k$.
</MathBox>

Let $V$ be a vector space over $\mathbb{F}$, and consider the contravariant tensor spaces

$$
  T^p (V) = T_0^p (V) = V^{\otimes p}
$$

For $p = 0$ we take $T^0 (V) = \mathbb{F}$. The external direct sum

$$
  T(V) = \bigoplus_{p=0}^\infty T^p (V)
$$

of theses tensor spaces if a vector space with the property that

$$
  T^p (V) \otimes T^q (V) = T^{p+q} (V)
$$

This is an example if a graded algebra, where $T^p (V)$ are the elements of grade $p$. The graded algebra $T(V)$ is called the tensor algebra over $V$. For covariant tensor spaces, note that

$$
  T_q (V) = (V^*)^{\otimes q} = T^q (V^*)
$$

Thus, the graded algebra

$$
  T(V^*) = \bigotimes_{q=0}^\infty T_q (V^*)
$$

forms the tensor algebra over $V^*$. 

## Symmetric and antisymmetric tensor spaces

<MathBox title='Symmetric, antisymmetric and alternating maps' boxType='definition'>
If $V$ and $W$ are vector spaces over $\mathbb{F}$, a map $F:V^n \to W$ is called
- *symmetric* if it is invariant under interchange of coordinate positions, i.e. 
$$
\begin{align*}
  &F(\mathbf{v}_1,\dots,\mathbf{v}_i,\dots,\mathbf{v}_j,\dots,\mathbf{v}_n) \\
  =& F(\mathbf{v}_1,\dots,\mathbf{v}_j,\dots,\mathbf{v}_i,\dots,\mathbf{v}_n)
\end{align*}
$$
for any $i\neq j$.
- *antisymmetric/skew-symmetric* if it changes sign under interchange of coordiate positions, i.e.
$$
\begin{align*}
  &F(\mathbf{v}_1,\dots,\mathbf{v}_i,\dots,\mathbf{v}_j,\dots,\mathbf{v}_n) \\
  =& -F(\mathbf{v}_1,\dots,\mathbf{v}_j,\dots,\mathbf{v}_i,\dots,\mathbf{v}_n)
\end{align*}
$$
- *alternate/alternating* if $\mathbf{v}_i = \mathbf{v}_j$ for some $i\neq j$ implies that $F(\mathbf{v}_1,\dots,\mathbf{v}_n) = 0$
</MathBox>

<MathBox title='Permutation' boxType='definition'>
A *permutation* of a set $N = \Set{1,\dots,n}$ for $n\in\N_+$ is a bijective function $\pi: N\to N$. The group (under composition) of all such permutations is denoted $S_n$ and is called the *symmetric group* on $n$ symbols. A *cycle* of length $k\in\N_+$ is a permutation of the form $(i_1,\dots,i_k)$, which sends $i_u$ to $i_{u+1}$ for $u = 1,\dots,k-1$ and also sends $i_k$ to $i_1$. All other elements of $N$ are left fixed. Every permutation is the product (composition) of disjoint cycles.

A *transposition* is a cycle $(i,j)$ of length $2$. Every cycle (and therefore every permutation) is the product of transpositions. In general, a permutation can be expressed as a product of transpositions in many ways. Howevery, the number of tranpositions is either alway even or always odd. Thus, we can define the *parity* of a permutation $\pi\in S_n$ to be the parity of the number of transpositions in any decomposition of $\pi$ as a product of transpositions. The *sign* of a permutation is defined bY

$$
  \mathrm{sgn}(\pi) = \begin{cases}
    1,& \quad \pi\text{ has even parity} \\
    -1,& \quad \pi\text{ has odd parity}
  \end{cases}
$$

If $\mathrm{sgn}(\pi) = 1$, then $\pi$ is an *even permutation* and if $\mathrm{sgn}(\pi) = -1$, then $\pi$ is an *odd permutation*. The sign of $\pi$ is often written $(-1)^\pi$.
</MathBox>

In terms of permutations, a map $F:V\to W$ between $\mathbb{F}$-vector spaces $V$ and $W$ is symmetric if and only if for all permutations $\pi\in S_n$

$$
  F(\mathbf{v}_1,\dots,\mathbf{v}_n) = F(\mathbf{v}_{\pi(1)},\dots,\mathbf{v}_{\pi(n)})
$$

Similarly, $F$ is antisymmetric if and only if

$$
  F(\mathbf{v}_1,\dots,\mathbf{v}_n) = (-1)^{\pi} F(\mathbf{v}_{\pi(1)},\dots,\mathbf{v}_{\pi(n)})
$$

<MathBox title='Wedge product' boxType='definition'>
Let $E = (e_i)_{i=1}^n$ be a sequence of independent variables. For $p\leq n$, let $\mathbb{F}_p^- [e_i]_{i=1}^n$ be the vector space over $\mathbb{F}$ with basis

$$
  A_p (E) = \Set{e_{i_1}\cdots e_{i_p} | i_1 <\cdots < i_p}
$$

consisting of all words of length $p$ over $E$ that are in ascending order. Let $F_0^- [e_i]_{i=1}^n = F\epsilon$, which we identify with $\mathbb{F}$ by identifying $\epsilon$ with $1\in\mathbb{F}$. Define a product on the direct sum

$$
  \mathbb{F}^- [e_i]_{i=1}^n = \bigoplus_{p=0}^n F_p^-
$$

as follows. First, the product $f\wedge g$ of monomials $f = x_1\cdots x_p \in F_p^-$ and $g = y_1\cdots y_q \in \mathbb{F}^-$ is defined as follows:
1. If $x_1\dots x_p y_1 \cdots y_q$ has a repeated factor, then $f\wedge g = 1$.
2. Otherwise, reorder $x_1\cdots x_p y_1 \cdots y_q$ in ascending order, say $z_1 \cdots z_{p+q}$ via the permutation $\sigma$ and set
$$
  f\wedge g = (-1)^\sigma z_1\cdots z_{p+q}
$$

Extende the product by distributivity to $\mathbb{F}^- [e_i]_{i=1}^n$. The resulting product makes $\mathbb{F}^- [e_i]_{i=1}^n$ into a (noncommutative) algebra over $\mathbb{F}$. This product is called the *wedge product* or *exterior product* on $\mathbb{F}^- [e_i]_{i=1}^n$.
</MathBox>

Let $B = \Set{e_i}_{i=1}^n$ be a basis for an $n$-dimensional vector space $V$. It is convenient to group the decomposable basis tensors $\mathbf{e}_{i_1} \otimes\cdots\otimes \mathbf{e}_{i_p}$ according to their index multiset. Specifically, for each multiset $M = \Set{i_1,\dots,i_p}$ with $1\leq i_k \leq n$, let $G_M$ be the set of all tensors $\mathbf{e}_{k_1} \otimes\cdots\otimes \mathbf{e}_{k_p}$ where $(k_1,\dots,k_p)$ is a permutation of $\Set{i_1,\dots,i_p}$.

If $\mathbf{v}\in T^p (V)$ has the form

$$
  \mathbf{v} = \sum_{i_1,\dots,i_p} \alpha_{i_1,\dots,i_p} \mathbf{e}_{i_1} \otimes\cdots\otimes \mathbf{e}_{i_p}
$$

where $\alpha_{i_1,\dots,i_p} \neq 0$, then let $G_M (\mathbf{v})$ be the subset of $G_M$ whose elements appear in the sum for $\mathbf{v}$. Let $S_M (\mathbf{v})$ denote the sum of terms of $\mathbf{v}$ associated with $G_M (\mathbf{v})$. Accordingly, $\mathbf{v}$ can be written in the form

$$
  \mathbf{v} = \sum_M S_M (\mathbf{v}) = \sum_M \left( \sum_{t\in G_M (\mathbf{v})} \alpha_t t \right)
$$

where the sum is over a collection of multisets $M$ with $S_M (\mathbf{v}) \neq 0$. Note also that $\alpha_t \neq 0$ since $t\in G_M (\mathbf{v})$. Finally, let $\mathbf{u}_M = \mathbf{e}_{i_1} \otimes\cdots\otimes \mathbf{e}_{i_p}$ be the unique member of $G_M$ for which $i_1 \leq\cdots\leq i_p$.

### Symmetric and antisymmetric tensors

Let $S_p$ be the symmetric group on $\Set{i,\dots,p}$ for $p\in\N_+$. For each $\sigma\in S_p$, the multilinear map $F_\sigma : V^p \to T^p (V)$ defined by

$$
  F_\sigma (x_1,\dots,x_p) = x_{\sigma_1} \otimes\codts\otimes x_{\sigma_p}
$$

determines a unique linear operator $\lambda_\sigma$ on $T^p (V)$ for which

$$
  \lambda_\sigma (x_1 \otimes\cdots\otimes x_p) = x_{\sigma_1} \otimes\codts\otimes x_{\sigma_p}
$$

Let $\Set{\mathbf{e}_i}_{i=1}^n$ be a basis for $V$. Since $\lambda_\sigma$ is a bijection of the basis

$$
  B = \Set{\mathbf{e}_{i_1} \otimes\cdots\otimes \mathbf{e}_{i_p} | \mathbf{e}_{i_j} \in B}
$$

it follows that $\lambda_\sigma$ is an isomorphism of $T^p (V)$. Note also that $\lambda_\sigma$ is a permutation of each $G_M$, meaning that the set $G_M$ are invariant under $\lambda_\sigma$.

<MathBox title='Symmetric and antisymmetric tensors' boxType='definition'>
Let $V$ be a finite-dimensional vector space.
1. A tensor $t\in T^p (V)$ is *symmetric* if $\lambda_\sigma t = t$ for all permutations $\sigma\in S_p$. The set of all symmetric tensors
$$
  ST^p (V) = \Set{t\in T^p (V) | \lambda_\sigma t = t,\,\forall \sigma\in S_p}
$$
is a subspace of $T^p (V)$, called the *symmetric tensor space* of degree $p$ over $V$. 
2. A tensor $t\in T^p (V)$ is *antisymmetric* if $\lambda_\sigma t = (-1)^\sigma t$. The set of all antisymmetric tensors
$$
  AT^p (V) = \Set{t\in T^p (V) | \lambda_\sigma t = (-1)^\sigma t,\,\forall \sigma\in S_p}
$$
is a subspace of $T^p (V)$, called the *antisymmetric tensor space* or *exterior product space* of degree $p$ over $V$.
</MathBox>

Let us write (anti)symmetric to denote a tensor that is either symmetric or antisymmetric. Since for any $s,t\in G_M$, there is a permutation $\lambda_\sigma$ taking $s$ to $t$, an (anti)symmetric tensor $\mathbf{v}$ must have $G_M (\mathbf{v}) = G_M$ and so

$$
  \mathbf{v} = \sum_M S_M (\mathbf{v}) = \sum_M \left( \sum_{t\in G_M} \alpha_t t \right)
$$

Since $\lambda_\sigma$ is a permutation of $G_M$, it follows that $\mathbf{v}$ is symmetric if and only if

$$
  \lambda_\sigma (S_M (\mathbf{v})) = S_M (\mathbf{v})
$$

for all $\sigma\in S_p$ and this holds if and only if the coefficients $\alpha_t$ of $S_M (\mathbf{v})$ are equal, say $\alpha_t = \alpha_M$ for all $t\in G_M$. Hence, the symmetric tensors are precisely the tensors of the form

$$
  \mathbf{v} = \sum_M \left( \alpha_M \sum_{t\in G_M} t \right)
$$

The tensor $\mathbf{v}$ is antisymmetric if and only if

$$
  \lambda_\sigma (S_M (\mathbf{v})) = (-1)^\sigma S_M (\mathbf{v})
$$

In this case, the coefficients $\alpha_t$ of $S_M (\mathbf{v})$ differ only by sign. Before examining this more closely, note that $M$ must be a set. If $M$ has an element $k$ of multiplicity greater than $1$, we can split $G_M$ into two disjoint Parts

$$
  G_M = G'_M \cup G''_M
$$

where $G'_M$ are the tensors that have $\mathbf{e}_k$ in positions $r$ and $s$

$$
  G'_M = \Set{\mathbf{e}_{i_1} \otimes\cdots\otimes \underbrace{\mathbf{}}_{\text{position }r} \otimes\cdots\otimes \underbrace{\mathbf{}}_{\text{position }s} \otimes\cdots\otimes \mathbf{e}_{i_p} }
$$

Then $\lambda_{(r,s)}$ fixes each element of $G'_M$ and sends the elements of $G''_M$ to other elements of $G''_M$. Hence applying $\lambda_{(r,s)}$ to the corresponding decomposition of $S_M (\mathbf{v})$

$$
  S_M (\mathbf{v}) = S'_M + S''_M
$$

gives

$$
  -(S'_M + S''_M) = -S_M = \lambda_{(1,2)} S_M = S'_M + \lambda_{(1,2)} S''_M
$$

and so $S'_M = 0$, hence $S_M (\mathbf{v}) = 0$. Thus. $M$ is a set.

Since for any $\sigma\in S_p$, we have $G_M = \Set{\lambda_\sigma t | t\in G_M}$, the antisymmetric tensor condition implies that

$$
\begin{align*}
  (-1)^\sigma \sum_{t\in G_M} \alpha t =& \lambda_\sigma \left( \sum_{t\in G_M} \alpha_t t \right) \\
  =& \sum_{t\in G_M} \alpha_t \lambda_\sigma t \\
  =& \sum_{t\in G_M} \alpha_{\lambda_\sigma^{-1}t} t
\end{align*}
$$

which holds if and only if $\alpha_{\lambda_\sigma^{-1}t} = (-1)^\sigma \alpha$, or equivalently

$$
  \alpha_{\lambda_\sigma t} = (-1)^\sigma \alpha_t
$$

for all $t\in G_M$ and $\sigma\in S_p$. Choosing $\mathbf{u} = \mathbf{u}_M = \mathbf{e}_{i_1} \otimes\cdots\otimes \mathbf{e}_{i_p}$, where $i_1 < \cdots< i_p$, as standard-bearer, if $\sigma_{u,t}$ denotes the permutation for which $\lambda_{\sigma_{u,t}} (u) = t$, then

$$
  \alpha_t = (-1)^{\sigma_{u,t}} \alpha_u
$$

Thus, $\mathbf{v}$ is antisymmetric if and only if it has the form

$$
  \mathbf{v} = \sum_M \left( \alpha_M \sum_{t\in G_M} (-1)^{\sigma_{u,t}} t \right)
$$

where $\alpha_M = \alpha_u \neq 0$ and the sum is over a family of sets. We can simplify the expression for (anti)symmetric tensors by representing the inner sums more succintly. In the symmetric case, define a surjective linear map $\tau: T^p (V) \to \mathbb{F}_p [\mathbf{e}_i]_{i=1}^n$ by

$$
  \tau(\mathbf{e}_{i_1} \otimes\cdots\otimes\mathbf{e}_{i_p}) = \mathbf{e}_{i_1} \wedge\cdots\wedge\mathbf{e}_{i_p}
$$

and extending by linearity. Since $\tau$ takes every member of $G_M$ to the same monomial $\tau \mathbf{u} = \mathbf{e}_{i_1} \wedge\cdots\wedge \mathbf{e}_{i_p}$, where $i_1 \leq\cdots\leq i_p$, we have

$$
\begin{align*}
  \tau\mathbf{v} = \tau\left( \sum_M \left( \alpha_M \sum_{t\in G_M} t \right) \right) \\
  =& \sum_M \alpha_M |G_M| \tau \mathbf{u}_M
\end{align*}
$$

In the antisymmetric case, define a surjective linear map $\tau: T^p (V) \to \mathbb{F}_p^- [\mathbf{e}_i]_{i=1}^n$ by

$$
  \tau(\mathbf{e}_{i_1} \otimes\cdots\otimes\mathbf{e}_{i_p}) = \mathbf{e}_{i_1} \vee\cdots\vee\mathbf{e}_{i_p}
$$

and extending by linearity. Since $\tau t = (-1)^{\sigma_{u, t}} \tau \mathbf{u}_M$ we have

$$
\begin{align*}
  \tau\mathbf{v} =& \sum_M \left( \alpha_M \sum_{t\in G_M} (-1)^{\sigma_{u,t}} \tau t \right) \\
  =& \sum_M \left( \alpha_M \sum_{t\in G_M} \tau \mathbf{u}_M \right) \\
  =& \sum_M \alpha_M |G_M| \tau \mathbf{u}_M
\end{align*}
$$

Thus, an for an (anti)symmetric tensor

$$
  \tau\mathbf{v} = \sum_M \alpha_M |G_M| \tau \mathbf{u}_M
$$

where $\mathbf{u}_M = \mathbf{e}_{i_1} \otimes\cdots\otimes \mathbf{e}_{i_p}$ with $i_1 \leq\cdots\leq i_p$ and
- $\tau \mathbf{u}_M = \mathbf{e}_{i_1} \wedge\cdots\wedge \mathbf{e}_{i_p}$ if $\mathbf{v}$ is symmetric
- $\tau \mathbf{u}_M = \mathbf{e}_{i_1} \vee\cdots\vee \mathbf{e}_{i_p}$ if $\mathbf{v}$ is antisymmetric

In either case, the monomials $\tau u_M$ are linearly independent for distinct (multi)sets $M$. Therefore, if $\tau\mathbf{v} = 0$ then $\alpha_M |G_M| = 0$ for all (multi)set $M$. Hence, if $\mathrm{char}(\mathbf{F}) = 0$, then $\alpha_M = 0$ and so $\mathbf{v} = 0$. This shows that the restricted maps $\tau|_{ST^p (V)}$ and $\tau|_{AT^p (V)}$ are isomorphisms. 

<MathBox title='' boxType='theorem'>
Let $V$ be an n-dimensional vector space over a field $\mathbb{F}$ with $\mathrm{char}(\mathbb{F}) = 0$.
1. The symmetric tensor space $ST^p (V)$ is isomorphic to the algebra $\mathbb{F}_p [\mathbf{e}_i]_{i=1}^n$ of homogenous polynomials, via the isomorphism
$$
  \tau \left(\sum_i \alpha_{i_1,\dots,i_p} \mathbf{e}_{i_1}\otimes\cdots\otimes\mathbf{e}_{i_p} \right) = \sum_i \alpha_{i_1,\dots,i_p} (\mathbf{e}_{i_1} \vee\cdots\vee \mathbf{e}_{i_p})
$$
2. For $p\leq n$, the antisymmetric tensor space $AT^p (V)$ is isomorphic to the algebra $\mathbb{F}_p^- [\mathbf{e}_i]_{i=1}^n$ of anticommutative homogeneous polynomials of degree $p$, via the isomorphism
$$
  \tau \left(\sum_i \alpha_{i_1,\dots,i_p} \mathbf{e}_{i_1}\otimes\cdots\otimes\mathbf{e}_{i_p} \right) = \sum_i \alpha_{i_1,\dots,i_p} (\mathbf{e}_{i_1} \wedge\cdots\wedge \mathbf{e}_{i_p})
$$
</MathBox>

The direct sum

$$
\begin{align*}
  ST(V) =& \bigoplus_{p=0}^\infty ST^p (V) \\
  \cong& \mathbb{F} [\mathbf{e}_i]_{i=1}^n
\end{align*}
$$

is called the *symmetric tensor algebra* of $V$ and the direct sum

$$
\begin{align*}
  AT(V) =& \bigoplus_{p=0}^\infty AT^p (V) \\
  \cong& \mathbb{F}^- [\mathbf{e}_i]_{i=1}^n
\end{align*}
$$

is called the *antisymmetric tensor algebra* or the *exterior algebra* of $V$. These vector spaces are graded algebras, where the product is defined using the vector space isomorphism described in the previous theorem to move the products of $\mathbb{F}[\mathbf{e}_i]_{i=1}^n$ and $\mathbb{F}^- [\mathbf{e}_i]_{i=1}^n$ to $ST(V)$ and $AT(V)$, respectively.

Thus, restricting the domains of the maps $\tau$ gives a nice description of the symmetric and antisymmetric tensor algebras, when $\mathrm{char}(\mathbb{F}) = 0$. However, there are many important fields, such as finite fields, that have nonzero characteristic. We can proceed in a different manner that holds regardless of the characteristic of the base field. Rather than restricting the domain of $\tau$ in order to get an isomorphism, we can factor out by the kernel of $\tau$. 

<MathBox title='' boxType='theorem'>
Let $V$ be a finite-dimensional vector space over a field $\mathbb{F}$.
1. The surjective linear map $\tau: T^p (V) \to\mathbb{F}_p [\mathbf{e}_i]_{i=1}^n$ defined by
$$
  \tau\left(\sum_i \alpha_{i_1,\dots,i_p} \mathbf{e}_{i_i} \otimes\cdots\otimes \mathbf{e}_{i_p} \right) = \sum_i \alpha_{i_1,\dots,i_p} \mathbf{e}_{i_i} \vee\cdots\vee \mathbf{e}_{i_p}
$$
has kernel
$$
  \ker(\tau) = I_p = \langle \lambda_\sigma (t) - t | t\in B, \sigma\in S_p \rangle
$$
and so
$$
  T^p (V) / I_p \cong \mathbb{F}_p [\mathbf{e}_i]_{i=1}^n
$$
2. The surjective linear map $\tau:T^p (V) \to\mathbb{F}_p^- [\mathbf{e}_i]_{i=1}^n$ defined by
$$
  \tau\left(\sum_i \alpha_{i_1,\dots,i_p} \mathbf{e}_{i_i} \otimes\cdots\otimes \mathbf{e}_{i_p} \right) = \sum_i \alpha_{i_1,\dots,i_p} \mathbf{e}_{i_i} \wedge\cdots\wedge \mathbf{e}_{i_p}
$$
has kernel
$$
  \ker(\tau) = I_p = \langle (-1)^\sigma\lambda_\sigma (t) - t | t\in B, \sigma\in S_p \rangle
$$
and so
$$
  T^p (V) / I_p \cong \mathbb{F}_p^- [\mathbf{e}_i]_{i=1}^n
$$

The vector space $T^p (V) / I$ is also referred to as the *antisymmetric tensor space* or *exterior product space* of degree $p$ of $V$.

<details>
<summary>Proof</summary>

Consider a tensor

$$
  \mathbf{v} = \sum_M S_M (V) = \sum_M \left(\sum_{t\in G_M(\mathbf{v})} \alpha_t t \right)
$$

Since $\tau$ sends elements of different groups $G_M (\mathbf{v}) = \Set{t_i}_{i=1}^k$ to different monomials in $\mathbb{F}_p [\mathbf{e}_i]_{i=1}^n$ or $\mathbb{F}_p^- [\mathbf{e}_i]_{i=1}^n$, it follows that $\mathbf{v}\in\ker(\tau)$ if and only if $\tau(S_M (\mathbf{v})) = 0$ for all $M$, that is, if and only if $\alpha_{t_1} \tau t +\cdots+ \alpha_{t_k} \tau t_k = 0$.

In the symmetric case, $\tau$ is constant on $G_M (\mathbf{v})$ and so $\mathbf{v}\in\ker(\tau)$ if and only if $\alpha_{t_1} +\cdots+ = 0$. In the antisymmetric case, $t\tau_k = (-1)^{\sigma_{1,j}} \tau t_1$ where $\lambda_{\sigma_{i,j}} = t_j$ and so $\mathbf{v}\in\ker(\tau)$ if and only if

$$
  (-1)^{\sigma_{1,1}} \alpha_{t_1} +\cdots+ (-1)^{\sigma_{1,k}} \alpha_{t_k} = 0
$$

In both cases, we solve for $\alpha_{t_1}$ and substitute into $S_M (\mathbf{v})$. In the symmetric case, $\alpha_{t_1} = -\alpha_{t_2} -\cdots -\alpha_{t_k}$ and so

$$
\begin{align*}
  S_M (\mathbf{v}) =& \alpha_{t_1} t_1 +\cdots+ \alpha_{t_k} t_k \\
  =& \alpha_{t_2}(t_2 - t_1) +\cdots+ \alpha_{t_k} (t_k - t_1)
\end{align*}
$$

In the antisymmetric case, $\alpha_{t_1} = -(-1)^{\sigma_{1,2}} \alpha_{t_2} -\cdots- (-1)^{\sigma_{1,k}}\alpha_{t_k}$ and so

$$
\begin{align*}
  S_M (\mathbf{v}) =& \alpha_{t_1} t_1 +\cdots+ \alpha_{t_k} t_k \\
  =& \alpha_{t_2}((-1)^{\sigma_{1,2}} t_2 - t_1) +\cdots+ \alpha_{t_k} ((-1)^{\sigma_{1,k}} t_k - t_1)
\end{align*}
$$

Since $t_i \in B$, it follows that $S_M (\mathbf{v}) = 0$ and therefore $\mathbf{v}$ is in the span of tensors of the form $\lambda_\sigma (t) - t$ in the symmetric case and $(-1)^\sigma \lambda_\sigma (t) - t$ in the antisymmetric case, where $\sigma\in S_p$ and $t\in B$. Hence, in the symmetric case

$$
  \ker(\tau) \subseteq I_p := \langle \lambda_\sigma (t) - t | t\in B, \sigma\in S_p \rangle
$$

and since $\tau(\lambda_\sigma(t) - t) = 0$, it follows that $\ker(\tau) = I_p$. In the antisymmetric case

$$
  \ker(\tau) \subseteq I_p := \langle (-1)^\sigma \lambda_\sigma (t) - t | t\in B, \sigma\in S_p \rangle
$$

and since $\tau((-1)^\sigma \lambda_\sigma (t) - 1) = 0$, it follows that $\ker(\tau) = I_p$.
</details>
</MathBox>

<MathBox title='' boxType='theorem'>
Let $V$ be a vector space of dimension $n$.
1. The dimension of the symmetric tensor space $ST^p (V)$ is equal to the number of monomials of degree $p$ in the variables $\mathbf{e}_1,\dots,\mathbf{e}_n$ and this is
$$
  \dim(ST^p (V_n)) = \binom{n+p-1}{p}
$$
2. The dimension of the exterior tensor space $AT^p (V)$ is equal to the number of words of length $p$ in ascending order over the alphabet $E = \Set{e_i}_{i=1}^n$ and this is

$$
  \dim(AT^p (V_n)) = \binom{n}{p}
$$

<details>
<summary>Proof</summary>

**(1):** The dimension is equal to the number of multisets of size $p$ taken from an underlying set $\Set{\mathbf{e_i}}_{i=1}^n$ of size $n$. Such multisets correspond bijectively to the solution, in nonnegative integers. of the equation

$$
  x_1 +\cdots+ x_n = p
$$

where $x_i$ is the multiplicity of $e_i$ in the multiset. To count the number of solutions, invent two symbols $x$ and $/$ to represent any solution $x_i = s_i$ as a sequence of the form

$$
  \underbrace{x \dots x}_{s_1 \text{ times}} /\dots/ \underbrace{x \dots x}_{s_n \text{ times}}
$$

Thus, the solutions correspond bijectively to sequences consisting of $p$ times $x$ and $n-1$ times $/$. To count the number of such sequences, note that such a sequence can be formed by considering $n+p - 1$ "blanks" and selecting $p$ of these blanks for $x$. This can be done in $\binom{n+p-1}{p}$ ways.
</details>
</MathBox>

# Tensor field

<MathBox title='Tensor field' boxType='definition'>
Let $V$ be an $n$-dimensional vector space. A tensor field of rank $(p,q)$ is a map $T:V\to T_q^p (V)$. The space of all tensor fields of rank $(p,q)$ is denoted $\mathcal{T}_q^p (V)$.
</MathBox>

<MathBox title='Tensor' boxType='definition'>
Let $V$ be finite-dimensional vector space and $V^*$ the corresponding dual vectors space. A tensor of rank $(m, n)$, with covariant rank $m$ and contravariant rank $n$, is a multilinear function $T: V^{*m} \times V^n \to\mathbb{F}$.

$$
  \bar{T}_{j_1\dots j_n}^{i_1\dots i_m} = T_{p_1\dots p_n}^{q_1\dots q_m} \frac{\partial\bar{x}^{i_1}}{\partial x^{q_1}}\cdots\frac{\partial\bar{x}^{i_m}}{\partial x^{q_m}}\frac{\partial x^{p_1}}{\partial \bar{x}^{j_1}}\dots\frac{\partial x^{q_n}}{\partial \bar{x}^{j_n}}
$$
</MathBox>

Given a vector space $V$, a tensor of type $(p, q)$, with contravariant order $p$ and covariant order $q$, is defined as an element of the tensor product of $p$ times $V$ and $q$ times its dual space $V^*$

$$
  T \in \left(\bigotimes_{i=1}^m V\right) \otimes \left(\bigotimes_{j=1}^n V^* \right) 
$$

The components of $T$ are coefficients determined by its basis, which is formed by the tensor product of the basis $\Set{\mathbf{e}_i }$ for $V$ and its dual basis $\Set{ \mathbf{\varepsilon}^j }$ for $V^*$

$$
  T = T_{j_1...j_q}^{i_1...i_p} \mathbf{e}_{i_1} \otimes ... \otimes \mathbf{e}_{i_p} \otimes \mathbf{\varepsilon}^{j_1} \otimes ... \otimes \mathbf{\varepsilon}^{j_q}
$$

When transforming a tensor under a change of basis, its contravariant components are backward transformed ($T_b = \tilde{A}$), while its covariant components are forward transformed ($T_f = A$)

$$
\begin{align}
  T &= T_{j_1...j_q}^{i_1...i_p} \mathbf{e}_{i_1} \otimes ... \otimes \mathbf{e}_{i_p} \otimes \mathbf{\varepsilon}^{j_1} \otimes ... \otimes \mathbf{\varepsilon}^{j_q} \\
  &= T_{j_1...j_q}^{i_1...i_p} \left( \tilde{A}_{i_1}^{i'_1} \tilde{\mathbf{e}}_{i'_1} \right) \otimes ... \otimes \left( \tilde{A}_{i_p}^{i'_p} \tilde{\mathbf{e}}_{i'_p} \right) \otimes \left( A_{j'_1}^{j_1} \tilde{\mathbf{\epsilon}}^{j'_1} \right) \otimes ... \otimes \left( A_{j'_q}^{j_q} \tilde{\mathbf{\epsilon}}^{j'_q} \right) \\
  &= \left(\tilde{A}_{i_1}^{i'_1}...\tilde{A}_{i_p}^{i'_p}T_{j_1...j_q}^{i_1...i_p} A_{j'_1}^{j_1}...A_{j'_q}^{j_q} \right) \tilde{\mathbf{e}}_{i'_1} \otimes ... \otimes \tilde{\mathbf{e}}_{i'_p} \otimes \tilde{\mathbf{\varepsilon}}^{j'_1} \otimes ... \otimes \tilde{\mathbf{\varepsilon}}^{j'_q} \\
  &= \tilde{T}_{j_1...j_q}^{i_1...i_p} \tilde{\mathbf{e}}_{i_1} \otimes ... \otimes \tilde{\mathbf{e}}_{i_p} \otimes \tilde{\mathbf{\varepsilon}}^{j_1} \otimes ... \otimes \tilde{\mathbf{\varepsilon}}^{j_q}
\end{align}
$$

giving the transformation rules

$$
\begin{align*}
  \tilde{T}_{j_1...j_q}^{i_1...i_p} &= \tilde{A}_{i'_1}^{i_1}...\tilde{A}_{i'_p}^{i_p} T_{j'_1...j'_q}^{i'_1...i'_p} A_{j_1}^{j'_1}...A_{j_q}^{j'_q} \\
  T_{j_1...j_q}^{i_1...i_p} &= A_{i'_1}^{i_1}...A_{i'_p}^{i_p}  \tilde{T}_{j'_1...j'_q}^{i'_1...i'_p} \tilde{A}_{j_1}^{j'_1}...\tilde{A}_{j_q}^{j'_q}
\end{align*}
$$

## Tensor operations

### Kronecker product

If $\mathbf{A}$ is an $m\times n$ matrix and $\mathbf{B}$ is a $p\times q$ matrix, then the Kronecker product $\mathbf{A}\otimes\mathbf{B}$ is the $pm\times qn$ block matrix 

$$
\begin{align*}
  \mathbf{A}\otimes\mathbf{B} =& \begin{bmatrix} 
    a_{11}\mathbf{B} & \cdots & a_{1n} \mathbf{B} \\
    \vdots & \ddots & \vdots \\
    a_{1m}\mathbf{B} & \cdots & a_{mn}\mathbf{B}
  \end{bmatrix} \\
  =& \left[\begin{array}{c|c|c} 
    \begin{matrix}
      a_{11}b_{11} & \cdots & a_{11}b_{1q} \\
      \vdots & \ddots & \vdots \\
      a_{11}b_{p1} & \cdots & a_{11}b_{pq}
    \end{matrix} & \cdots & \begin{matrix}
      a_{1n}b_{11} & \cdots & a_{1n}b_{1q} \\
      \vdots & \ddots & \vdots \\
      a_{1n}b_{p1} & \cdots & a_{1n}b_{pq}
    \end{matrix} \\ \hline
    \vdots & \ddots & \vdots \\ \hline
    \begin{matrix}
      a_{m1}b_{11} & \cdots & a_{n1}b_{1q} \\
      \vdots & \ddots & \vdots \\
      a_{m1}b_{p1} & \cdots & a_{m1}b_{pq}
    \end{matrix} & \cdots & \begin{matrix}
      a_{mn}b_{11} & \cdots & a_{mn}b_{1q} \\
      \vdots & \ddots & \vdots \\
      a_{mn}b_{p1} & \cdots & a_{mn}b_{pq}
    \end{matrix}
  \end{array} \right]
\end{align*}
$$

## Metric tensor

The metric tensor is a bilinear form $g : V \times V \to \R$ given by

$$
  g(\mathbf{v}, \mathbf{w}) \to g_{ij} v^i w^j 
$$

with the properties
- Symmetric: $g(\mathbf{u}, \mathbf{v}) = g(\mathbf{v}, \mathbf{u})$
- Positive definite: $g(\mathbf{v}, \mathbf{v}) = \| v \|^2 \geq 0 $
- Linear in both arguments separately
  - $\lambda g(\mathbf{u}, \mathbf{v}) = g(\lambda \mathbf{u}, \mathbf{v}) = g(\mathbf{u}, \lambda \mathbf{v})$
  - $g(\mathbf{t} + \mathbf{u}, \mathbf{v} + \mathbf{w}) = g(\mathbf{t}, \mathbf{v} + \mathbf{w}) + g(\mathbf{u}, \mathbf{v} + \mathbf{w}) = g(\mathbf{t}, \mathbf{v}) + g(\mathbf{t}, \mathbf{w}) + g(\mathbf{u}, \mathbf{v}) + g(\mathbf{u}, \mathbf{w})$

The metric tensor gets transformed as follows

$$
  \tilde{g}_{ij} = \langle \tilde{\mathbf{e}}_i, \tilde{\mathbf{e}}_j \rangle = \langle A_i^k \mathbf{e}_k, A_j^l \mathbf{e}_l \rangle = A_i^k A_j^l \langle \mathbf{e}_k, \mathbf{e}_l \rangle = A_i^k A_j^l g_{kl}
$$

The dual pair $\nu$ of a vector $\mathbf{v} \in V$ is given by 

$$
\begin{align*}
  \flat \mathbf{v} = \mathbf{v} \cdot \_ &= g(\mathbf{v}, \_) = g_{ik} \mathbf{\varepsilon}^i \mathbf{\varepsilon}^k \left( v^j \mathbf{e}_j \right) \\
  &= g_{ik} v^j \mathbf{\varepsilon}^i \mathbf{\varepsilon}^k \left( \mathbf{e}_j \right) \\
  &= g_{ik} v^j \mathbf{\varepsilon}^i \delta_j^k \\
  &= g_{ij}v^j \mathbf{\varepsilon}^i \\
  &= \nu_i \mathbf{\varepsilon}^i
\end{align*}
$$

The metric tensor can thus act as a map $g : V \to V*$ with the effect of lowering indices 

$$
  \nu_i = g_{ij} v^j
$$

An inverse metric tensor $\mathfrak{g} \in V \otimes V$ can be defined such that

$$
  \mathfrak{g}^{ki}g_{ij} = \delta_j^k
$$

The inverse metric tensor (contravariant) can act as a map $\mathfrak{g}: V^* \to V$ connecting a covector $\nu \in V^*$ to its vector pair

$$
  \#\nu = \mathfrak{g}(\nu, \_)
$$

with the effect of raising indices

$$
\begin{align*}
  \mathfrak{g}^{ki} \nu_i &= \mathfrak{g}^{ki} g_{ij} v^j \\
  \mathfrak{g}^{ki} \nu_i &= \delta_j^k v^j \\
  \mathfrak{g}^{ki} \nu_i &= v^k
\end{align*}
$$

# Lie bracket (commutator)

The Lie bracket measures the separation of vector field flow curves. A flow curve (integral curve) is a curve that is tangent to all vectors in a vector field. 

$$
\begin{align*}
  \left[ \mathbf{u}, \mathbf{v} \right] &= \mathbf{u}(\mathbf{v}) - \mathbf{v}(\mathbf{u}) \\
  &= u^i \partial_i \left( v^j \boldsymbol{\partial}_j \right) - v^i \partial_i \left( u^j \boldsymbol{\partial}_j \right) \\
  &= u^i \left[ \left( \partial_i v^j \right)\boldsymbol{\partial}_j + v^j \left( \partial_i \boldsymbol{\partial}_j \right) \right] - v^i \left[ \left( \partial_i u^j \right)\boldsymbol{\partial}_j + u^j \left( \partial_i \boldsymbol{\partial}_j \right) \right]
\end{align*}
$$

The Lie bracket of basis vectors always vanishes as the basis vector flow curves (coordinate lines) always close.

The Lie bracket is not linear for each input

$$
\begin{align*}
  \left[ \alpha \mathbf{u}, \mathbf{v} \right] &= \alpha \mathbf{u}\left( \mathbf{v} \right) - \mathbf{v}\left(\alpha \mathbf{u} \right) \\
  &= \alpha \mathbf{u}\left( \mathbf{v} \right) - \mathbf{v}\left( \alpha \right) - \alpha \mathbf{v}\left( \mathbf{u} \right) \\
  &= \alpha \left[ \mathbf{u}\left( \mathbf{v} \right) - \mathbf{v}\left( \mathbf{u} \right)  \right] - \mathbf{v}\left( \alpha \right)\mathbf{u} \\
  &= \alpha \left[ \mathbf{u}, \mathbf{v} \right] - \mathbf{v}\left( \alpha \right)\mathbf{u}
\end{align*}
$$

# Geodesics

The geodesic is the straightest possible path in a curved surface/space. In curved space, a straight path has zero tangential acceleration when we travel along it at constant speed, meaning that the accelaration vector is normal to the surface.

The acceleration along a path $\mathbf{R}(t)$ on a surface is given by the second order derivative in terms of the tangential basis.

$$
\begin{align*}
  \frac{\mathrm{d}^2\mathbf{R}}{\mathrm{d}t^2} &= \frac{\mathrm{d}}{\mathrm{d}t} \left( \frac{\mathrm{d}\mathbf{R}}{\mathrm{d}t} \right) \\
  &= \frac{\mathrm{d}}{\mathrm{d}t} \left( \frac{\mathrm{d}x^i}{\mathrm{d}t} \frac{\partial \mathbf{R}}{\partial x^i} \right) \\
  &= \frac{\mathrm{d}^2x^i}{\mathrm{d}t^2} \frac{\partial \mathbf{R}}{\partial x^i} + \frac{\mathrm{d}x^i}{\mathrm{d}t} \left( \frac{\mathrm{d}}{\mathrm{d}t} \frac{\partial \mathbf{R}}{\partial x^i} \right) \\
  &= \frac{\mathrm{d}^2x^i}{\mathrm{d}t^2} \frac{\partial \mathbf{R}}{\partial x^i} + \frac{\mathrm{d}x^i}{\mathrm{d}t} \left[ \left( \frac{\mathrm{d}x^j}{\mathrm{d}t} \frac{\partial}{\partial x^j} \right) \frac{\partial \mathbf{R}}{\partial x^i} \right] \\
  &= \underbrace{\frac{\mathrm{d}^2x^i}{\mathrm{d}t^2} \frac{\partial \mathbf{R}}{\partial x^i}}_{\mathrm{tangential}} + \frac{\mathrm{d}x^i}{\mathrm{d}t} \frac{\mathrm{d}x^j}{\mathrm{d}t} \frac{\partial^2 \mathbf{R}}{\partial x^i \partial x^j}
\end{align*}
$$

The second term has both tangential and normal component, of which it can be expanded into

$$
  \frac{\partial^2 \mathbf{R}}{\partial x^i \partial x^j} = \Gamma^k_{ij} \frac{\partial \mathbf{R}}{\partial x^k} + L_{ij} \mathbf{\hat{n}}
$$

where 
- $L_{ij}$ is the second fundamental form giving the normal components.
- $\Gamma_{ij}^k$ are Christoffel symbols giving the tangential components.

The normal vector $\mathbf{\hat{n}}$ is given by the cross product 

$$
  \frac{ \frac{\partial \mathbf{R}}{\partial x^i} \times \frac{\partial \mathbf{R}}{\partial x^j}}{\left\| \frac{\partial \mathbf{R}}{\partial x^i} \times \frac{\partial \mathbf{R}}{\partial x^j} \right\|}
$$

The Christoffel symbols is derived from the dot product

$$
\begin{align*}
  \frac{\partial^2 \mathbf{R}}{\partial x^i \partial x^j} \cdot \frac{\partial \mathbf{R}}{\partial x^l} &= \left( \Gamma^k_{ij} \frac{\partial \mathbf{R}}{\partial x^k} + L_{ij} \mathbf{\hat{n}} \right) \cdot \frac{\partial \mathbf{R}}{\partial x^l} \\
  &= \Gamma^k_{ij} \frac{\partial \mathbf{R}}{\partial x^k} \cdot \frac{\partial \mathbf{R}}{\partial x^l} = \Gamma^k_{ij} g_{kl}
\end{align*}
$$

By applying the inverse metric tensor, we get

$$
  \Gamma^m_{ij} = \frac{\partial^2 \mathbf{R}}{\partial x^i \partial x^j} \cdot \frac{\partial \mathbf{R}}{\partial x^l} \mathfrak{g}^{lm}
$$

The second fundamental form is derived from the dot product

$$
\begin{align*}
  \frac{\partial^2 \mathbf{R}}{\partial x^i \partial x^j} \cdot \mathbf{\hat{n}} &= \left( \Gamma^k_{ij} \frac{\partial \mathbf{R}}{\partial x^k} + L_{ij} \mathbf{\hat{n}} \right) \cdot \mathbf{\hat{n}} \\
  L_{ij} &= \frac{\partial^2 \mathbf{R}}{\partial x^i \partial x^j} \cdot \mathbf{\hat{n}}
\end{align*}
$$

The second derivative of the path $\mathbf{R}(t)$ can thus be decomposed into

$$
  \frac{\mathrm{d}^2\mathbf{R}}{\mathrm{d}t^2} = \underbrace{\left( \frac{\mathrm{d}^2x^k}{\mathrm{d}t^2} + \Gamma^k_{ij} \frac{\mathrm{d}x^i}{\mathrm{d}t} \frac{\mathrm{d}x^j}{\mathrm{d}t} \right) \frac{\partial \mathbf{R}}{\partial{x^k}}}_{\mathrm{tangential}} + \underbrace{L_{ij} \frac{\mathrm{d}x^i}{\mathrm{d}t}\frac{\mathrm{d}x^j}{\mathrm{d}t} \mathbf{\hat{n}}}_{\mathrm{normal}}
$$

## Geodesic equation

A geodesic curve has vanishing tangential acceleration, giving the geodesic equation

$$
  \frac{\mathrm{d}^2x^k}{\mathrm{d}t^2} + \Gamma^k_{ij} \frac{\mathrm{d}x^i}{\mathrm{d}t} \frac{\mathrm{d}x^j}{\mathrm{d}t} = 0
$$

# Covariant derivative

The covariant derivative operator provides an affine connection between tangent spaces in a curved space, through parallel transport of a vector field along a smooth curve on a surface.

The covariant derivative operator has the following properties
- Linearity in direction vector input:
$$
\begin{align*}
  \nabla_{\left(\alpha\boldsymbol{\partial}_1 + \beta\boldsymbol{\partial}_2 \right)} \mathbf{v} &= \left( \alpha\partial_1 + \beta\partial_2 \right) \mathbf{v} \\
  &= \alpha\left(\partial_1\mathbf{v}\right) + \beta\left( \partial_2  \mathbf{v} \right) \\
  &= \alpha \nabla_{\boldsymbol{\partial}_1} \mathbf{v} + \beta \nabla_{\boldsymbol{\partial}_2} \mathbf{v}
\end{align*}
$$

- Additive in vector field input
$$
\begin{align*}
  \nabla_{\boldsymbol{\partial}_i} \left( \mathbf{u} + \mathbf{v} \right) &= \partial_i \left( \mathbf{u} + \mathbf{v} \right) \\
  &= \partial_i \mathbf{u} + \partial_i \mathbf{v} \\
  &= \nabla_{\boldsymbol{\partial}_i} \mathbf{u} + \nabla_{\boldsymbol{\partial}_i} \mathbf{v}
\end{align*}
$$

- Product rule (Leibniz rule)
$$
\begin{align*}
  \nabla_{\boldsymbol{\partial}_i} \left( \alpha \mathbf{v} \right) &= \partial_i \left( \alpha \mathbf{v} \right) \\
  &= \left(\partial_i \alpha \right)\mathbf{v} + \alpha \left( \partial_i \mathbf{v} \right) \\
  &= \left( \nabla_{\boldsymbol{\partial}_i} \alpha \right)\mathbf{v} + \alpha\left( \nabla_{\boldsymbol{\partial}_i} \mathbf{v} \right)
\end{align*}
$$

- The covariant derivative of a scalar field equals the directional derivative
$$
  \nabla_{\boldsymbol{\partial}_i} \alpha = \frac{\partial \alpha}{\partial x^i}
$$

The Christoffel symbols (connection coefficients) themselves are covarant derivaties

$$
  \nabla_{\boldsymbol{\partial}_i} \boldsymbol{\partial}_j = \Gamma_{ij}^k \boldsymbol{\partial}_k 
$$

A general formula for the covariant derivative is given as follows

$$
\begin{align*}
  \nabla_{\mathbf{u}} \mathbf{v} &= \nabla_{u_i \partial_i} \left( v^j \boldsymbol{\partial}_j \right) \\
  &= u^i \nabla_{\partial_i} \left( v^j \boldsymbol{\partial}_j \right) \\
  &= u^i \left[ \nabla_{\partial_i} \left(v^j \right) \boldsymbol{\partial}_j + v^j \nabla_{\partial_i} \boldsymbol{\partial}_j \right] \\
  &= u^i \left[ \left(\partial_i v^k \right) \boldsymbol{\partial}_k + v^j \Gamma_{ij}^k \boldsymbol{\partial}_k \right] \\
  &= u^i \left( \partial_i v^k + v^j \Gamma_{ij}^k \right) \boldsymbol{\partial}_k
\end{align*}
$$

## Fundamental theorem of Riemannian geometry
There is a unique covariant derivative, called the Levi-Civita connection, that is torsion free and has metric compatibility:

- Torsion-free (vanishing Lie bracket)
$$
  \nabla_{\boldsymbol{\partial}_i} \boldsymbol{\partial}_j - \nabla_{\boldsymbol{\partial}_j} \boldsymbol{\partial}_i = \partial_i \partial_j - \partial_j \partial_i \equiv \left[ \boldsymbol{\partial}_i, \boldsymbol{\partial}_j \right] = 0
$$

- Metric compatibility (product rule of vector field dot product)
$$
  \nabla_{\boldsymbol{\partial}_i} \left( \mathbf{u} \cdot \mathbf{v} \right) = \left( \nabla_{\boldsymbol{\partial}_i} \mathbf{u} \right) \cdot \mathbf{v} + \mathbf{u} \cdot \left( \nabla_{\boldsymbol{\partial}_i} \mathbf{v} \right) = 0
$$

The torsion-free property implies symmetry in the lower indices of the Christoffel symbols
$$
\begin{gather*}
  \nabla_{\boldsymbol{\partial}_i} \boldsymbol{\partial}_j = \nabla_{\boldsymbol{\partial}_j} \boldsymbol{\partial}_i \\
  \Gamma_{ij}^k \boldsymbol{\partial}_k = \Gamma_{ji}^k \boldsymbol{\partial}_k \\
  \Gamma_{ij}^k = \Gamma_{ji}^k
\end{gather*}
$$

The metric compatibility property gives the following

$$
\begin{align*}
  \nabla_{\boldsymbol{\partial}_k} \left( \boldsymbol{\partial}_i \cdot \boldsymbol{\partial}_j \right) &= \partial_k \left(\boldsymbol{\partial}_i \cdot \boldsymbol{\partial}_j  \right) = \left( \nabla_{\boldsymbol{\partial}_k} \boldsymbol{\partial}_i \right) \cdot \boldsymbol{\partial}_j + \boldsymbol{\partial}_i \cdot \left( \nabla_{\boldsymbol{\partial}_k} \boldsymbol{\partial}_j \right) \\
  &= \Gamma_{ik}^l \left( \boldsymbol{\partial}_l \cdot \boldsymbol{\partial}_j \right) + \Gamma_{jk}^l \left( \boldsymbol{\partial}_i \cdot \boldsymbol{\partial}_l \right) \\
  \partial_k g_{ij} &= \Gamma_{ik}^l g_{jl} + \Gamma_{jk}^l g_{il}
\end{align*}
$$

The two properties give equal Christoffel terms by permuting indices (equal terms in matching colors)

$$
\begin{align*}
  \partial_k g_{ij} &= \color{blue}{\Gamma_{ik}^l g_{jl}} + \color{red}{\Gamma_{jk}^l g_{il}} \\
  \partial_j g_{ki} &= \color{red}{\Gamma_{kj}^l g_{il}} + \color{green}{\Gamma_{ij}^l g_{kl}} \\
  \partial_i g_{jk} &= \color{green}{\Gamma_{ji}^l g_{kl}} + \color{blue}{\Gamma_{ki}^l g_{jl}}
\end{align*}
$$

Combining the equations and rearranging, a formula for the Christoffel symbol is derived

$$
\begin{gather*}
  \Gamma_{jk}^l g_{li} + \Gamma_{kj}^l g_{li} = \partial_k g_{ij} + \partial_j g_{ki} - \partial_i g_{jk} \\
  2\Gamma_{jk}^l g_{li}\mathfrak{g}^{im} = \mathfrak{g}^{im} \left( \partial_k g_{ij} + \partial_j g_{ki} - \partial_i g_{jk} \right) \\
  \Gamma_{jk}^l \delta_l^m = \frac{1}{2} \mathfrak{g}^{im} \left( \partial_k g_{ij} + \partial_j g_{ki} - \partial_i g_{jk} \right) \\
  \Gamma_{jk}^m = \frac{1}{2} \mathfrak{g}^{im} \left( \partial_k g_{ij} + \partial_j g_{ki} - \partial_i g_{jk} \right)
\end{gather*}
$$

## Flat-space definition

In flat space, the covariant derivative of a vector field equals the ordrinary derivative, differentiating both the vector components and the basis vectors

$$
\begin{align*}
  \frac{\partial \mathbf{v}}{\partial x^i} &= \frac{\partial}{\partial x^i} \left( v^j \frac{\partial \mathbf{v}}{\partial x^j} \right) \\
  &= \frac{\partial v^j}{\partial x^i} \frac{\partial \mathbf{v}}{\partial x^j} + v^j \frac{\partial}{\partial x^i} \left( \frac{\partial \mathbf{v}}{\partial x^j}  \right) \\
  &= \left( \frac{\partial v^j}{\partial x^i} + v^j \Gamma_{ij}^k \right) \frac{\partial \mathbf{v}}{\partial x^k} 
\end{align*}
$$

## Extrinsic definition (parallel transport)

Parallel transport moves a vector along a smooth curve on a surface. The vector rate of change is normal to the surface.

The covariant derivative $\nabla_{\mathbf{w}}\mathbf{v}$ is the rate of change of a vector field $\mathbf{v}$ in a direction $\mathbf{w}$ with the normal component $\mathbf{n}$ subtracted. When the covariant derivative vanishes, the vector $\mathbf{v}$ is parallel transported in the direction $\mathbf{w}$.

$$
  \nabla_{\frac{\mathrm{d}}{\mathrm{d}t}}\mathbf{v} = \frac{\mathrm{d}\mathbf{v}}{\mathrm{d}t} - \mathbf{n}
$$

For a tangent vector field we get

$$
\begin{align*}
  \nabla_{\boldsymbol{\partial}_i}\mathbf{v} &= \frac{\partial \mathbf{v}}{\partial x^i} - \mathbf{n} = \frac{\partial}{\partial x^i} \left( v^j \frac{\partial \mathbf{v}}{\partial x^j} \right) - \mathbf{n} \\
  &= \frac{\partial v^j}{\partial x^i} \frac{\partial \mathbf{v}}{\partial x^j} + v^j \frac{\partial}{\partial x^i} \left( \frac{\partial \mathbf{v}}{\partial x^j}  \right) - \mathbf{n} \\
  &= \left[ \frac{\partial v^k}{\partial x^i} \frac{\partial \mathbf{v}}{\partial x^k} + v^j \left(\Gamma_{ij}^k \frac{\partial \mathbf{v}}{\partial x^k} + L_{ij}\hat{\mathbf{n}} \right) \right] - \mathbf{n} \\
  &= \left[ \frac{\partial v^k}{\partial x^i} + v^j \Gamma_{ij}^k  \right]\frac{\partial \mathbf{v}}{\partial x^k} + \underbrace{v^j L_{ij}\mathbf{\hat{n}}}_{=\mathbf{n}} - \mathbf{n} \\
  &= \left[ \frac{\partial v^k}{\partial x^i} + v^j \Gamma_{ij}^k  \right]\frac{\partial \mathbf{v}}{\partial x^k}
\end{align*}
$$

## Intrinsic definition

In intrinsic geometry, curves cannot be described by position and normal vectors with respect to an origin. The covariant derivative is given by


$$
\begin{align*}
  \nabla_{\boldsymbol{\partial}_i}\mathbf{v} &= \frac{\partial \mathbf{v}}{\partial x^i} \\
  &= \frac{\partial}{\partial x^i} \left( v^j \boldsymbol{\partial}_j \right) \\
  &= \frac{\partial v^j}{\partial x^i} \boldsymbol{\partial}_j + v^j \frac{\partial}{\partial x^i} \left( \boldsymbol{\partial}_j \right) \\
  &= \frac{\partial v^k}{\partial x^i} \boldsymbol{\partial}_k + v^j \Gamma_{ij}^k \boldsymbol{\partial}_k \\
  &= \left( \frac{\partial v^k}{\partial x^i} + v^j \Gamma_{ij}^k  \right) \boldsymbol{\partial}_k
\end{align*}
$$

The Christoffel symbols are derived from partial derivatives of the metric tensor

$$
\begin{align*}
  \frac{\partial}{\partial x^k} g_{ij} &= \frac{\partial}{\partial x^k} \left( \boldsymbol{\partial}_i \cdot \boldsymbol{\partial}_j \right) \\
  &= \frac{\partial}{\partial x^k} \left( \boldsymbol{\partial}_i \right) \cdot \boldsymbol{\partial}_j + \boldsymbol{\partial}_i \cdot \frac{\partial}{\partial x^k} \left( \boldsymbol{\partial}_j \right) \\
  &= \left( \Gamma_{ik}^l \boldsymbol{\partial}_l \right) \cdot \boldsymbol{\partial}_j + \boldsymbol{\partial}_i \cdot \left( \Gamma_{jk}^l \boldsymbol{\partial}_l \right) \\
  &= \Gamma_{ik}^l \left( \boldsymbol{\partial}_l \cdot \boldsymbol{\partial}_j \right) + \Gamma_{jk}^l \left( \boldsymbol{\partial}_i \cdot \boldsymbol{\partial}_l \right) \\
  &= \Gamma_{ik}^l g_{lj} + \Gamma_{jk}^l g_{il} \\
  &= \Gamma_{ik}^l g_{jl} + \Gamma_{jk}^l g_{il}
\end{align*}
$$

Permuting the indices gives the following set of PDEs with equal terms matching in color

$$
\begin{align*}
  \frac{\partial g_{ij}}{\partial x^k} &= \color{blue}{\Gamma_{ik}^l g_{jl}} + \color{red}{\Gamma_{jk}^l g_{il}} \\
  \frac{\partial g_{ki}}{\partial x^j} &= \color{red}{\Gamma_{kj}^l g_{il}} + \color{green}{\Gamma_{ij}^l g_{kl}} \\
  \frac{\partial g_{jk}}{\partial x^i} &= \color{green}{\Gamma_{ji}^l g_{kl}} + \color{blue}{\Gamma_{ki}^l g_{jl}}
\end{align*}
$$

The Christoffel symbols can be isolated with the following PDE combination

$$
\begin{gather*}
  \Gamma_{jk}^l g_{li} + \Gamma_{kj}^l g_{li} = \frac{\partial g_{ij}}{\partial x^k} + \frac{\partial g_{ki}}{\partial x^j} - \frac{\partial g_{jk}}{\partial x^i} \\
  2\Gamma_{jk}^l g_{li}\mathfrak{g}^{im} = \mathfrak{g}^{im} \left( \frac{\partial g_{ij}}{\partial x^k} + \frac{\partial g_{ki}}{\partial x^j} - \frac{\partial g_{jk}}{\partial x^i} \right) \\
  \Gamma_{jk}^l \delta_l^m = \frac{1}{2} \mathfrak{g}^{im} \left( \frac{\partial g_{ij}}{\partial x^k} + \frac{\partial g_{ki}}{\partial x^j} - \frac{\partial g_{jk}}{\partial x^i} \right) \\
  \Gamma_{jk}^m = \frac{1}{2} \mathfrak{g}^{im} \left( \frac{\partial g_{ij}}{\partial x^k} + \frac{\partial g_{ki}}{\partial x^j} - \frac{\partial g_{jk}}{\partial x^i} \right)
\end{gather*}
$$

By the symmetry $\frac{\partial^2}{\partial x^j \partial x^k} = \frac{\partial^2}{\partial x^k \partial x^j}$ we get

$$
  \frac{\partial g_{ij}}{\partial x^k} + \frac{\partial g_{ki}}{\partial x^j} - \frac{\partial g_{jk}}{\partial x^i} = 2 \left(\frac{\partial}{\partial x^i} \cdot \frac{\partial^2}{\partial x^j \partial x^k} \right)
$$

Hence, the extrinsic formula for the Christoffel symbols is retrieved from the intrinsic formula

$$
\begin{align*}
  \Gamma_{jk}^m &= \frac{1}{2} \mathfrak{g}^{im} \left[ 2 \left( \frac{\partial}{\partial x^i} \cdot \frac{\partial^2}{\partial x^j \partial x^k} \right) \right] \\
  &= \frac{\partial}{\partial x^i} \cdot \frac{\partial^2}{\partial x^j \partial x^k} \mathfrak{g}^{im}
\end{align*}
$$

## Geodesics

The covariant derivative gives an alternative definition of a geodesic as a curve resulting from parallel transporting a vector along itself

$$
  \nabla_{\mathbf{v}}\mathbf{v} = \mathbf{0}
$$

The godesic equation can be retrieved from the covariant derivative of a parametrized curve along itself

$$
\begin{align*}
  \nabla_{\frac{\mathrm{d}}{\mathrm{d}\lambda}} \frac{\mathrm{d}}{\mathrm{d}\lambda} &= \frac{\mathrm{d}}{\mathrm{d}\lambda} \left( \frac{\mathrm{d}}{\mathrm{d}\lambda} \right) \\
  &= \frac{\mathrm{d}}{\mathrm{d}\lambda} \left( \frac{\mathrm{d}u^j}{\mathrm{d}\lambda} \boldsymbol{\partial}_j \right) \\
  &= \frac{\mathrm{d}}{\mathrm{d}\lambda} \left( \frac{\mathrm{d}u^j}{\mathrm{d}\lambda} \right) \boldsymbol{\partial}_j + \frac{\mathrm{d}u^j}{\mathrm{d}\lambda} \frac{\mathrm{d}}{\mathrm{d}\lambda} \boldsymbol{\partial}_j \\
  &= \frac{\mathrm{d}^2 u^j}{\mathrm{d}\lambda^2} \boldsymbol{\partial}_j + \frac{\mathrm{d}u^j}{\mathrm{d}\lambda} \left( \frac{\mathrm{d}u^i}{\mathrm{d}\lambda} \boldsymbol{\partial}_i  \right) \boldsymbol{\partial}_j \\
  &= \frac{\mathrm{d}^2 u^j}{\mathrm{d}\lambda^2} \boldsymbol{\partial}_j + \frac{\mathrm{d}u^i}{\mathrm{d}\lambda} \frac{\mathrm{d}u^j}{\mathrm{d}\lambda} \nabla_{\boldsymbol{\partial}_i} \boldsymbol{\partial}_j \\
  &= \frac{\mathrm{d}^2 u^j}{\mathrm{d}\lambda^2} \boldsymbol{\partial}_j + \frac{\mathrm{d}u^i}{\mathrm{d}\lambda} \frac{\mathrm{d}u^j}{\mathrm{d}\lambda} \Gamma_{ij}^k \boldsymbol{\partial}_k \\
  &= \left( \frac{\mathrm{d}^2 u^k}{\mathrm{d}\lambda^2} + \frac{\mathrm{d}u^i}{\mathrm{d}\lambda} \frac{\mathrm{d}u^j}{\mathrm{d}\lambda} \Gamma_{ij}^k \right) \boldsymbol{\partial}_k
\end{align*}
$$

Hence the curve is geodesic if $\nabla_{\frac{\mathrm{d}}{\mathrm{d}\lambda}} \frac{\mathrm{d}}{\mathrm{d}\lambda} = \mathbf{0}$

# Torsion tensor

The torsion tensor measures the separation between parallel-transported vector fields

$$
\begin{align*}
  T\left(\mathbf{u}, \mathbf{v}\right) &= \nabla_{\mathbf{u}}\mathbf{v} - \nabla_{\mathbf{v}}\mathbf{u} - \left[ \mathbf{u}, \mathbf{v} \right] \\
  &= u^i \left( \partial_i v^k + v^j \Gamma_{ij}^k \right) \boldsymbol{\partial}_k - v^i \left( \partial_i u^k + u^j \Gamma_{ij}^k \right) \boldsymbol{\partial}_k - \left[ u^i \partial_i \left( v^j \boldsymbol{\partial}_j \right) - v^i \partial_i \left( u^j \boldsymbol{\partial}_j \right) \right] \\
  &= u^i \left( \partial_i v^k + v^j \Gamma_{ij}^k \right) \boldsymbol{\partial}_k - v^i \left( \partial_i u^k + u^j \Gamma_{ij}^k \right) \boldsymbol{\partial}_k - u^i \left[ \left( \partial_i v^k \right)\boldsymbol{\partial}_k + v^j \left( \partial_i \boldsymbol{\partial}_j \right) \right] + v^i \left[ \left( \partial_i u^k \right)\boldsymbol{\partial}_k + u^j \left( \partial_i \boldsymbol{\partial}_j \right) \right] \\
  &= u^i v^j \Gamma_{ij}^k \boldsymbol{\partial}_k - v^i u^j \Gamma_{ij}^k \boldsymbol{\partial}_k = u^i v^j \Gamma_{ji}^k \boldsymbol{\partial}_k - v^j u^i \Gamma_{ji}^k \boldsymbol{\partial}_k \\
  &= u^i v^j \left(\Gamma_{ij}^k - \Gamma_{ji}^k \right) \boldsymbol{\partial}_k = u^i v^j T_{ij}^k \boldsymbol{\partial}_k
\end{align*}
$$

An affine connection given by a covariant derivative is called torsion-free if the torsion tensor vanishes, $T\left(\mathbf{u}, \mathbf{v}\right) = \mathbf{0}$. Torsion-free means that parallel-transported vectors close properly and it implies that the connection coefficients (Christoffel symbols) are symmetric in the lower indices

$$
\begin{gather*}
  T_{ij}^k = \Gamma_{ij}^k - \Gamma_{ji}^k = 0 \\
  \Gamma_{ij}^k = \Gamma_{ji}^k
\end{gather*}
$$

# Riemann curvature tensor (Riemann-Christoffel tensor)

The Riemann curvature tensor $R^d_{abc}$ is of rank $(1,3)$.

## Holonomic definition

On all curved surfaces a coordinate system called the Riemann normal coordinates (local inertia frame) can be found at a point $p$ such that the metric tensor and the Christoffel symbols vanish.

Holonomy describes how vector fields get twisted during parallel transport. In this regard, the Riemann curvature tensor captures this twisting by measuring the rate of change of a parallel-transported vector field $DCBA\mathbf{w}$ relative to the original field $\mathbf{w}$ at the same point. Assuming the Levi-Civita connection applies such that $\left[ \mathbf{u}, \mathbf{v} \right] = 0$

$$
\begin{align*}
  R\left( \mathbf{u}, \mathbf{v} \right) \mathbf{w} &= \lim_{r,s \to 0} \frac{\mathbf{w} - DCBA\mathbf{w}}{rs} \\
  &= \lim_{r,s \to 0} \frac{1}{rs} \left( DCC^{-1}D^{-1}\mathbf{w} - DCBA\mathbf{w} \right) \\
  &= \lim_{r,s \to 0} \frac{1}{rs} DC \left( C^{-1}D^{-1}\mathbf{w} - BA\mathbf{w} \right) \\
  &= \lim_{r,s \to 0} \frac{1}{rs} DC \left[ \left( C^{-1}D^{-1}\mathbf{w} - C^{-1}\mathbf{w} + C^{-1}\mathbf{w} - \mathbf{w} \right) - \left( BA\mathbf{w} - B\mathbf{w} + B\mathbf{w} - \mathbf{w} \right) \right] \\
  &= \lim_{r,s \to 0} \frac{1}{rs} DC \left[ C^{-1}\left( D^{-1} \mathbf{w} - \mathbf{w} \right) + \left( C^{-1} \mathbf{w} - \mathbf{w} \right) - B\left(A \mathbf{w} - \mathbf{w} \right) + \left( B\mathbf{w} - \mathbf{w} \right)  \right] \\
  &= \lim_{r,s \to 0} DC \left[ \left( \frac{C^{-1}}{r} \underbrace{\frac{D^{-1} \mathbf{w} - \mathbf{w}}{s}}_{=\nabla_{\mathbf{v}}\mathbf{w}} + \frac{1}{s} \underbrace{\frac{C^{-1} \mathbf{w} - \mathbf{w}}{r}}_{=\nabla_{\mathbf{u}}\mathbf{w}} \right) - \left( \frac{B}{s} \underbrace{\frac{A \mathbf{w} - \mathbf{w}}{r}}_{=\nabla_{\mathbf{u}}\mathbf{w}} + \frac{1}{r} \underbrace{\frac{B\mathbf{w} - \mathbf{w}}{s}}_{=\nabla_{\mathbf{v}}\mathbf{w}} \right) \right] \\
  &= \lim_{r,s \to 0} DC \left[ \frac{C^{-1}}{r} \left( \nabla_{\mathbf{v}}\mathbf{w} \right) + \frac{1}{s} \left( \nabla_{\mathbf{u}}\mathbf{w} \right) - \frac{B}{s} \left( \nabla_{\mathbf{u}}\mathbf{w} \right) - \frac{1}{r} \left( \nabla_{\mathbf{v}}\mathbf{w} \right) \right] \\
  &= \lim_{r,s \to 0} DC \left( \frac{C^{-1} \nabla_{\mathbf{v}}\mathbf{w} - \nabla_{\mathbf{v}}\mathbf{w}}{r} - \frac{B \nabla_{\mathbf{u}}\mathbf{w} - \nabla_{\mathbf{u}}\mathbf{w}}{s} \right) \\
  &= \lim_{r,s \to 0} \underbrace{DC}_{=\mathrm{Id}} \left( \nabla_{\mathbf{u}}\nabla_{\mathbf{v}}\mathbf{w} - \nabla_{\mathbf{v}}\nabla_{\mathbf{u}}\mathbf{w} \right) \\
  &= \nabla_{\mathbf{u}}\nabla_{\mathbf{v}}\mathbf{w} - \nabla_{\mathbf{v}}\nabla_{\mathbf{u}}\mathbf{w}
\end{align*}
$$

In the case $\left[ \mathbf{u}, \mathbf{v} \right] \neq 0$, the Riemann curvature tensor is defined as

$$
  R\left( \mathbf{u}, \mathbf{v} \right) \mathbf{w} = \nabla_{\mathbf{u}}\nabla_{\mathbf{v}}\mathbf{w} - \nabla_{\mathbf{v}}\nabla_{\mathbf{u}}\mathbf{w} - \nabla_{\left[\mathbf{u}, \mathbf{v} \right]}\mathbf{w}
$$

The Riemann curvature tensor is linear in all inputs

$$
  R\left( \mathbf{u}, \mathbf{v} \right) \mathbf{w} = u^i v^j w^k R\left(\boldsymbol{\partial}_i, \boldsymbol{\partial}_j \right)\boldsymbol{\partial}_k
$$

Linearity in the first input is shown as follows

$$
\begin{align*}
  R\left( \mathbf{u}, \mathbf{v} \right) \mathbf{w} &= \nabla_{u^i \boldsymbol{\partial}_i} \nabla_{\mathbf{v}} \mathbf{w} - \nabla_{\mathbf{v}}\nabla_{u^i \boldsymbol{\partial}_i} \mathbf{w} - \nabla_{\left[u^i \boldsymbol{\partial}_i, \mathbf{v} \right]} \mathbf{w} \\
  &= \nabla_{u^i \boldsymbol{\partial}_i} \nabla_{\mathbf{v}} \mathbf{w} - \nabla_{\mathbf{v}}\nabla_{u^i \boldsymbol{\partial}_i} \mathbf{w} - \nabla_{u^i \left[ \boldsymbol{\partial}_i, \mathbf{v} \right] - \mathbf{v}\left(u^i\right) \boldsymbol{\partial}_i} \mathbf{w} \\
  &= u^i \nabla_{\boldsymbol{\partial}_i} \nabla_{\mathbf{v}} \mathbf{w} - \nabla_{\mathbf{v}} \left( u^i \nabla_{\boldsymbol{\partial}_i} \mathbf{w} \right) - u^i \nabla_{ \left[ \boldsymbol{\partial}_i, \mathbf{v} \right]} \mathbf{w} + \mathbf{v}\left(u^i\right) \nabla_{\boldsymbol{\partial}_i} \mathbf{w} \\
  &= u^i \nabla_{\boldsymbol{\partial}_i} \nabla_{\mathbf{v}} \mathbf{w} - \underbrace{\left( \nabla_{\mathbf{v}} u^i \right)}_{=\mathbf{v}\left( u^i\right)} \left( \nabla_{\boldsymbol{\partial}_i} \mathbf{w} \right) - u^i \left( \nabla_{\mathbf{v}}\nabla_{\boldsymbol{\partial}_i} \mathbf{w} \right) - u^i \nabla_{ \left[ \boldsymbol{\partial}_i, \mathbf{v} \right]} \mathbf{w} + \mathbf{v}\left(u^i\right) \left( \nabla_{\boldsymbol{\partial}_i} \mathbf{w} \right) \\
  &= u^i \nabla_{\boldsymbol{\partial}_i} \nabla_{\mathbf{v}} \mathbf{w} - u^i \left( \nabla_{\mathbf{v}}\nabla_{\boldsymbol{\partial}_i} \mathbf{w} \right) - u^i \nabla_{u^i \left[ \boldsymbol{\partial}_i, \mathbf{v} \right]} \mathbf{w} \\
  &= u^i \left( \nabla_{\boldsymbol{\partial}_i} \nabla_{\mathbf{v}} \mathbf{w} - \nabla_{\mathbf{v}}\nabla_{\boldsymbol{\partial}_i} \mathbf{w} - \nabla_{ \left[ \boldsymbol{\partial}_i, \mathbf{v} \right]} \mathbf{w} \right) \\
  &= u^i R\left( \boldsymbol{\partial}_i, \mathbf{v} \right) \mathbf{w}
\end{align*}
$$

Linearity in the second input can be similarly shown, while linearity in the third input is shown as follows

$$
\begin{align*}
  R\left( \mathbf{u}, \mathbf{v} \right) \mathbf{w} =& \nabla_{\mathbf{u}}\nabla_{\mathbf{v}}\left(w^i \boldsymbol{\partial}_i \right) - \nabla_{\mathbf{v}}\nabla_{\mathbf{u}}\left(w^i \boldsymbol{\partial}_i \right) - \nabla_{\left[\mathbf{u}, \mathbf{v} \right]}\left(w^i \boldsymbol{\partial}_i \right) \\
  =& \nabla_{\mathbf{u}} \left[ \left( \nabla_{\mathbf{v}} w^i \right) \boldsymbol{\partial}_i + w^i \left( \nabla_{\mathbf{v}} \boldsymbol{\partial}_i \right) \right] - \nabla_{\mathbf{v}} \left[ \left( \nabla_{\mathbf{u}} w^i \right) \boldsymbol{\partial}_i + w^i \left( \nabla_{\mathbf{u}} \boldsymbol{\partial}_i \right) \right] - \nabla_{\left[\mathbf{u}, \mathbf{v} \right]}\left(w^i \boldsymbol{\partial}_i \right) \\
  =& \left( \nabla_{\mathbf{u}} \nabla_{\mathbf{v}} w^i \right) \boldsymbol{\partial}_i + \left( \nabla_{\mathbf{v}} w^i \right) \left( \nabla_{\mathbf{u}} \boldsymbol{\partial}_i \right) + \left( \nabla_{\mathbf{u}} w^i \right) \left( \nabla_{\mathbf{v}} \boldsymbol{\partial}_i \right) + w^i \left( \nabla_{\mathbf{u}} \nabla_{\mathbf{v}} \boldsymbol{\partial}_i \right) \\
  &- \left( \nabla_{\mathbf{v}} \nabla_{\mathbf{u}} w^i \right)\boldsymbol{\partial}_i -  \left( \nabla_{\mathbf{v}} w^i \right) \left( \nabla_{\mathbf{u}} \boldsymbol{\partial}_i \right) - \left( \nabla_{\mathbf{u}} w^i \right) \left( \nabla_{\mathbf{v}} \boldsymbol{\partial}_i \right) - w^i \left( \nabla_{\mathbf{v}} \nabla_{\mathbf{u}} \boldsymbol{\partial}_i \right) \\
  &- \nabla_{\left[\mathbf{u}, \mathbf{v} \right]}\left(w^i \boldsymbol{\partial}_i \right) \\
  =& \left( \nabla_{\mathbf{u}} \nabla_{\mathbf{v}} w^i \right) \boldsymbol{\partial}_i - \left( \nabla_{\mathbf{v}} \nabla_{\mathbf{u}} w^i \right)\boldsymbol{\partial}_i + w^i \left( \nabla_{\mathbf{u}} \nabla_{\mathbf{v}} \boldsymbol{\partial}_i \right) - w^i \left( \nabla_{\mathbf{v}} \nabla_{\mathbf{u}} \boldsymbol{\partial}_i \right) \\
  &- \underbrace{\left( \nabla_{\left[\mathbf{u}, \mathbf{v} \right]} w^i \right)}_{\nabla_{\mathbf{u}}\nabla_{\mathbf{v}}w^i - \nabla_{\mathbf{v}}\nabla_{\mathbf{u}}w^i} \boldsymbol{\partial}_i - w^i \left( \nabla_{\left[\mathbf{u}, \mathbf{v} \right]} \boldsymbol{\partial}_i \right) \\
  =& w^i \left( \nabla_{\mathbf{u}} \nabla_{\mathbf{v}} \boldsymbol{\partial}_i \right) - w^i \left( \nabla_{\mathbf{v}} \nabla_{\mathbf{u}} \boldsymbol{\partial}_i \right) - w^i \left( \nabla_{\left[\mathbf{u}, \mathbf{v} \right]} \boldsymbol{\partial}_i \right) \\
  =& w^i \left( \nabla_{\mathbf{u}} \nabla_{\mathbf{v}} \boldsymbol{\partial}_i - \nabla_{\mathbf{v}} \nabla_{\mathbf{u}} \boldsymbol{\partial}_i - \nabla_{\left[\mathbf{u}, \mathbf{v} \right]} \boldsymbol{\partial}_i \right) \\
  =& w^i R\left( \mathbf{u}, \mathbf{v} \right) \boldsymbol{\partial}_i
\end{align*}
$$

## Geodesic deviation definition

The geodesic deviation of vector flow curves $\mathbf{v}$ separated by the vector field $\mathbf{s}$ is given by

$$
  \nabla_{\mathbf{v}}\nabla_{\mathbf{v}}\mathbf{s}
$$

Since the flow and separation curves form closed loops, a torsion-free connecting applies, giving 

$$
  \nabla_{\mathbf{v}}\mathbf{s} - \nabla_{\mathbf{s}}\mathbf{v} \equiv \left[ \mathbf{v}, \mathbf{s} \right] = 0 \\
  \nabla_{\mathbf{v}}\mathbf{s} = \nabla_{\mathbf{s}}\mathbf{v}
$$

Since the flow curves are geodesic the covariant derivative of $\mathbf{v}$ along itself vanishes

$$
\begin{align*}
  \mathbf{0} &= \nabla_{\mathbf{v}}\mathbf{v} \\
  &= \nabla_{\mathbf{s}}\nabla_{\mathbf{v}}\mathbf{v} \\
  &= \underbrace{\nabla_{\mathbf{s}}\nabla_{\mathbf{v}}\mathbf{v} - \nabla_{\mathbf{v}}\nabla_{\mathbf{s}}\mathbf{v}}_{=R(\mathbf{s}, \mathbf{v})\mathbf{v}} + \nabla_{\mathbf{v}}\underbrace{\nabla_{\mathbf{s}}\mathbf{v}}_{=\nabla_{\mathbf{v}}\mathbf{s}} \\
  &= R(\mathbf{s}, \mathbf{v})\mathbf{v} + \nabla_{\mathbf{v}}\nabla_{\mathbf{v}}\mathbf{s} \\
\end{align*}
$$

The geodesic deviation is thus given by

$$
  \nabla_{\mathbf{v}}\nabla_{\mathbf{v}}\mathbf{s} = -R(\mathbf{s}, \mathbf{v})\mathbf{v} = R(\mathbf{v}, \mathbf{s})\mathbf{v} 
$$

### Sectional curvature

Sectional curvature is defined as 

$$
  K(\mathbf{s}, \mathbf{v}) = \frac{\left[R(\mathbf{s}, \mathbf{v})\mathbf{v}\right] \cdot \mathbf{s}}{\left( \mathbf{s} \cdot \mathbf{s} \right) - \left( \mathbf{v} \cdot \mathbf{v} \right) - \left( \mathbf{s} \cdot \mathbf{v} \right)^2}
$$

where the denominator is a normalization factor equaling the area of the parallellogram spaned by the vector fields, equivalent of $\| \mathbf{s} \times \mathbf{v} \|^2$. The sign of the nominator denotes wheter the flow curves described by $\mathbf{v}$ are converging/diverging.

## Tensor components

The Riemann curvature tensor components can be derived as follows

$$
\begin{align*}
  R\left( \boldsymbol{\partial}_a, \boldsymbol{\partial}_b \right) \boldsymbol{\partial}_c &= \nabla_{\boldsymbol{\partial}_a} \nabla_{\boldsymbol{\partial}_b} \boldsymbol{\partial}_c - \nabla_{\boldsymbol{\partial}_b} \nabla_{\boldsymbol{\partial}_a} \boldsymbol{\partial}_c - \underbrace{\nabla_{\left[ \boldsymbol{\partial}_a, \boldsymbol{\partial}_b \right]}}_{=0} \boldsymbol{\partial}_c \\
  &= \nabla_{\boldsymbol{\partial}_a} \left( \Gamma_{bc}^i \boldsymbol{\partial}_i \right) - \nabla_{\boldsymbol{\partial}_b} \left( \Gamma_{ac}^j \boldsymbol{\partial}_j \right) \\
  &= \nabla_{\boldsymbol{\partial}_a} \left( \Gamma_{bc}^i \right) \boldsymbol{\partial}_i + \Gamma_{bc}^i \nabla_{\boldsymbol{\partial}_a} \boldsymbol{\partial}_i - \nabla_{\boldsymbol{\partial}_b} \left( \Gamma_{ac}^j \right) \boldsymbol{\partial}_j - \Gamma_{ac}^j \nabla_{\boldsymbol{\partial}_b} \boldsymbol{\partial}_j \\
  &= \partial_a \left( \Gamma_{bc}^i \right) \boldsymbol{\partial}_i - \partial_b \left( \Gamma_{ac}^j \right) \boldsymbol{\partial}_j + \Gamma_{bc}^i \Gamma_{ai}^k \boldsymbol{\partial}_k - \Gamma_{ac}^j \Gamma_{bj}^l \boldsymbol{\partial}_l \\
  &= \partial_a \left( \Gamma_{bc}^i \right) \boldsymbol{\partial}_d - \partial_b \left( \Gamma_{ac}^d \right) \boldsymbol{\partial}_d + \Gamma_{bc}^i \Gamma_{ai}^d \boldsymbol{\partial}_d - \Gamma_{ac}^j \Gamma_{bj}^d \boldsymbol{\partial}_d \\
  &= \left[ \partial_a \left( \Gamma_{bc}^i \right) - \partial_b \left( \Gamma_{ac}^d \right) + \Gamma_{bc}^i \Gamma_{ai}^d - \Gamma_{ac}^j \Gamma_{bj}^d \right] \boldsymbol{\partial}_d \\
  &= R_{cab}^d \boldsymbol{\partial}_d
\end{align*}
$$ 

## Symmetries

- 34-symmetry: $R^d_{c\color{red}{ba}} = -R^d_{c\color{red}{ab}}$
- Bianchi identity: $R^d_{cab} + R^d_{bca} + R^d_{abc} = 0$
- Second Bianchi identity: $R_{cab;i}^d + R_{cia;b}^d + R_{cbi;a}^d = 0$
- 12-symmetry: $R_{\color{red}{ba}cd} = -R_{\color{red}{ab}cd}$
- Flip: $R_{\color{red}{ab}\color{blue}{cd}} = -R_{\color{blue}{cd}\color{red}{ab}}$

## Transformation rule

The Riemann tensor transforms as follows

$$
  \tilde{R}_{abc}^d = J_a^i J_b^j J_c^k R_{ijk}^l \left(J^{-1}\right)_l^d
$$

# Ricci tensor

## Geodesic deviation definition

The Ricci tensor desribes volume changes along flow curves and can be defined as a sum of sectional curvature wrt an orthonormal basis

$$
\begin{align*}
  \mathrm{Ric}(\mathbf{v}, \mathbf{v}) &= \sum_{i=1}^{n-1} K(\boldsymbol{\partial}_i, \mathbf{v}) \\
  &= \frac{\left[R\left( \boldsymbol{\partial}_i, \mathbf{v} \right)\mathbf{v}\right] \cdot \boldsymbol{\partial}_i}{\left( \boldsymbol{\partial}_i \cdot \boldsymbol{\partial}_i\right) - \left( \mathbf{v} \cdot \mathbf{v} \right) - \left( \boldsymbol{\partial}_i \cdot \mathbf{v} \right)^2} \\
  &= \left[R\left( \boldsymbol{\partial}_i, \mathbf{v} \right)\mathbf{v}\right] \cdot \boldsymbol{\partial}_i \\
  &= \left[R\left( \boldsymbol{\partial}_i, v^j \boldsymbol{\partial}_j \right)v^k \boldsymbol{\partial}_k \right] \cdot \boldsymbol{\partial}_i \\
  &= v^j v^k \left[R\left( \boldsymbol{\partial}_i, \boldsymbol{\partial}_j \right) \boldsymbol{\partial}_k \right] \cdot \boldsymbol{\partial}_i \\
  &= v^j v^k R_{kij}^i \\
  &= v^j v^k R_{kj}
\end{align*}
$$

where $\mathbf{v} = \boldsymbol{\partial}_n$ is set as a direction vector field.

## Volume form definition

The volume form is a linear form measuring the volum spaned by a set of vectors

$$
  \omega (\mathbf{v}_1,...,\mathbf{v}_n) = \sqrt{\det g} \varepsilon_{ij...} \prod_{i=1}^n v_i^{\mu_i} = \omega \prod_{i=1}^n v_i^{\mu_i}
$$

where $\varepsilon$ is the Levi-Civita symbol.

The Ricci tensor can be derived from the covariant derivative of the volume form of separation vectors along a geodesic curve.

With the Levi-Civita connection, the first covariant derivative of the volume form of parallel-transported flow separation vectors along a geodesic path vanishes since volume is preserved

$$
\begin{align*}
  \nabla_{\mathbf{v}} \left[ \omega (\mathbf{s}_1,...,\mathbf{s}_n) \right] &= \nabla_{\mathbf{v}} \left( \sqrt{\det g} \varepsilon_{\mu_1...\mu_n} \prod_{i=1}^n s_i^{\mu_i} \right) \\
  &= \left( \nabla_{\mathbf{v}} \sqrt{\det g} \right)\varepsilon_{\mu_1...\mu_n} \prod_{i=1}^n s_i^{\mu_i} + \sqrt{\det g} \varepsilon_{\mu_1...\mu_n} \underbrace{\left( \nabla_{\mathbf{v}} s_j^{\mu_j} \right)}_{=0} \prod_{\substack{i=0\\ i\neq j}}^n s_i^{\mu_i}
\end{align*}
$$

Since the vectors are parallel-transported, $\nabla_{\mathbf{v}} s_j^{\mu_j} = 0$, and hence 

$$
  \left( \nabla_{\mathbf{v}} \sqrt{\det g} \right)\varepsilon_{\mu_1...\mu_n} = \nabla_{\mathbf{v}} \omega = 0
$$

The 2nd covariant derivative of the volume form becomes

$$
\begin{align*}
  \nabla_{\mathbf{v}}\nabla_{\mathbf{v}} \left[ \omega (\mathbf{s}_1,...,\mathbf{s}_n) \right] &= \nabla_{\mathbf{v}} \nabla_{\mathbf{v}} \left( \prod_{i=1}^n s_i^{\mu_i} \right) \underbrace{\sqrt{\det g} \varepsilon_{\mu_1...\mu_n} }_{\nabla_{\mathbf{v}} \omega = 0} \\
  &= \nabla_{\mathbf{v}} \left[ \left( \nabla_{\mathbf{v}} s_j^{\mu_j} \right) \prod_{\substack{i=1 \\ i\neq j}} s_i^{\mu_i} \right] \sqrt{\det g} \varepsilon_{\mu_1...\mu_n} \\
  &= \left[ \underbrace{\left( \nabla_{\mathbf{v}}\nabla_{\mathbf{v}} s_j^{\mu_j} \right)}_{=-R(\mathbf{s}, \mathbf{v})\mathbf{v}} \prod_{\substack{i=1 \\ i\neq j}} s_i^{\mu_i} \right] \sqrt{\det g} \varepsilon_{\mu_1...\mu_n} + \left[ \left( \nabla_{\mathbf{v}}s_j^{\mu_j} \right) \left( \nabla_{\mathbf{v}}s_k^{\mu_k} \right) \prod_{\substack{i=1 \\ i\neq j,k}} s_i^{\mu_i} \right] \sqrt{\det g} \varepsilon_{\mu_1...\mu_n} \\
  &= \left[ -R_{xyz}^{\mu_j} s_j^{y} v^x v^z \prod_{\substack{i=1 \\ i\neq j}} s_i^{\mu_i} \right] \sqrt{\det g} \underbrace{\varepsilon_{\mu_1...\mu_n}}_{\Rightarrow y = \mu_j} + \left[ \left( \nabla_{\mathbf{v}}s_j^{\mu_j} \right) \left( \nabla_{\mathbf{v}}s_k^{\mu_k} \right) \prod_{\substack{i=1 \\ i\neq j,k}} s_i^{\mu_i} \right] \sqrt{\det g} \varepsilon_{\mu_1...\mu_n} \\
  &= \left[ -R_{x\mu_j z}^{\mu_j} s_j^{\mu_j} v^x v^z \prod_{\substack{i=1 \\ i\neq j}} s_i^{\mu_i} \right] \sqrt{\det g} \varepsilon_{\mu_1...\mu_n} + \left[ \left( \nabla_{\mathbf{v}}s_j^{\mu_j} \right) \left( \nabla_{\mathbf{v}}s_k^{\mu_k} \right) \prod_{\substack{i=1 \\ i\neq j,k}} s_i^{\mu_i} \right] \sqrt{\det g} \varepsilon_{\mu_1...\mu_n} \\
  &= \left[ -R_{x\mu_j z}^{\mu_j} v^x v^z \prod_{i=1} s_i^{\mu_i} \right] \sqrt{\det g} \varepsilon_{\mu_1...\mu_n} + \left[ \left( \nabla_{\mathbf{v}}s_j^{\mu_j} \right) \left( \nabla_{\mathbf{v}}s_k^{\mu_k} \right) \prod_{\substack{i=1 \\ i\neq j,k}} s_i^{\mu_i} \right] \sqrt{\det g} \varepsilon_{\mu_1...\mu_n} \\
  &= -\underbrace{R_{xz}v^x v^z}_{\mathrm{Ric}(\mathbf{v}, \mathbf{v})} \omega (\mathbf{s}_1,...,\mathbf{s}_n) + \left[ \left( \nabla_{\mathbf{v}}s_j^{\mu_j} \right) \left( \nabla_{\mathbf{v}}s_k^{\mu_k} \right) \prod_{\substack{i=1 \\ i\neq j,k}} s_i^{\mu_i} \right] \sqrt{\det g} \varepsilon_{\mu_1...\mu_n}
\end{align*}
$$

The Ricci tensor thus measures the volume change along geodesics in curved space

## Ricci scalar

The Ricci scalar measures the amount by which the volume of a geodesic ball in curved space deviates from that of a standard ball in flat space.

$$
  R = g^{ij}R_{ij} = R_i^i
$$

## Properties

- Symmetry: $R_{ab} = R_{ba}$
- Contracted Bianchi identity: $R^{nm}_{;n} - \frac{1}{2}g^{mn}R_{;n} = G^{mn}_{;n} = 0$ where $G^{mn}$ is the Einstein tensor

The Ricci tensor is given by contractions in the second and third lower indices of the Riemann tensor

$$
  R_{aib}^i = R_{ab} \\
  R_{abi}^i = -R_{aib}^i = -R_{ab}
$$

the third possible contraction of the Riemann tensor is trivial

$$
  R_{\color{blue}{i}ab}^{\color{blue}{i}} = g^{\color{blue}{i}\color{red}{j}}R_{jiab} = -g^{\color{red}{j}\color{blue}{i}}R_{\color{blue}{i}\color{red}{j}ab} = -R_{\color{red}{j}ab}^{\color{red}{j}} = 0
$$

The symmetry of the Ricci tensor is shown from a special case of the Bianchi identity by setting $d = a$

$$
\begin{align*}
  R_{c\color{red}{a}b}^{\color{red}{a}} + R_{bc\color{red}{a}}^{\color{red}{a}} + \underbrace{R_{\color{red}{a}bc}^{\color{red}{a}}}_{=0} &= 0 \\
  R_{c\color{red}{a}b}^{\color{red}{a}} - R_{b\color{red}{a}c}^{\color{red}{a}} &= 0 \\
  R_{cb} - R_{bc} &= 0 \\
  R_{cb} &= R_{bc}
\end{align*}
$$