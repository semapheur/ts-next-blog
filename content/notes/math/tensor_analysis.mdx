---
title: 'Tensor Analysis'
subject: 'Mathematics'
showToc: true
---

# Einstein summation convetion

- Indices can not occur more twice
- Twice-repeated indices denote summation
- Free indices have the same range as summation indices

# Levi-Civita symbol

<MathBox title='Levi-Civita symbol' boxType='definition'>
In $n$-dimension, the Levi-Civita symbol is defined as

$$
\begin{align*}
  \varepsilon_{a_1 a_2\dots a_n} &:= \begin{cases}
    +1 &\textrm{ if } (a_i)_{i=1}^n \textrm{ is an even permutation of } (1, 2,\dots, n) \\
    -1 &\textrm{ if } (a_i)_{i=1}^n \textrm{ is an odd permutation of } (1, 2,\dots, n) \\
    0 &\textrm{ otherwise}
  \end{cases} \\
  &= \prod_{1\leq i < j \leq n} \mathrm{sgn}(a_j - a_i)
\end{align*}
$$
</MathBox>

## Two dimension

In two dimensions, the Levi-Civita symbol is defined as

$$
  \varepsilon_{ij} = \begin{cases}
    +1, &\quad (i, j) = (1,2) \\
    -1, &\quad (i, j) = (2,1) \\
    0, &\quad i=j
  \end{cases}
$$

**Product identity**

$$
  \varepsilon_{ij}\varepsilon^{mn} = \delta_i^m \delta_j^n - \delta_i \delta^n \delta_j^m
$$

## Three dimensions

In three dimensions, the Levi-Civita symbol is defined as

$$
  \varepsilon_{ijk} = \begin{cases}
    +1, &\quad (i, j, k) \in \{(1, 2, 3), (2, 3, 1), (3, 1, 2)\} \\
    -1, &\quad (i, j, k) \in \{(3, 2, 1), (1, 3, 2), (2, 1, 3)\} \\
    0, &\quad i=j \lor j=k \lor k=i
  \end{cases}
$$

**Product identity**

$$
\begin{align*}
  \delta_{ijklmn} = \varepsilon_{ijk}\varepsilon_{lmn} &= \begin{vmatrix} 
    \delta_{il} & \delta_{im} & \delta_{in} \\
    \delta_{jl} & \delta_{jm} & \delta_{jn} \\
    \delta_{kl} & \delta_{km} & \delta_{kn}
  \end{vmatrix} \\ 
  &= \delta_{il}(\delta_{jm}\delta_{kn} - \delta_{jn}\delta_{km}) - \delta_{im}(\delta_{jl}\delta_{kn} - \delta_{jn}\delta_{kl}) + \delta_{in}(\delta_{jl}\delta_{km} - \delta_{jm}\delta_{kl})
\end{align*}
$$

A special case of the result is

$$
  \delta_{jkmn} = \varepsilon_{ijk}\varepsilon_{imn} = \delta_{jm}\delta_{kn} - \delta_{jn}\delta_{km}
$$

# Vector space

<MathBox title='Vector space' boxType='definition'>
A vector space is a set $V$ over a field $\mathbb{F}$ equipped with the two closed operations 
- $+: V \times V \to V$ (vector addition)
- $\cdot: \mathbb{F} \times V \to V$ (scalar multiplication)
and has the following properties

- $V$ is an abelian group under vector addition, i.e. for all $\mathbf{u}, \mathbf{v}, \mathbf{w} \in V$
  - $\mathbf{u} + (\mathbf{v} + \mathbf{w}) = (\mathbf{u} + \mathbf{v}) + \mathbf{w}$ (associativity)
  - $\exists \mathbf{0} \in V : \mathbf{v} + \mathbf{0} = \mathbf{v}$ (identity element)
  - $\forall \mathbf{v} \; \exists -\mathbf{v} : \mathbf{v} + (-\mathbf{v}) = \mathbf{0}$ (additive inverse)
  - $\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$ (commutativity)
- Scalar multiplication is compatible, satisfying for all $\alpha, \beta\in\mathbf{F}$
  - $\alpha(\beta\mathbf{v}) = (\alpha\beta)\mathbf{v}$
  - $1\mathbf{v} = \mathbf{v}$
- Vector addition and scalar multiplication are related by distributivity
  - $\alpha (\mathbf{u} + \mathbf{v}) = \alpha\mathbf{u} + \alpha\mathbf{v}$
  - $(\alpha + \beta)\mathbf{v} = \alpha\mathbf{v} + \beta\mathbf{v}$

The elements of $V$ are called vectors, while the elements of $\mathbb{F}$ are called scalars.
</MathBox>

<MathBox title='Linear combination' boxType='definition'>
A linear combination of vectors $\{ \mathbf{v}_i \}_{i=1}^k \subseteq V$ for $k\in\mathbb{N}_+$ is a vector of the form

$$
  \sum_{i=1}^k \lambda_i \mathbf{v}_i,\; \lambda_i \in\mathbb{F}
$$
</MathBox>

<MathBox title='Linear subspace' boxType='definition'>
A linear subspace of an $\mathbb{F}$-vector space $V$ is a set $U\subseteq V$ that is itself an $\mathbb{F}$-vector. This means that $U$ must satisfy
- $\mathbf{0}\in U$
- $\mathbf{u}, \mathbf{v}\in U \implies \mathbf{u} + \mathbf{v} \in U$ (closed under vector addition)
- $\mathbf{u}\in U, \alpha\in\mathbb{F} \implies \alpha u \in U$ (closed under scalar multiplication)

The conditions imply that $U$ is closed under linear combinations, i.e. $\alpha u + \beta v \in U$ for all $\alpha,\beta\in\mathbb{R}$ and all $u, v \in U$.
</MathBox>

<MathBox title='Linear dependence and independence' boxType='definition'>
A set of vectors $S =\{ \mathbf{v}_i \}_{i=1}^k$ for $k\in\mathbb{N}_+$ of an $\mathbb{F}$-vector space $V$ is linearly dependent if there is a non-trivial linear combination for $\mathbf{0}\in V$. That is, there is a sequence of scalars $(\lambda_i \in\mathbb{F})_{i=1}^k$ that are not all equal to zero such that

$$
\begin{gather*}
  \sum_{i=1}^k \lambda_i \mathbf{v_i} = \mathbf{0} \\
  \iff \mathbf{v_j} = \sum_{i=1, i\neq j}^k \tilde{\lambda}_i \mathbf{v_i},\; 1 \leq j \leq k, \lambda_j \neq 0, \tilde{lambda}_i = \frac{-\lambda_i}{\lambda_j}
\end{gather*}  
$$

Equivalently, $S$ is linearly dependent if and only if one of its vectors is a linear combination of the others. The set $S$ is linearly independent if it is not linearly dependent, i.e. 

$$
  \sum_{i=1}^k \lambda_i \mathbf{v_i} = \mathbf{0} \iff \lambda_i = 0
$$
</MathBox>

<MathBox title='Span' boxType='definition'>
Given a subset $U$ of a $\mathbb{F}$-vector space $V$, the subspace spanned by $U$ is the the smallest set of all linear combinations of vectors in $U$. If $U$ is finite, i.e. for $k\in\mathbb{N}_+$ we can write $U = \{ \mathbf{u}_i \}_{i=1}^k$, the span of $U$ is the set

$$
  \mathrm{span}(U) := \left\{ \mathbf{v}\in V \mid \exists (\lambda_i \in \mathbf{F})_{i=1}^k : \mathbf{v} = \sum_{i=1}^k \lambda_i \mathbf{u}_i \right\}
$$

In particular $\mathrm{span}(\emptyset) := \{\mathbf{0}\}$.
</MathBox>

## Dual vector space

Given a vector space $V$ over a field $\mathbb{F}$, the dual space $V^*$ is defined as the set of all linear functionals (covectors) $\phi : V \to \mathbb{F}$.

The dual space $V^*$ itself becomes a vector space over $\mathbb{F}$ when equipped with the operations of addition and scalar multiplication satisfying $\forall \phi, \psi \in V^*, x \in V$ and $a \in \mathbb{F}$ 

$$
\begin{gather*}
  (\phi + \psi)(x) = \phi(x) + \psi(x) \\
  (a\phi)(x) = a(\phi(x))
\end{gather*}
$$

If $V$ is finite-dimensional, then $V^*$ has the same dimension as $V$.

## Basis

<MathBox title='Basis' boxType='definition'>
A basis $B$ of an $\mathbb{F}$-vector space $V$ is a linearly independent subset of $V$ spanning $V$. 
</MathBox>

A basis $B$ of a vector space $V$ over a field $\mathbb{F}$ is a linearly independent subset of $V$ that spans $V$.

Given a basis $B = \left\{ \mathbf{e}_i \right\}_{i \in I \subseteq \mathbb{N}}$, a new basis $\tilde{B} = \left\{ \tilde{\mathbf{e}}_i \right\}_{i \in I \subseteq \mathbb{N}}$ can be formed by transforming the old basis vectors, and vice versa

$$
\begin{align*}
  \tilde{\mathbf{e}}_i &= \sum_{j=1}^n A_i^j\mathbf{e}_j = A_i^j\mathbf{e}_j \\
  \mathbf{e}_i &= \sum_{j=1}^n \tilde{A}_i^j \tilde{\mathbf{e}}_j = \tilde{A}_i^j \tilde{\mathbf{e}}_j 
\end{align*}
$$

Basis vectors transform covariantly. The transforms are invertible (bijections) such that the compositions give the identity transform 

$$
\begin{gather*}
  A\tilde{A} = AA^{-1} = I \\
  \sum_{j=1}^n A_{ij}\tilde{A}_{ji} = \sum_{j=1}^n \tilde{A}_{ij}A_{ji} = \delta_{ij} = \begin{cases} 1, \quad i = j \\ 0, \quad i \neq j \end{cases}
\end{gather*}
$$

A vector $\mathbf{v}$ can be expressed as a linear combination of basis vectors

$$
\begin{align*}
  \mathbf{v} = \sum_{j=1}^n v_j \mathbf{e}_j = \sum_{j=1}^n v_j \left( \sum_{i=1}^n \tilde{A}_{ij} \tilde{\mathbf{e}}_i \right) = \sum_{i=1}^n \left( \sum_{j=1}^n \tilde{A}_{ij}v_j \right)\tilde{\mathbf{e}}_i = \sum_{j=1}^n \tilde{v}_j \tilde{\mathbf{e}}_j \\ 
  \mathbf{v} = \sum_{j=1}^n \tilde{v}_j \tilde{\mathbf{e}}_j = \sum_{j=1}^n \tilde{v}_j \left( \sum_{i=1}^n A_{ij} \mathbf{e}_i \right) = \sum_{i=1}^n \left( \sum_{j=1}^n A_{ij} \tilde{v}_j \right)\mathbf{e}_i = \sum_{j=1}^n v_j \mathbf{e}_j
\end{align*}
$$

Vector components are said to be contravariant as they transform inversely of the basis vectors. To signify this, vector components are denoted with superscript indices. 

$$
\begin{align*}
  v^j &= \sum_{j=1}^n A_{ij} \tilde{v}^j \\
  \tilde{v}^j &= \sum_{j=1}^n \tilde{A}_{ij}v^j
\end{align*}
$$

### Dual basis

Given a basis $B = \left\{ \mathbf{e}_i \right\}_{i \in I \subset \mathbb{N}}$ for a finite-dimensional vector space $V$, a basis $B^* = \left\{ \mathbf{e}^i \right\}_{i \in I \subset \mathbb{N}}$, called the dual basis, can be formed through the bi-orthogonality property

$$
  \mathbf{e}^i \left( \mathbf{e}_j \right) = \delta_j^i
$$

A new dual basis $\tilde{B}^*$ can be formed by transforming the old dual basis, $\tilde{\mathbf{e}}^i = \sum_{i=1}^n t_{ij}\mathbf{e}^j$. By the bi-orthogonality property

$$
\begin{align*}
  \tilde{\mathbf{e}}^i (\tilde{\mathbf{e}}_k) &= \sum_{j=1} t_{ij} \mathbf{e}^j \left( \tilde{\mathbf{e}}_k\right) \\
  &= \sum_{j=1}^n t_{ij} \mathbf{e}^j \left( \sum_{l=1}^n a_{lk} \mathbf{e}_l \right) \\
  &= \sum_{j=1}^n \sum_{l=1}^n t_{ij} a_{lk} \mathbf{e}^j (\mathbf{e}_l) \\
  &= \sum_{j=1}^n \sum_{l=1}^n t_{ij} a_{lk} \delta_j^l \\
  &= \sum_{j=1}^n \sum_{l=1}^n t_{ij} a_{jk} = \delta_k^i \quad \Rightarrow t_{ij} = \tilde{a}_{ij}
\end{align*}
$$

This shows that dual basis vectors transform covariantly, hence the superscripts

$$
\begin{gather*}
  \tilde{\mathbf{e}}^i = \sum_{j=1}^n \tilde{a}_{ij} \mathbf{e}^j \\
  \mathbf{e}^i = \sum_{j=1}^n a_{ij} \tilde{\mathbf{e}}^j
\end{gather*}
$$

A covector $\phi \in V^*$ can be represented as a linear combination of the dual basis vectors

$$
  \phi = \sum_{i=1}^n \phi_i \mathbf{e}^i = \sum_{i=1}^n \phi_i \left( a_{ij}\mathbf{e}^j \right) = \sum_{j=1}^n \left( \sum_{i=1}^n \phi_i a_{ij} \right) \tilde{\mathbf{e}}^j
$$

Covectors transform covariantly, hence the subscripts.

$$
\begin{gather*}
  \phi_j = \sum_{i=1}^n \tilde{a}_{ij}\tilde{\phi}_i \\
  \tilde{\phi}_j = \sum_{i=1}^n a_{ij}\phi_i
\end{gather*}
$$

## Linear transformation

<MathBox title='Linear transformation' boxType='definition'>
Let $V$ and $W$ be vector spaces over the same field $\mathbb{F}$. A function $T: V \to W$ is called a linear transformation if it preserve the operations of vector addition and scalar multiplication, i.e. for all $\mathbf{v}, \mathbf{w}\in V$ and $\alpha\in\mathbb{F}$
- $T(\mathbf{v} + \mathbf{w}) = T(\mathbf{v}) + T(\mathbf{w})$ 
- $T(\alpha\mathbf{v}) = T(\mathbf{v}) + T(\mathbf{w})$ for all $\mathbf{v}, \mathbf{w}\in V$

These conditions can be replaced by the single condition
$$
  T(\alpha v + \beta w) = \alpha T(v) + \beta T(\beta)
$$
for all $\alpha, \beta\in\mathbb{F}$ and $v, w\in V$.
</MathBox>

An isomorphism is a structure-preserving mapping between two vector spaces that can be reversed by an inverse mapping. That is, for a linear map $f: V \to W$ there exists an inverse map $g: W \to V$ such that the compositions $f \circ g: W \to W$ and $g \circ f: V \to V$ are identity maps.

An endomorphism is a linear map $f: V \to V$. An endomorphism that is also an isomorphism is called an automorphism.

For a vector space $V$, the outer product of its basis $B = \left\{ \mathbf{e}_i \right\}_{i \in I \subset \mathbb{N}}$ and its dual basis $B^* = \left\{ \mathbf{e}^i \right\}_{i \in I \subset \mathbb{N}}$ spans all endomorphic linear transforms. Every linear transform $L : V \to V$ can be writen as a linear combination of vector-covector pairs 

$$
  L = L_j^i \left(\mathbf{e}_i \otimes \mathbf{e}^j \right)
$$

The outer product itself forms a linear map

$$
\begin{align*}
  \mathbf{w} &= L(\mathbf{v}) = L_j^i \left(\mathbf{e}_i \otimes \mathbf{e}^j \right)(v^k \mathbf{e}_k) \\
  &= L_j^i v^k \mathbf{e}_i \mathbf{e}^j (\mathbf{e}_k) \\
  &= L_j^i v^k \mathbf{e}_i \delta_k^j \\
  &= L_j^i v^j \mathbf{e}_i
\end{align*}
$$

<MathBox title='Kernel and image' boxType='definition'>
Let $V$ and $W$ be vector spaces over the same field $\mathbb{F}$. If $T: V \to W$ is a linear transformation then

- the *kernel* (null space) of $T$ is the set $\mathrm{ker}(T) := \{ v\in V \mid T(v) = 0 \}$
- the *range* (image) of $T$ is the set $\mathrm{ran}(T) := \{ T(v) \mid v\in V\}$
</MathBox>

### Matrices

Every linear map between finite-dimensional vector spaces can be represented by a matrix. This can be shown as follows for a transform $L : V \subseteq \mathbb{F}^n \to W \subseteq \mathbb{F}^m$. For simplicity, it is assumed that the bases $B^V = \{\hat{\mathbf{v}}_i\}_{i \in I \subseteq \mathbb{N}}$ and $B^W = \{\hat{\mathbf{w}}_i\}_{i \in I \subseteq \mathbb{N}}$ are orthonormal

$$
\begin{align*}
  L(\mathbf{v}) &= L\left( \sum_{i=1}^n v^i \hat{\mathbf{v}}_i \right) = \sum_{i=1}^n v^i L(\hat{\mathbf{v}}_i) \\
  &= \sum_{i=1}^n v^i \sum_{j=1}^m \left( \sum_{k=1}^n L_k^j \hat{v}^k \right)\hat{\mathbf{w}}_j, \quad \sum_{k=1}^n L_k^j \hat{v}^k = L_k^j \delta^{ik} \\
  &= \sum_{j=1}^m \sum_{i=1}^n L_i^j v^i \hat{\mathbf{w}}_j = \sum_{j=1}^m w^j \hat{\mathbf{w}}_j = \mathbf{w}
\end{align*}    
$$

The output vector $\mathbf{w}$ is given by the matrix multiplication

$$
  w^j = \sum_{i=1}^n L_i^j v^i
$$

### Eigenvectors

Let $L$ be an endomorphic linear transformation $L: V \to V$. A non-zero vector $\mathbf{v} \neq \mathbf{0} \in V$ is an eigenvector of $T$ if

$$
  L(\mathbf{v}) = \lambda \mathbf{v}
$$

where $\lambda \in \mathbb{F}$ is the eigenvalue corresponding to $\mathbf{v}$.

The eigenvalue equation can be rearranged as

$$
  (L - \lambda \cdot \mathrm{Id})\mathbf{v} = \mathbf{0}
$$

If $V$ is a finite-dimensional vector space, the determinant of the composed transformation $L - \lambda \cdot \mathrm{Id}$ vanishes

$$
  \mathrm{det}(L - \lambda \cdot \mathrm{Id}) = 0
$$

The determinant gives a polynomial function in $\lambda$, called the characteristic polynomial of $L$.

### Linear map transform

A linear map can be transformed into a new basis through the steps
1. Transform the input vector (contravariant) from the new to the old basis through a forward transform
2. Apply the linear map to the transformed vector
3. Transform the output vector back into the new basis

This can be shown as follows for a linear map $L$

$$
\begin{align*}
  L(\tilde{\mathbf{e}}_i) &= L\left( \sum_{j=1}^n A_i^j \mathbf{e}_j \right) = \sum_{j=1}^n A_i^j L(\mathbf{e}_j) \\
  &= \sum_{j=1}^n A_i^j \sum_{k=1}^n L_j^k \mathbf{e}_k \\
  &= \sum_{j=1}^n \sum_{k=1}^n A_i^j t_j^k \sum_{l=1}^n \tilde{A}_k^l \tilde{\mathbf{e}}_l \\
  &= \sum_{l=1}^n \sum_{j=1}^n \sum_{k=1}^n \tilde{A}_k^l L_j^k A_i^j   \tilde{\mathbf{e}}_l = \sum_{l=1} \tilde{L}_i^l \tilde{\mathbf{e}}_l
\end{align*}
$$

The linear map gets transformed as follows

$$
\begin{gather*}
  \tilde{L}_i^l = \sum_{j=1}^n \sum_{k=1}^n \tilde{A}_k^l L_j^k A_i^j = \tilde{A}_k^l L_j^k A_i^j \\
\begin{aligned}
  \tilde{L} &= A^{-1}LA = \tilde{A}LA \\
  A\tilde{L}A^{-1} &= AA^{1}LAA^{-1} = L
\end{aligned}
\end{gather*}
$$

In tensor product notation, this can be derived as follows

$$
\begin{align*}
  L &= L_j^i \mathbf{e}_i \otimes \mathbf{\varepsilon}^j \\
  &= L_j^i \left(\tilde{A}_i^k \tilde{\mathbf{e}}_k \right) \otimes \left(A_l^j \tilde{\mathbf{\varepsilon}}^l \right) \\
  &= \left( \tilde{A}_i^k L_j^i A_l^j \right) \tilde{\mathbf{e}}_k \otimes \tilde{\mathbf{\varepsilon}}^l \\
  &= \tilde{L_j^i} \tilde{\mathbf{e}}_i \otimes \tilde{\mathbf{\varepsilon}}^j
\end{align*}
$$

### Rank-nullity theorem

For any matrix $M \in \mathbb{C}^{m\times n}$

$$
  \dim\left(\mathrm{Ran}(M) \right) + \dim\left(\mathrm{Ker}(M) \right) = n
$$

## Inner product (dot product)

$$
  \langle \mathbf{v}, \mathbf{w} \rangle = \mathbf{v} \cdot \mathbf{w} = \lVert \mathbf{v} \rVert \lVert \mathbf{w} \rVert \cos{\theta} = g_{ij}v^i v^j
$$

## Norm

The norm of a vector $\mathbf{v} \in V$ for a generalized basis $B = \left\{ \mathbf{e}_i \right\}_{i \in I \subset \mathbb{N}}$ is given by

$$
  \lVert \mathbf{v} \rVert^2 = \langle \mathbf{v}, \mathbf{v} \rangle = \mathbf{v}^* G \mathbf{v} = g_{ij}v^i v^j
$$

where $g_{ij}$ is the metric tensor

$$
  g_{ij} = \langle \mathbf{e}_i, \mathbf{e}_j \rangle
$$

The norm is invariant of coordinate changes

$$
\begin{align*}
  \lVert \mathbf{v} \rVert^2 &= \tilde{g}_{ij} \tilde{v}^i \tilde{v}^j \\
  &= \left( A_i^k A_j^l g_{kl} \right) \left( \tilde{A}_m^i v^m \right) \left( \tilde{A}_n^j v^n \right) \\
  &= g_{kl} v^m v^n \left( \tilde{A}_m^i A_i^k  \tilde{A}_n^j A_j^l \right) \\
  &= g_{kl} v^m v^n \left(\delta_m^k \delta_n^l \right) \\
  &= g_{ij} v^i v^j 
\end{align*}
$$

## Cross product (wedge product)
$$
\begin{gather*}
  \mathbf{a} \times \mathbf{b} = \begin{vmatrix} \mathbf{e}_1 & \mathbf{e}_2 & \mathbf{e}_3\\
  a_1 & a_2 & a_3 \\
  b_1 & b_2 & b_3 \end{vmatrix} = \varepsilon^i_{jk}\mathbf{e}_i a^j b^k  \\
  \lVert \mathbf{a} \times \mathbf{b} \rVert = \lVert \mathbf{a} \rVert \lVert \mathbf{b} \rVert \sin\theta \\
  \lVert \mathbf{a} \times \mathbf{b} \rVert^2 = \lVert \mathbf{a} \rVert^2 \lVert \mathbf{b} \rVert^2 - \left( \mathbf{a} \cdot \mathbf{b} \right)^2 = \lVert \mathbf{a} \rVert^2 \lVert \mathbf{b} \rVert^2 \left(1 - \cos^2\theta \right)
\end{gather*}
$$

Properties
- Jacobi identity
$$
  \mathbf{a} \times \left( \mathbf{b} \times \mathbf{c} \right) + \mathbf{b} \times \left( \mathbf{c} \times \mathbf{a} \right) + \mathbf{c} \times \left( \mathbf{a} \times \mathbf{b} \right) = \mathbf{0}
$$
- Scalar triple product
$$
  \mathbf{A} \cdot \left( \mathbf{B} \times \mathbf{C} \right) = \mathbf{B} \cdot \left( \mathbf{C} \times \mathbf{A} \right) = \mathbf{C} \cdot \left( \mathbf{A} \times \mathbf{B} \right)
$$

### Triple product

$$
\begin{align*}
  \left[\mathbf{a} \times \left( \mathbf{b} \times \mathbf{c} \right)\right]^i &= \varepsilon^i_{jk} a^j \varepsilon^k_{mn} b^m c^n \\ 
  &= \left(\delta^{im}\delta^{jn} - \delta^{in}\delta^{jm}\right)a^j b^m c^n \\
  &= b^i a^j c^j - c^i a^j b^j \\
  &= \left[\mathbf{b} \left(\mathbf{a} \cdot \mathbf{c} \right) - \mathbf{c}\left( \mathbf{a} \cdot \mathbf{b} \right) \right]^i
\end{align*}
$$

## Outer product (tensor product)

Given an $m$-dimensional vector $\mathbf{u}$ and an $n$-dimensional vector $\mathbf{v}$, their outer product is an $m \times n$ matrix 

$$
  \left(\mathbf{u} \otimes \mathbf{v}\right)_{ij} = \left(\mathbf{u} \mathbf{v}^{\dagger}\right)_{ij} = u_{i} v_{j}^*
$$

The outer product has the following properties $\forall \mathbf{u}, \mathbf{v} \in V$, $\psi, \phi \in V^*$  and $a \in \mathbb{F}$

- $a \left( \mathbf{v} \otimes  \right) = \left(a \mathbf{v} \right) \otimes \psi = \mathbf{v} \otimes \left(a \psi \right)$
- $\mathbf{v} \otimes \left( \psi + \phi \right) = \mathbf{v} \otimes \psi + \mathbf{v} \otimes \phi$
- $\left(\mathbf{u} + \mathbf{v} \right) \otimes \psi = \mathbf{u} \otimes \psi + \mathbf{v} \otimes \psi$

## Linear form/functional (covector)

A linear form is a linear map from a vector space to its field of scalars, $f : V \to \mathbb{F}$ (1-form).

Linear functionals are represented as row vectors in $\mathbb{R}^n$.

### Bilinear form

A bilinear form is a map $B: V \times V \to \mathbb{F}$ (2-form) that is linear in each argument separately $\forall \mathbf{u}, \mathbf{v}, \mathbf{w} \in V$ and $\lambda \in \mathbb{F}$

- $\lambda B(\mathbf{u}, \mathbf{v}) = B(\lambda \mathbf{u}, \mathbf{v}) = B(\mathbf{u}, \lambda \mathbf{u})$
- $B(\mathbf{u} + \mathbf{v}, \mathbf{w}) = B(\mathbf{u}, \mathbf{w}) + B(\mathbf{v}, \mathbf{w})$
- $B(\mathbf{u}, \mathbf{v} + \mathbf{w}) = g(\mathbf{u}, \mathbf{v}) + g(\mathbf{u}, \mathbf{w})$

Bilinear forms are formed by linear combinations of covector-covector pairs

$$
  B = B_{ij} \left( \mathbf{e}^i \otimes \mathbf{e}^j \right)
$$

With this definition a bilinear map can be expressed as follows

$$
\begin{align*}
  s &= B(\mathbf{u}, \mathbf{v}) \\
  &= B_{ij}\mathbf{e}^i \mathbf{e}^j \left(u^k \mathbf{e}_k, v^l \mathbf{e}_l \right) \\
  &= B_{ij}\mathbf{e}^i \left(u^k \mathbf{e}_k \right) \mathbf{e}^j \left( v^l \mathbf{e}_l \right) \\
  &= B_{ij} u^k v^l \mathbf{e}^i \left(\mathbf{e}_k \right) \mathbf{e}^j \left(\mathbf{e}_l \right) \\
  &= B_{ij} u^k v^l \delta_k^i \delta_l^j \\
  &= B_{ij} u^i v^j
\end{align*}
$$

Bilinear forms are transformed as follows

$$
\begin{align*}
  B &= B_{ij} \left( \mathbf{e}^i \otimes \mathbf{e}^j \right) = B_{ij} \left(A_k^i \tilde{\mathbf{e}}^k \right) \left( A_l^j \tilde{\mathbf{e}}^l \right) \\
  &= \left( A_k^i A_l^j B_{ij} \right) \tilde{\mathbf{e}}^k  \tilde{\mathbf{e}}^l \\
  &= \tilde{B}_{ij}  \tilde{\mathbf{e}}^i \tilde{\mathbf{e}}^j
\end{align*}
$$

giving the transformation rules

$$
\begin{align*}
  \tilde{B}_{ij} &= A_i^k A_j^l B_{kl} \\
  B_{ij} &= \tilde{A}_i^k \tilde{A}_j^l \tilde{B}_{kl}
\end{align*}
$$

# Tensor

<MathBox title='Tensor' boxType='definition'>
Let $V$ be finite-dimensional vector space and $V^*$ the corresponding dual vectors space. A tensor of rank $(m, n)$, with covariant rank $m$ and contravariant rank $n$, is a multilinear function $T: V^{*m} \times V^n \to\mathbb{F}$.

$$
  \bar{T}_{j_1\dots j_n}^{i_1\dots i_m} = T_{p_1\dots p_n}^{q_1\dots q_m} \frac{\partial\bar{x}^{i_1}}{\partial x^{q_1}}\cdots\frac{\partial\bar{x}^{i_m}}{\partial x^{q_m}}\frac{\partial x^{p_1}}{\partial \bar{x}^{j_1}}\dots\frac{\partial x^{q_n}}{\partial \bar{x}^{j_n}}
$$
</MathBox>

Given a vector space $V$, a tensor of type $(p, q)$, with contravariant order $p$ and covariant order $q$, is defined as an element of the tensor product of $p$ times $V$ and $q$ times its dual space $V^*$

$$
  T \in \left(\bigotimes_{i=1}^m V\right) \otimes \left(\bigotimes_{j=1}^n V^* \right) 
$$

The components of $T$ are coefficients determined by its basis, which is formed by the tensor product of the basis $\{\mathbf{e}_i \}$ for $V$ and its dual basis $\{ \mathbf{\varepsilon}^j \}$ for $V^*$

$$
  T = T_{j_1...j_q}^{i_1...i_p} \mathbf{e}_{i_1} \otimes ... \otimes \mathbf{e}_{i_p} \otimes \mathbf{\varepsilon}^{j_1} \otimes ... \otimes \mathbf{\varepsilon}^{j_q}
$$

When transforming a tensor under a change of basis, its contravariant components are backward transformed ($T_b = \tilde{A}$), while its covariant components are forward transformed ($T_f = A$)

$$
\begin{align}
  T &= T_{j_1...j_q}^{i_1...i_p} \mathbf{e}_{i_1} \otimes ... \otimes \mathbf{e}_{i_p} \otimes \mathbf{\varepsilon}^{j_1} \otimes ... \otimes \mathbf{\varepsilon}^{j_q} \\
  &= T_{j_1...j_q}^{i_1...i_p} \left( \tilde{A}_{i_1}^{i'_1} \tilde{\mathbf{e}}_{i'_1} \right) \otimes ... \otimes \left( \tilde{A}_{i_p}^{i'_p} \tilde{\mathbf{e}}_{i'_p} \right) \otimes \left( A_{j'_1}^{j_1} \tilde{\mathbf{\epsilon}}^{j'_1} \right) \otimes ... \otimes \left( A_{j'_q}^{j_q} \tilde{\mathbf{\epsilon}}^{j'_q} \right) \\
  &= \left(\tilde{A}_{i_1}^{i'_1}...\tilde{A}_{i_p}^{i'_p}T_{j_1...j_q}^{i_1...i_p} A_{j'_1}^{j_1}...A_{j'_q}^{j_q} \right) \tilde{\mathbf{e}}_{i'_1} \otimes ... \otimes \tilde{\mathbf{e}}_{i'_p} \otimes \tilde{\mathbf{\varepsilon}}^{j'_1} \otimes ... \otimes \tilde{\mathbf{\varepsilon}}^{j'_q} \\
  &= \tilde{T}_{j_1...j_q}^{i_1...i_p} \tilde{\mathbf{e}}_{i_1} \otimes ... \otimes \tilde{\mathbf{e}}_{i_p} \otimes \tilde{\mathbf{\varepsilon}}^{j_1} \otimes ... \otimes \tilde{\mathbf{\varepsilon}}^{j_q}
\end{align}
$$

giving the transformation rules

$$
\begin{align*}
  \tilde{T}_{j_1...j_q}^{i_1...i_p} &= \tilde{A}_{i'_1}^{i_1}...\tilde{A}_{i'_p}^{i_p} T_{j'_1...j'_q}^{i'_1...i'_p} A_{j_1}^{j'_1}...A_{j_q}^{j'_q} \\
  T_{j_1...j_q}^{i_1...i_p} &= A_{i'_1}^{i_1}...A_{i'_p}^{i_p}  \tilde{T}_{j'_1...j'_q}^{i'_1...i'_p} \tilde{A}_{j_1}^{j'_1}...\tilde{A}_{j_q}^{j'_q}
\end{align*}
$$

## Tensor operations

### Kronecker product

If $\mathbf{A}$ is an $m\times n$ matrix and $\mathbf{B}$ is a $p\times q$ matrix, then the Kronecker product $\mathbf{A}\otimes\mathbf{B}$ is the $pm\times qn$ block matrix 

$$
\begin{align*}
  \mathbf{A}\otimes\mathbf{B} =& \begin{bmatrix} 
    a_{11}\mathbf{B} & \cdots & a_{1n} \mathbf{B} \\
    \vdots & \ddots & \vdots \\
    a_{1m}\mathbf{B} & \cdots & a_{mn}\mathbf{B}
  \end{bmatrix} \\
  =& \left[\begin{array}{c|c|c} 
    \begin{matrix}
      a_{11}b_{11} & \cdots & a_{11}b_{1q} \\
      \vdots & \ddots & \vdots \\
      a_{11}b_{p1} & \cdots & a_{11}b_{pq}
    \end{matrix} & \cdots & \begin{matrix}
      a_{1n}b_{11} & \cdots & a_{1n}b_{1q} \\
      \vdots & \ddots & \vdots \\
      a_{1n}b_{p1} & \cdots & a_{1n}b_{pq}
    \end{matrix} \\ \hline
    \vdots & \ddots & \vdots \\ \hline
    \begin{matrix}
      a_{m1}b_{11} & \cdots & a_{n1}b_{1q} \\
      \vdots & \ddots & \vdots \\
      a_{m1}b_{p1} & \cdots & a_{m1}b_{pq}
    \end{matrix} & \cdots & \begin{matrix}
      a_{mn}b_{11} & \cdots & a_{mn}b_{1q} \\
      \vdots & \ddots & \vdots \\
      a_{mn}b_{p1} & \cdots & a_{mn}b_{pq}
    \end{matrix}
  \end{array} \right]
\end{align*}
$$

## Metric tensor

The metric tensor is a bilinear form $g : V \times V \to \mathbb{R}$ given by

$$
  g(\mathbf{v}, \mathbf{w}) \to g_{ij} v^i w^j 
$$

with the properties
- Symmetric: $g(\mathbf{u}, \mathbf{v}) = g(\mathbf{v}, \mathbf{u})$
- Positive definitive: $g(\mathbf{v}, \mathbf{v}) = \lVert v \rVert^2 \geq 0 $
- Linear in both arguments separately
  - $\lambda g(\mathbf{u}, \mathbf{v}) = g(\lambda \mathbf{u}, \mathbf{v}) = g(\mathbf{u}, \lambda \mathbf{v})$
  - $g(\mathbf{t} + \mathbf{u}, \mathbf{v} + \mathbf{w}) = g(\mathbf{t}, \mathbf{v} + \mathbf{w}) + g(\mathbf{u}, \mathbf{v} + \mathbf{w}) = g(\mathbf{t}, \mathbf{v}) + g(\mathbf{t}, \mathbf{w}) + g(\mathbf{u}, \mathbf{v}) + g(\mathbf{u}, \mathbf{w})$

The metric tensor gets transformed as follows

$$
  \tilde{g}_{ij} = \langle \tilde{\mathbf{e}}_i, \tilde{\mathbf{e}}_j \rangle = \langle A_i^k \mathbf{e}_k, A_j^l \mathbf{e}_l \rangle = A_i^k A_j^l \langle \mathbf{e}_k, \mathbf{e}_l \rangle = A_i^k A_j^l g_{kl}
$$

The dual pair $\nu$ of a vector $\mathbf{v} \in V$ is given by 

$$
\begin{align*}
  \flat \mathbf{v} = \mathbf{v} \cdot \_ &= g(\mathbf{v}, \_) = g_{ik} \mathbf{\varepsilon}^i \mathbf{\varepsilon}^k \left( v^j \mathbf{e}_j \right) \\
  &= g_{ik} v^j \mathbf{\varepsilon}^i \mathbf{\varepsilon}^k \left( \mathbf{e}_j \right) \\
  &= g_{ik} v^j \mathbf{\varepsilon}^i \delta_j^k \\
  &= g_{ij}v^j \mathbf{\varepsilon}^i \\
  &= \nu_i \mathbf{\varepsilon}^i
\end{align*}
$$

The metric tensor can thus act as a map $g : V \to V*$ with the effect of lowering indices 

$$
  \nu_i = g_{ij} v^j
$$

An inverse metric tensor $\mathfrak{g} \in V \otimes V$ can be defined such that

$$
  \mathfrak{g}^{ki}g_{ij} = \delta_j^k
$$

The inverse metric tensor (contravariant) can act as a map $\mathfrak{g}: V^* \to V$ connecting a covector $\nu \in V^*$ to its vector pair

$$
  \#\nu = \mathfrak{g}(\nu, \_)
$$

with the effect of raising indices

$$
\begin{align*}
  \mathfrak{g}^{ki} \nu_i &= \mathfrak{g}^{ki} g_{ij} v^j \\
  \mathfrak{g}^{ki} \nu_i &= \delta_j^k v^j \\
  \mathfrak{g}^{ki} \nu_i &= v^k
\end{align*}
$$

# Lie bracket (commutator)

The Lie bracket measures the separation of vector field flow curves. A flow curve (integral curve) is a curve that is tangent to all vectors in a vector field. 

$$
\begin{align*}
  \left[ \mathbf{u}, \mathbf{v} \right] &= \mathbf{u}(\mathbf{v}) - \mathbf{v}(\mathbf{u}) \\
  &= u^i \partial_i \left( v^j \boldsymbol{\partial}_j \right) - v^i \partial_i \left( u^j \boldsymbol{\partial}_j \right) \\
  &= u^i \left[ \left( \partial_i v^j \right)\boldsymbol{\partial}_j + v^j \left( \partial_i \boldsymbol{\partial}_j \right) \right] - v^i \left[ \left( \partial_i u^j \right)\boldsymbol{\partial}_j + u^j \left( \partial_i \boldsymbol{\partial}_j \right) \right]
\end{align*}
$$

The Lie bracket of basis vectors always vanishes as the basis vector flow curves (coordinate lines) always close.

The Lie bracket is not linear for each input

$$
\begin{align*}
  \left[ \alpha \mathbf{u}, \mathbf{v} \right] &= \alpha \mathbf{u}\left( \mathbf{v} \right) - \mathbf{v}\left(\alpha \mathbf{u} \right) \\
  &= \alpha \mathbf{u}\left( \mathbf{v} \right) - \mathbf{v}\left( \alpha \right) - \alpha \mathbf{v}\left( \mathbf{u} \right) \\
  &= \alpha \left[ \mathbf{u}\left( \mathbf{v} \right) - \mathbf{v}\left( \mathbf{u} \right)  \right] - \mathbf{v}\left( \alpha \right)\mathbf{u} \\
  &= \alpha \left[ \mathbf{u}, \mathbf{v} \right] - \mathbf{v}\left( \alpha \right)\mathbf{u}
\end{align*}
$$

# Geodesics

The geodesic is the straightest possible path in a curved surface/space. In curved space, a straight path has zero tangential acceleration when we travel along it at constant speed, meaning that the accelaration vector is normal to the surface.

The acceleration along a path $\mathbf{R}(t)$ on a surface is given by the second order derivative in terms of the tangential basis.

$$
\begin{align*}
  \frac{\mathrm{d}^2\mathbf{R}}{\mathrm{d}t^2} &= \frac{\mathrm{d}}{\mathrm{d}t} \left( \frac{\mathrm{d}\mathbf{R}}{\mathrm{d}t} \right) \\
  &= \frac{\mathrm{d}}{\mathrm{d}t} \left( \frac{\mathrm{d}x^i}{\mathrm{d}t} \frac{\partial \mathbf{R}}{\partial x^i} \right) \\
  &= \frac{\mathrm{d}^2x^i}{\mathrm{d}t^2} \frac{\partial \mathbf{R}}{\partial x^i} + \frac{\mathrm{d}x^i}{\mathrm{d}t} \left( \frac{\mathrm{d}}{\mathrm{d}t} \frac{\partial \mathbf{R}}{\partial x^i} \right) \\
  &= \frac{\mathrm{d}^2x^i}{\mathrm{d}t^2} \frac{\partial \mathbf{R}}{\partial x^i} + \frac{\mathrm{d}x^i}{\mathrm{d}t} \left[ \left( \frac{\mathrm{d}x^j}{\mathrm{d}t} \frac{\partial}{\partial x^j} \right) \frac{\partial \mathbf{R}}{\partial x^i} \right] \\
  &= \underbrace{\frac{\mathrm{d}^2x^i}{\mathrm{d}t^2} \frac{\partial \mathbf{R}}{\partial x^i}}_{\mathrm{tangential}} + \frac{\mathrm{d}x^i}{\mathrm{d}t} \frac{\mathrm{d}x^j}{\mathrm{d}t} \frac{\partial^2 \mathbf{R}}{\partial x^i \partial x^j}
\end{align*}
$$

The second term has both tangential and normal component, of which it can be expanded into

$$
  \frac{\partial^2 \mathbf{R}}{\partial x^i \partial x^j} = \Gamma^k_{ij} \frac{\partial \mathbf{R}}{\partial x^k} + L_{ij} \mathbf{\hat{n}}
$$

where 
- $L_{ij}$ is the second fundamental form giving the normal components.
- $\Gamma_{ij}^k$ are Christoffel symbols giving the tangential components.

The normal vector $\mathbf{\hat{n}}$ is given by the cross product 

$$
  \frac{ \frac{\partial \mathbf{R}}{\partial x^i} \times \frac{\partial \mathbf{R}}{\partial x^j}}{\left\lVert \frac{\partial \mathbf{R}}{\partial x^i} \times \frac{\partial \mathbf{R}}{\partial x^j} \right\rVert}
$$

The Christoffel symbols is derived from the dot product

$$
\begin{align*}
  \frac{\partial^2 \mathbf{R}}{\partial x^i \partial x^j} \cdot \frac{\partial \mathbf{R}}{\partial x^l} &= \left( \Gamma^k_{ij} \frac{\partial \mathbf{R}}{\partial x^k} + L_{ij} \mathbf{\hat{n}} \right) \cdot \frac{\partial \mathbf{R}}{\partial x^l} \\
  &= \Gamma^k_{ij} \frac{\partial \mathbf{R}}{\partial x^k} \cdot \frac{\partial \mathbf{R}}{\partial x^l} = \Gamma^k_{ij} g_{kl}
\end{align*}
$$

By applying the inverse metric tensor, we get

$$
  \Gamma^m_{ij} = \frac{\partial^2 \mathbf{R}}{\partial x^i \partial x^j} \cdot \frac{\partial \mathbf{R}}{\partial x^l} \mathfrak{g}^{lm}
$$

The second fundamental form is derived from the dot product

$$
\begin{align*}
  \frac{\partial^2 \mathbf{R}}{\partial x^i \partial x^j} \cdot \mathbf{\hat{n}} &= \left( \Gamma^k_{ij} \frac{\partial \mathbf{R}}{\partial x^k} + L_{ij} \mathbf{\hat{n}} \right) \cdot \mathbf{\hat{n}} \\
  L_{ij} &= \frac{\partial^2 \mathbf{R}}{\partial x^i \partial x^j} \cdot \mathbf{\hat{n}}
\end{align*}
$$

The second derivative of the path $\mathbf{R}(t)$ can thus be decomposed into

$$
  \frac{\mathrm{d}^2\mathbf{R}}{\mathrm{d}t^2} = \underbrace{\left( \frac{\mathrm{d}^2x^k}{\mathrm{d}t^2} + \Gamma^k_{ij} \frac{\mathrm{d}x^i}{\mathrm{d}t} \frac{\mathrm{d}x^j}{\mathrm{d}t} \right) \frac{\partial \mathbf{R}}{\partial{x^k}}}_{\mathrm{tangential}} + \underbrace{L_{ij} \frac{\mathrm{d}x^i}{\mathrm{d}t}\frac{\mathrm{d}x^j}{\mathrm{d}t} \mathbf{\hat{n}}}_{\mathrm{normal}}
$$

## Geodesic equation

A geodesic curve has vanishing tangential acceleration, giving the geodesic equation

$$
  \frac{\mathrm{d}^2x^k}{\mathrm{d}t^2} + \Gamma^k_{ij} \frac{\mathrm{d}x^i}{\mathrm{d}t} \frac{\mathrm{d}x^j}{\mathrm{d}t} = 0
$$

# Covariant derivative

The covariant derivative operator provides an affine connection between tangent spaces in a curved space, through parallel transport of a vector field along a smooth curve on a surface.

The covariant derivative operator has the following properties
- Linearity in direction vector input:
$$
\begin{align*}
  \nabla_{\left(\alpha\boldsymbol{\partial}_1 + \beta\boldsymbol{\partial}_2 \right)} \mathbf{v} &= \left( \alpha\partial_1 + \beta\partial_2 \right) \mathbf{v} \\
  &= \alpha\left(\partial_1\mathbf{v}\right) + \beta\left( \partial_2  \mathbf{v} \right) \\
  &= \alpha \nabla_{\boldsymbol{\partial}_1} \mathbf{v} + \beta \nabla_{\boldsymbol{\partial}_2} \mathbf{v}
\end{align*}
$$

- Additive in vector field input
$$
\begin{align*}
  \nabla_{\boldsymbol{\partial}_i} \left( \mathbf{u} + \mathbf{v} \right) &= \partial_i \left( \mathbf{u} + \mathbf{v} \right) \\
  &= \partial_i \mathbf{u} + \partial_i \mathbf{v} \\
  &= \nabla_{\boldsymbol{\partial}_i} \mathbf{u} + \nabla_{\boldsymbol{\partial}_i} \mathbf{v}
\end{align*}
$$

- Product rule (Leibniz rule)
$$
\begin{align*}
  \nabla_{\boldsymbol{\partial}_i} \left( \alpha \mathbf{v} \right) &= \partial_i \left( \alpha \mathbf{v} \right) \\
  &= \left(\partial_i \alpha \right)\mathbf{v} + \alpha \left( \partial_i \mathbf{v} \right) \\
  &= \left( \nabla_{\boldsymbol{\partial}_i} \alpha \right)\mathbf{v} + \alpha\left( \nabla_{\boldsymbol{\partial}_i} \mathbf{v} \right)
\end{align*}
$$

- The covariant derivative of a scalar field equals the directional derivative
$$
  \nabla_{\boldsymbol{\partial}_i} \alpha = \frac{\partial \alpha}{\partial x^i}
$$

The Christoffel symbols (connection coefficients) themselves are covarant derivaties

$$
  \nabla_{\boldsymbol{\partial}_i} \boldsymbol{\partial}_j = \Gamma_{ij}^k \boldsymbol{\partial}_k 
$$

A general formula for the covariant derivative is given as follows

$$
\begin{align*}
  \nabla_{\mathbf{u}} \mathbf{v} &= \nabla_{u_i \partial_i} \left( v^j \boldsymbol{\partial}_j \right) \\
  &= u^i \nabla_{\partial_i} \left( v^j \boldsymbol{\partial}_j \right) \\
  &= u^i \left[ \nabla_{\partial_i} \left(v^j \right) \boldsymbol{\partial}_j + v^j \nabla_{\partial_i} \boldsymbol{\partial}_j \right] \\
  &= u^i \left[ \left(\partial_i v^k \right) \boldsymbol{\partial}_k + v^j \Gamma_{ij}^k \boldsymbol{\partial}_k \right] \\
  &= u^i \left( \partial_i v^k + v^j \Gamma_{ij}^k \right) \boldsymbol{\partial}_k
\end{align*}
$$

## Fundamental theorem of Riemannian geometry
There is a unique covariant derivative, called the Levi-Civita connection, that is torsion free and has metric compatibility:

- Torsion-free (vanishing Lie bracket)
$$
  \nabla_{\boldsymbol{\partial}_i} \boldsymbol{\partial}_j - \nabla_{\boldsymbol{\partial}_j} \boldsymbol{\partial}_i = \partial_i \partial_j - \partial_j \partial_i \equiv \left[ \boldsymbol{\partial}_i, \boldsymbol{\partial}_j \right] = 0
$$

- Metric compatibility (product rule of vector field dot product)
$$
  \nabla_{\boldsymbol{\partial}_i} \left( \mathbf{u} \cdot \mathbf{v} \right) = \left( \nabla_{\boldsymbol{\partial}_i} \mathbf{u} \right) \cdot \mathbf{v} + \mathbf{u} \cdot \left( \nabla_{\boldsymbol{\partial}_i} \mathbf{v} \right) = 0
$$

The torsion-free property implies symmetry in the lower indices of the Christoffel symbols
$$
\begin{gather*}
  \nabla_{\boldsymbol{\partial}_i} \boldsymbol{\partial}_j = \nabla_{\boldsymbol{\partial}_j} \boldsymbol{\partial}_i \\
  \Gamma_{ij}^k \boldsymbol{\partial}_k = \Gamma_{ji}^k \boldsymbol{\partial}_k \\
  \Gamma_{ij}^k = \Gamma_{ji}^k
\end{gather*}
$$

The metric compatibility property gives the following

$$
\begin{align*}
  \nabla_{\boldsymbol{\partial}_k} \left( \boldsymbol{\partial}_i \cdot \boldsymbol{\partial}_j \right) &= \partial_k \left(\boldsymbol{\partial}_i \cdot \boldsymbol{\partial}_j  \right) = \left( \nabla_{\boldsymbol{\partial}_k} \boldsymbol{\partial}_i \right) \cdot \boldsymbol{\partial}_j + \boldsymbol{\partial}_i \cdot \left( \nabla_{\boldsymbol{\partial}_k} \boldsymbol{\partial}_j \right) \\
  &= \Gamma_{ik}^l \left( \boldsymbol{\partial}_l \cdot \boldsymbol{\partial}_j \right) + \Gamma_{jk}^l \left( \boldsymbol{\partial}_i \cdot \boldsymbol{\partial}_l \right) \\
  \partial_k g_{ij} &= \Gamma_{ik}^l g_{jl} + \Gamma_{jk}^l g_{il}
\end{align*}
$$

The two properties give equal Christoffel terms by permuting indices (equal terms in matching colors)

$$
\begin{align*}
  \partial_k g_{ij} &= \color{blue}{\Gamma_{ik}^l g_{jl}} + \color{red}{\Gamma_{jk}^l g_{il}} \\
  \partial_j g_{ki} &= \color{red}{\Gamma_{kj}^l g_{il}} + \color{green}{\Gamma_{ij}^l g_{kl}} \\
  \partial_i g_{jk} &= \color{green}{\Gamma_{ji}^l g_{kl}} + \color{blue}{\Gamma_{ki}^l g_{jl}}
\end{align*}
$$

Combining the equations and rearranging, a formula for the Christoffel symbol is derived

$$
\begin{gather*}
  \Gamma_{jk}^l g_{li} + \Gamma_{kj}^l g_{li} = \partial_k g_{ij} + \partial_j g_{ki} - \partial_i g_{jk} \\
  2\Gamma_{jk}^l g_{li}\mathfrak{g}^{im} = \mathfrak{g}^{im} \left( \partial_k g_{ij} + \partial_j g_{ki} - \partial_i g_{jk} \right) \\
  \Gamma_{jk}^l \delta_l^m = \frac{1}{2} \mathfrak{g}^{im} \left( \partial_k g_{ij} + \partial_j g_{ki} - \partial_i g_{jk} \right) \\
  \Gamma_{jk}^m = \frac{1}{2} \mathfrak{g}^{im} \left( \partial_k g_{ij} + \partial_j g_{ki} - \partial_i g_{jk} \right)
\end{gather*}
$$

## Flat-space definition

In flat space, the covariant derivative of a vector field equals the ordrinary derivative, differentiating both the vector components and the basis vectors

$$
\begin{align*}
  \frac{\partial \mathbf{v}}{\partial x^i} &= \frac{\partial}{\partial x^i} \left( v^j \frac{\partial \mathbf{v}}{\partial x^j} \right) \\
  &= \frac{\partial v^j}{\partial x^i} \frac{\partial \mathbf{v}}{\partial x^j} + v^j \frac{\partial}{\partial x^i} \left( \frac{\partial \mathbf{v}}{\partial x^j}  \right) \\
  &= \left( \frac{\partial v^j}{\partial x^i} + v^j \Gamma_{ij}^k \right) \frac{\partial \mathbf{v}}{\partial x^k} 
\end{align*}
$$

## Extrinsic definition (parallel transport)

Parallel transport moves a vector along a smooth curve on a surface. The vector rate of change is normal to the surface.

The covariant derivative $\nabla_{\mathbf{w}}\mathbf{v}$ is the rate of change of a vector field $\mathbf{v}$ in a direction $\mathbf{w}$ with the normal component $\mathbf{n}$ subtracted. When the covariant derivative vanishes, the vector $\mathbf{v}$ is parallel transported in the direction $\mathbf{w}$.

$$
  \nabla_{\frac{\mathrm{d}}{\mathrm{d}t}}\mathbf{v} = \frac{\mathrm{d}\mathbf{v}}{\mathrm{d}t} - \mathbf{n}
$$

For a tangent vector field we get

$$
\begin{align*}
  \nabla_{\boldsymbol{\partial}_i}\mathbf{v} &= \frac{\partial \mathbf{v}}{\partial x^i} - \mathbf{n} = \frac{\partial}{\partial x^i} \left( v^j \frac{\partial \mathbf{v}}{\partial x^j} \right) - \mathbf{n} \\
  &= \frac{\partial v^j}{\partial x^i} \frac{\partial \mathbf{v}}{\partial x^j} + v^j \frac{\partial}{\partial x^i} \left( \frac{\partial \mathbf{v}}{\partial x^j}  \right) - \mathbf{n} \\
  &= \left[ \frac{\partial v^k}{\partial x^i} \frac{\partial \mathbf{v}}{\partial x^k} + v^j \left(\Gamma_{ij}^k \frac{\partial \mathbf{v}}{\partial x^k} + L_{ij}\hat{\mathbf{n}} \right) \right] - \mathbf{n} \\
  &= \left[ \frac{\partial v^k}{\partial x^i} + v^j \Gamma_{ij}^k  \right]\frac{\partial \mathbf{v}}{\partial x^k} + \underbrace{v^j L_{ij}\mathbf{\hat{n}}}_{=\mathbf{n}} - \mathbf{n} \\
  &= \left[ \frac{\partial v^k}{\partial x^i} + v^j \Gamma_{ij}^k  \right]\frac{\partial \mathbf{v}}{\partial x^k}
\end{align*}
$$

## Intrinsic definition

In intrinsic geometry, curves cannot be described by position and normal vectors with respect to an origin. The covariant derivative is given by


$$
\begin{align*}
  \nabla_{\boldsymbol{\partial}_i}\mathbf{v} &= \frac{\partial \mathbf{v}}{\partial x^i} \\
  &= \frac{\partial}{\partial x^i} \left( v^j \boldsymbol{\partial}_j \right) \\
  &= \frac{\partial v^j}{\partial x^i} \boldsymbol{\partial}_j + v^j \frac{\partial}{\partial x^i} \left( \boldsymbol{\partial}_j \right) \\
  &= \frac{\partial v^k}{\partial x^i} \boldsymbol{\partial}_k + v^j \Gamma_{ij}^k \boldsymbol{\partial}_k \\
  &= \left( \frac{\partial v^k}{\partial x^i} + v^j \Gamma_{ij}^k  \right) \boldsymbol{\partial}_k
\end{align*}
$$

The Christoffel symbols are derived from partial derivatives of the metric tensor

$$
\begin{align*}
  \frac{\partial}{\partial x^k} g_{ij} &= \frac{\partial}{\partial x^k} \left( \boldsymbol{\partial}_i \cdot \boldsymbol{\partial}_j \right) \\
  &= \frac{\partial}{\partial x^k} \left( \boldsymbol{\partial}_i \right) \cdot \boldsymbol{\partial}_j + \boldsymbol{\partial}_i \cdot \frac{\partial}{\partial x^k} \left( \boldsymbol{\partial}_j \right) \\
  &= \left( \Gamma_{ik}^l \boldsymbol{\partial}_l \right) \cdot \boldsymbol{\partial}_j + \boldsymbol{\partial}_i \cdot \left( \Gamma_{jk}^l \boldsymbol{\partial}_l \right) \\
  &= \Gamma_{ik}^l \left( \boldsymbol{\partial}_l \cdot \boldsymbol{\partial}_j \right) + \Gamma_{jk}^l \left( \boldsymbol{\partial}_i \cdot \boldsymbol{\partial}_l \right) \\
  &= \Gamma_{ik}^l g_{lj} + \Gamma_{jk}^l g_{il} \\
  &= \Gamma_{ik}^l g_{jl} + \Gamma_{jk}^l g_{il}
\end{align*}
$$

Permuting the indices gives the following set of PDEs with equal terms matching in color

$$
\begin{align*}
  \frac{\partial g_{ij}}{\partial x^k} &= \color{blue}{\Gamma_{ik}^l g_{jl}} + \color{red}{\Gamma_{jk}^l g_{il}} \\
  \frac{\partial g_{ki}}{\partial x^j} &= \color{red}{\Gamma_{kj}^l g_{il}} + \color{green}{\Gamma_{ij}^l g_{kl}} \\
  \frac{\partial g_{jk}}{\partial x^i} &= \color{green}{\Gamma_{ji}^l g_{kl}} + \color{blue}{\Gamma_{ki}^l g_{jl}}
\end{align*}
$$

The Christoffel symbols can be isolated with the following PDE combination

$$
\begin{gather*}
  \Gamma_{jk}^l g_{li} + \Gamma_{kj}^l g_{li} = \frac{\partial g_{ij}}{\partial x^k} + \frac{\partial g_{ki}}{\partial x^j} - \frac{\partial g_{jk}}{\partial x^i} \\
  2\Gamma_{jk}^l g_{li}\mathfrak{g}^{im} = \mathfrak{g}^{im} \left( \frac{\partial g_{ij}}{\partial x^k} + \frac{\partial g_{ki}}{\partial x^j} - \frac{\partial g_{jk}}{\partial x^i} \right) \\
  \Gamma_{jk}^l \delta_l^m = \frac{1}{2} \mathfrak{g}^{im} \left( \frac{\partial g_{ij}}{\partial x^k} + \frac{\partial g_{ki}}{\partial x^j} - \frac{\partial g_{jk}}{\partial x^i} \right) \\
  \Gamma_{jk}^m = \frac{1}{2} \mathfrak{g}^{im} \left( \frac{\partial g_{ij}}{\partial x^k} + \frac{\partial g_{ki}}{\partial x^j} - \frac{\partial g_{jk}}{\partial x^i} \right)
\end{gather*}
$$

By the symmetry $\frac{\partial^2}{\partial x^j \partial x^k} = \frac{\partial^2}{\partial x^k \partial x^j}$ we get

$$
  \frac{\partial g_{ij}}{\partial x^k} + \frac{\partial g_{ki}}{\partial x^j} - \frac{\partial g_{jk}}{\partial x^i} = 2 \left(\frac{\partial}{\partial x^i} \cdot \frac{\partial^2}{\partial x^j \partial x^k} \right)
$$

Hence, the extrinsic formula for the Christoffel symbols is retrieved from the intrinsic formula

$$
\begin{align*}
  \Gamma_{jk}^m &= \frac{1}{2} \mathfrak{g}^{im} \left[ 2 \left( \frac{\partial}{\partial x^i} \cdot \frac{\partial^2}{\partial x^j \partial x^k} \right) \right] \\
  &= \frac{\partial}{\partial x^i} \cdot \frac{\partial^2}{\partial x^j \partial x^k} \mathfrak{g}^{im}
\end{align*}
$$

## Geodesics

The covariant derivative gives an alternative definition of a geodesic as a curve resulting from parallel transporting a vector along itself

$$
  \nabla_{\mathbf{v}}\mathbf{v} = \mathbf{0}
$$

The godesic equation can be retrieved from the covariant derivative of a parametrized curve along itself

$$
\begin{align*}
  \nabla_{\frac{\mathrm{d}}{\mathrm{d}\lambda}} \frac{\mathrm{d}}{\mathrm{d}\lambda} &= \frac{\mathrm{d}}{\mathrm{d}\lambda} \left( \frac{\mathrm{d}}{\mathrm{d}\lambda} \right) \\
  &= \frac{\mathrm{d}}{\mathrm{d}\lambda} \left( \frac{\mathrm{d}u^j}{\mathrm{d}\lambda} \boldsymbol{\partial}_j \right) \\
  &= \frac{\mathrm{d}}{\mathrm{d}\lambda} \left( \frac{\mathrm{d}u^j}{\mathrm{d}\lambda} \right) \boldsymbol{\partial}_j + \frac{\mathrm{d}u^j}{\mathrm{d}\lambda} \frac{\mathrm{d}}{\mathrm{d}\lambda} \boldsymbol{\partial}_j \\
  &= \frac{\mathrm{d}^2 u^j}{\mathrm{d}\lambda^2} \boldsymbol{\partial}_j + \frac{\mathrm{d}u^j}{\mathrm{d}\lambda} \left( \frac{\mathrm{d}u^i}{\mathrm{d}\lambda} \boldsymbol{\partial}_i  \right) \boldsymbol{\partial}_j \\
  &= \frac{\mathrm{d}^2 u^j}{\mathrm{d}\lambda^2} \boldsymbol{\partial}_j + \frac{\mathrm{d}u^i}{\mathrm{d}\lambda} \frac{\mathrm{d}u^j}{\mathrm{d}\lambda} \nabla_{\boldsymbol{\partial}_i} \boldsymbol{\partial}_j \\
  &= \frac{\mathrm{d}^2 u^j}{\mathrm{d}\lambda^2} \boldsymbol{\partial}_j + \frac{\mathrm{d}u^i}{\mathrm{d}\lambda} \frac{\mathrm{d}u^j}{\mathrm{d}\lambda} \Gamma_{ij}^k \boldsymbol{\partial}_k \\
  &= \left( \frac{\mathrm{d}^2 u^k}{\mathrm{d}\lambda^2} + \frac{\mathrm{d}u^i}{\mathrm{d}\lambda} \frac{\mathrm{d}u^j}{\mathrm{d}\lambda} \Gamma_{ij}^k \right) \boldsymbol{\partial}_k
\end{align*}
$$

Hence the curve is geodesic if $\nabla_{\frac{\mathrm{d}}{\mathrm{d}\lambda}} \frac{\mathrm{d}}{\mathrm{d}\lambda} = \mathbf{0}$

# Torsion tensor

The torsion tensor measures the separation between parallel-transported vector fields

$$
\begin{align*}
  T\left(\mathbf{u}, \mathbf{v}\right) &= \nabla_{\mathbf{u}}\mathbf{v} - \nabla_{\mathbf{v}}\mathbf{u} - \left[ \mathbf{u}, \mathbf{v} \right] \\
  &= u^i \left( \partial_i v^k + v^j \Gamma_{ij}^k \right) \boldsymbol{\partial}_k - v^i \left( \partial_i u^k + u^j \Gamma_{ij}^k \right) \boldsymbol{\partial}_k - \left[ u^i \partial_i \left( v^j \boldsymbol{\partial}_j \right) - v^i \partial_i \left( u^j \boldsymbol{\partial}_j \right) \right] \\
  &= u^i \left( \partial_i v^k + v^j \Gamma_{ij}^k \right) \boldsymbol{\partial}_k - v^i \left( \partial_i u^k + u^j \Gamma_{ij}^k \right) \boldsymbol{\partial}_k - u^i \left[ \left( \partial_i v^k \right)\boldsymbol{\partial}_k + v^j \left( \partial_i \boldsymbol{\partial}_j \right) \right] + v^i \left[ \left( \partial_i u^k \right)\boldsymbol{\partial}_k + u^j \left( \partial_i \boldsymbol{\partial}_j \right) \right] \\
  &= u^i v^j \Gamma_{ij}^k \boldsymbol{\partial}_k - v^i u^j \Gamma_{ij}^k \boldsymbol{\partial}_k = u^i v^j \Gamma_{ji}^k \boldsymbol{\partial}_k - v^j u^i \Gamma_{ji}^k \boldsymbol{\partial}_k \\
  &= u^i v^j \left(\Gamma_{ij}^k - \Gamma_{ji}^k \right) \boldsymbol{\partial}_k = u^i v^j T_{ij}^k \boldsymbol{\partial}_k
\end{align*}
$$

An affine connection given by a covariant derivative is called torsion-free if the torsion tensor vanishes, $T\left(\mathbf{u}, \mathbf{v}\right) = \mathbf{0}$. Torsion-free means that parallel-transported vectors close properly and it implies that the connection coefficients (Christoffel symbols) are symmetric in the lower indices

$$
\begin{gather*}
  T_{ij}^k = \Gamma_{ij}^k - \Gamma_{ji}^k = 0 \\
  \Gamma_{ij}^k = \Gamma_{ji}^k
\end{gather*}
$$

# Riemann curvature tensor (Riemann-Christoffel tensor)

The Riemann curvature tensor $R^d_{abc}$ is of rank $(1,3)$.

## Holonomic definition

On all curved surfaces a coordinate system called the Riemann normal coordinates (local inertia frame) can be found at a point $p$ such that the metric tensor and the Christoffel symbols vanish.

Holonomy describes how vector fields get twisted during parallel transport. In this regard, the Riemann curvature tensor captures this twisting by measuring the rate of change of a parallel-transported vector field $DCBA\mathbf{w}$ relative to the original field $\mathbf{w}$ at the same point. Assuming the Levi-Civita connection applies such that $\left[ \mathbf{u}, \mathbf{v} \right] = 0$

$$
\begin{align*}
  R\left( \mathbf{u}, \mathbf{v} \right) \mathbf{w} &= \lim_{r,s \to 0} \frac{\mathbf{w} - DCBA\mathbf{w}}{rs} \\
  &= \lim_{r,s \to 0} \frac{1}{rs} \left( DCC^{-1}D^{-1}\mathbf{w} - DCBA\mathbf{w} \right) \\
  &= \lim_{r,s \to 0} \frac{1}{rs} DC \left( C^{-1}D^{-1}\mathbf{w} - BA\mathbf{w} \right) \\
  &= \lim_{r,s \to 0} \frac{1}{rs} DC \left[ \left( C^{-1}D^{-1}\mathbf{w} - C^{-1}\mathbf{w} + C^{-1}\mathbf{w} - \mathbf{w} \right) - \left( BA\mathbf{w} - B\mathbf{w} + B\mathbf{w} - \mathbf{w} \right) \right] \\
  &= \lim_{r,s \to 0} \frac{1}{rs} DC \left[ C^{-1}\left( D^{-1} \mathbf{w} - \mathbf{w} \right) + \left( C^{-1} \mathbf{w} - \mathbf{w} \right) - B\left(A \mathbf{w} - \mathbf{w} \right) + \left( B\mathbf{w} - \mathbf{w} \right)  \right] \\
  &= \lim_{r,s \to 0} DC \left[ \left( \frac{C^{-1}}{r} \underbrace{\frac{D^{-1} \mathbf{w} - \mathbf{w}}{s}}_{=\nabla_{\mathbf{v}}\mathbf{w}} + \frac{1}{s} \underbrace{\frac{C^{-1} \mathbf{w} - \mathbf{w}}{r}}_{=\nabla_{\mathbf{u}}\mathbf{w}} \right) - \left( \frac{B}{s} \underbrace{\frac{A \mathbf{w} - \mathbf{w}}{r}}_{=\nabla_{\mathbf{u}}\mathbf{w}} + \frac{1}{r} \underbrace{\frac{B\mathbf{w} - \mathbf{w}}{s}}_{=\nabla_{\mathbf{v}}\mathbf{w}} \right) \right] \\
  &= \lim_{r,s \to 0} DC \left[ \frac{C^{-1}}{r} \left( \nabla_{\mathbf{v}}\mathbf{w} \right) + \frac{1}{s} \left( \nabla_{\mathbf{u}}\mathbf{w} \right) - \frac{B}{s} \left( \nabla_{\mathbf{u}}\mathbf{w} \right) - \frac{1}{r} \left( \nabla_{\mathbf{v}}\mathbf{w} \right) \right] \\
  &= \lim_{r,s \to 0} DC \left( \frac{C^{-1} \nabla_{\mathbf{v}}\mathbf{w} - \nabla_{\mathbf{v}}\mathbf{w}}{r} - \frac{B \nabla_{\mathbf{u}}\mathbf{w} - \nabla_{\mathbf{u}}\mathbf{w}}{s} \right) \\
  &= \lim_{r,s \to 0} \underbrace{DC}_{=\mathrm{Id}} \left( \nabla_{\mathbf{u}}\nabla_{\mathbf{v}}\mathbf{w} - \nabla_{\mathbf{v}}\nabla_{\mathbf{u}}\mathbf{w} \right) \\
  &= \nabla_{\mathbf{u}}\nabla_{\mathbf{v}}\mathbf{w} - \nabla_{\mathbf{v}}\nabla_{\mathbf{u}}\mathbf{w}
\end{align*}
$$

In the case $\left[ \mathbf{u}, \mathbf{v} \right] \neq 0$, the Riemann curvature tensor is defined as

$$
  R\left( \mathbf{u}, \mathbf{v} \right) \mathbf{w} = \nabla_{\mathbf{u}}\nabla_{\mathbf{v}}\mathbf{w} - \nabla_{\mathbf{v}}\nabla_{\mathbf{u}}\mathbf{w} - \nabla_{\left[\mathbf{u}, \mathbf{v} \right]}\mathbf{w}
$$

The Riemann curvature tensor is linear in all inputs

$$
  R\left( \mathbf{u}, \mathbf{v} \right) \mathbf{w} = u^i v^j w^k R\left(\boldsymbol{\partial}_i, \boldsymbol{\partial}_j \right)\boldsymbol{\partial}_k
$$

Linearity in the first input is shown as follows

$$
\begin{align*}
  R\left( \mathbf{u}, \mathbf{v} \right) \mathbf{w} &= \nabla_{u^i \boldsymbol{\partial}_i} \nabla_{\mathbf{v}} \mathbf{w} - \nabla_{\mathbf{v}}\nabla_{u^i \boldsymbol{\partial}_i} \mathbf{w} - \nabla_{\left[u^i \boldsymbol{\partial}_i, \mathbf{v} \right]} \mathbf{w} \\
  &= \nabla_{u^i \boldsymbol{\partial}_i} \nabla_{\mathbf{v}} \mathbf{w} - \nabla_{\mathbf{v}}\nabla_{u^i \boldsymbol{\partial}_i} \mathbf{w} - \nabla_{u^i \left[ \boldsymbol{\partial}_i, \mathbf{v} \right] - \mathbf{v}\left(u^i\right) \boldsymbol{\partial}_i} \mathbf{w} \\
  &= u^i \nabla_{\boldsymbol{\partial}_i} \nabla_{\mathbf{v}} \mathbf{w} - \nabla_{\mathbf{v}} \left( u^i \nabla_{\boldsymbol{\partial}_i} \mathbf{w} \right) - u^i \nabla_{ \left[ \boldsymbol{\partial}_i, \mathbf{v} \right]} \mathbf{w} + \mathbf{v}\left(u^i\right) \nabla_{\boldsymbol{\partial}_i} \mathbf{w} \\
  &= u^i \nabla_{\boldsymbol{\partial}_i} \nabla_{\mathbf{v}} \mathbf{w} - \underbrace{\left( \nabla_{\mathbf{v}} u^i \right)}_{=\mathbf{v}\left( u^i\right)} \left( \nabla_{\boldsymbol{\partial}_i} \mathbf{w} \right) - u^i \left( \nabla_{\mathbf{v}}\nabla_{\boldsymbol{\partial}_i} \mathbf{w} \right) - u^i \nabla_{ \left[ \boldsymbol{\partial}_i, \mathbf{v} \right]} \mathbf{w} + \mathbf{v}\left(u^i\right) \left( \nabla_{\boldsymbol{\partial}_i} \mathbf{w} \right) \\
  &= u^i \nabla_{\boldsymbol{\partial}_i} \nabla_{\mathbf{v}} \mathbf{w} - u^i \left( \nabla_{\mathbf{v}}\nabla_{\boldsymbol{\partial}_i} \mathbf{w} \right) - u^i \nabla_{u^i \left[ \boldsymbol{\partial}_i, \mathbf{v} \right]} \mathbf{w} \\
  &= u^i \left( \nabla_{\boldsymbol{\partial}_i} \nabla_{\mathbf{v}} \mathbf{w} - \nabla_{\mathbf{v}}\nabla_{\boldsymbol{\partial}_i} \mathbf{w} - \nabla_{ \left[ \boldsymbol{\partial}_i, \mathbf{v} \right]} \mathbf{w} \right) \\
  &= u^i R\left( \boldsymbol{\partial}_i, \mathbf{v} \right) \mathbf{w}
\end{align*}
$$

Linearity in the second input can be similarly shown, while linearity in the third input is shown as follows

$$
\begin{align*}
  R\left( \mathbf{u}, \mathbf{v} \right) \mathbf{w} =& \nabla_{\mathbf{u}}\nabla_{\mathbf{v}}\left(w^i \boldsymbol{\partial}_i \right) - \nabla_{\mathbf{v}}\nabla_{\mathbf{u}}\left(w^i \boldsymbol{\partial}_i \right) - \nabla_{\left[\mathbf{u}, \mathbf{v} \right]}\left(w^i \boldsymbol{\partial}_i \right) \\
  =& \nabla_{\mathbf{u}} \left[ \left( \nabla_{\mathbf{v}} w^i \right) \boldsymbol{\partial}_i + w^i \left( \nabla_{\mathbf{v}} \boldsymbol{\partial}_i \right) \right] - \nabla_{\mathbf{v}} \left[ \left( \nabla_{\mathbf{u}} w^i \right) \boldsymbol{\partial}_i + w^i \left( \nabla_{\mathbf{u}} \boldsymbol{\partial}_i \right) \right] - \nabla_{\left[\mathbf{u}, \mathbf{v} \right]}\left(w^i \boldsymbol{\partial}_i \right) \\
  =& \left( \nabla_{\mathbf{u}} \nabla_{\mathbf{v}} w^i \right) \boldsymbol{\partial}_i + \left( \nabla_{\mathbf{v}} w^i \right) \left( \nabla_{\mathbf{u}} \boldsymbol{\partial}_i \right) + \left( \nabla_{\mathbf{u}} w^i \right) \left( \nabla_{\mathbf{v}} \boldsymbol{\partial}_i \right) + w^i \left( \nabla_{\mathbf{u}} \nabla_{\mathbf{v}} \boldsymbol{\partial}_i \right) \\
  &- \left( \nabla_{\mathbf{v}} \nabla_{\mathbf{u}} w^i \right)\boldsymbol{\partial}_i -  \left( \nabla_{\mathbf{v}} w^i \right) \left( \nabla_{\mathbf{u}} \boldsymbol{\partial}_i \right) - \left( \nabla_{\mathbf{u}} w^i \right) \left( \nabla_{\mathbf{v}} \boldsymbol{\partial}_i \right) - w^i \left( \nabla_{\mathbf{v}} \nabla_{\mathbf{u}} \boldsymbol{\partial}_i \right) \\
  &- \nabla_{\left[\mathbf{u}, \mathbf{v} \right]}\left(w^i \boldsymbol{\partial}_i \right) \\
  =& \left( \nabla_{\mathbf{u}} \nabla_{\mathbf{v}} w^i \right) \boldsymbol{\partial}_i - \left( \nabla_{\mathbf{v}} \nabla_{\mathbf{u}} w^i \right)\boldsymbol{\partial}_i + w^i \left( \nabla_{\mathbf{u}} \nabla_{\mathbf{v}} \boldsymbol{\partial}_i \right) - w^i \left( \nabla_{\mathbf{v}} \nabla_{\mathbf{u}} \boldsymbol{\partial}_i \right) \\
  &- \underbrace{\left( \nabla_{\left[\mathbf{u}, \mathbf{v} \right]} w^i \right)}_{\nabla_{\mathbf{u}}\nabla_{\mathbf{v}}w^i - \nabla_{\mathbf{v}}\nabla_{\mathbf{u}}w^i} \boldsymbol{\partial}_i - w^i \left( \nabla_{\left[\mathbf{u}, \mathbf{v} \right]} \boldsymbol{\partial}_i \right) \\
  =& w^i \left( \nabla_{\mathbf{u}} \nabla_{\mathbf{v}} \boldsymbol{\partial}_i \right) - w^i \left( \nabla_{\mathbf{v}} \nabla_{\mathbf{u}} \boldsymbol{\partial}_i \right) - w^i \left( \nabla_{\left[\mathbf{u}, \mathbf{v} \right]} \boldsymbol{\partial}_i \right) \\
  =& w^i \left( \nabla_{\mathbf{u}} \nabla_{\mathbf{v}} \boldsymbol{\partial}_i - \nabla_{\mathbf{v}} \nabla_{\mathbf{u}} \boldsymbol{\partial}_i - \nabla_{\left[\mathbf{u}, \mathbf{v} \right]} \boldsymbol{\partial}_i \right) \\
  =& w^i R\left( \mathbf{u}, \mathbf{v} \right) \boldsymbol{\partial}_i
\end{align*}
$$

## Geodesic deviation definition

The geodesic deviation of vector flow curves $\mathbf{v}$ separated by the vector field $\mathbf{s}$ is given by

$$
  \nabla_{\mathbf{v}}\nabla_{\mathbf{v}}\mathbf{s}
$$

Since the flow and separation curves form closed loops, a torsion-free connecting applies, giving 

$$
  \nabla_{\mathbf{v}}\mathbf{s} - \nabla_{\mathbf{s}}\mathbf{v} \equiv \left[ \mathbf{v}, \mathbf{s} \right] = 0 \\
  \nabla_{\mathbf{v}}\mathbf{s} = \nabla_{\mathbf{s}}\mathbf{v}
$$

Since the flow curves are geodesic the covariant derivative of $\mathbf{v}$ along itself vanishes

$$
\begin{align*}
  \mathbf{0} &= \nabla_{\mathbf{v}}\mathbf{v} \\
  &= \nabla_{\mathbf{s}}\nabla_{\mathbf{v}}\mathbf{v} \\
  &= \underbrace{\nabla_{\mathbf{s}}\nabla_{\mathbf{v}}\mathbf{v} - \nabla_{\mathbf{v}}\nabla_{\mathbf{s}}\mathbf{v}}_{=R(\mathbf{s}, \mathbf{v})\mathbf{v}} + \nabla_{\mathbf{v}}\underbrace{\nabla_{\mathbf{s}}\mathbf{v}}_{=\nabla_{\mathbf{v}}\mathbf{s}} \\
  &= R(\mathbf{s}, \mathbf{v})\mathbf{v} + \nabla_{\mathbf{v}}\nabla_{\mathbf{v}}\mathbf{s} \\
\end{align*}
$$

The geodesic deviation is thus given by

$$
  \nabla_{\mathbf{v}}\nabla_{\mathbf{v}}\mathbf{s} = -R(\mathbf{s}, \mathbf{v})\mathbf{v} = R(\mathbf{v}, \mathbf{s})\mathbf{v} 
$$

### Sectional curvature

Sectional curvature is defined as 

$$
  K(\mathbf{s}, \mathbf{v}) = \frac{\left[R(\mathbf{s}, \mathbf{v})\mathbf{v}\right] \cdot \mathbf{s}}{\left( \mathbf{s} \cdot \mathbf{s} \right) - \left( \mathbf{v} \cdot \mathbf{v} \right) - \left( \mathbf{s} \cdot \mathbf{v} \right)^2}
$$

where the denominator is a normalization factor equaling the area of the parallellogram spaned by the vector fields, equivalent of $\lVert \mathbf{s} \times \mathbf{v} \rVert^2$. The sign of the nominator denotes wheter the flow curves described by $\mathbf{v}$ are converging/diverging.

## Tensor components

The Riemann curvature tensor components can be derived as follows

$$
\begin{align*}
  R\left( \boldsymbol{\partial}_a, \boldsymbol{\partial}_b \right) \boldsymbol{\partial}_c &= \nabla_{\boldsymbol{\partial}_a} \nabla_{\boldsymbol{\partial}_b} \boldsymbol{\partial}_c - \nabla_{\boldsymbol{\partial}_b} \nabla_{\boldsymbol{\partial}_a} \boldsymbol{\partial}_c - \underbrace{\nabla_{\left[ \boldsymbol{\partial}_a, \boldsymbol{\partial}_b \right]}}_{=0} \boldsymbol{\partial}_c \\
  &= \nabla_{\boldsymbol{\partial}_a} \left( \Gamma_{bc}^i \boldsymbol{\partial}_i \right) - \nabla_{\boldsymbol{\partial}_b} \left( \Gamma_{ac}^j \boldsymbol{\partial}_j \right) \\
  &= \nabla_{\boldsymbol{\partial}_a} \left( \Gamma_{bc}^i \right) \boldsymbol{\partial}_i + \Gamma_{bc}^i \nabla_{\boldsymbol{\partial}_a} \boldsymbol{\partial}_i - \nabla_{\boldsymbol{\partial}_b} \left( \Gamma_{ac}^j \right) \boldsymbol{\partial}_j - \Gamma_{ac}^j \nabla_{\boldsymbol{\partial}_b} \boldsymbol{\partial}_j \\
  &= \partial_a \left( \Gamma_{bc}^i \right) \boldsymbol{\partial}_i - \partial_b \left( \Gamma_{ac}^j \right) \boldsymbol{\partial}_j + \Gamma_{bc}^i \Gamma_{ai}^k \boldsymbol{\partial}_k - \Gamma_{ac}^j \Gamma_{bj}^l \boldsymbol{\partial}_l \\
  &= \partial_a \left( \Gamma_{bc}^i \right) \boldsymbol{\partial}_d - \partial_b \left( \Gamma_{ac}^d \right) \boldsymbol{\partial}_d + \Gamma_{bc}^i \Gamma_{ai}^d \boldsymbol{\partial}_d - \Gamma_{ac}^j \Gamma_{bj}^d \boldsymbol{\partial}_d \\
  &= \left[ \partial_a \left( \Gamma_{bc}^i \right) - \partial_b \left( \Gamma_{ac}^d \right) + \Gamma_{bc}^i \Gamma_{ai}^d - \Gamma_{ac}^j \Gamma_{bj}^d \right] \boldsymbol{\partial}_d \\
  &= R_{cab}^d \boldsymbol{\partial}_d
\end{align*}
$$ 

## Symmetries

- 34-symmetry: $R^d_{c\color{red}{ba}} = -R^d_{c\color{red}{ab}}$
- Bianchi identity: $R^d_{cab} + R^d_{bca} + R^d_{abc} = 0$
- Second Bianchi identity: $R_{cab;i}^d + R_{cia;b}^d + R_{cbi;a}^d = 0$
- 12-symmetry: $R_{\color{red}{ba}cd} = -R_{\color{red}{ab}cd}$
- Flip: $R_{\color{red}{ab}\color{blue}{cd}} = -R_{\color{blue}{cd}\color{red}{ab}}$

## Transformation rule

The Riemann tensor transforms as follows

$$
  \tilde{R}_{abc}^d = J_a^i J_b^j J_c^k R_{ijk}^l \left(J^{-1}\right)_l^d
$$

# Ricci tensor

## Geodesic deviation definition

The Ricci tensor desribes volume changes along flow curves and can be defined as a sum of sectional curvature wrt an orthonormal basis

$$
\begin{align*}
  \mathrm{Ric}(\mathbf{v}, \mathbf{v}) &= \sum_{i=1}^{n-1} K(\boldsymbol{\partial}_i, \mathbf{v}) \\
  &= \frac{\left[R\left( \boldsymbol{\partial}_i, \mathbf{v} \right)\mathbf{v}\right] \cdot \boldsymbol{\partial}_i}{\left( \boldsymbol{\partial}_i \cdot \boldsymbol{\partial}_i\right) - \left( \mathbf{v} \cdot \mathbf{v} \right) - \left( \boldsymbol{\partial}_i \cdot \mathbf{v} \right)^2} \\
  &= \left[R\left( \boldsymbol{\partial}_i, \mathbf{v} \right)\mathbf{v}\right] \cdot \boldsymbol{\partial}_i \\
  &= \left[R\left( \boldsymbol{\partial}_i, v^j \boldsymbol{\partial}_j \right)v^k \boldsymbol{\partial}_k \right] \cdot \boldsymbol{\partial}_i \\
  &= v^j v^k \left[R\left( \boldsymbol{\partial}_i, \boldsymbol{\partial}_j \right) \boldsymbol{\partial}_k \right] \cdot \boldsymbol{\partial}_i \\
  &= v^j v^k R_{kij}^i \\
  &= v^j v^k R_{kj}
\end{align*}
$$

where $\mathbf{v} = \boldsymbol{\partial}_n$ is set as a direction vector field.

## Volume form definition

The volume form is a linear form measuring the volum spaned by a set of vectors

$$
  \omega (\mathbf{v}_1,...,\mathbf{v}_n) = \sqrt{\det g} \varepsilon_{ij...} \prod_{i=1}^n v_i^{\mu_i} = \omega \prod_{i=1}^n v_i^{\mu_i}
$$

where $\varepsilon$ is the Levi-Civita symbol.

The Ricci tensor can be derived from the covariant derivative of the volume form of separation vectors along a geodesic curve.

With the Levi-Civita connection, the first covariant derivative of the volume form of parallel-transported flow separation vectors along a geodesic path vanishes since volume is preserved

$$
\begin{align*}
  \nabla_{\mathbf{v}} \left[ \omega (\mathbf{s}_1,...,\mathbf{s}_n) \right] &= \nabla_{\mathbf{v}} \left( \sqrt{\det g} \varepsilon_{\mu_1...\mu_n} \prod_{i=1}^n s_i^{\mu_i} \right) \\
  &= \left( \nabla_{\mathbf{v}} \sqrt{\det g} \right)\varepsilon_{\mu_1...\mu_n} \prod_{i=1}^n s_i^{\mu_i} + \sqrt{\det g} \varepsilon_{\mu_1...\mu_n} \underbrace{\left( \nabla_{\mathbf{v}} s_j^{\mu_j} \right)}_{=0} \prod_{\substack{i=0\\ i\neq j}}^n s_i^{\mu_i}
\end{align*}
$$

Since the vectors are parallel-transported, $\nabla_{\mathbf{v}} s_j^{\mu_j} = 0$, and hence 

$$
  \left( \nabla_{\mathbf{v}} \sqrt{\det g} \right)\varepsilon_{\mu_1...\mu_n} = \nabla_{\mathbf{v}} \omega = 0
$$

The 2nd covariant derivative of the volume form becomes

$$
\begin{align*}
  \nabla_{\mathbf{v}}\nabla_{\mathbf{v}} \left[ \omega (\mathbf{s}_1,...,\mathbf{s}_n) \right] &= \nabla_{\mathbf{v}} \nabla_{\mathbf{v}} \left( \prod_{i=1}^n s_i^{\mu_i} \right) \underbrace{\sqrt{\det g} \varepsilon_{\mu_1...\mu_n} }_{\nabla_{\mathbf{v}} \omega = 0} \\
  &= \nabla_{\mathbf{v}} \left[ \left( \nabla_{\mathbf{v}} s_j^{\mu_j} \right) \prod_{\substack{i=1 \\ i\neq j}} s_i^{\mu_i} \right] \sqrt{\det g} \varepsilon_{\mu_1...\mu_n} \\
  &= \left[ \underbrace{\left( \nabla_{\mathbf{v}}\nabla_{\mathbf{v}} s_j^{\mu_j} \right)}_{=-R(\mathbf{s}, \mathbf{v})\mathbf{v}} \prod_{\substack{i=1 \\ i\neq j}} s_i^{\mu_i} \right] \sqrt{\det g} \varepsilon_{\mu_1...\mu_n} + \left[ \left( \nabla_{\mathbf{v}}s_j^{\mu_j} \right) \left( \nabla_{\mathbf{v}}s_k^{\mu_k} \right) \prod_{\substack{i=1 \\ i\neq j,k}} s_i^{\mu_i} \right] \sqrt{\det g} \varepsilon_{\mu_1...\mu_n} \\
  &= \left[ -R_{xyz}^{\mu_j} s_j^{y} v^x v^z \prod_{\substack{i=1 \\ i\neq j}} s_i^{\mu_i} \right] \sqrt{\det g} \underbrace{\varepsilon_{\mu_1...\mu_n}}_{\Rightarrow y = \mu_j} + \left[ \left( \nabla_{\mathbf{v}}s_j^{\mu_j} \right) \left( \nabla_{\mathbf{v}}s_k^{\mu_k} \right) \prod_{\substack{i=1 \\ i\neq j,k}} s_i^{\mu_i} \right] \sqrt{\det g} \varepsilon_{\mu_1...\mu_n} \\
  &= \left[ -R_{x\mu_j z}^{\mu_j} s_j^{\mu_j} v^x v^z \prod_{\substack{i=1 \\ i\neq j}} s_i^{\mu_i} \right] \sqrt{\det g} \varepsilon_{\mu_1...\mu_n} + \left[ \left( \nabla_{\mathbf{v}}s_j^{\mu_j} \right) \left( \nabla_{\mathbf{v}}s_k^{\mu_k} \right) \prod_{\substack{i=1 \\ i\neq j,k}} s_i^{\mu_i} \right] \sqrt{\det g} \varepsilon_{\mu_1...\mu_n} \\
  &= \left[ -R_{x\mu_j z}^{\mu_j} v^x v^z \prod_{i=1} s_i^{\mu_i} \right] \sqrt{\det g} \varepsilon_{\mu_1...\mu_n} + \left[ \left( \nabla_{\mathbf{v}}s_j^{\mu_j} \right) \left( \nabla_{\mathbf{v}}s_k^{\mu_k} \right) \prod_{\substack{i=1 \\ i\neq j,k}} s_i^{\mu_i} \right] \sqrt{\det g} \varepsilon_{\mu_1...\mu_n} \\
  &= -\underbrace{R_{xz}v^x v^z}_{\mathrm{Ric}(\mathbf{v}, \mathbf{v})} \omega (\mathbf{s}_1,...,\mathbf{s}_n) + \left[ \left( \nabla_{\mathbf{v}}s_j^{\mu_j} \right) \left( \nabla_{\mathbf{v}}s_k^{\mu_k} \right) \prod_{\substack{i=1 \\ i\neq j,k}} s_i^{\mu_i} \right] \sqrt{\det g} \varepsilon_{\mu_1...\mu_n}
\end{align*}
$$

The Ricci tensor thus measures the volume change along geodesics in curved space

## Ricci scalar

The Ricci scalar measures the amount by which the volume of a geodesic ball in curved space deviates from that of a standard ball in flat space.

$$
  R = g^{ij}R_{ij} = R_i^i
$$

## Properties

- Symmetry: $R_{ab} = R_{ba}$
- Contracted Bianchi identity: $R^{nm}_{;n} - \frac{1}{2}g^{mn}R_{;n} = G^{mn}_{;n} = 0$ where $G^{mn}$ is the Einstein tensor

The Ricci tensor is given by contractions in the second and third lower indices of the Riemann tensor

$$
  R_{aib}^i = R_{ab} \\
  R_{abi}^i = -R_{aib}^i = -R_{ab}
$$

the third possible contraction of the Riemann tensor is trivial

$$
  R_{\color{blue}{i}ab}^{\color{blue}{i}} = g^{\color{blue}{i}\color{red}{j}}R_{jiab} = -g^{\color{red}{j}\color{blue}{i}}R_{\color{blue}{i}\color{red}{j}ab} = -R_{\color{red}{j}ab}^{\color{red}{j}} = 0
$$

The symmetry of the Ricci tensor is shown from a special case of the Bianchi identity by setting $d = a$

$$
\begin{align*}
  R_{c\color{red}{a}b}^{\color{red}{a}} + R_{bc\color{red}{a}}^{\color{red}{a}} + \underbrace{R_{\color{red}{a}bc}^{\color{red}{a}}}_{=0} &= 0 \\
  R_{c\color{red}{a}b}^{\color{red}{a}} - R_{b\color{red}{a}c}^{\color{red}{a}} &= 0 \\
  R_{cb} - R_{bc} &= 0 \\
  R_{cb} &= R_{bc}
\end{align*}
$$