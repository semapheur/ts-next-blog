---
title: 'Statistics'
subject: 'Mathematics'
showToc: true
references:
  - book_siegrist_2021
---

# Statistical model

A statistical model describes the process of generating sample data from a population, usually in the form of mathematical relationship between one or more random variables and deterministic parameters.

<MathBox title='Statistical model' boxType="definition">
A statistical model can formally be defined by a pair $(S,\mathcal{P})$ where $S$ is the sample space and $\mathcal{P}$ is a set of probability distributions on $S$. The set $\mathcal{P}$ is almost always parametrized

$$
  \mathcal{P} = \Set{ P_\theta : \theta\in\Theta }
$$

where $\Theta$ is the parameter space of the model. The observed outcome of a statistical experiment has the form $\boldsymbol{x} = (x_i)_{i=1}^n \in S$, where $x_i$ is the vector of measurements for the $i$-th object. 

</MathBox> 

<MathBox title='Empirical distribution' boxType="definition">
Suppose $\boldsymbol{x} = (x_i)_{i=1}^{n\in\N}\in S$ is a sample. The empirical distribution associated with $\boldsymbol{X}$ assigns the probability $\frac{1}{n}$ at each $x_i$. If the sample values are distinct, the empirical distribution is the discrete uniform distribution. Generally, if $x$ occurs $k$ times in the sample data, the empirical distribution assigns probability $\frac{k}{n}$ to $x$. Thus, every finite data set defines a probability distribution.
</MathBox> 

<MathBox title="Statistics" boxType="definition">
Suppose $\boldsymbol{x} = (x_i)_{i=1}^{n\in\N}\in S$ is a sample. A statistic $w = w(\boldsymbol{x}):S\to \Theta$ is an observable function of the sample $\boldsymbol{x}$ where $\Theta$ is the parameter space.  
</MathBox> 

<MathBox title='Equivalence of statistics' boxType="proposition">
Suppose $\boldsymbol{x} = (x_i)_{i=1}^{n\in\N}\in S$ is a sample. Statistics $u$ and $v$ on $\boldsymbol{x}$ are equivalent if and only if for any $\boldsymbol{x},\boldsymbol{y}\in S$, $u(\boldsymbol{x}) = u(\boldsymbol{y})$ if and only if $v(\boldsymbol{x}) = v(\boldsymbol{y})$. This defines a equivalence relation on the collection of statistics for a given statistical model, which satisfies for arbitrary statistics $u, v$ and $w$

1. $u$ is equivalent to $u$ (reflexivity)
2. if $u$ is equivalent to $v$, then $v$ is equivalent to $u$ (symmetry)
3. if $u$ is equivalent to $v$ and $v$ is equivalent to $w$, then $u$ is equivalent to $w$ (transitivity)
</MathBox>

## Statistical inference

There are two broad branches of statistics
- descriptive statistics
- inferential statistics

Descriptive statistics refers to methods for summarizing and displaying sample data. The methods usually involve computing various statistics. In the context of descriptive statistics, the term parameter refers to a characteristic of the entire population.

Inferential statistics describes a statistical experiment as a random process with a probability measure $\mathbb{P}$ on an underlying sample space. The sample $\boldsymbol{x}$ of the experiment is an observed value of a random variable $\boldsymbol{X}$ with unknown distribution defined on this probability space. The goal of statical inference is to draw inferences about the distribution of $\boldsymbol{X}$ from the observed value $\boldsymbol{x}$. In inferential statistics, a statistic is itself a random variable, while a parameter refers to a characteristic of the distribution of $\boldsymbol{X}$.

<MathBox title='Random sample' boxType="definition">
Suppose $\boldsymbol{X} = (X_i)_{i=1}^{n\in\N}$ is an observable random variable for a statistical experiment. The most common and important special case of the inferential statistical model occurs when $\boldsymbol{X}$ is a sequence of independent and identically distributed (i.i.d.) random variables. In this case $\boldsymbol{X}$ represent independent copies of an underlying measurement vector $X$, and $\boldsymbol{X}$ is called a random sample of size $n$ from the distribution of $X$.
</MathBox>

<MathBox title="Parameter" boxType="definition">
Suppose $\boldsymbol{X} = (X_i)_{i=1}^{n\in\N}$ is an observable random variable for a statistical experiments. A parameter $\boldsymbol{\theta}$ is a function of distribution of $\boldsymbol{X}$ taking values in a parameter space $\Theta$.
</MathBox>

Typically, the distribution of an obervable random variable $\boldsymbol{X}$ will have $k\in\N_+$ real parameters of interest, so that $\boldsymbol{\theta} = (\theta_i)_{i=1}^{k\in\N}\in T\subseteq\R^k$

# Random samples

<MathBox title='Law of large numbers' boxType="theorem">
Let $(X_i)_{i=1}^{n\in\N}$ be a sequence of independent and identically distributed integrable random variables with expected value $\mathbb{E}(X_i) = \mu$. Suppose $\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i$.

**Weak law of large numbers (Kinchin's law):** The sample average $\bar{X}_i$ converges in probability to $\mu$, i.e. $\bar{X}_n \xrightarrow[\textrm{i.p.}]{n\to\infty} \mu$, or for every $\varepsilon > 0$

$$
\begin{gather*}
  \lim_{n\to\infty} \mathbb{P}\left(|\bar{X}_n - \mu| < \varepsilon \right) = 1 \\
  \iff \lim_{n\to\infty} \mathbb{P}\left(|\bar{X}_n - \mu| > \varepsilon \right) = 0
\end{gather*}
$$

**Strong law of large numbers (Kolmogorov's law):** The sample average $\bar{X}_i$ converges almost surely to $\mu$, i.e. $\bar{X}_n \xrightarrow[\textrm{a.s.}]{n\to\infty}_ \mu$, or

$$
  \mathbb{P}\left(\lim_{n\to\infty} \bar{X}_n = \mu \right) = 1
$$

<details>
<summary>Proof</summary>

The weak law follows from Chebyshev's inequality

$$
  \mathbb{P}\left(|\bar{X}_n - \mu| > \varepsilon \right)\leq\frac{\mathrm{var}(\bar{X}_n)}{\varepsilon^2} = \frac{\sigma^2}{n\varepsilon^2} \xrightarrow{n\to\infty} 0
$$

The strong law is proved in three major steps. The first step is to show that with probability $1$ that $\bar{X}_{n^2} \xrightarrow{n\to\infty}\mu$. From Chebyshev's inequality,

$$
  \mathbb{P}\left(|\bar{X}_{n^2} - \mu| > \varepsilon \right)\leq\frac{\mathrm{var}(\bar{X}_n)}{\varepsilon^2} = \left(\frac{\sigma}{n\varepsilon}\right)^2 \xrightarrow{n\to\infty} 0
$$

Since $\sum_{n\N} \left(\frac{\sigma}{n\varepsilon}\right)^2 < \infty$, it follows from the first Borelli-Cantelli lemma that for every $\varepsilon > 0$

$$
  \mathbb{P}\left(|\bar{X}_{n^2} - \mu| > \varepsilon \textrm{ for infinitely many }n\in\N_+ \right) = 0
$$

From Boole's inequality it follows that for some rational $\varepsilon > 0$

$$
  \mathbb{P}\left(|\bar{X}_{n^2} - \mu| > \varepsilon \textrm{ for infinitely many }n\in\N_+ \right) = 0
$$

showing that $\bar{X}_{n^2}$ converges almost surely to $\mu$.

In the second step, we show that if the underlying sampling variable is nonnegative, so that $\mathbb{P}(X\geq 0) = 1$, then $\bar{X}_n\xrightarrow[\textrm{a.s.}]{n\to\infty}\mu$. Let $Y_n = \sum_{i=1}^n X_i$ so that $\bar{X}_n = Y_n / n$. Note first that $Y_n$ is almost surely increasing in $n$. For $n\in\N$ be the unique positive integer such that $k_n^2 \leq n < (k_n + 1)^2$. From the increasing propery it follows that (almost surely)

$$
  \frac{Y_{k_n^2}}{(k_n + 1)^2} \leq \frac{Y_n}{n} \leq \frac{Y_{(k_n + 1)^2}}{k_n^2}
$$

From the first step

$$
  \frac{Y_{k_n^2}}{(k_n + 1)^2} = \frac{Y_{k_n^2}}{k_n^2}\frac{k_n^2}{(k_n + 1)^2} \xrightarrow[\textrm{a.s.}]{n\to\infty}\mu
$$

Similarly,

$$
  \frac{Y_{(k_n + 1)^2}}{k_n^2} = \frac{Y_{(k_n + 1)^2}}{(k_n + 1)^2}\frac{(k_n + 1)^2}{k_n^2} \xrightarrow[\textrm{a.s.}]{n\to\infty}\mu
$$

By the squeeze theorem for limits it follows that $\bar{X}_n = Y_{n}/n \xrightarrow[\textrm{a.s.}]{n\to\infty}\mu$.

Finally, we relax the condition that the underlying sampling variable is $X$ is nonnegative. From step two, it follows that

$$
\begin{align*}
  \frac{1}{n}\sum_{i=1}^n X_i^+ &\xrightarrow[\textrm{a.s.}]{n\to\infty} \mathbb{E}(X^+) \\
  \frac{1}{n}\sum_{i=1}^n X_i^- &\xrightarrow[\textrm{a.s.}]{n\to\infty} \mathbb{E}(X^-)
\end{align*}
$$

From the linearity of expected value

$$
\begin{align*}
  \frac{1}{n}\sum_{i=1}^n X_i =& \frac{1}{n}\sum_{i=1}^n \left(X_i^+ - X_i^+\right) \\
  =& \frac{1}{n}\sum_{i=1}^n X_i^+ - \frac{1}{n}\sum_{i=1}^n X_i^- \\
  \xrightarrow& [\textrm{a.s.}]{n\to\infty} \mathbb{E}(X^+) - \mathbb{E}(X^+) = \mathbb{E}(X^+ - X^-) = \mathbb{E}(X) = \mu
\end{align*}
$$
</details>
</MathBox>

## Partial sum process

<MathBox title='Partial sum process' boxType="definition">
Suppose $\boldsymbol{X} = (X_n)_{n\in\N}$ is a sequence of independent, identically distributed random variables with common probability density function $f$, mean $\mu$ and standard deviation $\sigma\in(0,\infty)$. Let $Y_n = \sum_{i=1}^n X_i$ with $Y_0 = 0$. The random process $\boldsymbol{Y} = (Y_n)_{n\in\N_+}$ is called the partial sum process associated with $\boldsymbol{X}$.
</MathBox>

<MathBox title='Properties of partial sum processes' boxType="proposition" tag="1">
Let $\boldsymbol{Y} = (Y_n)_{n\in\N_+}$ be a partial sum process for a sample variable $X$ with mean $\mu$, variance $\sigma^2$ and probability density function $f$. If $m, n\in\N$ with $m\leq n$ then

1. $Y_n - Y_m$ has the same distribution as $Y_{n-m}$, implying that $\boldsymbol{Y}$ has stationary increments
2. if $(n_i)_{i\in\N}$ is an increasing sequence then $(Y_{n_i} - Y_{n_{i-i}})_{i\in\N}$ is a sequence of independent random variables, implying that $\mathbb{Y}$ has independent increments
3. $\mathbb{E}(Y_n) = n\mu$
4. $\mathrm{var}(Y_n) = n\sigma^2$
5. $\mathrm{cov}(Y_m, Y_n) = m\sigma^2$
6. $\mathrm{cor}(Y_m, Y_n) = \sqrt{\frac{m}{n}}$
7. $\mathbb{E}(Y_m Y_n) = m\sigma^2 + mn\mu^2$
8. the probability density function of $Y_n$ is the convolution power of $f$ of order $n$, i.e. $f^{*n}$
    a. if $(n_i)_{i=1}^{k\in\N}$ is a strictly increasing sequence then $(Y_{n_i})_{i=1}^{k\in\N}$ has joint probability density function for $\boldsymbol{y} = (y_i)_{i=1}^{k\in\N} \in\R^k$
$$
  f_{n_1, n_2,\dots,n_k} (\boldsymbol{y}) = f^{*n_1}(y_1)\prod_{i=2}^k f^{*(n_i - n_{i-1})}(y_i - y_{i-1})
$$
8. if $\mathbf{X}$ has moment generating function $G$, then $Y_n$ has moment generating function $G^n$

<details>
<summary>Proof</summary>

1. Note that $Y_n - Y_m = \sum_{i=m+1}^{n} X_i$, which is the sum of $n-m$ independent variables, each with the common distribution. Conversely, $Y_{n-m}$ is also the sum of $n-m$ independent variables, each with the common distribution.

2. The terms in the sequence of increments $(Y_{n_i} - Y_{n_{i-i}})$ are sums over disjoint collections of terms in the sequence $\boldsymbol{X}$. Since the sequence $\boldsymbol{X}$ is independent, so is the sequence of increments 

3. This follows from the linear property of expected value 

$$
  \mathbb{E}(Y_n) = \mathbb{E}\left(\sum_{i=i}^n X_i \right) = \sum_{i=1}^n \mathbb{E}(X_i) = n\mu
$$

4. By independence 

$$
  \mathrm{var}(Y_n) = \mathrm{var}\left(\sum_{i=i}^n X_i \right) = \sum_{i=1}^n \mathrm{var}(X_i) = n\sigma^2
$$

5. Note that $Y_n = Y_m + (Y_n - Y_m)$ giving

$$
\begin{align*}
  \mathrm{cor}(Y_m, Y_n) =& \mathrm{cor}(Y_m, Y_m) + \mathrm{cor}(Y_m, Y_n - Y_m) \\
  =& \mathrm{var}(Y_m) + 0 \\
  =& m\sigma^2
\end{align*}
$$

6.
$$
\begin{align*}
  \mathrm{cor}(Y_m, Y_n) =& \frac{\mathrm{cov}(Y_m, Y_n)}{\mathrm{sd}(Y_m) \mathrm{sd}(Y_m)} \\
  =& \frac{m\sigma^2}{\sqrt{m\sigma^2}\sqrt{n\sigma^2}} = \sqrt{\frac{m}{n}}
\end{align*}
$$

7.
$$
  \mathbb{E}(Y_m Y_n) = \mathrm{cov}(Y_m, Y_n) + \mathbb{E}(Y_m)\mathbb{E}(Y_n) = m\sigma^2 + m\mu n\mu
$$

8. The probability density function (PDF) of a sum of independent variables is the convolution of the PDFs of the terms.

9. This follows from the fact that the generating function of a sum of independent variables is the product of the generating functions of the terms.
</details>
</MathBox>

## Central limit theorem

<MathBox title='Central limit theorem' boxType="theorem">
Let $\boldsymbol{Y}$ be a partial sum process of a sample variable $X$ with mean $\mu$, variance $\sigma^2$ and characteristic function

$$
  \chi_n (t) = \mathbb{E}\left[\exp\left(it\frac{X - \mu}{\sigma} \right)\right],\quad t\in\R
$$

Define the common standard score 

$$
  Z_n = \frac{Y_n - n\mu}{\sqrt{n}\sigma} = \frac{\bar{X}_n - \mu}{\sigma/\sqrt{n}}
$$

with characteristic function

$$
  \chi_n (t) = \mathbb{E}(e^{itZ_n}),\quad t\in\R
$$

In the limit $n\to\infty$, the distribution of $Z_n$ converges to the standard normal distribution, i.e.

$$
  \lim_{n\to\infty} \chi_n (t) = e^{-t^2/2}
$$

<details>
<summary>Proof</summary>

Noting that

$$
  Z_n = \frac{\bar{X}_n - \mu}{\sigma/\sqrt{n}} = \sum_{i=1}^n \frac{X_i - \mu}{\sqrt{n}\sigma}
$$

we may rewrite

$$
\begin{align*}
  \chi_n (t) =& \mathbb{E}\left[\exp\left(itZ_n\right)\right] = \mathbb{E}\left[\exp\left(i\frac{t}{\sqrt{n}}\sum_{i=1}^n \frac{X_i - \mu}{\sigma} \right)\right] \\
  =& \mathbb{E}\left[\prod_{i=1}^n\exp\left(i\frac{t}{\sqrt{n}} \frac{X_i - \mu}{\sigma} \right)\right] = \prod_{i=1}^n \mathbb{E}\left[\exp\left(i\frac{t}{\sqrt{n}} \frac{X_i - \mu}{\sigma} \right)\right] \\
  =& \chi^n \left(\frac{t}{\sqrt{n}}\right)
\end{align*}
$$

Noting that $\chi(0)=1$, $\chi'(0)=0$, $\chi''(0) = -1$, the second order Taylor expansion of $chi\left(\frac{t}{\sqrt{n}}\right)$ about $t=0$ is

$$
  \chi\left(\frac{t}{\sqrt{n}}\right) = 1 - \frac{t^2}{2n} + o\left(\frac{t^2}{n} \right)
$$

In the limit $n\to\infty$, the higher order terms $o\left(\frac{t^2}{n}\right)$ vanish and noting that $e^x = \lim_{n\to\infty} \left(1 + \frac{x_n}{n} \right)^n$, we get

$$
  \lim_{n\to\infty} \chi^n \left(\frac{t}{\sqrt{n}}\right) = e^{-\frac{t^2}{2}}
$$
</details>
</MathBox>

## Sample moments

### Sample mean

<MathBox title='Sample mean' boxType="definition">
Suppose $\boldsymbol{x}\in S\subseteq\R^n$ is a sample of size $n\in\N_+$ from a real-valued variable. The *sample mean* is simply the arithmetic average of the sample values

$$
  m \equiv \bar{x} := \frac{1}{n}\sum_{i=1}^n x_i
$$
</MathBox>

<MathBox title='Properties of sample mean' boxType="proposition" tag="2">
Suppose $\boldsymbol{x}, \boldsymbol{y}\in S\subseteq\R^n$ are real-valued samples of size $n\in\N_+$, and that $a, b\in\R$ are constants. The sample mean has the following properties

1. **Linearity:** $m(a\boldsymbol{x} + b\boldsymbol{y}) = am(\boldsymbol{x}) + bm(\boldsymbol{y})$
2. **Positivity:** if $x_i \geq 0$ for each $i$ then $m(\boldsymbol{x})\geq 0$. The inequality becomes strict if $x_j > 0$ for some $j$.
3. **Ordering:** if $x_i \leq y_i$ for each $i$ then $m(\boldsymbol{x})\leq m(\boldsymbol{y})$. The inequality becomes strict if $x_j < y_j$ for some $j$.
4. If $\boldsymbol{c}$ is a sample of a constant $c\in\R$ then $m(\boldsymbol{c}) = c$.

<details>
<summary>Proof</summary>

1. 
$$
\begin{align*}
  m(a\boldsymbol{x} + b\boldsymbol{y}) =& \frac{1}{n}\sum_{i=1}^n (a x_i + by_i) \\
  =& \frac{a}{n}\sum_{i=1}^n x_i + \frac{b}{n}\sum_{i=1}^n y_i \\
  =& am(\boldsymbol{x}) + bm(\boldsymbol{y})
\end{align*}
$$

2. This follows immediately from the definition.

3. Note that $x_i \leq y_i \iff y_i - x_i \geq 0$ for each $i$, such that $0\leq m(\boldsymbol{y}-\boldsymbol{x}) = m(\boldsymbol{y}) - m(\boldsymbol{x})$ and hence $m(\boldsymbol{x})\leq m(\boldsymbol{y})$.

4. Note that

$$
  m(\boldsymbol{c}) = \frac{1}{n}\sum_{i=1}^n c_i = \frac{nc}{n} = c
$$
</details>
</MathBox>

#### Empirical statistics

<MathBox title='Relative frequency' boxType="definition">
Suppose $\boldsymbol{x}\in S$ is a sample of size $n\in\N_+$. For $A\subseteq S$, the frequency of $A$ corresponding $\boldsymbol{x}$ is the number of data values that are in $A$

$$
  n(A) = \sum_{i=1}^n \mathbf{1}(x_i \in A)
$$

The relative frequency of $A$ corresponding to $\boldsymbol{x}$ is the proportion of sample values that are in $A$

$$
  p(A) = \frac{n(A)}{n} = \frac{1}{n}\sum_{i=1}^n \mathbf{1}(x_i \in A)
$$
</MathBox>

<MathBox title='Empirical probability distribution' boxType="proposition">
The relative frequency function $p:S\to[0,\infty)$ is a probability measure on $S$ which satisfies

1. $p(A)\geq 0$ for every $A\subseteq S$
2. $p(S) = 1$
3. if $\Set{A_j}_{j\in J\subseteq\N}$ is a countable collection of pairwise disjoint subsets of $S$, then

$$
  p\left(\bigcup_{j\in J} A_j \right) = \sum_{j\in J} p(A_j)
$$

The empirical probability distribution is a discrete distribution that assigns probability $\frac{1}{n}$ at each sample value. If the sample values are distinct, the empirical distribution is the discrete uniform distribution.

<details>
<summary>Proof</summary>

The 1st and 2nd properties are obvious. For the third property, note that since the sets are disjoint

$$
\begin{align*}
  p\left(\bigcup_{i\in I} A_i \right) =& \frac{1}{n}\sum_{i=1}^n \mathbf{1}\left(x_i \in \bigcup_{j\in J} \right) \\
  =& \frac{1}{n}\sum_{i=1}^n \sum_{j\in J}\mathbf{1}(x_i \in A_j) \\
  =& \sum_{j\in J}\frac{1}{n}\sum_{i=1}^n \mathbf{1}(x_i\in A_j) \\
  =& \sum_{j\in J} p(A_j)
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Empirical density' boxType="definition">
Suppose $\boldsymbol{x}\in S\subseteq\R^n$ is a sample of size $n\in\N_+$.
For $A\subseteq S\subseteq\R^d$ with standard measure $\lambda_n (A) = \int_A 1\;\d x > 0$, the *empirical density* of $A$ corresponding to $\boldsymbol{x}$ is

$$
  D(A) = \frac{p(A)}{\lambda_n (A)} = \frac{1}{n\lambda_d (A)} \sum_{i=1}^n \mathbf{1}(x_i \in A)
$$
</MathBox>

<MathBox title='Empirical distribution function' boxType="definition">
Suppose $\boldsymbol{x}\in S\subseteq\R^n$ is a sample of size $n\in\N_+$. For $x\in\R$, let $F(x)$ denote the relative frequency of $(-\infty, x]$ corresponding to $\boldsymbol{x}$. The function $F(x)$ is the sample mean of the data $\Set{ \mathbf{1}(x_i \leq x)}_{i=1}^n$

$$
  F(x) = p((-\infty, x]) = \frac{1}{n}\sum_{i=1}^n \mathbf{1}(x_i\leq x)
$$

and defines a distribution function that satisfies
1. $F$ increases from $0$ to $1$
2. $F$ is a step function with jumps at the distinct sample values
</MathBox>

<MathBox title='Empirical discrete density function' boxType="definition">
Suppose $\boldsymbol{x}\in S\subseteq\R^n$ is a sample of size $n\in\N_+$.
For $x\in S$, let $f(x)$ be the relative frequency of $x$ corresponding the sample $\boldsymbol{x}$. The function $f$ is a sample mean for the data $\Set{\mathbf{1}(x_i = x)}_{i=1}^n$

$$
  f(x) = p(\Set{x}) = \frac{1}{n}\sum_{i=1}^n \mathbf{1}(x_i = x)
$$

and defines a discrete probability density function satisfying
1. $f(x)\geq 0$
2. $\sum_{x\in S} f(x) = 1$

If the underlying population variable is real-valued, then the sample mean is the expected value computed relative to the empirical density function

$$
  \frac{1}{n}\sum_{i=1}^n x_i = \sum_{x\in S} xf(x)
$$

<details>
<summary>Proof</summary>

Note that 

$$
\begin{align*}
  \sum_{x\in S} xf(x) =& \sum_{x\in S} x\frac{1}{n}\sum_{i=1}^n \mathbf{1}(x_i = x) \\
  =& \frac{1}{n}\sum_{i=1}^n \sum_{x\in S} x\mathbf{1}(x_i = x) \\
  =& \frac{1}{n}\sum_{i=1}^n x_i
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Empirical continuous density function' boxType="definition">
Suppose $\boldsymbol{x}\in S\subseteq\R^n$ is a sample of size $n\in\N_+$. Let $\mathcal{A} = \Set{A_j}_{j\in J}$ be a partition of $S$ into a countable number of subsets, each of positive, finite measure. Let $f$ be the function on $S$ defined by the rule that $f(x)$ is empirical density of $A_j$, corresponding to the data set $\mathbf{x}$, for each $x\in A_j$

$$
  f(x) = D(A_j) = \frac{p(A_j)}{\lambda_n (A_j)} = \frac{1}{n\lambda_d (A_j)}\sum_{i=1}^n \mathbf{1}(x_i\in A_j)
$$

The function $f$ is a continuous probability density function satisfying
1. $f(x)\geq 0$
2. $\int_S f(x)\;\d x = 1$

<details>
<summary>Proof</summary>

Note that 

$$
\begin{align*}
  \int_S f(x)\;\d x =& \sum_{j\in J}\int_{A_j} f(x)\;\d x \\
  =& \sum_{j\in J} \lambda_k (A_j) \frac{p(A_j)}{\lambda_k(A_j)} \\
  =& \sum_{j\in J}p(A_j) = 1
\end{align*}
$$
</details>
</MathBox>

### Sample variance

<MathBox title='Sample variance' boxType="definition">
Suppose $\boldsymbol{x}\in S\subseteq\R^n$ is a sample of size $n\in\N_+$ from a real-valued variable. The *sample variance* is defined as the mean square deviation

$$
\begin{equation*}
\begin{split}
  s^2 =& \frac{1}{n-1}\sum_{i=1}^n (x_i - m)^2 \\
  =& \frac{1}{n-1}\sum_{i=1}^n x_i^2 - \frac{n}{n-1}m^2 = \frac{n}{n-1}\left[m(\boldsymbol{x^2}) - m^2(\boldsymbol{x})\right] \\
  =& \frac{1}{2n(n - 1)}\sum_{i=1}^n \sum_{j=1}^n (x_i - x_j)^2
\end{split}
\tag{\label{equation-8}}
\end{equation*}
$$

where $\mathbf{x}^2 = (x_i^2)_{i=1}^n$.

<details>
<summary>Details</summary>

The first alternate form of the sample variance is found from

$$
\begin{align*}
  \sum_{i=1}^n (x_i - m)^2 =& \sum_{i=1}^n (x_i^2 - 2mx_i + m^2) \\
  =& \sum_{i=1}^n x_i^2 - 2m\sum_{i=1}^n x_i - \sum_{i=1}^n m^2 \\
  =& \sum_{i=1}^n x_i^2 2nm^2 + nm^2 \\
  =& \sum_{i=1}^n x_i^2 - nm^2
\end{align*}
$$

The second alternate form of the sample variance is found from

$$
\begin{align*}
  \frac{1}{2n}\sum_{i=1}^n \sum_{j=1}^n (x_i - x_j)^2 =& \frac{1}{2n}\sum_{i=1}^n \sum_{j=1}^n (x_i - m + m - x_j)^2 \\
  =& \frac{1}{2n}\sum_{i=1}^n\sum_{j=1}^n \left[(x_i - m)^2 + 2(x_i - m)(m - x_j) + (m - x_j)^2\right] \\
  =& \frac{1}{2n}\sum_{i=1}^n \sum_{j=1}^n (x_i - m)^2 + \frac{1}{n} \sum_{i=1}^n \sum_{j=1}^n (x_i - m)(m - x_j) + \frac{1}{2n}\sum_{i=1}^n \sum_{j=1}^n (m - x_j)^2 \\
  =& \frac{1}{2}\sum_{i=1}^n (x_i - m)^2 + 0 + \frac{1}{2}\sum_{j=1}^n (m - x_j)^2 \\
  =& \sum_{i=1}^n (x_i - m)^2
\end{align*}
$$
</details>
</MathBox>

The reason for dividing by $n - 1$ rather than $n$ in $\eqref{equation-8}$ is that there are only $n-1$ degrees of freedom in the set of deviations as the sum of all deviations vanishes

$$
\begin{align*}
  \sum_{i=1}^n (x_i - m) =& \sum_{i=1}^n x_i - \sum_{i=1}^n m \\
  =& nm - nm = 0
\end{align*}
$$

<MathBox title='Properties of the sample variance' boxType="proposition">
Suppose $\boldsymbol{x}\in S\subseteq\R^n$ is a sample of size $n\in\N_+$ from a real-valued variable. The sample variance $s^2$ has the following properties

1. $s^2 \geq 0$ and $s^2 = 0$ if and only if $x_i = x_j$ for each $i,j$ (positive definiteness)
2. if $c\in\R$ is a constant then $s^2(c\boldsymbol{x}) = c^2 s^2(\boldsymbol)$ and $s(c\boldsymbol{x}) = |c|s(\boldsymbol{x})$
3. if $\boldsymbol{c}$ is a sample of size $n$ from a constant $c$ then $s^2(\boldsymbol{x} + \boldsymbol{c}) = s^2(\boldsymbol{x})$
<details>
<summary>Details</summary>

2. Recall from Proposition $\ref{proposition-2}$ that $m(c\boldsymbol{x}) = cm(\boldsymbol{x})$ so that

$$
\begin{align*}
  s^2 (c\boldsymbol{x}) =& \frac{1}{n-1}\sum_{i=1}^n \left[ cx_i - cm(\boldsymbol{x})\right]^2 \\
  =& \frac{1}{n - 1}\sum_{i=1}^n c^2\left[x_i - m(\boldsymbol{x})\right]^2 \\
  =& c^2 s^2 (\boldsymbol{x})
\end{align*}
$$

3. Recall from Proposition $\ref{proposition-2}$ that $m(\boldsymbol{x} + \boldsymbol{c}) = m(\boldsymbol{x}) + c$ so that

$$
\begin{align*}
  s^2(\boldsymbol{x} + \boldsymbol{c}) =& \frac{1}{n-1}\sum_{i=1}^n \left( (x_i + c) - (m(\boldsymbol{x}) + c) \right)^2 \\
  =& \frac{1}{n-1}\sum_{i=1}^n \left[ x_i - m(\boldsymbol{x}) \right]^2 = s^2(\boldsymbol{x})
\end{align*}
$$
</details>
</MathBox>

### Sample covariance

<MathBox title='Sample covariance' boxType="definition">
Suppose that $x$ and $y$ are real-valued variables for a population and that $((x_i, y_i))_{i=1}^n$ is an observed sample of size $n$ from $(x,y)$. Let $\mathbf{x} = (x_i)_{i=1}^n$ and $\mathbf{y} = (y_i)_{i=1}^n$ denote the samples from $x$ and $y$, respectively.

The sample coveriance is defined as

$$
\begin{equation*}
\begin{split}
  s(\mathbf{x}, \mathbf{y}) =& \frac{1}{n-1} \sum_{i=1}^n [x_i - m(\mathbf{x})][y_i - m(\mathbf{y})] \\
  =& \frac{1}{n-1} \sum_{i=1}^n x_i y_i - \frac{n}{n-1} m(\mathbf{x}) m(\mathbf{y}) = \frac{n}{n-1} [m(\mathbf{xy}) - m(\mathbf{x}) m(\mathbf{y})] \\
  =& \frac{1}{2n(n-1)} \sum_{i=1}^n \sum_{j=1}^n (x_i - x_j)(y_i - y_j)
\end{split}
\tag{\label{equation-9}}
\end{equation*}
$$

<details>
<summary>Proof</summary>

The first alternate form is found from

$$
\begin{align*}
  \sum_{i=1}^n [x_i - m(\mathbf{x})][y_i - m(\mathbf{y})] =& \sum_{i=1}^n [x_i y_i - x_i m(\mathbf{y}) - y_i m(\mathbf{x}) + m(\mathbf{x}) m(\mathbf{y})] \\
  =& \sum_{i=1}^n x_i y_i - m(\mathbf{y}) \sum_{i=1}^n x_i - m(\mathbf{x}) \sum_{i=1}^n y_i + nm(\mathbf{x}) m(\mathbf{y}) \\
  =& \sum_{i=1}^n x_i y_i - nm(\mathbf{y})m(\mathbf{x}) - nm(\mathbf{x})m(\mathbf{y}) + nm(\mathbf{x})m(\mathbf{y}) \\
  =& \sum_{x=1}^n x_i y_i - nm(\mathbf{x})m(\mathbf{y})
\end{align*}
$$

The second alternate form is found by noting

$$
\begin{align*}
  \sum_{i=1}^n \sum_{j=1}^n (x_i - x_j)(y_i - y_j) =& \frac{1}{2n} \sum_{i=1}^n \sum_{j=1}^n [x_i - m(\mathbf{x}) + m(\mathbf{x}) - x_j][y_i - m(\mathbf{y}) + m(\mathbf{y}) - y_j] \\
  =& \sum_{j=1}^n \sum_{j=1} \left([x_i - m(\mathbf{x})][y_i - m(\mathbf{y})] + [x_i - m(\mathbf{x})][m(\mathbf{y}) - y_j] \right. \\
  &+ \left. [m(\mathbf{x}) - x_j][y_i - m(\mathbf{y})] + [m(\mathbf{x}) - x_j][m(\mathbf{y}) - y_j] \right)
\end{align*}
$$

We compute the sums term by term. The first term is

$$
  \sum_{i=1}^n \sum_{j=1}^n [x_i - m(\mathbf{x})][y_i - m(\mathbf{y})] = n \sum_{i=1}^n [x_i - m(\mathbf{x})][y_i - m(\mathbf{y})]
$$

The second two sums are $0$, while the last sum is

$$
\begin{align*}
  \sum_{i=1}^n [m(\mathbf{x}) - x_j] [m(\mathbf{y}) - y_j] =& n\sum_{j=1}^n [m(\mathbf{x}) - x_j][m(\mathbf{y}) - y_j] \\
  =& n \sum_{i=1}^n [x_i - m(\mathbf{x})][y_i - m(\mathbf{y})]
\end{align*}
$$

Combining the results leads to

$$
  \sum_{i=1}^n \sum_{j=1}^n (x_i - x_j)(y_i - y_j) = 2n \sum_{i=1}^n [x_i - m(\mathbf{x})][y_i - m(\mathbf{y})]
$$

Dividing by $2n(n-1)$, we retrieve the sample covariance

$$
  \frac{1}{2n(n-1)}\sum_{i=1}^n \sum_{j=1}^n (x_i - x_j)(y_i - y_j) = 2n \sum_{i=1}^n [x_i - m(\mathbf{x})][y_i - m(\mathbf{y})] = \frac{1}{n-1} \sum_{i=1}^n \sum_{j=1}^n [x_i - m(\mathbf{x})][y_i - m(\mathbf{y})] = s(\mathbf{x}, \mathbf{y})
$$
</details>
</MathBox>

<MathBox title='Properties of the sample covariance' boxType="proposition">
Suppose that $\mathbf{x}, \mathbf{y}, \mathbf{z} \in S\subseteq R^n$ are independent samples of size $n \in\N_+$ from a real-valued variable. The sample covariance $s(\mathbf{x}, \mathbf{y})$ has the following properties

1. **Variance identity:** $s(\mathbf{x}, \mathbf{x}) = s^2 (\mathbf{x})$
2. **Symmetry:** $s(\mathbf{x}, \mathbf{y}) = s(\mathbf{y}, \mathbf{x})$
3. **Bilinearity:** For all $\mathbf{x}, \mathbf{y}, \mathbf{z} \in\R^n$ 
$$
  s(\mathbf{x} + \mathbf{y}, \mathbf{z}) = s(\mathbf{x}, \mathbf{z}) + s(\mathbf{y}, \mathbf{z})
$$

For all $c \in \R$
$$
  s(c\mathbf{x}, \mathbf{y}) = cs(\mathbf{x}, \mathbf{y})
$$ 

Consequently for finite samples $\set{\mathbf{x}_i}_{i=1}^k$ and $\set{\mathbf{y}_j}_{j=1}^l$ and coefficients $\set{a_i}_{i=1}^k$ and $\set{b_j}_{i=1}^l$, then

$$
  s\left(\sum_{i=1}^k a_i \mathbf{x}_i, \sum_{j=1}^l b_j \mathbf{y}_j \right) = \sum_{i=1}^k \sum_{j=1}^l a_i b_j s(\mathbf{x}_i, \mathbf{y}_j)
$$

5. If $\mathbf{c}$ is a constant data set then $s(\mathbf{x}, \mathbf{c}) = 0$


6. **Quadratic identity**

$$
  s^2 (\mathbf{x} + \mathbf{y}) = s^2 (\mathbf{x}) + 2s(\mathbf{x}, \mathbf{y}) + s^2 (\mathbf{y})
$$

7. $-s(\mathbf{x}) s(\mathbf{y}) \leq s(\mathbf{x}, \mathbf{y}) \leq s(\mathbf{x}) s(\mathbf{y})$

<details>
  <summary>Proof</summary>

1. By definition
$$
  s(\mathbf{x}, \mathbf{x}) = \frac{1}{n-1} \sum_{i=1}^n (x_i - m(\mathbf{x})^2 = s^2 (\mathbf{x})
$$

2. This follows since multiplication is commutative
$$
\begin{align*}
  s(\mathbf{x}, \mathbf{y}) = \frac{1}{n-1} \sum_{i=1}^n [x_i - m(\mathbf{x})][y_i - m(\mathbf{y})] \\
  =& \frac{1}{n-1} \sum_{i=1}^n [y_i - m(\mathbf{y})][x_i - m(\mathbf{x})] = s(\mathbf{y}, \mathbf{x})
\end{align*}
$$

3. Recall from Proposition $\ref{proposition-2}$ that $m(\mathbf{x} + \mathbf{y}) = m(\mathbf{x}) + m(\mathbf{y})$. Hence
$$
\begin{align*}
  s(\mathbf{x} + \mathbf{y}, \mathbf{z}) =& \frac{1}{n-1} \sum_{i=1}^n [x_i + y_i - m(\mathbf{x} + \mathbf{y})][z_i - m(\mathbf{z})] \\
  =& \frac{1}{n-1} \sum_{i=1}^n ([x_i - m(\mathbf{x})] + [y_i - m(\mathbf{y})])[z_i - m(\mathbf{z})] \\
  =& \frac{1}{n-1} \sum_{i=1}^n [x_i - m(\mathbf{x})][z_i - m(\mathbf{z})] + \frac{1}{n-1} \sum_{i=1}^n [y_i - m(\mathbf{y})][z_i - m(\mathbf{z})] \\
  =& s(\mathbf{x}, \mathbf{z}) + s(\mathbf{y}, \mathbf{z})
\end{align*}
$$

Recall from Proposition $\ref{proposition-2}$ that $m(c\mathbf{x}) = cm(\mathbf{x})$. Hence
$$
\begin{align*}
  s(c\mathbf{x}, \mathbf{y}) =& \frac{1}{n-1} \sum_{i=1}^n [cx_i - m(c\mathbf{x})][y_i - m(\mathbf{y})] \\
  =& \frac{1}{n-1} \sum_{i=1} [cx_i - cm(\mathbf{x})][y_i - m(\mathbf{y})] = cs(\mathbf{x}, \mathbf{y})
\end{align*}
$$

Bilinearity follows from symmetry.

5. If $c_i = c$, then $m(\mathbf{c}) = c$ and hence $c_i - m(\mathbf{c}) = 0$ for each $i$.

6. From the preceding results
$$
\begin{align*}
  s^2 (\mathbf{x} + \mathbf{y}) =& s(\mathbf{x} + \mathbf{y}, \mathbf{x} + \mathbf{y}) \\
  =& s(\mathbf{x}, \mathbf{x}) + s(\mathbf{x}, \mathbf{y}) + s(\mathbf{y}, \mathbf{x}) + s(\mathbf{y}, \mathbf{y}) \\
  =& s^2 (\mathbf{x}) + 2s(\mathbf{x}, \mathbf{y}) + s^2 (\mathbf{y})
\end{align*}
$$
</details>
</MathBox>

### Sample correlation

Assuming that the data vectors are not constant, so that the standard deviations are positive, the sample correlation is defined by

$$
  r(\mathbf{x}, \mathbf{y}) = \frac{s(\mathbf{x}, \mathbf{y})}{s(\mathbf{x}) s(\mathbf{y})}
$$

<details>
  <summary>Details</summary>

The standard scores of $\mathbf{x}$ and $\mathbf{y}$ are, respectively

$$
\begin{align*} 
  \mathbf{u} =& \frac{1}{s(\mathbf{x})}[\mathbf{x} - m(\mathbf{x})] \\
  \mathbf{v} =& \frac{1}{s(\mathbf{y})}[\mathbf{y} - m(\mathbf{y})]
\end{align*}
$$

The correlation between $\mathbf{x}$ and $\mathbf{y}$ is the covariance of their standard scores, i.e. $r(\mathbf{x}, \mathbf{y}) = s(\mathbf{u}, \mathbf{v})$.
</details>

which satisfies the properties

1. **Symmetry:** $r(\mathbf{x}, \mathbf{y}) = r(\mathbf{y}, \mathbf{x})$
2. If $c \neq 0$ is a constant then
$$
  r(c\mathbf{x}, \mathbf{y}) = \begin{cases}
  r(\mathbf{x}, \mathbf{y}),\quad& c > 0 \\
  -r(\mathbf{x}, \mathbf{y}),quad& c < 0
  \end{cases}
$$

3. If $\mathbf{c}, \mathbf{d} \in\R^d$ are constant vectors, then $r(\mathbf{x} + \mathbf{c}, \mathbf{y} + \mathbf{d}) = r(\mathbf{x}, \mathbf{y})$
4. $|r(\mathbf{x}, \mathbf{y})| \leq 1$
5. $r(\mathbf{x}, \mathbf{y}) = -1$ if and only if the sample points lie on a line with negative slope
6. $r(\mathbf{x}, \mathbf{y}) = 1$ if and only if the sample points lie on a line with positive slope

<details>
  <summary>Proof</summary>

2. By the scaling property of covariance
$$
\begin{align*}
  r(c\mathbf{x}, \mathbf{y}) =& \frac{s(c\mathbf{x}, \mathbf{y})}{s(c\mathbf{x}) s(\mathbf{y})} \\
  =& \frac{cs(\mathbf{x}, \mathbf{y})}{|c| s(\mathbf{x}) s(\mathbf{y})} = \frac{c}{|c|} r(\mathbf{x}, \mathbf{y})
\end{align*}
$$

3.
$$
\begin{align*}
  r(\mathbf{x} + \mathbf{c}, \mathbf{y} + \mathbf{d}) =& \frac{s(\mathbf{x} + \mathbf{c}, \mathbf{y} + \mathbf{d})}{s(\mathbf{x} + \mathbf{c})s(\mathbf{y} + \mathbf{d})} \\
  =& \frac{s(\mathbf{x}, \mathbf{y})}{s(\mathbf{x})s(\mathbf{y})} = r(\mathbf{x}, \mathbf{y})
\end{align*}
$$
</details>

## Loss function

<MathBox title='Mean square error function' boxType="definition">
Suppose $\boldsymbol{x}\in S\subseteq\R^n$ is a sample of size $n\in\N_+$ from a real-valued variable. For $a\in\R$, the means square error function is defined by

$$
  \mathrm{mse}(a) = \frac{1}{n-1}\sum_{i=1}^n (x_i - a)^2
$$

The graph of $\mathrm{mse}$ is a convex parabola with minimum value $s^2$ at $a = m$, the sample mean.

<details>
<summary>Details</summary>

Taking the derivative of $\mathrm{mse}$ gives

$$
\begin{align*}
  \frac{\d}{\d a}\mathrm{mse}(a) =& -\frac{2}{n-1}\sum_{i=1}^n (x_i - a) \\
  =& -\frac{2}{n-1}(nm - na)
\end{align*}
$$

which shows that $\mathrm{mse}$ is minimized when $a = m$
</details>
</MathBox>

<MathBox title='Mean absolute error function' boxType="definition">
Suppose $\boldsymbol{x}\in S\subseteq\R^n$ is a sample of size $n\in\N_+$ from a real-valued variable. For $a\in\R$, the means absolute error function is defined by

$$
  \mathrm{mae}(a) = \frac{1}{n-1}\sum_{i=1}^n |x_i - a|
$$
</MathBox>

<MathBox title='Sample standard score' boxType="definition">
Suppose $\boldsymbol{x}\in S\subseteq\R^n$ is a sample of size $n\in\N_+$ from a real-valued variable. The standar score associated with $x_i$ is defined as

$$
  z_i = \frac{(x_i - m)}{s}
$$

The sample of standard scores $\boldsymbol(z) = \frac{\boldsymbol{x} - \boldsymbol{m}}{s}$ has mean $0$ and variance $1$.
</MathBox>

## Linear regression (least squares problem)

Suppose that $x$ and $y$ are real-valued variables for a population and that $((x_i, y_i))_{i=1}^n \subset \R^2$ is an observed sample of size $n$ from $(x,y)$. Let $\mathbf{x} = (x_i)_{i=1}^n$ and $\mathbf{y} = (y_i)_{i=1}^n$ denote the samples from $x$ and $y$, respectively.

We seek the line $\hat{y}(x) = a + bx$ that minimizes the quadratic loss

$$
\begin{equation*}
  Q(a, b) := \sum_{i=1}^n (y_i - a - bx_i)^2
\tag{\label{equation-12}}
\end{equation*}
$$

Assuming $s^2 (\mathbf{x}) > 0$, then $Q$ is a strictly convex function of $(a,b)$, thus admitting a unique minimizer. The minimal point of $Q$ is given by

$$
\begin{align*}
  b(\mathbf{x}, \mathbf{y}) =& \frac{s(\mathbf{x}, \mathbf{y})}{s^2 (\mathbf{x})} \\
  a(\mathbf{x}, \mathbf{y}) =& m(\mathbf{y}) - b(\mathbf{x}, \mathbf{y}) m(\mathbf{x}) = m(\mathbf{y}) - \frac{s(\mathbf{x}, \mathbf{y})}{s^2 (\mathbf{x})} m(\mathbf{x})
\end{align*}
$$

<details>
<summary>Proof</summary>

Differentiating $\eqref{equation-11}$ with respect to $a$ and $b$ yields

$$
\begin{align*}
  \frac{\partial Q}{\partial a} =& -2 \sum_{i=1}^n (y_i - a - bx_i) \\
  \frac{\partial Q}{\partial b} =& -2 \sum_{i=1}^n x_i (y_i - a - bx_i)
\end{align*}
$$

Setting the derivatives equal to zero results in the normal equations

$$
\begin{align*}
  \sum_{i=1}^n (y_i - a - bx_i) =& 0 \\
  \sum_{i=1}^n x_i (y_i - a - bx_i) =& 0
\end{align*}
$$

The first equation gives
 
$$
  a = m(\mathbf{y}) - bm(\mathbf{x})
$$

Substituting this into the second equation and solving for $b$ yields

$$
  b = \frac{n[m(\mathbf{xy}) - m(\mathbf{x}) m(\mathbf{y})]}{n[m(\mathbf{x}^2) - m^2 (\mathbf{x})]}
$$

Dividing the numerator and denominator in the last expression by $n - 1$ and using the computational formula above, we see that 

$$
  b = \frac{s(\mathbf{x}, \mathbf{y})}{s^2 (\mathbf{x})}
$$
</details>

The least squares regression line of $y$ on $x$ is therefore

$$
  \hat{y}(x) = m(\mathbf{y}) + \frac{s(\mathbf{x}, \mathbf{y})}{s^2 (\mathbf{x})}[x - m(\mathbf{x})]
$$

The minimum mean square error is

$$
  \operatorname{mse}[a(\mathbf{x}, \mathbf{y}), b(\mathbf{x}, \mathbf{y})] = s(\mathbf{y})^2 [1 - r(\mathbf{x}, \mathbf{y})]
$$

Thus, the total sample variance decomposes as

$$
  s^2 (\mathbf{y}) = s^2 (\mathbf{y}) r^2 (\mathbf{x}, \mathbf{y}) + s^2 (\mathbf{y}) (1 - r^2 (\mathbf{x}, \mathbf{y})
$$

where
- $s^2 (\mathbf{y}) r^2 (\mathbf{x}, \mathbf{y})$ is the explained variance
- $s^2 (\mathbf{y}) (1 - r^2 (\mathbf{x}, \mathbf{y}))$ is the residual variance

The statistics $r^2 (\mathbf{x}, \mathbf{y})$, called the sample coefficient of determination, represents the fraction of the total sample variance of $y$ explained by the linear regression on $x$.

The sample regression line with predictor variable $x$ and response variable $y$ is not the same as the sample regression line with predictor variable $y$ and response variable $x$. Specifically, the slopes for these two cases are given by

$$
  b_{y|x} = \frac{s(\mathbf{x}, \mathbf{y})}{s^2 (\mathbf{x})},\quad b_{x|y} = \frac{s(\mathbf{x}, \mathbf{y})}{s^2 (\mathbf{y})}
$$

These coincide only in the degenerate case $|\mathbf{r}(\mathbf{x}, \mathbf{y})| = 1$, when all sample points lie exactly on a line. In that case the residual variance is zero and the relationship is perfectly linear.

### Regression on a constant

Consider minimizing

$$
  Q(a) = \sum_{i=1}^n (y_i - a)^2
$$

Differentiation gives $a = m(\mathbf{y})$ and the minimum values of the mean squared error is

$$
  \operatorname{mse}(a) = \frac{1}{1 - n} \sum_{i=1}^n (y_i - m(\mathbf{y})^2 = s^2 (\mathbf{y})
$$

Thus, adding the linear term $bx$ reduces the mean squared error by $s^2 (\mathbf{y}) r^2 (\mathbf{x}, \mathbf{y})$, giving a fractional reduction of $r^2 (\mathbf{x}, \mathbf{y})$. If $r(\mathbf{x}, \mathbf{y}) = 0$, i.e. $\mathbf{x}$ and $\mathbf{y}$ are uncorrelated, the least squares regression line is the horizontal line

$$
  \hat{y}(x) = m(\mathbf{y})
$$

In this case the predictor $x$ provides no linear explanatory power for $y$.

### Sums of squares

We can define different sums of square statistics
- **Total sum of squares:**
$$
  \operatorname{sst}(\mathbf{y}) = \sum_{i=1}^n [y_i - m(\mathbf{y})]^2
$$

- **Regression sum of squares:**
$$
  \operatorname{ssr}(\mathbf{x}, \mathbf{y}) = \sum_{i=1}^n [\hat{y}_i - m(\mathbf{y})]^2
$$

- **Error sum of squares:**
$$
  \operatorname{sse}(\mathbf{x}, \mathbf{y}) = \sum_{i=1}^n (y_i - \hat{y}_i)^2
$$

The sums of squares are related as follows:
1. $\operatorname{ssr}(\mathbf{x}, \mathbf{y}) = r^2 (\mathbf{x}, \mathbf{y}) \operatorname{sst}(\mathbf{y})$
2. $\mathrm{sst}(\mathbf{y}) = \operatorname{ssr}(\mathbf{x}, \mathbf{y}) + \operatorname{sse}(\mathbf{x}, \mathbf{y})$

<details>
<summary>Proof</summary>

1. By definition of $\operatorname{sst}$ and $r$, we see that

$$
  r^2 (\mathbf{x}, \mathbf{y}) \operatorname{sst}(\mathbf{y}) = \frac{s^2 (\mathbf{x}, \mathbf{y})}{s^2 (\mathbf{x})}
$$

From the regression equation, we get

$$
  [\hat{y}_i - m(\mathbf{y})]^2 = \frac{s^2 (\mathbf{x}, \mathbf{y})}{s^4 (\mathbf{x})}[x_i - m(\mathbf{x})]^2
$$

Summing over $i$ gives

$$
  \operatorname{ssr}(\mathbf{x}, \mathbf{y}) = \sum_{i=1}^n [\hat{y}_i - m(\mathbf{y})]^2 = \frac{s^2 (\mathbf{x}, \mathbf{y})}{s^2 (\mathbf{x})}
$$

Hence

$$
  \operatorname{ssr}(\mathbf{x}, \mathbf{y}) = r^2 (\mathbf{x}, \mathbf{y}) \operatorname{sst}(\mathbf{y})
$$

2. Multiplying the result above by $n-1$ gives

$$
\begin{align*}
  \operatorname{sse}(\mathbf{x}, \mathbf{y}) = \operatorname{sst}(\mathbf{y}) - r^2 (\mathbf{x}, \mathbf{y}) \operatorname{sst}(\mathbf{y}) \\
  =& \operatorname{sst}(\mathbf{y}) - \operatorname{ssr}(\mathbf{x}, \mathbf{y})
\end{align*}
$$
</details>

In terms of sums of squares, the coefficient of determination is given by

$$
  r^2 (\mathbf{x}, \mathbf{y}) = \frac{\operatorname{ssr}(\mathbf{x}, \mathbf{y})}{\operatorname{sst}(\mathbf{y})}
$$

We can average $\operatorname{sse}$ by dividing by its degrees of freedom and then take the square root to obtain a standard error

$$
  \operatorname{se}(\mathbf{x}, \mathbf{y}) = \sqrt{\frac{\operatorname{sse}(\boldsymbol{x}, \mathbf{y})}{n - 2}}
$$

# Point estimation

<MathBox title="Estimator" boxType="definition">
Let $\theta\in \Theta\subseteq\R$ be an unknown real parameter. A real-valued statistic $U = u(\mathbf{X})$ that is used to estimate $\theta$ is called an estimator of $\theta$. The estimator is a random variable whose moments generally depend on $\theta$.

If $U$ is an estimator for $\theta$, the following can be defined
1. $U-\theta$ is the error
2. $\operatorname{bias}(U) = \mathbb{E}(U - \theta) = \mathbb{E}(U) - \theta$ is the bias of $U$
3. $\mathrm{mse}(U) = \mathbb{E}\left[(U - \theta)^2\right]$ is the mean square error of $U$

Depending on the sign of $\operatorname{bias}(U)$ we say that
1. $U$ is *unbiased* if $\operatorname{bias}(U) = 0$, or equivalently $\mathbb{E}(U) = \theta$
2. $U$ is negatively biased if $\operatorname{bias}(U)\leq 0$, or equivalently $\mathbb{E}(U)\leq\theta$
3. $U$ is positively biased if $\operatorname{bias}(U)\geq 0$, or equivalently $\mathbb{E}(U)\geq\theta$
</MathBox>

<MathBox title='Estimator bias' boxType="definition">
Let $\theta\in \Theta\subseteq\R$ be an unknown real parameter. If $U$ is an estimator for $\theta$, the following can be defined
1. $U-\theta$ is the error
2. $\operatorname{bias}(U) = \mathbb{E}(U - \theta) = \mathbb{E}(U) - \theta$ is the bias of $U$
3. $\mathrm{mse}(U) = \mathbb{E}\left[(U - \theta)^2\right] = \mathrm{var}(U) + \operatorname{bias}^2(U)$ is the mean square error of $U$.

<details>
<summary>Details</summary>

$$
\begin{align*}
  \mathbb{E}\left[(U - \theta)^2\right] =& \mathrm{var}(U-\theta) + \left[\mathbb{E}(U-\theta)\right]^2 \\
  =& \mathrm{var}(U) + \operatorname{bias}^2(U) 
\end{align*}
$$
</details>
</MathBox>

<MathBox title="Estimator efficiency" boxType="definition">
Let $U$ and $V$ be unbiased estimators of a parameter $\theta\in \Theta\subseteq\R$. Then
1. $U$ is *more efficient than* $V$ if $\mathrm{var}(U)\leq\mathrm{var}(V)$
2. the *relative efficiency** of $U$ with respect to $V$ is

$$
  \mathrm{eff}(U, V) = \frac{\mathrm{var}(V)}{\mathrm{var}(U)}
$$
</MathBox>

<MathBox title="" boxType="proposition">
Suppose that $\theta$ is a parameter with possible values in $T \subseteq (0,\infty)$ (with at least two points), and hat $U$ is a statistic with values in $T$. If $U^2$ is an unbiased estimator of $\theta^2$, then $U$ is a negatively biased estimator of $\theta$.

<details>
<summary>Proof</summary>

Note that

$$
\begin{align*}
  \operatorname{var}(U) =& \mathbb{E}(U^2) - [\mathbb{E}(U)]^2 \\
  =& \theta^2 - [\mathbb{E}(U)]^2
\end{align*}
$$

Since $T$ has at least two points, $U$ cannot be deterministic so $\operatorname{var}{U} > 0$. It follows that $[\mathbb{E}(U)]^2 < \theta^2$, so $\mathbb{E}(U) < \infty$ for $\infty \in T$.
</details>
</MathBox>

## Asymptotic properties

<MathBox title="Asymptotically unbiased" boxType="definition">
The sequence of estimators $\boldsymbol{U} = (U_i)_{i=1}^{n\in\N}$ is *asymptotically unbiased* if $\operatorname{bias}(U_n)\xrightarrow{n\to\infty} 0$ for every $\theta\in T$, or equivalently $\mathbb{E}(U_n)\xrightarrow{n\to\infty}\theta$. 
</MathBox>

<MathBox title='Asymptotic relative efficiency' boxType="definition">
Suppose $\boldsymbol{U} = (U_i)_{i=1}^{n\in\N}$ and $\boldsymbol{V} = (V_i)_{i=1}^{n\in\N}$ are two sequences of estimators that are asymptotically unbiased. The *asymptotic relative efficiency* of $\boldsymbol{U}$ to $\boldsymbol{V}$ is

$$
  \lim_{n\to\infty}\mathrm{eff}(U_n, V_n) = \lim_{n\to\infty}\frac{\mathrm{var}(V_n)}{\mathrm{var}(U_n)}
$$

assuming that the limit exists.
</MathBox>

<MathBox title="Consistency" boxType="definition">
Suppose $\boldsymbol{U} = (U_i)_{i=1}^{n\in\N}$ is a sequence of estimators for $\theta\in \Theta\subseteq\R$. Then
1. $\boldsymbol{U}$ is *consistent* if $U_n\xrightarrow{n\to\infty}\theta$ for each $\theta\in T$, i.e. $\mathbb{P}\left(|U_n - \theta| > \varepsilon \right) \xrightarrow{n\to\infty} 0$ for every $\varepsilon > 0$.
2. $\boldsymbol{U}$ is *mean-square consistent* if $\mathrm{var}(U_n) = \mathbb{E}\left[(U_n - \theta)^2 \right]$  
</MathBox>

<MathBox title='Consistency relation' boxType="proposition">
Suppose $\boldsymbol{U} = (U_i)_{i=1}^{n\in\N}$ is a sequence of estimators for $\theta\in \Theta\subseteq\R$. If $\mathbb{U}$ is mean-square consistent then $\mathbb{U}$ is consistent and asymptotically unbiased.

<details>
<summary>Proof</summary>

From Markov's inequality

$$
\begin{align*}
  \mathbb{P}\left(|U_n - \theta| > \varepsilon \right) =& \mathbb{P}\left[(U_n - \theta)^2 > \varepsilon \right] \\
  \leq& \frac{\mathbb{E}\left[ (U_n  - \theta)^2 \right]}{\varepsilon^2} \xrightarrow{n\to\infty} 0
\end{align*}
$$
</details>
</MathBox>

## The method of moments

Consider a statistical experiment with an observable real random value $X$. The distribution of $X$ has $k$ unknown real parameters $\boldsymbol{\theta} = (\theta_i)_{i=1}^k\in\Theta\subseteq\R^k$. Repeating the experiment $n$ times generates a random sample of size $n$ from the distribution of $X$ in the form $\boldsymbol{X} = (X_i)_{i=1}^n$. Thus, $\boldsymbol{X}$ is a sequence of independent random variables, each with the distribution of $X$. 

The method of moments is a technique fro constructing estimators of the parameters that is based on matching the sample moments with the corresponding distribution moments. The $j$-th moment of $X$ about $0$ is a function of $\boldsymbol{\theta}$

$$
  \mu^{(j)}(\boldsymbol{\theta}) = \mathbb{E}(X^j),\quad j\in\N_+
$$

where $\mu^{(1)}(\boldsymbol{\theta})$ is just the mean of $X$. The $j$-th sample moment about $0$ takes the form

$$
  M^{(j)}(\boldsymbol{X}) = \frac{1}{n}\sum_{i=1}^n X_i^j
$$

where $M^{(1)}(\boldsymbol{X})$ is the sample mean $\bar{X}_n$.

To construct the method of moments estimators $\boldsymbol{W} = (W_i)_{i=1}^k$ for the parameters $\boldsymbol{\theta} = (\theta_i)_{i=1}^k$ respectively, we consider the equations

$$
  \mu^{(j)}(\boldsymbol{W}) = M^{(j)}(\boldsymbol{X})
$$

consecutively for $j$ until we are able to solve for $\boldsymbol{W}$ in terms of $M^{(j)}$.

## Maximum likelihood estimator

<MathBox title='Likelihood function' boxType="definition">
Suppose $\boldsymbol{X}\in S$ is an observable random variable for a statistical experiment, depending on unknown parameters $\boldsymbol{\theta} = (\theta_i)_{i=1}^{k\in\N}\in\Theta\subseteq\R^k$. Let $f_{\boldsymbol{\theta}}$ denote the probability density function of $\boldsymbol{X}$ for $\boldsymbol{\theta}$. The likelihood function at $\boldsymbol{x}\in S$ is the function $L_{\boldsymbol{x}}:\Theta\to[0,\infty)$ given by

$$
  L_{\boldsymbol{x}} (\boldsymbol{\theta}) = f_{\boldsymbol{\theta}}(\boldsymbol{x})
$$

The logarithm of the likelihood function, called log-likelihood function, at $\boldsymbol{x}\in S$ is the function $\ln L_{\boldsymbol{x}}$ given by

$$
  \ell_{\boldsymbol{x}}(\boldsymbol{\theta}) := \ln L_{\boldsymbol{x}}(\boldsymbol{\theta}) = \ln f_{\boldsymbol{\theta}}(\boldsymbol{x})
$$

The likelihood function is the function obtained by reversing the roles of $\boldsymbol{x}$ and $\theta$ in the probability density function, i.e. we view $\theta$ as the variable and $\boldsymbol{x}$ as the given information.
</MathBox>

<MathBox title='Maximum likelihood estimator' boxType="definition">
Suppose $\boldsymbol{X}\in S$ is an observable random variable depending on epending on unknown parameters $\boldsymbol{\theta} = (\theta_i)_{i=1}^{k\in\N}\in\Theta\subseteq\R^k$. If the maximum of the likelihood function $L_{\boldsymbol{x}}(\boldsymbol{\theta})$ occurs at $u(\mathbf{x})\in\Theta$ for each $\mathbf{x}\in S$, the statistic $u(\mathbf{X})$ is a *maximum likelihood estimator* of $\theta$. 

Since the logarithm is a strictly monotonic function, the maximum of the log-likelihood function $\ell_{\boldsymbol{x}} (\boldsymbol{\theta})$ occurs for the same $u(\mathbf{x})\in\Theta$ as the maximum of $L_{\boldsymbol{x}} (\boldsymbol{\theta})$.  

The *maximum likelihood estimation* (MLE) for $\boldsymbol{\theta}$ is the values of $\boldsymbol{\theta}\in\Theta$ that maximizes the likelihood function $L_{\boldsymbol{x}}$, i.e.

$$
  \hat{\boldsymbol{\theta}}_{\mathrm{MLE}} = \argmax_{\boldsymbol{\theta}\in\Theta} L_{\boldsymbol{x}} (\boldsymbol{\theta}) = \argmax_{\boldsymbol{\theta}\in\Theta} \ell_{\boldsymbol{x}} (\boldsymbol{\theta})
$$

Intuitively, the maximum likelihood estimation$\hat{\boldsymbol{\theta}}_{\mathrm{MLE}}$ is the parameter that would have most likely produced the observed data. 
</MathBox>

If the likelihood function is differentiable, the maximum likelihood estimation can be found by solving

$$
  \frac{\partial}{\partial \theta_i} L_{\boldsymbol{x}}(\boldsymbol{\theta}) = 0,\; i\in\Set{1,2,\dots,n}
$$

or equivalently

$$
  \frac{\partial}{\partial \theta_i} \ln L_{\boldsymbol{x}}(\boldsymbol{\theta}) = 0,\; i\in\Set{1,2,\dots,n}
$$

The maximum likelhood estimation can be reformulated as a minimization problem by introducing the *negative log-likelihood*

$$
  \operatorname{NLL}_{\boldsymbol{x}}(\boldsymbol{\theta}) := -\ln L_{\boldsymbol{x}} (\boldsymbol{\theta}) = -\sum_{i=1}^n \ln g_{\boldsymbol{\theta}}(x_i)
$$

The maximum likelhood estimation is found from minimizing the negative log-likelihood

$$
  \hat{\boldsymbol{\theta}} = \argmin_{\boldsymbol{\theta}\in\Theta} \operatorname{NLL}_{\boldsymbol{x}}(\boldsymbol{\theta})
$$

<MathBox title='Reparametrized likelihood function' boxType="definition">
Suppose $h:\Theta\to\Lambda$ and let $\boldsymbol{\lambda} = h(\boldsymbol{\theta})$ denote the new parameter. Define the likelihood function for $\lambda$ at $\boldsymbol{x}\in S$ by

$$
  \hat{L}_{\boldsymbol{x}}(\boldsymbol{\lambda}) = \max\Set{ L_{\boldsymbol{x}}(\boldsymbol{\theta}) | \boldsymbol{\theta}\in h^{-1}(\boldsymbol{\lambda}) },\quad \boldsymbol{\lambda}\in\Lambda
$$

If $v(\boldsymbol{x})\in\Lambda$ maximized $\hat{L}_{\boldsymbol{x}}$ for each $\boldsymbol{x}\in S$ then $V = v(\boldsymbol{X})$ is a maximum likelihood estimator of $\lambda$. Conversely, if $U = u(\boldsymbol{x})\in\Theta$ is a maximum likelihood estimator for $\boldsymbol{\theta}$, then $V = h(U)$ is a maximum likelihood estimator for $\boldsymbol{\lambda}$. This is known as the invariance property.
</MathBox>

### Random samples

Suppose that $\mathbf{X} = (X_i)_{i=1}^n$ is a random sample of size $n$ from the distribution of a random variable $X$ taking values in $\R$, with probability density function $g_{\boldsymbol{\theta}}$ for $\boldsymbol{\theta}\in\Theta$. Then $\mathbf{X}$ takes values in $S = \R^n$ and since the $X_1$ are independently sampled from the same distribution, the likelihood and log-likelihood functions for $\boldsymbol{x} = (x_i)_{i=1}^n \in S$ are

$$
\begin{align*}
  L_{\boldsymbol{x}} =& \prod_{i=1}^n g_{\boldsymbol{\theta}}(x_i),\; \boldsymbol{\theta}\in\Theta \\
  \ln L_{\boldsymbol{x}} (\theta) =& \sum_{i=1}^n \ln g_{\boldsymbol{\theta}}(x_i),\; \boldsymbol{\theta}\in\Theta
\end{align*}
$$

In this case, the maximum likelihood estimation becomes

$$
  \hat{\boldsymbol{\theta}} = \argmax_{\boldsymbol{\theta}\in\Theta} \sum_{i=1}^n \ln g_{\boldsymbol{\theta}} (x_i)
$$

## Bayesian estimator

Consider a statistical experiment with an observable random variable $\boldsymbol{X}\in S$, whose distribution depends on parameters $\boldsymbol{\theta}\in T$. *Bayesian analysis* models the parameters $\boldsymbol{\theta}$ with a random variable $\boldsymbol{\Theta}$ that has a specified distribution on the parameter space $T$. This distribution is called the *prior distribution* of $\boldsymbol{\Theta}$ and reflects knowledge of the parameters $\boldsymbol{\theta}$ before sampling data. After observing $\boldsymbol{X} = \boldsymbol{x}\in S$, Bayes' theorem is used to compute the conditional distribution of $\boldsymbol{\Theta}$ given $\boldsymbol{X} = \boldsymbol{x}$. This distribution is called the *posterior distribution* of $\boldsymbol{\Theta}$ and reflects updated inferences of $\boldsymbol{\theta}$ given new information.

<MathBox title='Posterior distribution' boxType="definition">
Let $\boldsymbol{X}\in S$ be an observable random variable with probability density function $f$. Suppose the *prior distrubution* of $\boldsymbol{\Theta}$ on $T$ has probability density function $h$, and that given $\boldsymbol{\Theta} = \boldsymbol{\theta}\in T$, the conditional probability density function of $\boldsymbol{X}$ on $S$ is $f(\cdot|\boldsymbol{\theta})$. Then the probability density function of the posterior distribution of $\boldsymbol{\Theta}$ given $\boldsymbol{X} = \boldsymbol{x}\in S$ is

$$
  h(\boldsymbol{\theta}|\boldsymbol{x}) = \frac{h(\boldsymbol{\theta})f(\boldsymbol{x}|\boldsymbol{\theta})}{f(\boldsymbol{x})}
$$

where the unconditional probability function $f$ is defined as follows, in the discrete and continuous cases, respectively

$$
\begin{align*}
  f(\boldsymbol{x}) =& \sum_{\boldsymbol{\theta}\in T} h(\boldsymbol{\theta})f(\boldsymbol{x}|\boldsymbol{\theta}) \\
  f(\boldsymbol{x}) =& \int_T h(\boldsymbol{\theta})f(\boldsymbol{x}|\boldsymbol{\theta})\;\d\boldsymbol{\theta}
\end{align*}
$$

The most import and common special case occurs when $\boldsymbol{X} = (X_i)_{i=1}^{n\in\N}$ is a random sample of size $n$ from the distribution of an obervable random variable $X$. If $X$ is real-valued and has probability density function $g(\cdot|\boldsymbol{\theta})$ for a given $\boldsymbol{\theta}\in T$. In this case, $S=\R^n$ and the probability function $f(\cdot|\boldsymbol{\theta})$ of $\boldsymbol{X}$ given $\boldsymbol{\theta}$ is

$$
  f(\boldsymbol{x}|\boldsymbol{\theta}) = \prod_{i=1}^n g(x_i\mid \boldsymbol{\theta})
$$

<details>
<summary>Details</summary>

Note that the joint probability density function of $(\boldsymbol{X},\boldsymbol{\Theta})$ is a mapping on $f: S\times T\to [0, \infty)$ given by

$$
  (\boldsymbol{x}, \boldsymbol{\theta})\mapsto h(\boldsymbol{\theta})f(\boldsymbol{x}|\boldsymbol{\theta})
$$

The function $f(\boldsymbol{x})$ is the marginal probability density function of $\boldsymbol{X}$. 
</details>
</MathBox>

If the parameter space has finite measure $c$, then one possible prior distribution is the uniform distribution on $T$. If $\theta\in T\subseteq\R$ the probability density function is given by $h(\theta) = \frac{1}{c}$. This distribution reflects no prior knowledge about the parameter, and is called the *non-informative* prior distribution.

<MathBox title='Bayesian estimator' boxType="definition">
Suppose $\theta\in T\subseteq\R$ is a real-valued parameter for an observable random variable $\boldsymbol{X}$. The conditional expected value $\mathbb{E}(\theta|\boldsymbol{X})$ is the Bayesian estimator for $\theta$, which takes the form, in the discrete and continuous cases, respectively

$$
\begin{align*}
  \mathbb{E}(\theta|\boldsymbol{X} = \boldsymbol{x}) =& \sum_{\theta\in T} \theta h(\theta|\boldsymbol{x}) \\
  \mathbb{E}(\theta|\boldsymbol{X} = \boldsymbol{x}) =& \int_T \theta h(\theta|\boldsymbol{x})\;\d\boldsymbol{\theta}
\end{align*}
$$

The definitions of bias and mean square get conditioned on $\Theta = \theta\in T$. If $U$ is a Bayesian estimator for $\theta$ then

1. The *bias* of $\operatorname{bias}(U\mid \theta) = \mathbb{E}(U-\theta|\Theta = \theta)$
2. The *mean square error* of $U$ is $\mathrm{mse}(U\mid \theta) = \mathbb{E}\left[(U-\theta)^2 | \Theta = \theta \right] = \mathrm{var}(U\mid \theta) + \operatorname{bias}^2(U\mid \theta)$
</MathBox>

<MathBox title='Properties of Bayesian estimators' boxType="proposition">
Let $\boldsymbol{U} = (U_n)_{n\in\N_+}$ be a sequence of Baysian estimators of $\theta\in T\subseteq\R$. Then
1. $\boldsymbol{U}$ is *asymptotically unbiased* if $\operatorname{bias}(U_n\mid \theta)\xrightarrow{n\to\infty} 0$
2. $\boldsymbol{U}$ is *mean-square consistent* if $\mathrm{mse}(U_n\mid \theta)\xrightarrow{n\to\infty} 0$
</MathBox>

## Single variable models

Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space and let $X:\Omega\to\R$ be an integrable real random variable, i.e. $\mathbb{E}(|X|) < \infty$, with mean $\mathbb{E}(X) = \mu$ and variance $\operatorname{var}(X) = \sigma^2$. We sample from the distribution of $X$ to produce a sequence $\mathbf{X} = (X_i)_{i\in I\subset\N}$ of independent and identically distributed variables. For each $n\in\N_+$, we take a random sample $\mathbf{X}_n = (X_i)_{i=1}^n$ of size $n$ from the distribution of $X$.

### Mean estimator

The natural estimator of the distribution mean $\mu$ is the sample mean

$$
  M_n = \frac{1}{n} \sum_{i=1}^n X_i
$$

which is a partial sum process $Y_n = \sum_{i=1}^n X_i$ scaled by $1/n$. By Proposition $\ref{proposition-1-3}$, the expectation of $M_n$ is

$$
  \mathbb{E}(M_n) = n \frac{1}{n} \mathbb{E}(X_i) = \mu
$$

showing that the sample mean $M_n$ is an unbiased estimator of the population mean $\mathbb{E}(X)$. By Proposition $\ref{proposition-1-4}$, the variance of $M_n$ is

$$
\begin{equation*}
\begin{split}
  \operatorname{var}(M_n) =& \operatorname{var}\left(\frac{1}{n} Y_n \right) \\
  =& \frac{1}{n^2} \underbrace{\operatorname{var}(Y_n)}_{n\sigma^2} = \frac{\sigma^2}{n}
\end{split}
\tag{\label{equation-1}}
\end{equation*}
$$

### Variance estimator

For the variance estimators, we assume additionally that the third and fourth central moments of $X$ are finite, i.e. 
- $\mu_3 = \mathbb{E}[(X - \mu)^3] < \infty$
- $\mu_4 = \mathbb{E}[(X - \mu)^4] < \infty$

#### Special sample variance

If $\mu$ is known, a natural estimator of $\sigma^2$ is a special version of the sample variance, defined by

$$
  W_n^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \mu)^2
$$

which satisfies the properties
1. **Expectation:** $\mathbb{E}(W_n^2) = \sigma^2$
2. **Variance:** 
$$
\begin{equation*}
  \operatorname{var}(W_n^2) = \frac{1}{n} (\mu_4 - \sigma^4)
\tag{\label{equation-10}}
\end{equation*}
$$

3. $\lim_{n\to\infty} W_n^2 = \sigma^2$ with probability $1$
4. The distribution of $\sqrt{n} (W_n^2 - \sigma^2)/\sqrt{\mu_4 - \sigma^4}$ converges to the standard normal distribution as $n\to\infty$
5. $\mathbb{E}(W_n) \leq \sigma$
6. $\operatorname{cov}(M_n, W_n^2) = \mu_3/n$
7. $\operatorname{cor}(M_n, W_n^2) = \mu_3/\sqrt{\sigma^2 (\mu_4 - \sigma^4)}$

<details>
  <summary>Proof</summary>

1. Since $\mathbb{E}[(X - \mu)^2] = \sigma^2$, the expectation of $W_n^2$ is
$$
  \mathbb{E}(W_n^2) = \frac{1}{n} \sum_{i=1}^n \mathbb{E}[(X_i - \mu)^2] = \sigma^2
$$

2. Setting $Y_i := (X_i - \mu)^2$, we have

$$
\begin{align*}
  \operatorname{var}(W_n^2) =& \operatorname{var}\left(\frac{1}{n} \sum_{i=1}^n Y_i \right) \\
  =& \frac{1}{n^2} \sum_{i=1}^n \operatorname{var} (Y_i) = \frac{1}{n} \operatorname{var}(Y) \\
  =& \frac{1}{n} [\mathbb{E}(Y^2) - \mathbb{E}(Y)^2] \\
  =& \frac{1}{n} (\mathbb{E}[(X - \mu)^4] - \sigma^4)
\end{align*}
$$

5. This follows from the unbiased property and Jensen's inequality. Since $\omega\mapsto\sqrt{\omega}$ is concave downward on $[0,\infty)$, we have 
$$
\begin{align*}
  \mathbb{E}(W) =& \mathbb{E}(\sqrt{W^2}) \leq \sqrt{\mathbb{E}(W^2)} \\
  =& \sqrt{\sigma^2} = \sigma
\end{align*}
$$

6. From bilinearity of covariance and by independence
$$
\begin{align*}
  \operatorname{cov}(M_n, W_n^2) =& \operatorname{cov}\left(\frac{1}{n} \sum_{i=1}^n X_i, \frac{1}{n} \sum_{j=1}^n (X_j - \mu)^2 \right) \\
  =& \frac{1}{n^2} \sum_{i=1}^n \operatorname{cov}[X_i, (X_i - \mu)^2]
\end{align*}
$$

Furthermore
$$
\begin{align*}
  \operatorname{cov}[X_i, (X_i - \mu)^2] =& \operatorname{cov}[X_i - \mu, (X_i - \mu)^2] \\
  =& \mathbb{E}[(X_i - \mu)^3] - \mathbb{E}(X_i - \mu)\mathbb{E}[(X_i - \mu)^2] \\
  =& \mu_3
\end{align*}
$$

7. The follows from item **(6)**, the unbiased property and that $\operatorname{var}(M) = \sigma^2 /n$
</details>

In particular, property **(1)** means that $W_n^2$ is an unbiased estimator of $\sigma^2$. From Property **(2)**, it follows that $W_n^2$ is consistent estimator of $\sigma^2$.

#### Standard sample variance

If $\mu$ is uknown, a natural estimator of the distribution variance is the standard version of the sample variance, defined by

$$
  S_n^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - M_n)^2
$$

which satisfies the properties

1. **Expectation:** $\mathbb{E}(S_n^2) = \sigma^2$
2. **Variance:**
$$
\begin{equation*}
  \operatorname{var}(S_n^2) = \frac{1}{n} \left(\mu_4 - \frac{n-3}{n-1} \sigma^4 \right)
\tag{\label{equation-11}}
\end{equation*}
$$

3. $\lim_{n\to\infty} S^2 = \sigma^2$ with probability $1$
4. $\mathbb{E}(S) \leq\sigma$
5. $\operatorname{cov}(M_n, S_n^2) = \mu_3/n$
6. $\operatorname{cor}(M_n, S_n^2) = \frac{\mu_4}{\sigma\sqrt{\mu_4 - \sigma^4 (n-3)/(n-1)}}$

<details>
<summary>Proof</summary>

1. Write $X_i - M_n = (X_i - \mu) - (M_n - \mu)$ and expand

$$
\begin{equation*}
\begin{split}
  \sum_{i=1}^n (X_i - M_n)^2 =& \sum_{i=1}^n (X_i - \mu)^2 - 2(M_n - \mu) \underbrace{\sum_{i=1}^n (X_i - \mu)}_{n (M_n - \mu)} + n(M_n - \mu)^2 \\
  =& \sum_{i=1}^n (X_i - \mu)^2 - n(M_n - \mu)^2 
\end{split}
\tag{\label{equation-3}}
\end{equation*}
$$

Taking the expectation yields

$$
\begin{align*}
  \mathbb{E}\left[\sum_{i=1}^n (X_i - M_n)^2 \right] =& \sum_{i=1}^n \underbrace{\mathbb{E}[(X_i - \mu)^2]}_{=\sigma^2} - n \underbrace{\mathbb{E}[(M_n - \mu)^2]}_{=\sigma^2/n} \\
  =& n\sigma^2 - n\frac{\sigma^2}{n} = (1-n)\sigma^2
\end{align*}
$$

where it follows from $\eqref{equation-1}$ that

$$
  \operatorname{var}(M_n) = \mathbb{E}[(M_n - \mu)^2] - \underbrace{\mathbb{E}(M_n - \mu)}_{=0}^2 = \mathbb{E}\mathbb{E}[(M_n - \mu)^2] = \frac{\sigma^2}{n}
$$

Consequently, we get

$$
  \mathbb{E}(S_n^2) = \frac{1}{n-1} \mathbb{E}\left[\sum_{i=1}^n (X_i - M_n)^2 \right] = \sigma^2
$$

2. Let 

$$
\begin{align*}
  Y_i =& X_i - \mu \\
  A =& \sum_{i=1}^n Y_i^2 \\
  B =& \sum_{i=1}^n Y_i
\end{align*}
$$

then

$$
  M_n - \mu = \frac{B}{n},\quad n(M_n - \mu)^2 = \frac{B^2}{n}
$$

Using the identity $\eqref{equation-3}$, the sample variance estimator $S_n^2$ takes the form

$$
  S_n^2 = \frac{1}{n-1} \left(A - \frac{B^2}{n} \right)
$$

Taking the variance gives

$$
\begin{equation*}
  \var(S_n^2) = \frac{1}{(n - 1)^2} \left(\operatorname{var}(A) + \frac{1}{n^2} \operatorname{var} (B^2) - \frac{2}{n} \operatorname{cov} (A, B^2) \right)
\tag{\label{equation-7}}
\end{equation*}
$$

We now compute each term separately.

**Computing $\operatorname{var}(A)$:**

From $\eqref{equation-2}$, we have

$$
\begin{equation*}
  \operatorname{var}(A) = \operatorname{var}\left(\sum_{i=1}^n Y_i^2 \right) = n(E[(X - \mu)^4] - \sigma^4)
\tag{\label{equation-4}}
\end{equation*}
$$

**Computing $\operatorname{var}(B)$:**

We first need $\mathbb{E}(B^2)$ and $\mathbb{E}(B^4)$. Since $Y_i$ are centered and independent

$$
  \mathbb{E}(B^2) = n\sigma^2
$$

For the fourth moment, expand

$$
  B^4 = \sum_i Z_i^4 + 6 \sum_{i < j} Z_i^2 Z_j^2
$$

taking the expactation

$$
\begin{align*}
  \mathbb{E}(B^4) =& n \mathbb{E}[(X - \mu)^4] + 6\binom{n}{2} \sigma^4 \\
  =& n\mu_4 + 3n(n-1)\sigma^4
\end{align*}
$$

The variance of $B^2$ is then

$$
\begin{equation*}
\begin{split}
  \operatorname{var}(B^2) =& \mathbb{E}(B^4) - (\mathbb{E}(B^2))^2 \\
  =& n\mathbb{E}[(X - \mu)^4] + 3n(n-1)\sigma^4 - n^2 \sigma^4 \\
  =& n\mu_4 + (2n^2 - 3n)\sigma^4
\end{split}
\tag{\label{equation-5}}
\end{equation*}
$$

**Computing $\operatorname{cov}(A, B^2)$:**

Since

$$
  B^2 = \sum_{j=1}^n Z_j^2 + 2 \sum_{i < j} Z_i Z_j
$$

all mixed odd terms vanish under expectation, giving

$$
  \mathbb{E}(AB^2) = n\mathbb{E}[(X - \mu)^4] + n(n - 1)\sigma^4
$$

Thus

$$
\begin{equation*}
\begin{split}
  \operatorname{cov}(A, B^2) =& \mathbb{E}(AB^2) - \underbrace{\mathbb{E}(A)}_{=n\sigma^2} \underbrace{\mathbb{E}(B^2)}_{=n\sigma^2} \\
  =& n\mu_4 - n\sigma^4
\end{split}
\tag{\label{equation-6}}
\end{equation*}
$$

**Computing $\operatorname{var}(S_n^2)$:**

Inserting $\eqref{equation-4}$, $\eqref{equation-5}$ and $\eqref{equation-6}$ into $\eqref{equation-7}$ gives

$$
\begin{align*}
  \operatorname{var}(S_n^2) =& \frac{1}{(n-1)^2} \left(n\mu_4 - \sigma^4) + \frac{1}{n^2} (n\mu_4 + (2n^2 - 3n)\sigma^4) - \frac{2}{n}n(\mu_4 - \sigma^4) \right) \\
  =& \frac{1}{n} \left(\mu_4 - \frac{n-3}{n-1} \sigma^4 \right)
\end{align*}
$$

3. This follows from the strong law of large numbers. We have
$$
\begin{align*}
  S^2 =& \frac{1}{n-1} \sum_{i=1}^n X_i^2 - \frac{n}{n-1} M^2 \\
  =& \frac{n}{n-1} [M(\mathbf{X}^2) - M^2 (\mathbf{X})]
\end{align*}
$$

With probability $1$, we have $\lim_{n\to\infty} M(\mathbf{X}^2) = \sigma^2 + \mu^2$ and $\lim_{n\to\infty} M^2 (\mathbf{X}) = \mu^2$ and the result follows.
</details>

As for $W_n^2$, Property **(1)** shows that $S_n^2$ is an unbiased estimator of $\sigma^2$. Property **(3)** also means that $S_n^2$ is a consisten estimator of $\sigma^2$.

Comparing the estimators $W_n^2$ and $S_n^2$ we have

1. $\operatorname{var}(W_n^2) < \operatorname{var}(S_n^2)$
2. $\operatorname{cov}(W_n^2, S_n^2) = (\mu_4 - \sigma^4)/n$
3. $\operatorname{cor}(W_n^2, S_n^2) = \sqrt{\frac{\mu_4 - \sigma^4}{\mu_4 - \sigma^4 (n-3)/(n-1)}}$

<details>
<summary>Proof</summary>

1. Substracting $\eqref{equation-11}$ from $\eqref{equation-10}$ give
$$
  \operatorname{var}(S_n^2) - \operatorname{var}(W_n^2) = \frac{2\sigma^4}{n(n-1)} > 0
$$

Since $\sigma^2 \geq 0$ and $n\geq 2$, it follows that
$$
  \operatorname{var}(W_n^2) < \operatorname{var}(S_n^2)
$$

2. Using the decomposition
$$
  \operatorname{cov}(W_n^2, S_n^2) = \frac{1}{2n^2(n-1)} \sum_{i=1}^n \sum_{j=1}^n \sum_{k=1} \operatorname{cov}[(X_i - \mu)^2, (X_j - X_k)^2]
$$

We compute the covariance in this sum by considering disjoint cases
- $\operatorname{cov}[(X_i - \mu)^2] = 0$ if $j=k$ and there are $n^2$ such terms
- $\operatorname{cov}[(X_i - \mu)^2] = 0$ if $i, j, k$ are distinct, and there are $n(n-1)(n-2)$ such terms
- $\operatorname{cov}[(X_i - \mu)^2] = \mu_4 - \sigma^4$ if $j \neq k$ and there are $2n(n-1)$ such terms

3. By definition

$$
  \operatorname{cor}(W_n^2, S_n^2) = \frac{\operatorname{cov}(W_n^2, S_n^2)}{\sqrt{\operatorname{var}(W_n^2) \operatorname{var}(S_n^2)}}
$$

Substituting $\eqref{equation-10}$, $\eqref{equation-11}$ and item **(2)**, yields

$$
\begin{align*}
  \operatorname{cor}(W_n^2, S_n^2) =& \frac{\frac{\mu_4 - \sigma^4}{n}}{\sqrt{\frac{\mu_4 - \sigma^4}{n}\frac{1}{n} (\mu_4 - \frac{n-3}{n-1} \sigma^4)}} \\
  =& \sqrt{\frac{\mu_4 - \sigma^4}{\mu_4 - \frac{n-3}{n-1} \sigma^4}}
\end{align*}
$$
</details>

Note that $\lim_{n\to\infty} \operatorname{cor}(W_n^2, S_n^2) = 1$, meaning that both estimators are asymptotically equivalent.

## Bivariate models

# Hyptheses testing

<MathBox title='Statistical hypothesis' boxType="definition">
A statistical hypothesis is a statement about the distribution of a sample $\mathbf{X}$. Equivalently, a statistical hypothesis specifies a set of possible distributions of $\mathbf{X}$: the set of distributions for which the statement is true. A hypothesis that specifies a single distribution for $\mathbf{X}$ is called simple; a hypothesis that specifies more than one distribution for $\mathbf{X}$ is called composite.
</MathBox>

In hypothesis testing, the goal is to see if there is sufficient statistical evidence to reject a presumed *null hypothesis* in favour of a conjectured *alternative hypothesis*. The null hypothesis is usually denoted $H_0$, while the alternative hypothesis is usually denoted $H_1$.

<TableFigure caption="Hypothesis test">
| State/decision | Fail to reject $H_0$ | Reject $H_0$ |
| --- | --- | --- |
| $H_0$ true | Correct | Type 1 error |
| $H_1$ true | Type 2 error | Correct |
</TableFigure>

An hypothesis test is a statistical decision; the conclusion will either be to reject the null hypothesis in favour of the alternative, or to fail to reject the null hypothesis. The decision is based on the observed value $\mathbf{x}$ of the sample $\mathbf{X}$. Thus, we will find an appropriate subset $\mathbf{R}$ of the sample space $S$ and reject $H_0$ if and only if $\mathbf{x}\in R$. The set $R$ is known as the rejection/critical region.

An hypothesis test is in a sense a statistical analogy to proof by contradiction. Suppose $H_1$ is a logical statement and that $H_0$ is its negation. One way that we can prove $H_1$ is to assume $H_0$ and work our way logically to a contradiction. Similarly, in a hypothesis test we assume $H_0$ and then see if the data $\mathbf{x}$ are sufficiently at odds with that assumption that we feel justified in rejecting $H_0$ in favor of $H_1$.

Often, the critical region is defined in terms of a statistic $w(\mathbf{X})$, known as a *test statistic*, where $w:S\to T$ is a function from the sample space $S$ into another set $T$. We find an appropriate rejection region $R_T\subseteq T$ and reject $H_0$ when the observed value $w(\mathbf{x})\in R_T$. Thus, the rejection region in $S$ is $R = w^{-1}(R_T) = \Set{\mathbf{x}\in S | w(\mathbf{x})\in R_T}$. The use of a statistic often allows significant data reduction when the dimension of the test statistic is much smaller than the dimension of the data vector.

<MathBox title='Test errors' boxType="definition">
1. A *type 1 error* is rejecting the null hypothesis $H_0$ when $H_0$ is true.
2. A *type 2 error* is failing to reject the null hypothesis $H_0$ when the alternative hypothesis $H_1$ is true.
</MathBox>

If $H_0$ is true, then $\mathbb{P}(\mathbf{X}\in R)$ is the probability of a type 1 error for this distribution. If $H_0$ is composite, then $H_0$ specifies a variety of different distributions for $\mathbf{X}$ and thus there is a set of type 1 error probabilities.

<MathBox title='Significance level' boxType="definition">
The maximum probability of a type 1 error, over the set of distributions specified by $H_0$, is the *significance level* of the test or the $size$ of the critical region.
</MathBox>

The significance level is often denoted by $\alpha$. Usually, the rejection region is constructed so that the significance level is a prescribed, small value (typically $0.1, 0.05, 0.01$).

If $H_1$ is true, then $\mathbf{P}(\mathbf{X}\notin R)$ is the probability of a type 2 error for this distribution. If $H_1$ is composite, then $H_1$ specifies a variety of different distributions for $\mathbf{X}$, and thus there will be a set of type 2 error probabilities. Generally, there is a tradeoff between the type 1 and type 2 error probabilities. If we reduce the probability of a type 1 error, by making the rejection region $R$ smaller, we necessarily increase the probability of a type 2 error because the complementary region $S\setminus R$ is larger.

The extreme case can give us some insight. First, consider the decision rule in which we never reject $H_0$, regardless of the evidence $\mathbf{x}$. This corresponds to the rejection region $R=\emptyset$. A type 1 error is impossible, so the significance level is $0$. On the other hand, the probability for a type 2 error is $1$ for any distribution defined by $H_1$. At the other extreme, consider the decision rule in which we always reject $H_0$, regardless of the evidence $\mathbf{x}$. This corresponds to the rejection region $R = S$. A type 2 error is impossible, but now the probability of a type 1 error is $1$ for any distribution defined by $H_0$.

## Power

<MathBox title="Power" boxType="definition">
If $H_1$ is true, so that the distribution of $\mathbf{X}$ is specified by $H_1$, then $\mathbb{P}(\mathbf{X}\in R)$, the probability of rejecting $H_0$ is the *power* of the test for that distribution.
</MathBox>

The power of the test for a distribution specified by $H_1$ is the probability of making the correct decision.

<MathBox title='Uniformly powerful test' boxType="definition">
Suppose we have two tests, corresponding to rejection regions $R_1$ and $R_2$, respectively, each having significance level $\alpha$. The test with $R_1$ is *uniformly more powerful* than the test with region $R_2$ if for every distribution $\mathbf{X}$ specified by $H_1$

$$
  \mathbb{P}(\mathbf{X}\in R_1)\geq\mathbb{P}(\mathbf{X}\in R_2)
$$

If a test has significance level $\alpha$ and is uniformly more powerful than any other test with same significance level $\alpha$, then the test is the *uniformly most powerful test* at level $\alpha$.
</MathBox>

## $P$-value

<MathBox title='P-value' boxType="definition">
The $P$-value of the observed value $\mathbf{x}$ of $\mathbf{X}$, denoted $P(\mathbf{x})$, is defined to be the smallest $\alpha$ for which $\mathbf{x}\in R_\alpha$, i.e. the smallest significance level for which $H_0$ is rejected given $\mathbf{X} = \mathbf{x}$. Note that $P(\mathbf{x})$ is a statistic.

If $t(\mathbf{X})$ is an observed test statistic from an uknown distribution $T$, the $P$-value is what the prior probability would be of observing a test-statistic value at least as extreme as $t$ if the null hypothesis $H_0$ were true. That is:
- $P = \mathbb{P}(T \geq t | H_0)$ for a one-sided right-tail test statistic distribution
- $P = \mathbb{P}(T \leq t | H_0)$ for a one-sided left-tail test-statistic distribution
- $P = 2\min\Set{\mathbb{P}(T \geq t : H_0), \mathbb{P}(T \leq t : H_0)}$ for a two-sided test-statistic distribution. If the distribution of $T$ is symmetric about zero, then $P = \mathbb{P}(|T| \geq |t| : H_0)$
</MathBox>

Knowing $P(\mathbf{x})$ allows us to test $H_0$ at any significance level for the given data $\mathbf{x}$. If $P(\mathbf{x})\leq\alpha$ then we would reject $H_0$ at significance level $\alpha$; if $P(\mathbf{x})> \alpha$ then we fail to reject $H_0$ at significance level $\alpha$.

<MathBox title='The relation of $P$-values to posterior probabilities' boxType="definition">
A $P$-value is often misinterpreted as the likelihood of the data under the null hypothesis, so small value are intepreted to mean that $H_0$ is unlikely. This is reasoned as follows:

> If $H_0$ is true, then this test statistic would probably not occur. This statistic did occur. Therefore $H_0$ is probably false.

However, this is invalid reasoning. To see why, consider the following example:

> If a person is an American, then he is probably not a member of Congress. This person is a member of Congress. Therefore he is probably not an American.

This is obviously fallacious reasoning. By contrast, the following logical argument is valid reasoning:

> If a person is a Martian, then he is not a member of Congress. This person is a member of Congress. Therefore he is not a Martian.

The difference between these two cases is that the Martian example is using deduction, that is, reasoning forward from logical definitions to their consequences. More precisely, this example uses a rule from logic called *modus tollens*, in which we start out with a definition of the form $A\implies B$; when we observe $\lnot B$, we can conclude $\lnot A$. By contrast, the American example concerns *induction*, that is, reasoning backwards from observed evidence to probable (but not necessarily true) causes using statistical regularities, not logical definitions.

To perform induction, we need to probabilistic inference. Using Bayes' rule the probability of the null hypothesis given the observed data is

$$
  p(M_0 | \mathcal{S}) = \frac{p(\mathcal{S}|M_0)p(M_0)}{p(\mathcal{S}|M_0)p(M_0)p(M_0) + p(\mathcal{S}|M_1)p(M_1)}
$$

If the prior is uniform, i.e. $p(M_0) = p(M_1) = 0.5$, this can be rewritten in terms of the likelihood ratio $\operatorname{LR} = p(\mathcal{D}|M_0) / p(\mathcal{D}|M_1)$ as

$$
  p(M_0|\mathcal{D}) = \frac{\operatorname{LR}}{\operatorname{LR} + 1}
$$

In the American example, $\mathcal{S}$ is the observation that the person is a member of Congress. The null hypothesis $M_0$ is that the person is American, and the alternative hypothesis $M_1$ is that the person is not American. We assume that $p(\mathcal{S}|M_0)$ is low, since most Americans are not members of Congress. However, in this case $p(\mathcal{S}|M_1) = 0$ since only Americans can be members of Congress. Hence $\operatorname{LR} = \infty$, si $p(M_0|\mathcal{D}) = 1$, as intuition suggests. Note that NHST ignores $p(\mathcal{S}|M_1)$ as well as the prior $p(M_0)$, so it gives the wrong results.
</MathBox>

## Test of an unknown parameter

An important special class of hypothesis testing occurs when the distribution of the sample $\mathbf{X}$ depends on a parameter $\theta$ taking values in a parameter space $\Theta$. In this case the hypotheses take the form

$$
  H_0:\theta\in\Theta_0 \textrm{ versus } H_1:\theta\notin\Theta_0
$$

where $\Theta_0$ is a prescribed subset of the parameter space $\Theta$. In this setting, the probabilities of making an error or a correct decision depend on the true value of $\theta$. If $R$ is the rejection region, then the power function $Q$ is given by

$$
  Q(\theta) = \mathbb{P}_\theta (\mathbf{X}\in R),\quad \theta\in\Theta
$$

<MathBox title='Properties of power functions' boxType="proposition">
The power function $Q:\Theta\to[0,1]$ satisfies the following properties:
1. $Q(\theta)$ is the probability of a type 1 error when $\theta\in\Theta_0$.
2. $\max\Set{Q(\theta) : \theta\in\Theta_0 }$ is the significance level of the test.
3. $1-Q(\theta)$ is the probability of a type 2 error when $\theta\notin\Theta_0$.
4. $Q(\theta)$ is the power of the test when $\theta\notin\Theta_0$
</MathBox>

<MathBox title='Hypotheses tests for real parameters' boxType="definition">
Suppose $\theta\in\R$ is a real parameter and $\theta_0\in\Theta$ a specified value. Hypotheses tests for $\theta$ fall into three categories
1. Two-sided test: $H_0:\theta = \theta_0\textrm{ versus }H_1:\theta\neq\theta_0$
2. Left-sided test: $H_0:\theta \geq \theta_0\textrm{ versus }H_1:\theta < \theta_0$
3. Right-sided test: $H_0:\theta \leq \theta_0\textrm{ versus }H_1:\theta > \theta_0$ 
</MathBox>

## Equivalence with confidence sets

<MathBox title='' boxType="proposition">
Suppose $C(\mathbf{x})$ is a $1 - \alpha$ level confidence set for $\theta$. Consider the hypothesis

$$
  H_0:\theta = \theta_0 \textrm{ versus }H_1:\theta\neq\theta_0
$$

The following test has significance level $\alpha$ for the hypothesis: Reject $H_0$ if and only if $\theta_0\notin C(\mathbf{x})$.

<details>
<summary>Proof</summary>

By definition, $\mathbb{P}[\theta\in C(\mathbf{X})] = 1 - \alpha$. Thus, if $H_0$ is true so that $\theta = \theta_0$, then the probability of a type 1 error is $\mathbb{P}[\theta\notin C(\mathbf{X})] = \alpha$.
</details>
</MathBox>

In each case below, the confidence interval has confidence level $1 - \alpha$ and the test has significance level $\alpha$.
1. Suppose $L(\mathbf{X})$ is a confidence lower bound for $\theta$. Reject $H_0 : \theta\leq\theta_0$ versus $H_1: \theta > \theta_0$ if and only if $\theta_0 < L(\mathbf{X})$.
2. Suppose $U(\mathbf{X})$ is a confidence upper bound for $\theta$. Reject $H_0 : \theta\geq\theta_0$ versus $H_1: \theta < \theta_0$ if and only if $\theta_0 > U(\mathbf{X})$.
3. Suppose $[L(\mathbf{X}), U(\mathbf{X})]$ is a two-sided confidence interval for $\theta$. Reject $H_0 : \theta = \theta_0$ versus $H_1: \theta \neq \theta_0$ if and only if $\theta_0 < L(\mathbf{X})$ or $\theta_0 > U(\mathbf{X})$.

Confidence sets of an unknown parameter $\theta$ are often constructed through a *pivot variable*, i.e. a random variable $W(\mathbf{X},\theta)$ that depends on the data vector $\mathbf{X}$ and the parameter $\theta$, but whose distribution does not depend on $\theta$ and is known. In this case, a natural test statistic for the basic test given above is $W(\mathbf{X},\theta_0)$.

## Likelihood ratio tests

Let $\mathbf{X}$ be an observable random variable taking values in a set $S$. Suppose $\mathbf{X}$ has one of two possible distributions. This gives the following two simple hypotheses
- Null hypothessis $H_0:\mathbf{X}$ has probability density function $f_0$
- Alternative hypothesis $H_1:\mathbf{X}$ has probability density function $f_1$

<MathBox title='Likelihood ratio function' boxType="definition" tag="1">
Let $\mathbf{X}$ be a random variable in $S$. If $f_0, f_1: S\to (0,\infty)$ are probability density functions in $S$, the *likelihood ratio function* $L:S\to(0,\infty)$ is defined by

$$
  L(\mathbf{x}) = \frac{f_0 (\mathbf{x})}{f_1 (\mathbf{x})},\; \mathbf{x}\in S
$$

The statistic $L(\mathbf{X})$ is the *likelihood ratio statistic*.
</MathBox>

The likelihood ratio test is based on the fact that if we observe $\mathbf{X} = \mathbf{x}$, then the condition $f_1(\mathbf{x}) > f_0 (\mathbf{x})$ is evidence in favour of the alternative hypothesis $H_1$; the opposite inequality is evidence against $H_1$. Consequently, we reject $H_0$ if and only if $L \leq l$, where $l$ is a constant to be determined. The significance level of the test is $\alpha = \mathbb{P}_0 (L \leq l)$.

As usual, we can try to construct a test by choosing $l$ so that $\alpha$ is a prescribed value. If $\mathbf{X}$ has a discrete distribution, this will only be possible when $\alpha$ is a value of the distribution function of $L(\mathbf{X})$.

An important special case of this model occurs when the distribution of $\mathbf{X}$ depends on a parameter $\theta$ that has two possible values. Thus, the parameter space is $\set{\theta_0, \theta_1}$, and $f_i$ denotes the probability density function of $\mathbf{X}$ when $\theta = \theta_i$ for $i\in\set{0,1}$. In this case, the hypotheses are equivalent to $H_0 : \theta = \theta_0$ versus $H_1 : \theta = \theta_1$.

Another important special case is when $\mathbf{X} = (X_i)_{i=1}^n$ is a random sample of size $n$ from a distribution of an underlying random variable $X$ taking values in a set $R$. In this case, $S = R^n$ and the probability density function $f:R^n \to (0,\infty)$ of $\mathbf{X}$ has the form

$$
  f(x_1,\dots,x_n) = \prod_{i=1}^n g(x_i),\; (x_1,\dots,x_n) \in S
$$

where $g$ is the probability density function of $X$. In this case, the hypotheses simplify to
- $H_0: X$ has probability density function $g_0$
- $H_1: X$ has probability density function $g_1$

and the likelihood statistic is

$$
  L(X_1,\dots,X_n) = \prod_{i=1}^n \frac{g_0 (X_i)}{g_1 (X_i)}
$$

### Neyman-Pearson lemma

The Neyman-Pearson lemma shows that the the likelihood ratio test is a most powerful test.

<MathBox title='Neyman-Pearson lemma' boxType="lemma">
Let $\mathbf{X}$ be a random variable in $S$. Consider that test with rejection regions $R = \Set{\mathbf{x}\in S | L(\mathbf{x}) \leq l}$ and an arbitrary $A\subseteq S$. If the size of $R$ is at least as large as the size of $A$, then the test with rejection region $R$ is more powerful than the test with rejection region $A$. That is, if $\mathbb{P}_0 (\mathbf{X}\in R) \geq \mathbb{P}_0 (\mathbf{X}\in A)$ then $\mathbb{P}_1 (\mathbf{X}\in A)$.

<details>
<summary>Proof</summary>

Note that from the definitions of $R$ and $L$ (Definition $\ref{definition-1})$, the following inequalities hold

$$
\begin{align*}
  \mathbb{P}_0 (\mathbf{X}\in A) \leq& l\mathbb{P}_1 (\mathbf{X}\in A),\; A\subseteq R \\
  \mathbb{P}_0 (\mathbf{X}\in A) \geq& l\mathbf{P}_1 (\mathbf{X}\in A),\; A\subseteq R^c
\end{align*}
$$

For arbitrary $A\subseteq S$, write $R = (R \cap A) \cup (R\setminus A)$ and $A = (A \cap R) \cup (A\setminus R)$. From the additivity of probability and the inequalities above, it follows that

$$
  \mathbb{P}_1 (\mathbf{X}\in R) - \mathbb{P}_1 (\mathbf{X}\in A) \geq \frac{1}{l} [\mathbf{P}_0 (\mathbf{X}\in R) - \mathbb{P}_0 (\mathbf{X} \in A)]
$$

Hence, if $\mathbb{P}_0 (\mathbf{X}\in R) \geq \mathbb{P}_0 (\mathbf{X} \in A)$ then $\mathbb{P}_1 (\mathbf{X}\in R) \geq \mathbb{P}_1 (\mathbf{X}\in A)$.
</details>
</MathBox>

### Generalized likelihood ratio

The likelihood ratio statistic can be generalized to composite hypotheses. Suppose the probability density function $f_\theta$ of the data variable $\mathbf{X}$ depends on a parameter $\theta$, taking values in a parameter space $\Theta$. Consider the hypotheses $\theta \in \Theta_0$ versus $\theta \notin \Theta_0$, where $\Theta_0 \subseteq\Theta$. In this case that likelihood ratio function $L:S\to (0,\infty)$ is defined as

$$
  L(\mathbf{x}) = \frac{\sup\Set{f_\theta (\mathbf{x})|\theta\in\Theta_0}}{\sup\Set{f_\theta (\mathbf{x})|\theta\in\Theta}}
$$
