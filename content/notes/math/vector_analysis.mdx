---
title: 'Vector Analysis'
subject: 'Mathematics'
showToc: true
---

# Curvilinear coordinates

| System | $q_1$ | $q_2$ | $q_3$ | $h_1$ | $h_2$ | $h_3$ |
|---|:-:|:-:|:-:|:-:|:-:|:-:|
| Cartesian | $x$ | $y$ | $z$ | $1$ | $1$ | $1$ | 
| Spherical | $r$ | $\theta$ | $\phi$ | $1$ | $r$ | $r\sin\theta$ |
| Cylindrical | $s$ | $\phi$ | $z$ | $1$ | $s$ | $1$ |

Consider the 3-dimensional Euclidean space. In Cartesian coordinates the position of a point $P$ is given by

$$
  \mathbf{r} = x_1 \mathbf{e}_1 + x_2 \mathbf{e}_2 + x_3 \mathbf{e}_3
$$

where $\mathbf{e}_i := \frac{\partial\mathbf{r}}{\partial x_i}$ are the standard basis vectors. The point $P$ can also be defined by curviliner coordinates $q_i$ that are related to the Cartesian coordinates by the functions

$$
\begin{split}
  x_1 = f_1(q_1, q_2, q_3) 
\end{split}\quad
\begin{split}
  x_2 = f_2(q_1, q_2, q_3) 
\end{split}\quad
\begin{split}
    x_3 = f_3(q_1, q_2, q_3) 
\end{split}
$$

Assuming that the curvilinear coordinate system is orthogonal, its basis vectors $\mathbf{h}_i := \frac{\partial\mathbf{r}}{\partial q_i}$ are mutually perpendicular such that

$$
  \mathbf{h_i}\cdot\mathbf{h_j} = \delta_{ij} = \begin{cases}
    1 \quad i = j \\ 
    0 \quad i \neq j
  \end{cases}
$$

The norm of the basis vectors are given by $\left| \mathbf{h}_i \right| = \sqrt{\frac{\partial\mathbf{r}}{\partial q_i} \cdot \frac{\partial\mathbf{r}}{\partial q_i}}$. An orthonormal basis $\Set{\mathbf{b}_i }_{i=1}^3$ can thus be formed by setting

$$
  {\mathbf{b}}_i = \frac{\mathbf{h_i}}{h_i} \quad h_j := \sqrt{\frac{\partial\mathbf{r}}{\partial q_i} \cdot \frac{\partial\mathbf{r}}{\partial q_i}}
$$

The total differential change in $\mathbf{r}$ is then given by 

$$
  \mathrm{d}\mathbf{r} = \sum_{i=1}^3 \left(\frac{\partial\mathbf{r}}{\partial q_i}\right)\mathrm{d}q_i = \sum_{i=1}^3 h_i \mathrm{d}q_i \mathbf{b}_i
$$

From the total differential change we can form the differential volume element by

$$
  \mathrm{d}V = \prod_{i=1}^3 \left(\mathrm{d}\mathbf{r} \right)_i = h_1 h_2 h_3 \;\mathrm{d}q_1\;\mathrm{d}q_2\;\mathrm{d}q_3
$$

and differential surface elements by

$$
  \mathrm{d}\mathbf{a}_i = (\mathrm{d}\mathbf{r})_j \cdot (\mathrm{d}\mathbf{r})_k \mathbf{b}_i = h_j h_k \;\mathrm{d}q_j\;\mathrm{d}q_k \mathbf{b}_i
$$

## Gradient

Consider a scalar function $f(q_i) : \R^3 \to\R$. The total derivative of $f$ is given by

$$
  \mathrm{d}f := \nabla f \cdot\mathrm{d}\mathbf{r} = \sum_{i=1}^3 \frac{\partial f}{\partial q_i}\mathrm{d}q_i
$$

Evaluating the inner product $\nabla f\cdot \mathrm{d}\mathbf{r}$ we get

$$
  \mathrm{d}f = \nabla f \cdot\mathrm{d}\mathbf{r} = \nabla f \cdot \left( \sum_{i=1}^3 h_i \mathrm{d}q_i \mathbf{b}_i \right) = \sum_{i=1}^3 h_i \left( \nabla f \cdot\mathbf{b}_i \right)\mathrm{d}q_i
$$

Equating the terms of $\mathrm{d}f$ we see that

$$
  \nabla f \cdot \mathbf{b}_i = \frac{1}{h_i}\frac{\partial f}{\partial q_i}
$$

and hence the gradient of $f$ wrt. to the curvilinear coordinates is given by

$$
  \nabla f = \sum_{i=1}^3 \left(\nabla f \cdot \mathbf{b}_i \right)\mathbf{b}_i = \sum_{i=1}^3 \frac{1}{h_i}\frac{\partial f}{\partial q_i}\mathbf{b}_i
$$

## Divergence

Suppose $\mathbf{A}: \R^3 \to\R^3$ is a vector field with components $\mathbf{A} = \sum_{i=1}^3 A_i\mathbf{b}_i$. The divergence of $\mathbf{A}$ can be derived by writing out

$$
\begin{align*}
  \nabla\cdot\mathbf{A} &= \nabla\cdot\left( A_1 \mathbf{b}_1 + A_2 \mathbf{b}_2 + A_3 \mathbf{b}_3 \right) \\
  &= \nabla\cdot\left( h_2 h_3 A_1 \frac{\mathbf{b}_1}{h_2 h_3} + h_1 h_3 A_2 \frac{\mathbf{b}_2}{h_1 h_3} + h_1 h_2 A_3 \frac{\mathbf{b}_3}{h_1 h_2} \right) \\
  &= \nabla\cdot\left( h_2 h_3 A_1 \frac{\mathbf{b}_1}{h_2 h_3} \right) + \nabla\cdot\left( h_1 h_3 A_2 \frac{\mathbf{b}_2}{h_1 h_3} \right) + \nabla\cdot\left( h_1 h_2 A_3 \frac{\mathbf{b}_3}{h_1 h_2} \right)
\end{align*}
$$

Setting $f:= h_j h_k A_i$ and $\mathbf{v}:= \frac{\mathbf{b}_i}{h_j h_k}$, each divergence term takes the form $\nabla\cdot(f\mathbf{v})$ allowing us to apply the identity

$$
  \nabla\cdot(f\mathbf{v}) = \mathbf{v}\cdot\nabla f + f\nabla\cdot\mathbf{v}
$$

It can be further shown that $\nabla\cdot\mathbf{v} = \nabla\cdot\left( \frac{\mathbf{b}_i}{h_j h_k} \right) = 0$, by recalling that $\nabla q_i = \frac{\mathbf{b}_i}{h_i}$ and $\mathbf{b}_i \times \mathbf{b}_j = \sum_{k=1}^3 \varepsilon_{ijk}\mathbf{b}_k$ where $\varepsilon_{ijk}$ is the Levi-Civita symbol, so that

$$
  \nabla q_j \times\nabla q_k = \frac{\mathbf{b}_j \times\mathbf{b}_k}{h_j h_k} = \frac{\mathbf{b}_i}{h_j h_k}
$$

Taking the divergence of the left hand side, we may use the identity

$$
  \nabla\cdot(\mathbf{u}\times\mathbf{v}) = \mathbf{v}\cdot(\nabla\times\mathbf{u}) - \mathbf{u}\cdot(\nabla\times\mathbf{v})
$$

with $\mathbf{u}:= \nabla q_j$ and $\mathbf{v}:= q_k$. However, because gradients are curl free, i.e. $\nabla\times(\nabla f) = \mathbf{0}$ we conclude that

$$
\begin{align*}
  \nabla\cdot\left( \frac{\mathbf{b}_i}{h_j h_k} \right) &= \nabla\cdot\left(\nabla q_j \times\nabla q_k\right) \\
  &= \nabla q_k \left(\nabla \times \nabla q_j \right) - \nabla q_j \cdot\left( \nabla\times \nabla q_k \right) \\
  &= 0
\end{align*}
$$

It therefore follows that

$$
  \nabla\left( h_j h_k A_i \frac{\mathbf{b}_i}{h_j h_k} \right) = \frac{\mathbf{b}_i}{h_j h_k}\cdot \nabla(h_j h_k A_i) = \frac{1}{h_i h_j h_k}\frac{\partial}{\partial q_i}\left( h_j h_k A_i \right)
$$

where we have used that $\mathbf{q}_i \cdot \nabla f = \frac{1}{h_i}\frac{\partial f}{\partial q_i}$. In conclusion, the divergence of $\mathbf{A}$ wrt. curvilinear coordinates is given by

$$
  \nabla\cdot\mathbf{A} := \frac{1}{h_1 h_2 h_3}\left[\frac{\partial}{\partial q_1}\left(h_2 h_3 A_1\right) + \frac{\partial}{\partial q_2}\left(h_1 h_3 A_2\right) + \frac{\partial}{\partial q_3}\left(h_1 h_2 A_3\right)\right]
$$

## Curl

The curl of $\mathbf{A}$ can be derived by writing out

$$
\begin{align*}
  \nabla\times\mathbf{A} &= \nabla\times\left(\sum_{i=1}^3 A_i \mathbf{b}_i \right) \\
  &= \nabla\times\left(\sum_{i=1}^3 h_i A_i \frac{\mathbf{b}_i}{h_i} \right) \\
  &= \sum_{i=1}^3 \nabla\times\left(h_i A_i \frac{\mathbf{b}_i}{h_i} \right)
\end{align*}
$$

Setting $f:= h_i A_i$ and $\mathbf{v} = \frac{\mathbf{b}_i}{h_i}$, each curl term takes the form $\nabla\times (f\mathbf{v})$, allowing us to apply the identity

$$
  \nabla\times(f\mathbf{v}) = -\mathbf{v}\times\nabla f + f\nabla\times\mathbf{v}
$$

Since $\nabla\times\left(\frac{\mathbf{b}_i}{h_i}\right) = \nabla\times\left(\nabla q_i \right) = 0$, we are left with

$$
  \nabla\times\mathbf{A} = \sum_{i=1}^3 -\frac{\mathbf{b}_i}{h_i}\times\nabla\left(h_i A_i\right) = \sum_{i=1}^3 -\frac{\mathbf{b}_i}{h_i}\times\left[\sum_{j=1}^3 \frac{1}{h_j}\frac{\partial}{\partial q_j}\left( h_i A_i \right)\mathbf{b}_j \right] 
$$

Applyting the relation $\mathbf{b}_i \times \mathbf{b}_j = \sum_{k=1}^3 \varepsilon_{ijk}\mathbf{b}_k$, we conclude that

$$
  \nabla\times\mathbf{A} = \frac{\mathbf{q}_1}{h_2 h_3}\left[\frac{\partial}{\partial q_2}\left(h_3 A_3 \right) - \frac{\partial}{\partial q_3}\left(h_2 A_2 \right)\right] + \frac{\mathbf{q}_2}{h_1 h_3}\left[\frac{\partial}{\partial q_3}\left(h_1 A_1 \right) - \frac{\partial}{\partial q_1}\left(h_3 A_3 \right)\right] + \frac{\mathbf{q}_3}{h_1 h_2}\left[\frac{\partial}{\partial q_1}\left(h_2 A_2 \right) - \frac{\partial}{\partial q_2}\left(h_1 A_1 \right)\right]
$$

## Laplacian

The Laplacian of a scalar function $f:\R^3 \to\R$ is defined as the divergence of $\nabla f$. Using the results for the gradient and dirvergence in curvilinear coordinates, we find that

$$
\begin{align*}
  \nabla^2 f &:= \nabla\cdot\left(\nabla f \right) = \nabla\cdot\left(\sum_{i=1}^3 \frac{1}{h_1}\frac{\partial f}{\partial q_i}\mathbf{b}_i \right) \\
  &= \nabla\cdot\left( \frac{h_2 h_3}{h_1} \frac{\partial f}{\partial q_1} \frac{\mathbf{b}_1}{h_2 h_3} + \frac{h_1 h_3}{h_2} \frac{\partial f}{\partial q_2} \frac{\mathbf{b}_2}{h_1 h_3} + \frac{h_1 h_2}{h_3} \frac{\partial f}{\partial q_3} \frac{\mathbf{b}_3}{h_1 h_2} \right) \\
  &= \nabla\cdot\left( \frac{h_2 h_3}{h_1} \frac{\partial f}{\partial q_1} \frac{\mathbf{b}_1}{h_2 h_3}\right)  + \nabla\cdot\left(\frac{h_1 h_3}{h_2} \frac{\partial f}{\partial q_2} \frac{\mathbf{b}_2}{h_1 h_3}\right) + \nabla\cdot\left( \frac{h_1 h_2}{h_3} \frac{\partial f}{\partial q_3} \frac{\mathbf{b}_3}{h_1 h_2} \right) \\
  &= \frac{\mathbf{b_1}}{h_2 h3}\cdot\left(\frac{h_2 h_3}{h_1}\frac{\partial f}{\partial q_1} \right) + \frac{\mathbf{b_2}}{h_1 h3}\cdot\left(\frac{h_1 h_3}{h_2} \frac{\partial f}{\partial q_2} \right) + \frac{\mathbf{b_3}}{h_1 h2}\cdot\left(\frac{h_1 h_2}{h_3} \frac{\partial f}{\partial q_3} \right) \\
  &= \frac{1}{h_1 h_2 h_3}\left[ \frac{\partial}{\partial q_1}\left( \frac{h_2 h_3}{h_1}\frac{\partial f}{\partial q_1} \right) + \frac{\partial}{\partial q_2}\left( \frac{h_1 h_3}{h_2}\frac{\partial f}{\partial q_2} \right) + \frac{\partial}{\partial q_3}\left( \frac{h_1 h_2}{h_3}\frac{\partial f}{\partial q_3} \right) \right]
\end{align*}
$$

# Paths and curves

<MathBox title='Path and curve' boxType='definition'>
A continuous function $\alpha: I\subseteq\R\to\R^n$ is called a path. The image of $\alpha$ is called a curve $C$

$$
  C = \Set{ \mathbf{x}\in\R^n | \mathbf{x} = \alpha(t)\; t\in I }
$$

We say that $\alpha$ is a parametrization of the curve. We often refer to the input variable $t\in I$ as time, so that $\alpha(t)$ describes the position of a moving object at time $t$. 
</MathBox>

## Lines

Given a point $\mathbf{a}\in\R^n$ and a non-zero vector $\mathbf{v}\in\R$, the line through $\mathbf{a}$ and parallel to $\mathbf{v}$ is parametrized by $\alpha: \R\to\R^n$ where

$$
  \alpha(t) = \mathbf{a} + t\mathbf{v}
$$

## Calculus

### Derivative
<MathBox title='Derivative of paths' boxType='definition'>
Given a path $\alpha: I\to\R^n$ the derivative of $\alpha$ is defined by

$$
\begin{align*}
  \alpha'(t) &= \lim_{h\to 0} \frac{\alpha(t + h) - \alpha(t)}{h} \\
  &= \lim_{h\to 0} \frac{1}{h}\left(\left[x_1(t + h),\dots,x_n(t + h) \right] - \left[x_1(t), \dots,x_n(t) \right]\right) \\
  &= \lim_{h\to 0} \left( \frac{x_1(t+h) - x_1(t)}{h},\dots,\frac{x_n(t+h) - x_n(t)}{h} \right) \\
  &= \left( x_1'(t), \dots, x_n'(t) \right)
\end{align*}
$$

provided the limit exists. The derivative is also called the velocity $\mathbf{v}(t) := \alpha'(t) = \frac{\mathrm{d}\alpha}{\mathrm{d}t}$ of $\alpha$. The norm of $\mathbf{v}$ is called the speed $v$ of $\alpha$ and is defined as

$$
  v(t) := \lVert \mathbf{v}(t) \rVert
$$
</MathBox>

<MathBox title='Product rules' boxType='proposition'>
If $\alpha, \beta: I\to\R^n$ are differentiable paths and $f:I\to\R$ a differentiable function, the following product rules hold

- Dot product rule: $(\alpha\cdot\beta)' = \alpha'\cdot\beta + \alpha\cdot\beta'$
  - In the special case $\alpha = \beta$ we get $(\alpha\cdot\beta)' = (\lVert\alpha\rVert^2)' = 2\alpha \alpha\cdot\alpha'$
- Cross product rule: For $\alpha, \beta: I\to\R^3: $(\alpha\times\beta)' = \alpha'\times\beta + \alpha\times\beta'$
- Scalar multiplication: $(f\alpha)' = f'\alpha + f\alpha'$

<details>
<summary>Proof</summary>

**Dot product rule**
Note that $\alpha\cdot\beta : \R^n \timesmathbb{R}^n \to\R$ is real-valued, so that its derivative is given by

$$
  (\alpha\cdot\beta)' = \lim_{h\to 0} \frac{1}{h}\left[\alpha(t + h)\cdot\beta(t + h) - \alpha(t)\cdot\beta(t)\right]
$$

The quotient can be rewritten as

$$
\begin{align*}
  \frac{1}{h}\left[\alpha(t + h)\cdot\beta(t + h) - \alpha(t)\cdot\beta(t)\right] =& \frac{1}{h}\left[\alpha(t + h)\cdot\beta(t + h) - \alpha(t)\cdot\beta(t + h) \\
  &+ \alpha(t)\cdot\beta(t + h) - \alpha(t)\cdot\beta(t) \right] \\
  =& \frac{\alpha(t + h) - \alpha(t)}{h}\cdot\beta(t + h) + \alpha(t)\cdot\frac{\beta(t + h) - \beta(t)}{h}
\end{align*}
$$
</details>
</MathBox>

### Arclength
<MathBox title='Arclength' boxType='definition'>
The arclength of a differentiable path $\alpha: [a, b]\to\R^n$ is defined as

$$
  s(t) := \int_a^b v(t)\;\mathrm{d}t = \int_a^b \lVert\mathbf{v}(t)\rVert\;\mathrm{d}t
$$
</MathBox>

For a curve $\mathbf{R}:[a, b] \mapsto \R^n$, the length is defined as the limit of the sum of line segments, as the partition approaches infinity

$$
  \lim_{\Delta t \to \infty} \sum_i \frac{\left\lVert \mathbf{R} \left(t_{i + 1} \right) - \mathbf{R} \left(t_i \right) \right\rVert}{\Delta t} \Delta t = \int \left\lVert \frac{\mathrm{d}\mathbf{\mathbf{R}}}{\mathrm{d}t} \right\rVert \mathrm{d}t
$$

The line segment norm is given by

$$
\begin{align*}
  \left\lVert \frac{\mathrm{d}\mathbf{R}}{\mathrm{d}t} \right\rVert^2 = \frac{\mathrm{d}\mathbf{R}}{\mathrm{d}t} \cdot \frac{\mathrm{d}\mathbf{R}}{\mathrm{d}t} &= \left( \frac{\mathrm{d}x^i}{\mathrm{d}t} \frac{\partial \mathbf{R}}{\partial x^i} \right) \cdot \left( \frac{\mathrm{d}x^j}{\mathrm{d}t} \frac{\partial \mathbf{R}}{\partial x^j} \right) \\
  &= \frac{\mathrm{d}x^i}{\mathrm{d}t} \frac{\mathrm{d}x^j}{\mathrm{d}t} \left( \frac{\partial \mathbf{R}}{\partial x^i} \cdot \frac{\partial \mathbf{R}}{\partial x^j} \right) \\
  &= \frac{\mathrm{d}x^i}{\mathrm{d}t} \frac{\mathrm{d}x^j}{\mathrm{d}t} g_{ij}
\end{align*}
$$

where $g$ is the metric tensor. As a bilinear form, the metric tensor is transformed by two Jacobians

$$
  \tilde{g}_{ij} = \frac{\partial x^k}{\partial \tilde{x}^i} \frac{\partial x^l}{\partial \tilde{x}^j} g_{kl} = J_i^k J_j^l g_{kl}
$$


$$
  \left\lVert \frac{\mathrm{d}\mathbf{r}}{\mathrm{d}t} \right\rVert = \sqrt{\sum_i \sum_j \frac{\mathrm{d}x_i}{\mathrm{d}t} \frac{\mathrm{d}x_j}{\mathrm{d}t} \left( \frac{\partial \mathbf{r}}{\partial x_i} \frac{\partial \mathbf{r}}{\partial x_j} \right)} - \sqrt{\frac{\mathrm{d}x_i}{\mathrm{d}t} \frac{\mathrm{d}x_j}{\mathrm{d}t} \left( \frac{\partial \mathbf{r}}{\partial x_i} \frac{\partial \mathbf{r}}{\partial x_j} \right)}
$$

## Frenet-Serret formula

<MathBox title='Frenet vectors' boxType='definition'>
Let $C$ be a curve in $\R^3$ parametrized by the differentiable path $\alpha: I\to\R^3$. Then, the following vectors can be defined

- the unit tangent vector: $\mathbf{T}(t) = \frac{1}{\lVert\alpha'(t)\rVert}\alpha'(t)$ if $\alpha'(t) \neq \mathbf{0}$
- the principal normal vector: $\mathbf{N}(t) = \frac{1}{\lVert\mathbf{T}'\rVert}\mathbf{T}'(t)$ if $\mathbf{T}'(t) \neq \mathbf{0}$
- the binormal vector: $\mathbf{B}(t) = \mathbf{T}(t)\times\mathbf{N}(t)$

The vectors are called the Frenet vectors of $\alpha$.
</MathBox>

<MathBox title='Lemma' boxType='lemma'>
The derivative of the the binormal vector $\mathbf{B}'$ is always a scalar multiple of the principal normal $\mathbf{N}$, i.e. $\mathbf{B}'(t) = c(t)\mathbf{N}(t)$ where $c(t):\R\to\R$ is a scalar function.

<details>
<summary>Proof</summary>

If suffices to show that $\mathbf{B}'$ is orthogonal to (a) $\mathbf{B}$ and (b) $\mathbf{T}$. The only vectors orthogonal to both are precisely the scalar multiples of $\mathbf{N}$.

(a) Since $\mathbf{B}$ is a unit vector, $\lVert \mathbf{B} \rVert$ is constant. Hence $\mathbf{B}$ and its derivative $\mathbf{B}'$ are always orthogonal.

(b) By definition $\mathbf{B} = \mathbf{T}\times\mathbf{N}$, so using the cross product rule
$$
\begin{align*}
  \mathbf{B}' &= (\mathbf{T}\times\mathbf{N})' = \mathbf{T}'\times\mathbf{N} + \mathbf{T}\times\mathbf{N}' \\
  &= \left(\lVert\mathbf{T}'\rVert\mathbf{N}\right)\times\mathbf{N} + \mathbf{T}\times\mathbf{N}' = \\
  &= \mathbf{T}\times\mathbf{N}'
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Curvature and torsion' boxType='definition'>
Let $C$ be a curve in $\R^3$ parametrized by the differentiable path $\alpha: I\to\R^3$. Then, the curvature $\kappa$ of $C$ is defined as

$$
    \kappa(t) := \frac{\lVert \mathbf{T}(t) \rVert}{v(t)} = \frac{\lVert \mathbf{T}(t) \rVert}{\lVert \alpha'(t) \rVert}
$$

Given that $\mathbf{B}'(t) = c(t)\N$, the torsion $\tau$ of $C$ is defined as

$$
    \tau(t) := -\frac{c(t)}{v(t)}
$$
</MathBox>

<MathBox title='Frenet-Serret formulas' boxType='proposition'>
Let $\alpha:I\to\R^3$ be a differentiable path with speed $v$, Frenet vectors $(\mathbf{T}, \mathbf{N}, \mathbf{B})$, curvature $\kappa$ and torsion $\tau$. Assume that $v\neq 0$ and $\mathbf{T}' \neq\mathbf{0}$ so that the Frenet vectors are defined for all $t$. Then

$$
\begin{align*}
  \mathbf{T}' &= \kappa v\mathbf{N} \\
  \mathbf{N}' &= -\kappa v \mathbf{T} + \tau v\mathbf{B} \\
  \mathbf{B}' &= -\tau v\mathbf{N}
\end{align*}
$$

or in matrix form

$$
  \begin{bmatrix} \mathbf{T}' \\ \mathbf{N}' \\ \mathbf{B}' \end{bmatrix} = \begin{bmatrix} 
      0 & \kappa & 0 \\ 
      -\kappa & 0 & \tau \\
      0 & -\tau & 0
  \end{bmatrix} \cdot \begin{bmatrix} \mathbf{T} \\ \mathbf{N} \\ \mathbf{B} \end{bmatrix}
$$

<details>
<summary>Proof</summary>

The first equation follows from the definitions $\mathbf{N} = \frac{1}{\lVert\mathbf{T}\rVert}\mathbf{T}'$ and $\kappa = \frac{\lVert\mathbf{T}'\rVert}{v}$, giving

$$
  \mathbf{T}' = \lVert\mathbf{T}'\rVert\mathbf{N} = \kappa v\mathbf{N}
$$

For the third equation, we know that $\tau = -\frac{c}{v}$ such that

$$
  \mathbf{B}' = c\mathbf{N} = -\tau v\mathbf{N}
$$

The second equation follows from the cross product of $\mathbf{N} = \mathbf{B}\times\mathbf{N}$ and the first and third equations

$$
\begin{align*}
  \mathbf{N}' &= (\mathbf{B}\times\mathbf{T})' = \mathbf{B}'\times\mathbf{T} + \mathbf{B}\times\mathbf{T}' \\
  &= -\tau v\mathbf{N}\times\mathbf{T} + \mathbf{B}\times(\kappa v\mathbf{N}) = -\tau v(\underbrace{\mathbf{N}\times\mathbf{T}}_{=-\mathbf{B}}) + \kappa v(\underbrace{\mathbf{B} + \mathbf{N}}_{-\mathbf{T}}) \\
  &= -\kappa v \mathbf{T} + \tau v\mathbf{B}
\end{align*}
$$
</details>
</MathBox>

<MathBox title='$v\kappa\tau$ theorem' boxType='theorem'>
Let $\alpha,\beta :I\to\R^3$ be differentiable paths with nonzero $v$ and $\mathbf{T}'$ so that the Frenet vectors are defined for all $t$. Then either path can be translated and rotated so that it fits exactly on top of the other if and only if they have the same speed $v$, curvature $\kappa$ and torsion $\tau$. In this case, the two paths are said to be congruent to each other. 

<details>
<summary>Proof</summary>

**Translations and rotations preserve $v, \kappa, \tau$**<br/>
Suppose a path $\alpha:I\to\R^3$ is translated by a constant $\mathbf{c}\in\R^3$ to obtain a new path $\beta(t) = \alpha(t) + \mathbb{c}$. Then $\beta'(t) = \alpha'(t)$, so $\alpha$ and $\beta$ have the same velocity and therefore the same speed. It follows that $\beta$ and $\alpha$ have the same Frenet vectors and thus the same curvature and torsion. Hence, speed, curvature and torsion are preserved under translations.

Suppose $\alpha$ is rotated in $\R^3$ to obtain a new path $\beta(t) = R(\alpha(t) + \mathbf{c})$ where $R$ is a rotation matrix such that $RR^T = I$. Then intuitively, the velocities $\alpha'$ and $\beta'$ are related by the same rotation, and so are the Frenet vectors. The matrix $Q$ whose rows are the Frenet vectors also transforms as $Q\mapsto QR$ such that

$$
  \frac{\mathrm{d}(QR)}{\mathrm{d}t}(QM)^T = \frac{\mathrm{d}Q}{\mathrm{d}t}RR^T Q^T = \frac{\mathrm{d}Q}{\mathrm{d}t} Q^T
$$

Hence the entries $\kappa$ and $\tau$ of $\frac{\mathrm{d}Q}{\mathrm{d}t}Q^T$ (Frenet-Serret formulas) are preserved under rotations. 

**Paths with same $v, \kappa, \tau$ are congruent**<br/>
Suppose the paths $\alpha, \beta$ have the same speed, curvature and tension. The two paths are congruent if one can be rigidly moved to the other through translation and rotation. Consider a point $a\in I$. We first translate $\alpha$ by a constant vector $\mathbf{c} = \beta(a) - \alpha(a)$ to get a new path $\gamma(t) = \alpha(t) + \mathbf{d}$ such that $\gamma(a) = \beta(a) =: \mathbf{x}_0$. As a translate, the $v,\kappa,\tau$ of $\gamma$ are preserved. 

The unit tangents  to $\gamma$ and $\beta$ at $\mathbf{x}_0$ may not be equal, but $\gamma$ can be rotated until they are. Next, $\gamma$ can be rotated again about this tangent direction until the principal normals align at $\mathbf{x}_0$. The binormals are automatically the same, since they are the cross products of the unit tangents and the principal normal. Under these transformations, the final path $\tilde{\alpha}$ satisfies the following three conditions
- $\tilde{\alpha}(a) = \beta(a) = \mathbf{x}_0$
- $\tilde{\alpha}$ and $\beta$ have the same Frenet vectors at $\mathbf{x}_0$
- $\tilde{\alpha}$ and $\beta$ have the same speed, curvature and torsion for all $t$

To complete the proof, it remains to show that $\tilde{\alpha} = \beta$ using the Frenet-Serret formulas. We denote the Frenet vectors of $\tilde{\alpha}$ by $\left(\tilde{\mathbf{T}}, \tilde{\mathbf{N}}, \tilde{\mathbf{B}}\right)$ and those of $\beta$ by $\left(\mathbf{T}, \mathbf{N}, \mathbf{B}\right)$. Using similar notation, we further have $\tilde{v} = v$, $\tilde{\kappa}= \kappa$ and $\tilde{\tau} = \tau$ by construction. Let $\theta$ be the angle between $\tilde{\mathbf{T}}$ and $\mathbf{T}$. Then 

$$
\begin{gather*}
  \tilde{\mathbf{T}}(t)\cdot\mathbf{T}(t) = \lVert\tilde{\mathbf{T}}(t)\rVert \cdot \lVert\mathbf{T}(t)\rVert\cos\theta \\
  \cos\theta \begin{cases} = 1, \quad \theta = 0 \iff \tilde{\mathbf{T}}(t) = \mathbf{T}(t) \\ \leq 1, \quad \textrm{otherwise} \end{cases}
\end{gather*}
$$

Noting the same for $\tilde{\mathbf{N}}(t)\cdot\mathbf{N}(t)$ and $\tilde{\mathbf{B}}(t)\cdot\mathbf{B}(t)$, we may define $f:I\to\R$ by

$$
\begin{align*}
  f(t) &= \tilde{\mathbf{T}}(t)\cdot\mathbf{T}(t) + \tilde{\mathbf{N}}(t)\cdot\mathbf{N}(t) + \tilde{\mathbf{B}}(t)\cdot\mathbf{B}(t) \\
  &\begin{cases} 
    = 3 \quad \iff \tilde{\mathbf{T}}(t) = \mathbf{T}(t), \tilde{\mathbf{N}}(t) = \mathbf{N}(t), \tilde{\mathbf{B}}(t) = \mathbf{B}(t) \\
    \leq 3 \quad \textrm{otherwise}
  \end{cases}
\end{align*}
$$

Note that by construction $f(a) = 3$. Differentiating $f$ and using the Frenet-Serret formulas gives

$$
\begin{align*}
  f' =& \left(\tilde{\mathbf{T}}'\cdot\mathbf{T} + \tilde{\mathbf{T}}\cdot\mathbf{T}' \right) + \left(\tilde{\mathbf{N}}'\cdot\mathbf{N} + \tilde{\mathbf{N}}\cdot\mathbf{N}' \right) + \left(\tilde{\mathbf{B}}'\cdot\mathbf{B} + \tilde{\mathbf{B}}\cdot\mathbf{B}' \right) \\
  =& \left(\kappa v\tilde{\mathbf{N}}\cdot\mathbf{T} + \tilde{\mathbf{T}}\cdot\kappa v \mathbf{N} \right) \\
   &+ \left[\left(-\kappa v \tilde{\mathbf{T}} + \tau v\tilde{\mathbf{B}}\right)\cdot\mathbf{N} + \tilde{\mathbf{N}}\cdot\left(-\kappa v\mathbf{T} + \tau\mathbf{B}\right)\right] \\
  &+ \left[\left(-\tau v\tilde{\mathbf{N}}\right)\cdot\mathbf{B} + \tilde{\mathbf{B}}\cdot\left(-\tau v \mathbf{N}\right)\right] \\
  =& \kappa v \left[\left(\tilde{\mathbf{N}}\cdot\mathbf{T} - \tilde{\mathbf{N}}\cdot\mathbf{T} \right) + \left(\tilde{\mathbf{T}}\cdot\mathbf{N} - \tilde{\mathbf{T}}\cdot\mathbf{N}\right)\right] \\
   &+ \tau v \left[\left(\tilde{\mathbf{B}}\cdot\mathbf{N}\right) - \left(\tilde{\mathbf{B}}\cdot\mathbf{N}\right) + \left(\tilde{\mathbf{N}}\cdot\mathbf{B} - \tilde{\mathbf{N}}\cdot\mathbf{B}\right) \right] \\
  =& 0
\end{align*}
$$

Hence $f$ is a constant function. Since $f(a) = 3$ it follows that $f(t) = 3$ for all $t$. This implies that $\tilde{\mathbf{T}}(t) = \mathbf{T}(t)$ for all $t$. By the definition of the unit tangent, this gives

$$
  \frac{\tilde{\alpha}'(t)}{v(t)} = \frac{\beta'(t)}{v(t)} \iff \tilde{\alpha}'(t) = \beta'(t) \; \forall t
$$

Consequently $\tilde{a}$ and $\beta$ differ by a constant vector. Since $\tilde{a}(a) = \beta(a) = \mathbf{x}_0$ this constant is zero, giving $\tilde{\alpha}(t) = \beta(t)$ for all $t$.
</details>
</MathBox>

# Real-valued functions (scalar fields)

## Graphs and level sets

<MathBox title='Graph' boxType='definition'>
Let $f:A\subset \R^n \to\R$ be a scalar function. The graph of $f$ is the set

$$
  \Set{ \left(x_i \right)_{i=0}^{n+1} \in\R^{n+1} | \left(x_i \right)_{i=1}^n \in A \textrm{ and } x_{n+1} = f(x_1,x_2,\dots,x_n) }
$$
</MathBox>

<MathBox title='Level set' boxType='definition'>
Let $f:A\subset \R^n \to\R$ be a scalar function. For a given $c \in f[A] \subseteq\R$, its corresponding level set is the set of all points in the domain at which $f$ has a value of $c$, i.e. $\Set{\mathbf{x}\in A : f(\mathbf{x}) = c }$. For functions of two variables, level sets are also called level curves and, for functions of three variables, level surfaces.
</MathBox>

## Open sets

<MathBox title='Open ball' boxType='definition'>
Consider a point $\mathbf{a}\in\R^n$ and assume $r > 0$. The open ball centered at $\mathbf{a}$ and with radius $r$ is defined as
$$
  B_r(\mathbf{a}) = \Set{ \mathbf{x}\in\R | \lVert \mathbf{x} - \mathbf{a} \rVert < r }
$$
</MathBox>

Consider a subset $U\subseteq\R^n$. The subset is called 
- open if, given any point $\mathbf{a}\in U$, there exists a positive real number $r$ such that $B_r(\mathbf{a})\subset U$
- closed if its complement $U^c = \R^n \backslash U = \Set{\mathbf{x}\in\R^n | \mathbf{x}\notin U}$ is open.

## Continuity

<MathBox title='Continuous real-valued function' boxType='definition'>
Let $f: U\subseteq\R^n \to\R$ be a real-valued function and consider a point $\mathbf{a}\in U$. Then, $f$ is called continuous at $\mathbf{a}$ if, given any open ball $B_\epsilon (f(\mathbf{a}))$ about $f(\mathbf{a})$, there exists an open ball $B_\delta (\mathbf{a})$ about $\mathbf{a}$ such that

$$
  f(B_\delta (\mathbf{a})) \subset B_\epsilon (f(\mathbf{a}))
$$

Equivalently, writing out the definition of the open ball, $f$ is continuous at $\mathbf{a}$ if, given any $\epsilon > 0$, there is a $\delta > 0$ such that

$$
  \lVert\mathbf{x} - \mathbf{a}\rVert < \delta : |f(\mathbf{x}) - f(\mathbf{a}) | < \epsilon
$$

The function $f$ is simply called continuous if it is continuous at every point of its domain $U$.
</MathBox>

<MathBox title='Properties of continuous real-valued functions' boxType='proposition'>
Let $f, g: U\to\R$ be real-valued functions defined on an open set $U\subseteq\R^n$. If $f$ and $g$ are continuous at a point $\mathbf{a}\in U$, then so are
- the sum $f + g$
- $cf$ for any scalar $c\in\R$
- the product $fg$
- the quotient $f/g$, assuming $g(\mathbf{a})\neq 0$

<details>
<summary>Proof</summary>

**Sum**
Given $\epsilon > 0$, we need to find $\delta > 0$ such that 

$$
  \mathbf{x}\inB_\delta (\mathbf{a}) \implies \left|\left[f(\mathbf{x}) + g(\mathbf{x})\right] - \left[f(\mathbf{a}) + g(\mathbf{a})\right]\right| < \epsilon 
$$

The absolute values of the $f$ and $g$ contributions can be separated by applying the triangle inequality

$$
\begin{align*}
  \left|\left[f(\mathbf{x}) + g(\mathbf{x})\right] - \left[f(\mathbf{a}) + g(\mathbf{a})\right]\right| &= \left|\left[f(\mathbf{x}) - f(\mathbf{a})\right] + \left[g(\mathbf{x}) - g(\mathbf{a})\right]\right| \\
  &\overset{\triangle}{\leq} \left|f(\mathbf{x}) - f(\mathbf{a})\right| + \left|g(\mathbf{x}) - g(\mathbf{a})\right|
\end{align*}
$$

Since $f$ and $g$ are continuous at $\mathbf{a}$, then given any $\epsilon_f = \epsilon_g = \frac{\epsilon}{2}$, there are $\delta_f > 0$ and $\delta_g > 0$ such that 

$$
\begin{align*}
  \left|f(\mathbf{x}) - f(\mathbf{a})\right| < \epsilon_f \land \mathbf{x}\inB_{\delta_f}(\mathbf{a}) \\
  \left|g(\mathbf{x}) - g(\mathbf{a})\right| < \epsilon_g \land \mathbf{x}\inB_{\delta_g}(\mathbf{a})
\begin{align*}
$$

Taking $\delta = \min\Set{\delta_f, \delta_g}$, we get that

$$
  \mathbf{x}\inB_\delta (\mathbf{a}) \implies \left|\left[f(\mathbf{x}) + g(\mathbf{x})\right] - \left[f(\mathbf{a}) + g(\mathbf{a})\right]\right| < \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon
$$
</details>
</MathBox>

<MathBox title='Composition preserves continuity' boxType='proposition'>
Suppose $U\subseteq\R^n$ and $V\subseteq\R$ are open sets, and let $f:U\to\R$ and $g:V\to\R$ be functions such that $f(\mathbf{x})\in V$ for all $\mathbf{x}\in U$. This guarantees that the composition $g\circ f:U\to\R$ is defined. If $f$ and $g$ are continuous, so is the composition $f\circ g$.

<details>
<summary>Proof</summary>

Consider a point $\mathbf{a}\in U$ and let $\epsilon > 0$ be given. Since $g$ is continuous at the point $f(\mathbf{a})$, there is a $\delta' > 0$ such that

$$
  g\left(B_{\delta'}\left[f(\mathbf{a})\right]\left) \subset B_\epsilon \left(g[f(\mathbf{a})]\right)
$$

Since $f$ is continuous at $\mathbf{a}$, treating $\delta'$ as the given input, there is a $\delta > 0$ such that

$$
  f\left[B_\delta (\mathbf{a}) \right] \subset B_{\delta'} \left[ f(\mathbf{a})\right]
$$

Linking these two steps gives

$$
  (g\circ f)\left[B_\delta (\mathbf{a}) \right] = g\left(f\left[B_\delta (\mathbf{a})\right]\right) \subset g\left(B_{\delta'}\left[f(\mathbf{a})\right]\left) \subset B_\epsilon \left(g[f(\mathbf{a})]\right)
$$
</details>
</MathBox>

## Limits

<MathBox title='Limit' boxType='definition'>
Consider a point $\mathbf{a}\in U \subseteq\R^n$ where $U$ is an open set. If $f:\R^n \to\R$ is a real-valued function that is defined on $U$, except possibly at $\mathbf{a}$, then the limit of $f$ at $\mathbf{a}$, denoted $\lim_{\mathbf{x}\to\mathbf{a}}$, exists if there is a number $L$, such that the function $\tilde{f}:U\to\R$ defined by

$$
  \tilde{f} = \begin{cases}
    f(\mathbf{x}),\quad \mathbf{x}\neq\mathbf{a} \\
    L, \quad \mathbf{x}=\mathbf{a}
  \end{cases}
$$

is continuous at $\mathbf{a}$, in which case we write $\lim_{\mathbf{x}\to\mathbf{a}} f(\mathbf{x}) = L$.

Alternatively, using the definition of continuity to $\mathbf{f}$ then $\lim_{\mathbf{x}\to\mathbf{a}} f(\mathbf{x}) = L$ if for any $\epsilon > 0$, there exists a $\delta > 0$ such that $|f\mathbf{x} - L| < \epsilon$ whenever $\lVert\mathbf{x} - \mathbf{a}\rVert$, except possibly when $\mathbf{x} = \mathbf{a}$.
</MathBox>

<MathBox title='Properties of limits' boxType='proposition'>
Suppose that $\lim_{\mathbf{x}\to\mathbf{a}}f(\mathbf{x})$ and $\lim_{\mathbf{x}\to\mathbf{a}}f(\mathbf{x})$ both exist for real-valued functions $f,g:\R^n \to\R$.
1. $\lim_{\mathbf{x}\to\mathbf{a}}[f(\mathbf{x}) + g(\mathbf{x})] = \lim_{\mathbf{x}\to\mathbf{a}}f(\mathbf{x}) + \lim_{\mathbf{x}\to\mathbf{a}}g(\mathbf{x})$
2. $\lim_{\mathbf{x}\to\mathbf{a}}cf(\mathbf{x}) = c \lim_{\mathbf{x}\to\mathbf{a}}f(\mathbf{x})$ for any scalar $c\in\R$
3. $\lim_{\mathbf{x}\to\mathbf{a}}f(\mathbf{x} g(\mathbf{x})) = \lim_{\mathbf{x}\to\mathbf{a}}f(\mathbf{x}) \lim_{\mathbf{x}\to\mathbf{a}}g(\mathbf{x})$
4. $\lim_{\mathbf{x}\to\mathbf{a}}\frac{f(\mathbf{x})}{g(\mathbf{x})} = \frac{\lim_{\mathbf{x}\to\mathbf{a}}f(\mathbf{x})}{\lim_{\mathbf{x}\to\mathbf{a}}g(\mathbf{x})}$ if $\lim_{\mathbf{x}\to\mathbf{a}}g(\mathbf{x}) \neq 0$
</MathBox>

## Differential calculus

### Partial derivative

In the one-variable case, a real-valued function $f:I\subseteq\R\to\R$ can be linearly approximated at $a\in I$ by a secant (affine linear function)

$$
  \ell(t) = \frac{f(x)-f(a)}{x - a} (t - a) + f(a) 
$$

In the limit $x\to a$, the secant converges to the tangent of $f$ at $a$. If this limit exists, we say that $f$ is differentiable at $a$ with derivative equal to the slope of $\ell$

$$
  f'(a) := \lim_{x\to a} \frac{f(x) - f(a)}{x - a} \\
$$

This can be rewritten as

$$
\begin{align*}
  0 =& \lim_{x\to a} \left( \frac{f(x) - f(a)}{x - a} - f'(a) \right) \\
  =& \lim_{x\to a} \frac{f(x) - f(a) - f'(a)(x-a)}{x - a} \\
  =& \lim_{x\to a} \frac{f(x) - \ell(x)}{x - a} 
\end{align*}
$$

Generalizing to the multivariable case, we expect a function $f:U\subseteq\R^n \to\R$ to be differentiable at a point $\mathbf{a}\in U$ if there is a linear affine function $\ell:\R^n \to\R$ given by $\ell(\mathbf{x}) = T(\mathbf{x}) + b$ such that

$$
  \lim_{\mathbf{x}\to\mathbf{a}} \frac{f(\mathbf{x}) - \ell(\mathbf{x})}{\lVert \mathbf{x} - \mathbf{a}\rVert} = 0
$$

<MathBox title='Affine function' boxType='definition'>
A function $\ell:\R^n \to R^m$ is called an *affine function* if it has the form $\ell(\mathbf{x}) = T(\mathbf{x}) + \mathbf{b}$, where $T:\R^n \to\R^m$ is a linear transformation and $\mathbf{b}\in\R^m$.
</MathBox>

The affine function $\ell$ represents the linear approximation of $f$ at $\mathbf{a}$ such that $f(\mathbf{a}) = \ell(\mathbf{a}) = T(\mathbf{a}) + b$. Thus, $b = f(\mathbf{a}) - T(\mathbf{a})$ giving

$$
\begin{align*}
  \ell(\mathbf{x}) =& T(\mathbf{x}) + f(\mathbf{a}) - T(\mathbf{a}) \\
  =& f(\mathbf{a}) + T(\mathbf{x} - \mathbf{a})
\end{align*}
$$

The difference quotient can thus be rewritten as

$$
  \lim_{\mathbf{x}\to\mathbf{a}} \frac{f(\mathbf{x}) - f(\mathbf{a}) - T(\mathbf{x} - \mathbf{a})}{\lVert \mathbf{x} - \mathbf{a}\rVert} = 0
$$

Suppose that $\mathbf{x}$ approaches $\mathbf{a}$ in the $x_i$-direction, i.e. $\mathbf{x} = \mathbf{a} + h\mathbf{e}_i$. Then $\mathbf{x} - \mathbf{a} = h\mathbf{e}_1$ and $\lVert \mathbf{x} - \mathbf{a} \rVert = |h|$. For the difference quotient to hold true then we require in both limits $h\downarrow 0$ and $h\uparrow 0$ that

$$
\begin{align*}
  0 =&\lim_{h\to 0} \frac{f(\mathbf{a} + h\mathbf{e}_i) - f(\mathbf{a}) - T(h\mathbf{e}_i)}{h} \\
  =& \lim_{h\to 0} \frac{f(\mathbf{a} + h\mathbf{e}_i) - f(\mathbf{a}) - hT(\mathbf{e}_i)}{h} 
\end{align*}
$$

where $T(h\mathbf{e}_i) = hT(\mathbf{e}_i)$ follows from the linearity of $T$. Solving for for $T(\mathbf{e}_i)$ gives

$$
\begin{align*}
  T(\mathbf{e}_i) =& \lim_{h\to 0} \frac{f(\mathbf{a} + h\mathbf{e}_i) - f(\mathbf{a})}{h} \\
  =& \lim_{h\to 0} \frac{f(a_i,\dots,a_i + h,\dots,a_n) - f(a_1,\dots,a_i,\dots,a_n)}{h}
\end{align*}
$$

This limit of the difference quotient is analogous to the one-variable derivative where only $x_i$ varies while $x_j = a_j$ for $j\neq i$ are held fixed. This limit is called the partial derivative of $f$ at $\mathbf{a}$ with respect to $x_i$, denoted $\frac{\partial}{\partial x_i}(\mathbf{a})$.

The linear transformation $T:\R^n \to\R$ takes the form of a $1\times n$ matrix

$$
  \mathrm{D}f(\mathbf{a}) = \begin{bmatrix} \frac{\partial f}{\partial x^1}(\mathbf{a}) & \cdots & \frac{\partial f}{\partial x_n}(\mathbf{a}) \end{bmatrix}
$$

which is called the *total derivative* of $f$ at $\mathbf{a}$. The total derivative is also known as the Jacobian matrix.

<MathBox title='Partial derivative' boxType='definition'>
Let $f:U\to\R$ be a real-valued function defined on an open set $U\in\R^n$ and consider a point $\mathbf{a}\in U$. The *partial derivative* of $f$ at $\mathbf{a}$ with respect to $x_i$ is defined by

$$
\begin{align*}
  \frac{\partial f}{\partial x_i}(\mathbf{a}) =& \lim_{h\to 0} \frac{f(\mathbf{a} + h\mathbf{e}_i) - f(\mathbf{a})}{h} \\
  =& \lim_{h\to 0} \frac{f(a_i,\dots,a_i + h,\dots,a_n) - f(a_1,\dots,a_i,\dots,a_n)}{h}
\end{align*}
$$
</MathBox>

<MathBox title='Differentiability' boxType='definition'>
A function $f:U\subseteq\R^n \to\R$ is *differentiable* at a point $\mathbf{a}\in U$ if

$$
  \lim_{\mathbf{x}\to\mathbf{a}}\frac{f(\mathbf{x}) - f(\mathbf{a}) - \mathrm{D}f(\mathbf{a})(\mathbf{x} - \mathbf{a})}{\lVert \mathbf{x} - \mathbf{a}\rVert} = 0
$$

If this limit exists, the matrix $\mathrm{D}f(\mathbf{a})$ is called the *derivative* of $f$ at $\mathbf{a}$. It is also known as the *Jacobian matrix*. The affine function $\ell(\mathbf{x}) = f(\mathbf{a}) + \mathrm{D}f(\mathbf{a})(\mathbf{x}-\mathbf{a})$ is called the *first-order affine transformation* of $f$ at $\mathbf{a}$.
</MathBox>

### Partial derivative (basis vector)

Partial derivatives represent basis vectors

$$
  \mathbf{e}_i \equiv \frac{\partial}{\partial \mathbf{x}^i} \equiv \boldsymbol{\partial}_i
$$

A basis of partial derivatives can be transformed to a new basis $\tilde{\mathbf{e}}_i \equiv \frac{\partial}{\partial \tilde{\mathbf{x}}^i}$ by the chain rule

$$
  \frac{\partial}{\partial \tilde{\mathbf{x}}^i} = \frac{\partial x^j}{\partial \tilde{x}^i} \frac{\partial}{\partial \mathbf{x}^j} = J_i^j \frac{\partial}{\partial \mathbf{x}^j}
$$

where $J$ is the Jacobian matrix of the coordinate transform.

Tangent vectors of a curve can be expressed as linear combination of partial derivatives

$$
  \frac{\mathrm{d}}{\mathrm{d} \lambda} = \frac{\mathrm{d}x^i}{\mathrm{d} \lambda} \frac{\partial }{\partial \mathbf{x}^i} 
$$

Tangent vectors can be transformed to a new basis. The tangent vector derivative components transform contravariantly

$$
\begin{align*}
  \frac{\mathrm{d}}{\mathrm{d} \lambda} = \frac{\mathrm{d}x^i}{\mathrm{d} \lambda} \frac{\partial }{\partial \mathbf{x}^i} &= \frac{\mathrm{d}x^i}{\mathrm{d} \lambda} \left( \frac{\partial \tilde{x}^j}{\partial x^i} \frac{\partial}{\partial \tilde{\mathbf{x}}^j} \right) \\
  &= \left( \frac{\mathrm{d}x^i}{\mathrm{d} \lambda} \frac{\partial \tilde{x}^j}{\partial x^i} \right) \frac{\partial}{\partial \tilde{\mathbf{x}}^j} \\
  &= \frac{\mathrm{d} \tilde{x}^i}{\mathrm{d} \lambda} \frac{\partial}{\partial \tilde{\mathbf{x}}^i}
\end{align*}
$$

## Gradient (vector field)

If a scalar function $f:U\subseteq\R^n \to\R$ is differentiable at $\mathbf{a}\in U$, its total derivative at this point is the linear transformation $\mathrm{D}f(\mathbf{a}):\R^n \to\R$ in the form of the $1\times n$ Jacobian matrix

$$
  \mathrm{D}f(\mathbf{a}) = \begin{bmatrix} \frac{\partial f}{\partial x^1}(\mathbf{a}) & \cdots & \frac{\partial f}{\partial x^n}(\mathbf{a}) \end{bmatrix}
$$

Transposing $\mathrm{D}f(\mathbf{a})$ gives a vector ($n\times 1$ matrix) called the *gradient* of $f$ at $\mathbf{a}$, denoted

$$
\begin{align*}
  \nabla f(\mathbf{a}) =& \mathrm{D}f(\mathbf{a})^T \\
  =& \begin{bmatrix} \frac{\partial f}{\partial x^1}(\mathbf{a}) & \cdots & \frac{\partial f}{\partial x^n}(\mathbf{a}) \end{bmatrix}^T \\
  =& \begin{bmatrix} \frac{\partial f}{\partial x_i}(\mathbf{a}) \\ \vdots \\ \frac{\partial f}{\partial x^n}(\mathbf{a}) \end{bmatrix}
\end{align*}
$$

On a sidenote, since $\mathrm{D}f(\mathbf{a})$ maps vectors in $\R^n$ into $\R$ it is technically a differential 1-form, or a linear functional, identified with dual vector space $(\R^n)^*$, which is the set of all linear functionals. The total derivative $\mathrm{D}f(\mathbf{a})$ and the gradient $\nabla f(\mathbf{a})$ are thus dual to each other. 

<MathBox title='Gradient' boxType='definition'>
If a function $f:U\subseteq\R^n \to\R$ is totally differentiable $\mathbf{a}\R^n$, then the gradient of $f$ at $\mathbf{a}$ is defined as

$$
  \nabla f(\mathbf{a}) = \mathrm{grad}(f(\mathbf{a})) := \begin{bmatrix} \frac{\partial f}{\partial x_i}(\mathbf{a}) \\ \vdots \\ \frac{\partial f}{\partial x^n}(\mathbf{a}) \end{bmatrix}
$$

which is the transpose of the total derivative (Jacobian) $\mathrm{D}f(\mathbf{a})$.
</MathBox>

<MathBox title='Properties of the del operator' boxType='proposition'>
Let $f,g:\R^n \to\R$ be scalar functions and $\alpha\in\R$ a scalar. The del operator $\nabla$ has the following properties. 

1. Linearity
$$
\begin{gather*}
  \nabla(f + g) = \nabla f + \nabla g \\
  \nabla (\alpha f) = \alpha \nabla f
\end{gather*}
$$
2. $\nabla(fg) = g\nabla f + f\nabla g$ **(product rule):**
3. $\nabla\left(\frac{f}{g} \right) = \frac{g\nabla f - f\nabla g}{g^2}
$ **(quotient rule)**
</MathBox>

The gradient (del) operator maps a scalar field to a vector field. 

$$
  \nabla f = \left( \nabla f \right)^i \frac{\partial}{\partial x^i}
$$

The gradient operator and differential form are a dual pair

$$
\begin{align*}
  \mathrm{d}f = \nabla f \cdot \_ &= g\left( \nabla f, \_ \right) \\ 
  &= \left[ g_{ij} \left( \mathrm{d}x^i \otimes \mathrm{d}x^j \right) \right] \left[ \left( \nabla f \right)^k \frac{\partial}{\partial x^k} \right] \\
  &= g_{ij} \left( \nabla f \right)^k \mathrm{d}x^i \left( \frac{\partial}{\partial x^k} \right) \mathrm{d}x^j \\
  &= g_{ij} \left( \nabla f \right)^k \delta_k^i \mathrm{d}x^j \\
  &= g_{ij} \left( \nabla f \right)^i  \mathrm{d}x^j
\end{align*}
$$

The covector components are given by

$$
  \frac{\partial f}{\partial x^j} = \left( \nabla f \right)^i g_{ij}
$$

The gradient components are retrieved by applying the inverse metric tensor

$$
\begin{align*}
  \frac{\partial f}{\partial x^j} \mathfrak{g}^{jk} &= \left( \nabla f \right)^i g_{ij} \mathfrak{g}^{jk} \\
  &= \left( \nabla f \right)^i \delta_i^k \\
  &= \left( \nabla f \right)^k &= \frac{\partial f}{\partial x^j} \mathfrak{g}^{jk}
\end{align*}    
$$

The gradient can thus be expressed as

$$
  \nabla f = \left( \nabla f \right)^i \frac{\partial}{\partial x^k} = \mathfrak{g}^{jk} \frac{\partial f}{\partial x^j} \frac{\partial}{\partial x^k} 
$$

For a Cartesian basis $g_{ij} = \mathfrak{g}_{ij} = \delta_{ij}$ and the gradient simplifies to

$$
  \nabla f = \frac{\partial f}{\partial x^i} \frac{\partial}{\partial x^i}
$$

### Directional derivative (gradient/derivative dual pairs)

The directional derivative is defined as

$$
  \mathrm{d}f (\mathbf{v}) \equiv \nabla_\mathbf{v} f = \lim_{h \to 0} \frac{f(\mathbf{x} + \mathbf{h} \cdot \mathbf{v}) - f(\mathbf{x})}{\mathbf{h}} = \mathbf{v} \cdot \nabla f
$$

This implies that the gradient (vector) $\nabla f$ and the differential form (covector) $\mathrm{d}f$ are dual pairs

$$
\begin{align*}
  \mathrm{d}f(\mathbf{v}) = \nabla f \cdot \mathbf{v} &= \left[ \left( \nabla f\right)^i \frac{\partial}{\partial x^i} \right] \cdot \left(v^j \frac{\partial}{\partial x^j} \right) \\
  &= \left( \nabla f \right)^i v^j \left( \frac{\partial}{\partial x^i} \cdot \frac{\partial}{\partial x^j} \right) \\
  &= \left( \nabla f \right)^i v^j g_{ij}
\end{align*}
$$

Alternatively

$$
\begin{align*}
  \nabla f \cdot \mathbf{v} &= g\left(\nabla f, \mathbf{v} \right) \\
  &= \left[ g_{ij} \left( \mathrm{d}x^i \otimes \mathrm{d}x^j \right) \right] \left[ \left( \nabla f \right)^k \frac{\partial}{\partial x^k} , v^l \frac{\partial}{\partial x^l} \right] \\
  &= g_{ij} \mathrm{d}x^i \left[ \left( \nabla f \right)^k \frac{\partial}{\partial x^k} \right] \mathrm{d}x^j \left( v^l \frac{\partial}{\partial x^l} \right) \\
  &= g_{ij} \left( \nabla f \right)^k v^l \mathrm{d}x^i \left( \frac{\partial}{\partial x^k} \right) \mathrm{d}x^j \left( \frac{\partial}{\partial x^l} \right) \\
  &= g_{ij} \left( \nabla f \right)^k v^l \delta_k^i \delta_l^j \\
  &= g_{ij} \left( \nabla f \right)^i v^j
\end{align*}
$$

## Integral calculus

### Gradient theorem

The gradient theorem is also known as the fundamental theorem of calculus for line integrals

$$
  \int_{C[a, b]} \nabla f \cdot \mathrm{d}\mathbf{r} = f(b) - f(a)
$$

### Partial integration

$$
\begin{gather*}
\begin{aligned}
  \nabla \cdot \left(f \mathbf{A} \right) &= f\left( \nabla \cdot \mathbf{A} \right) + \mathbf{A} \cdot \left( \nabla f \right) \\
  \int \nabla \cdot \left(f \mathbf{A} \right) \mathrm{d}\tau &= \int f\left( \nabla \cdot \mathbf{A} \right) \mathrm{d}\tau + \int \mathbf{A} \cdot \left( \nabla f \right) \mathrm{d}\tau = \oint f\mathbf{A} \cdot \mathrm{d}\mathbf{a}
\end{aligned} \\
  \int_\mathcal{V} \nabla \cdot \left(f \mathbf{A} \right) \mathrm{d}\tau = -\int_\mathcal{V} \mathbf{A} \cdot \left( \nabla f \right) \mathrm{d}\tau + \oint_\mathcal{S} f\mathbf{A} \cdot \mathrm{d}\mathbf{a}
\end{gather*}
$$

# Vector fields

## Differential calculus

### Divergence

$$
  \nabla\cdot\mathbf{v} = \frac{\partial v^i}{\partial x_i} \equiv \partial_i v^i
$$

The divergence of the curl of any vector field $\mathbf{A}$ always vanishes.

$$
\begin{align*}
  \nabla\cdot \left( \nabla \times \mathbf{A} \right) = \varepsilon_{ijk} \partial_i \partial_j A_k = -\varepsilon_{ikj} \partial_i \partial_j A_k = -\varepsilon_{ikj} \partial_j \partial_i A_k = -\varepsilon_{ijk} \partial_j \partial_i A_k = 0
\end{align*}
$$

Product rule

$$
  \nabla\cdot \left(\psi \mathbf{f}  \right) = \nabla\psi \cdot \mathbf{f} + \psi \nabla\cdot\mathbf{f}
$$

### Curl

$$
\begin{gather*}
  \nabla \times \mathbf{v} = \begin{vmatrix} \hat{\mathbf{x}} & \hat{\mathbf{y}} & \hat{\mathbf{z}} \\
  \frac{\partial}{\partial x} & \frac{\partial}{\partial y} & \frac{\partial}{\partial z} \\
  v^x & v^y & v^z \end{vmatrix} = \varepsilon^{ij}_{k}\mathbf{e}_i \frac{\partial v^k}{\partial x_j} \equiv \varepsilon^{ij}_{k}\mathbf{e}_i\partial_j v^k
\end{gather*}
$$

The curl of the gradient of any scalar field $\phi$ is always the zero vector field

$$
  \left[\nabla \times (\nabla \phi)\right]^i = \varepsilon_{ijk}\partial_j \partial_k \phi = -\varepsilon_{ikj}\partial_j \partial_k \phi = -\varepsilon_{ikj}\partial_k \partial_j \phi = -\varepsilon_{ijk}\partial_j \partial_k \phi = 0
$$

Curl of curl

$$
\begin{align*}
  \left[ \nabla \times \left( \nabla \times \mathbf{v} \right) \right]^i &= \varepsilon_{ijk}\partial_j \varepsilon_{kmn}\partial_m v^n \\
  &= \varepsilon_{kij}\varepsilon_{kmn}\partial_i \partial_m v^n \\
  &= \left( \delta_{im}\delta_{jn} - \delta_{in}\delta_{jm} \right)\partial_j \partial_m v^n \\
  &= \partial_j \partial_i v^j - \partial_j \partial_j v^i \\
  &= \partial_i \partial_j v^j - \partial_j \partial_j v^i \\
  &= \left[ \nabla \left(\nabla \cdot \mathbf{v}\right) - \nabla^2 \mathbf{v} \right]^i
\end{align*}
$$

### Integral calculus

### Green's theorem

$$
  \oint_C (P \mathrm{d}x + Q \mathrm{d}y) = \iint_D \left( \frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} \right)\mathrm{d}x \mathrm{d}y  
$$

### Stokes' theorem (curl)

Let $\mathbf{F} = \left[P(x, y, z), Q(x, y, z), R(x, y, z) \right]$ be a continuously differentiable vector field defined in a region $\Sigma \in \R^3$ with boundary $\partial\Sigma$, then

$$
\begin{align*}
  \iint_\Sigma \left(\nabla \times \mathbf{F}\right) \cdot \mathrm{d}\mathbf{f} &= \oint_{\partial\Sigma} \mathbf{F} \cdot \mathrm{d}\mathbf{l} \\
  \iint_\Sigma \left[ \left( \frac{\partial R}{\partial y} - \frac{\partial Q}{\partial z} \right) \mathrm{d}y\mathrm{d}z + \left( \frac{\partial P}{\partial z} - \frac{\partial R}{\partial x} \right) \mathrm{d}z\mathrm{d}x + \left( \frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} \right) \mathrm{d}x\mathrm{d}y \right] &= \oint_{\partial\Sigma} \left( P \mathrm{d}x + Q \mathrm{d}y + R P \mathrm{d}z \right)
\end{align*}
$$

### Gauss' theorem (flux)

Let $\mathbf{F}$ be a continuously differentiable vector field defined on a compact subset $V \in \R^n$ with the smooth boundary $\S$, then

$$
  \iiint_V \left(\nabla \cdot \mathbf{F} \right) \mathrm{d}V = \oiint_S \left( \mathbf{F} \cdot \mathbf{\hat{n}} \right) \mathrm{d}S
$$


#### Divergence of inverse-square functions

The divergence of an inverse square function vanishes
$$
  \nabla \cdot \left( \frac{\hat{\mathbf{r}}}{r^2} \right) = \frac{1}{r^2}\frac{\partial}{\partial r} \left(r^2 \frac{1}{r^2} \right) = 0
$$

The surface integral over a sphere of radius $R$ is however

$$
  \oint \left( \frac{\hat{\mathbf{r}}}{R^2} \right) \cdot \left( R^2 \sin\theta \;\mathrm{d}\theta \;\mathrm{d}\phi \,\hat{\mathbf{r}} \right) = \left( \int_0^\pi \sin\theta \;\mathrm{d}\theta \right) \left( \int_0^{2\pi} \mathrm{d}\phi \right) = 4\pi
$$

By the divergence theorem

$$
    \int_V \nabla \cdot \left( \frac{\hat{\mathbf{r}}}{r^2} \right) \mathrm{d}V = \oint \left( \frac{\hat{\mathbf{r}}}{R^2} \right) \cdot \mathrm{d}\mathbf{a} = 4\pi \\
    \implies \nabla \cdot \left( \frac{\hat{\mathbf{r}}}{r^2} \right) = 4\pi \delta^3 \left( \mathbf{r} \right)
$$

### Green's first identity

Green's first identity is derived from the divergence theorem applied to the vector field $\mathbf{F} = \psi \nabla \phi$. Using the divergence product rule gives

$$
  \nabla\cdot\left(\psi\nabla\phi \right) = \nabla\psi \cdot \nabla \phi + \psi \nabla \cdot \left(\nabla \phi\right) = \nabla\psi \cdot \nabla \phi + \psi \nabla^2 \phi
$$

Applying the divergence theorem gives

$$
\begin{align*}
  \int_\mathcal{V} \nabla \cdot \left( \psi \nabla \phi \right)\mathrm{d}V &= \int_\mathcal{V} \left( \nabla\psi \cdot \nabla \phi + \psi \nabla^2 \phi \right) \mathrm{d}V \\
  &= \oint_\mathcal{S} \psi \left(\nabla\phi \cdot \mathbf{n} \right) \mathrm{d}S = \oint_\mathcal{S} \psi \nabla \phi \cdot \mathrm{d}\mathbf{S}
\end{align*}
$$

### Green's second identity

If $\phi$ and $\psi$ are both twice continuously differentiable on $U \in \R^3$, and $\epsilon$ is once continuously differentiable, one may choose the vector field $\mathbf{F} = \psi \epsilon\nabla \phi - \phi\epsilon\nabla\psi$ to obtain

$$
  \int_U \left[ \psi \nabla \cdot \left( \epsilon\nabla\phi \right) - \phi \nabla \cdot \left( \epsilon\nabla\psi \right) \right] \mathrm{d}V = \oint_{\partial U} \epsilon \left( \psi \frac{\partial\phi}{\partial\mathbf{n}} - \phi \frac{\partial\psi}{\partial\mathbf{n}} \right) \mathrm{d}S
$$

In the special case of $\epsilon = 1$, the identity reduces to

$$
  \int_U \left( \psi \nabla^2 \phi - \phi \nabla^2 \psi \right)\mathrm{d}V = \oint_{\partial U} \left( \psi \frac{\partial\phi}{\partial\mathbf{n}} - \phi \frac{\partial\psi}{\partial\mathbf{n}} \right) \mathrm{d}S = \oint_{\partial U} \left( \psi \nabla \phi - \phi\nabla\psi \right) \mathrm{d}\mathbf{S}
$$

# Vector-valued functions

## Jacobian matrix (basis transform)

The Jacobian of a vector-valued function $\mathbf{f} : \mathbf{R}^n \mapsto \mathbf{R}^m$, is the matrix of all its first-order partial derivatives with entries $J_{ij} = \frac{\partial f_i}{\partial x_j}$, or explicitly

$$
  J = \begin{bmatrix} \frac{\partial \mathbf{f}}{\partial x_i} & \dots & \frac{\partial \mathbf{f}}{\partial x_n} \end{bmatrix} = \begin{bmatrix} \nabla^T f_1 \\ \dots \\ \nabla^T f_m \end{bmatrix} = \begin{bmatrix} \frac{\partial f_1}{\partial x_1} & \dots & \frac{\partial f_1}{\partial x_n} \\ \vdots & \ddots & \vdots \\ \frac{\partial f_m}{\partial x_1} & \dots & \frac{\partial f_m}{\partial x_n} \end{bmatrix}
$$

### Polar-Cartesian transformation

The transformation from polar coordinates $(r, \theta)$ to Cartesian coordinates $(x, y)$ is given by the function $\mathbf{f} : \R \times [0, 2\pi) \mapsto \R^2$ with components

$$
  \mathbf{f}(r, \theta) = \left[ r \cos{\theta}, r \sin{\theta} \right] \equiv (x, y)
$$

The Jacobian becomes

$$
  J = \begin{bmatrix} \frac{\partial x}{\partial r} & \frac{\partial y}{\partial r} \\ \frac{\partial x}{\partial \theta} & \frac{\partial y}{\partial \theta} \end{bmatrix} = \begin{bmatrix} \cos{\theta} & \sin{\theta} \\ - r \sin{\theta} & r \cos{\theta} \end{bmatrix}
$$

representing a linear transform of the Cartesian coordinate basis vectors, $J\mathbf{e}_{xy} = \mathbf{e}_{r\theta}$, written out

$$
\begin{align*}
  \begin{bmatrix} \mathbf{e}_r \\ \mathbf{e}_\theta \end{bmatrix} \equiv \begin{bmatrix} \frac{\partial \mathbf{R}}{\partial r} \\ \frac{\partial \mathbf{R}}{\partial \theta} \end{bmatrix} &= \begin{bmatrix} \frac{\partial x}{\partial r} & \frac{\partial y}{\partial r} \\ \frac{\partial x}{\partial \theta} & \frac{\partial y}{\partial \theta} \end{bmatrix} \cdot \begin{bmatrix} \frac{\partial \mathbf{R}}{\partial x} \\ \frac{\partial \mathbf{R}}{\partial y} \end{bmatrix} = \begin{bmatrix} \frac{\partial x}{\partial r} & \frac{\partial y}{\partial r} \\ \frac{\partial x}{\partial \theta} & \frac{\partial y}{\partial \theta} \end{bmatrix} \cdot \begin{bmatrix} \mathbf{e}_x \\ \mathbf{e}_y \end{bmatrix} \\
  &= \begin{bmatrix} \frac{\partial x}{\partial r}\frac{\partial \mathbf{R}}{\partial x} + \frac{\partial y}{\partial r}\frac{\partial \mathbf{R}}{\partial y} \\ \frac{\partial x}{\partial \theta}\frac{\partial \mathbf{R}}{\partial x} + \frac{\partial y}{\partial \theta}\frac{\partial \mathbf{R}}{\partial y} \end{bmatrix} \\
  &\equiv \begin{bmatrix} \frac{\partial x}{\partial r}\mathbf{e}_x + \frac{\partial y}{\partial r}\mathbf{e}_y \\ \frac{\partial x}{\partial \theta}\mathbf{e}_x + \frac{\partial y}{\partial \theta} \mathbf{e}_y \end{bmatrix}
\end{align*}
$$

### Cartesian-polar transformation

The transformation from Cartesian coordinates $(x, y)$ to polar coordinates $(r, \theta)$ is given by the function $\mathbf{f} : \R^2 \mapsto \R^+ \times [0, 2\pi)$ with components

$$
  \mathbf{f}(x, y) = \left[\sqrt{x^2 + y^2}, \arctan{\left( \frac{y}{x} \right)} \right] \equiv (r, \theta)
$$

The Jacobian becomes

$$
  J = \begin{bmatrix} \frac{\partial r}{\partial x} & \frac{\partial r}{\partial y} \\ \frac{\partial \theta}{\partial x} & \frac{\partial \theta}{\partial y} \end{bmatrix} = \begin{bmatrix} \frac{x}{\sqrt{x^2 + y^2}} & -\frac{y}{x^2 + y^2} \\ \frac{y}{\sqrt{x^2 + y^2}} & \frac{x}{x^2 + y^2} \end{bmatrix} = \begin{bmatrix} \frac{x}{r} & -\frac{y}{r^2} \\ \frac{y}{r} & \frac{x}{r^2} \end{bmatrix} 
$$

representing a linear transform of the polar coordinate basis vectors

$$
\begin{align*}
  \begin{bmatrix} \mathbf{e}_x \\ \mathbf{e}_y \end{bmatrix} \equiv \begin{bmatrix} \frac{\partial \mathbf{R}}{\partial x} \\ \frac{\partial \mathbf{R}}{\partial y} \end{bmatrix} &= \begin{bmatrix} \frac{\partial r}{\partial x} & \frac{\partial r}{\partial y} \\ \frac{\partial \theta}{\partial x} & \frac{\partial \theta}{\partial y} \end{bmatrix} \cdot \begin{bmatrix} \frac{\partial \mathbf{R}}{\partial r} \\ \frac{\partial \mathbf{R}}{\partial \theta} \end{bmatrix} = \begin{bmatrix} \frac{\partial r}{\partial x} & \frac{\partial r}{\partial y} \\ \frac{\partial \theta}{\partial x} & \frac{\partial \theta}{\partial y} \end{bmatrix} \cdot \begin{bmatrix} \mathbf{e}_r \\ \mathbf{e}_\theta \end{bmatrix} \\
  &= \begin{bmatrix} \frac{\partial r}{\partial x}\frac{\partial \mathbf{R}}{\partial r} + \frac{\partial \theta}{\partial x}\frac{\partial \mathbf{R}}{\partial \theta} \\ \frac{\partial r}{\partial y}\frac{\partial \mathbf{R}}{\partial r} + \frac{\partial \theta}{\partial y}\frac{\partial \mathbf{R}}{\partial \theta} \end{bmatrix} \\
  &\equiv \begin{bmatrix} \frac{\partial r}{\partial x}\mathbf{e}_r + \frac{\partial \theta}{\partial x}\mathbf{e}_\theta \\ \frac{\partial r}{\partial y}\mathbf{e}_r + \frac{\partial \theta}{\partial y} \mathbf{e}_\theta \end{bmatrix}
\end{align*}
$$

### Differential form (covector field)

A differential form $\mathrm{d}f$ maps a scalar field $f$ to a covector field (level sets). The covector field is a dual space (linear functionals) equipped with addition and scalar multiplication

$$
\begin{gather*}
  \mathrm{d}f(\mathbf{u} + \mathbf{v}) = \mathrm{d}f(\mathbf{u}) + \mathrm{d}f(\mathbf{v}) \\
  \mathrm{d}f(c\mathbf{u}) = c \left[ \mathrm{d}f(\mathbf{u}) \right]
\end{gather*}
$$

The linear functional $\mathrm{d}f(\mathbf{v})$ can geometrically be interpreted as a directional derivative.

$$
    \mathrm{d}f(\mathbf{v}) \equiv \nabla_\mathbf{v} = \mathbf{v} \cdot \nabla f
$$

The dual basis covectors $\epsilon^i \equiv \mathrm{d}x^i$ are formed by the bi-orthogonality property

$$
  \epsilon^i \left( \mathbf{e}_j \right) \equiv \mathrm{d}x^i \left( \frac{\partial}{\partial x^j} \right) = \frac{\partial x^i}{\partial x^j} = \delta_j^i
$$

The dual basis covectors transform covariantly

$$
  \mathrm{d}\tilde{x}^i = \frac{\partial \tilde{x}^i}{\partial x} \mathrm{d}x^j = \left( J^{-1} \right)_i^j \mathrm{d}x^j
$$

A differential form can be expressed as a linear combination of the basis covectors $\mathrm{d}x^i$. The derivative of a curve tangent vector is given by

$$
\begin{align*}
  \mathrm{d} f \left( \frac{\mathrm{d}}{\mathrm{d}\lambda} \right) &= \frac{\mathrm{d}f}{\mathrm{d}\lambda} \\
  &= \frac{\partial f}{\partial x^i} \frac{\mathrm{d} x^i}{\mathrm{d} \lambda} \\
  &= \frac{\partial f}{\partial x^i} \mathrm{d}x^i \left( \frac{\mathrm{d}}{\mathrm{d} \lambda} \right) \\
\end{align*}
$$

giving 

$$
  \mathrm{d}f = \frac{\partial f}{\partial x^i} \mathrm{d}x^i
$$

The differential form components transform covariantly using the chain rule

$$
  \frac{\partial f}{\partial \tilde{x}^i} = \frac{\partial x^j}{\partial \tilde{x}^i} \frac{\partial f}{\partial x^i} = J_i^j \frac{\partial f}{\partial x^i}
$$