---
title: 'Vector Analysis'
subject: 'Mathematics'
showToc: true
---

# Euclidean topology

## Open sets

<MathBox title='Open ball' boxType='definition'>
Consider a point $\mathbf{a}\in\R^n$ and assume $r > 0$. The open ball centered at $\mathbf{a}$ and with radius $r$ is defined as
$$
  B_r(\mathbf{a}) = \Set{ \mathbf{x}\in\R | \| \mathbf{x} - \mathbf{a} \| < r }
$$
</MathBox>

Consider a subset $U\subseteq\R^n$. The subset is called 
- open if, given any point $\mathbf{a}\in U$, there exists a positive real number $r$ such that $B_r(\mathbf{a})\subset U$
- closed if its complement $U^c = \R^n \backslash U = \Set{\mathbf{x}\in\R^n | \mathbf{x}\notin U}$ is open.

<MathBox title='Interior, exterior and boundary point' boxType='definition'>
Let $U$ be a subset of $\R^n$. A point $\mathbf{p}\in U$ is called:
1. an *interior point* if there is open ball $B_r(\mathbf{p})$ that is a subset of $U$, i.e. $B_r(\mathbf{p})\subseteq U$. The set of all interior points of $U$ is denoted $\mathrm{int}(U)$.
2. an *exterior point* if there is an open ball $B_r(\mathbf{p})$ that is a subset of the complement of $U$, i.e. $B_r(\mathbf{p}) \subseteq \R^n \setminus U$. The set of all exterior points of $U$ is denoted $\mathrm{ext}(U)$.
3. a *boundary point* if every $B_r(\mathbf{p})$ is both a subset of $U$ and its complement, i.e. $B_r(\mathbf{p}) \cap U \neq\emptyset$ and $B_r(\mathbf{p}) \cap U \neq\emptyset$. The set of all boundary points of $U$ is denoted $\partial U$.
</MathBox>

## Sequences

<MathBox title='Convergence' boxType='definition'>
A sequence $(\mathbf{x}_n)_{n\in\N}$ in $\R^m$ converges to a point $\mathbf{a}\in\R^m$ if for every $\varepsilon > 0$, there is $N\in\N$ such that $|\mathbf{x}_n - \mathbf{a}| < \varepsilon$ for all $n \geq N$. In this case we write

$$
  \lim_{n\to\infty} \mathbf{x}_n = \mathbf{a}
$$
</MathBox>

<MathBox title='Properties of convergence' boxType='proposition'>
Suppose that $(\mathbf{x}_n)_{n\in\N}$ and $(\mathbf{y}_n)_{n\in\N}$ are sequences in $\R^m$ with $\lim_{n\to\infty} \mathbf{x}_n = \mathbf{x}$ and $\lim_{n\to\infty} \mathbf{y}_n = \mathbf{y}$. Then the following sequences converge
1. $(c\mathbf{x}_n)_{n\in\N}$ for every $c\in\R$ with $\lim_{n\to\infty} (c\mathbf{x}_n) = c\mathbf{x}$
2. $(\mathbf{x}_n + \mathbf{y}_n)$ with $\lim_{n\to\infty} (\mathbf{x}_n + \mathbf{y}_n) = \mathbf{x} + \mathbf{y}$
3. $(\mathbf{x}_n - \mathbf{y}_n)$ with $\lim_{n\to\infty} (\mathbf{x}_n - \mathbf{y}_n) = \mathbf{x} - \mathbf{y}$
4. $(\mathbf{x}_n \cdot \mathbf{y}_n)$ with $\lim_{n\to\infty} (\mathbf{x}_n \cdot \mathbf{y}_n) = \mathbf{x} \cdot \mathbf{y}$

<details>
<summary>Proof</summary>

**(2):** Since $\lim_{n\to\infty} \mathbf{x}_n = \mathbf{x}$, there is $N_1 \in\N$ such that $\| \mathbf{x}_n - \mathbf{x} \| < \frac{\epsilon}{2}$ for all $n\geq N_1$. Similary, since $\lim_{n\to\infty} \mathbf{y}_n = \mathbf{y}$, there is $N_2 \in\N$ such that $\| \mathbf{y}_n - \mathbf{y} \| < \frac{\epsilon}{2}$ for all $n\geq N_1$. For $N = \max\Set{N_1, N_2}$, we then get using the triangle inequality

$$
\begin{align*}
  \|(\mathbf{x}_n + \mathbf{y}_n) - (\mathbf{x} + \mathbf{y}) \| =& \|(\mathbf{x}_n - \mathbf{x}) + (\mathbf{y}_n - \mathbf{y}) \| \\
  \overset{\triangle}{\leq}& \|\mathbf{x}_n - \mathbf{x}\| + \|\mathbf{y}_n - \mathbf{y}_n| \\
  < \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon
\end{align*}
$$

**(4):** Using the triangle and Cauchy-Schwarz inequalities we get

$$
\begin{align*}
  |\mathbf{x}_n \cdot \mathbf{y}_n - \mathbf{x}\cdot\mathbf{y}| =& |\mathbf{x}_n \cdot \mathbf{y}_n - \mathbf{x}\cdot\\mathbf{y}_n + \mathbf{x}\cdot\mathbf{y}_n - \mathbf{x}\cdot\mathbf{y} |
  \overset{\triangle}{\leq}& |\mathbf{x}_n \cdot \mathbf{y}_n - \mathbf{x}\cdot\mathbf{y}_n| + |\mathbf{x}\cdot\mathbf{y}_n - \mathbf{x}\cdot\mathbf{y} | \\
  \leq& \| \mathbf{x}_n - \mathbf{x} \| \cdot \| \mathbf{y}_n \| + \| \mathbf{x} \| \cdot \| \mathbf{y}_n - \mathbf{y} \|
\end{align*}
$$

Assume that $\| \mathbf{x} \| \neq 0$. Since $\lim_{n\to\infty} \mathbf{y}_n = \mathbf{y}$ there is $N_1\in\N$ such that $\| \mathbf{y}_n - \mathbf{y} \|\frac{\varepsilon}{2 \|\mathbf{x}\|}$ for all $n\geq N_1$.

Since $\lim_{n\to\infty} \mathbf{y}_n = \mathbf{y}$, there is $N_2 \in\N$ such that $\|\mathbf{y}_n - \mathbf{y}\| \leq 1$ when $n \geq N_2$. Thus

$$
\begin{align*}
  \| \mathbf{y}_n \| =& \| \mathbf{y} + (\mathbf{y}_n - \mathbf{y}) \| \\
  \overset{\triangle}{\leq}& \| \mathbf{y} \| + \| \mathbf{y}_n - \mathbf{y} \| \\
  \leq& \| \mathbf{y} + 1 \|

\end{align*}
$$

Since $\lim_{n\to\infty} \mathbf{x}_n = \mathbf{x}$, there exists $N_3 \in\N$ such that $\| \mathbf{x}_n - \mathbf{x} \| \leq \frac{\varepsilon}{2(\|\mathbf{y}\| + 1)}$. If $n\geq\max\Set{N_1,N_2}$ then

$$
  \| \mathbf{x}_n - \mathbf{x} \|\cdot \| \mathbf{y}_n \| < \frac{\varepsilon}{2(|\mathbf{y}| + 1|)} |\mathbf{y}| + 1| = \frac{\varepsilon}{2}
$$

Choosing $N = \max\Set{N_1, N_2, N_3}$, then for $n \geq N$ we have

$$
\begin{align*}
  | \mathbf{x}_n \cdot \mathbf{y}_n - \mathbf{x}\cdot\mathbf{y} | \leq& \| \mathbf{x}_n - \mathbf{x} \|\cdot\| \mathbf{y}_n \| + \| \mathbf{x} \|\cdot \| \mathbf{y}_n - \mathbf{y} \| \\
  <& \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon
\end{align*}
$$
</details>
</MathBox>


# Paths and curves

<MathBox title='Path and curve' boxType='definition'>
A continuous function $\alpha: I\subseteq\R\to\R^n$ is called a path. The image of $\alpha$ is called a curve $C$

$$
  C = \Set{ \mathbf{x}\in\R^n | \mathbf{x} = \alpha(t)\; t\in I }
$$

We say that $\alpha$ is a parametrization of the curve. We often refer to the input variable $t\in I$ as time, so that $\alpha(t)$ describes the position of a moving object at time $t$. 
</MathBox>

## Lines

Given a point $\mathbf{a}\in\R^n$ and a non-zero vector $\mathbf{v}\in\R$, the line through $\mathbf{a}$ and parallel to $\mathbf{v}$ is parametrized by $\alpha: \R\to\R^n$ where

$$
  \alpha(t) = \mathbf{a} + t\mathbf{v}
$$

## Calculus

### Derivative
<MathBox title='Derivative of paths' boxType='definition'>
Given a path $\alpha: I\to\R^n$ the derivative of $\alpha$ is defined by

$$
\begin{align*}
  \alpha'(t) &= \lim_{h\to 0} \frac{\alpha(t + h) - \alpha(t)}{h} \\
  &= \lim_{h\to 0} \frac{1}{h}\left(\left[x_1(t + h),\dots,x_n(t + h) \right] - \left[x_1(t), \dots,x_n(t) \right]\right) \\
  &= \lim_{h\to 0} \left( \frac{x_1(t+h) - x_1(t)}{h},\dots,\frac{x_n(t+h) - x_n(t)}{h} \right) \\
  &= \left( x_1'(t), \dots, x_n'(t) \right)
\end{align*}
$$

provided the limit exists. The derivative is also called the velocity $\mathbf{v}(t) := \alpha'(t) = \frac{\mathrm{d}\alpha}{\mathrm{d}t}$ of $\alpha$. The norm of $\mathbf{v}$ is called the speed $v$ of $\alpha$ and is defined as

$$
  v(t) := \| \mathbf{v}(t) \|
$$
</MathBox>

<MathBox title='Product rules' boxType='proposition'>
If $\alpha, \beta: I\to\R^n$ are differentiable paths and $f:I\to\R$ a differentiable function, the following product rules hold

- Dot product rule: $(\alpha\cdot\beta)' = \alpha'\cdot\beta + \alpha\cdot\beta'$
  - In the special case $\alpha = \beta$ we get $(\alpha\cdot\beta)' = (\|\alpha\|^2)' = 2\alpha \alpha\cdot\alpha'$
- Cross product rule: For $\alpha, \beta: I\to\R^3: $(\alpha\times\beta)' = \alpha'\times\beta + \alpha\times\beta'$
- Scalar multiplication: $(f\alpha)' = f'\alpha + f\alpha'$

<details>
<summary>Proof</summary>

**Dot product rule**
Note that $\alpha\cdot\beta : \R^n \timesmathbb{R}^n \to\R$ is real-valued, so that its derivative is given by

$$
  (\alpha\cdot\beta)' = \lim_{h\to 0} \frac{1}{h}\left[\alpha(t + h)\cdot\beta(t + h) - \alpha(t)\cdot\beta(t)\right]
$$

The quotient can be rewritten as

$$
\begin{align*}
  \frac{1}{h}\left[\alpha(t + h)\cdot\beta(t + h) - \alpha(t)\cdot\beta(t)\right] =& \frac{1}{h}\left[\alpha(t + h)\cdot\beta(t + h) - \alpha(t)\cdot\beta(t + h) \\
  &+ \alpha(t)\cdot\beta(t + h) - \alpha(t)\cdot\beta(t) \right] \\
  =& \frac{\alpha(t + h) - \alpha(t)}{h}\cdot\beta(t + h) + \alpha(t)\cdot\frac{\beta(t + h) - \beta(t)}{h}
\end{align*}
$$
</details>
</MathBox>

### Arclength
<MathBox title='Arclength' boxType='definition'>
The arclength of a differentiable path $\alpha: [a, b]\to\R^n$ is defined as

$$
  s(t) := \int_a^b v(t)\;\mathrm{d}t = \int_a^b \|\mathbf{v}(t)\|\;\mathrm{d}t
$$
</MathBox>

For a curve $\mathbf{R}:[a, b] \mapsto \R^n$, the length is defined as the limit of the sum of line segments, as the partition approaches infinity

$$
  \lim_{\Delta t \to \infty} \sum_i \frac{\left\| \mathbf{R} \left(t_{i + 1} \right) - \mathbf{R} \left(t_i \right) \right\|}{\Delta t} \Delta t = \int \left\| \frac{\mathrm{d}\mathbf{\mathbf{R}}}{\mathrm{d}t} \right\| \mathrm{d}t
$$

The line segment norm is given by

$$
\begin{align*}
  \left\| \frac{\mathrm{d}\mathbf{R}}{\mathrm{d}t} \right\|^2 = \frac{\mathrm{d}\mathbf{R}}{\mathrm{d}t} \cdot \frac{\mathrm{d}\mathbf{R}}{\mathrm{d}t} &= \left( \frac{\mathrm{d}x^i}{\mathrm{d}t} \frac{\partial \mathbf{R}}{\partial x^i} \right) \cdot \left( \frac{\mathrm{d}x^j}{\mathrm{d}t} \frac{\partial \mathbf{R}}{\partial x^j} \right) \\
  &= \frac{\mathrm{d}x^i}{\mathrm{d}t} \frac{\mathrm{d}x^j}{\mathrm{d}t} \left( \frac{\partial \mathbf{R}}{\partial x^i} \cdot \frac{\partial \mathbf{R}}{\partial x^j} \right) \\
  &= \frac{\mathrm{d}x^i}{\mathrm{d}t} \frac{\mathrm{d}x^j}{\mathrm{d}t} g_{ij}
\end{align*}
$$

where $g$ is the metric tensor. As a bilinear form, the metric tensor is transformed by two Jacobians

$$
  \tilde{g}_{ij} = \frac{\partial x^k}{\partial \tilde{x}^i} \frac{\partial x^l}{\partial \tilde{x}^j} g_{kl} = J_i^k J_j^l g_{kl}
$$


$$
  \left\| \frac{\mathrm{d}\mathbf{r}}{\mathrm{d}t} \right\| = \sqrt{\sum_i \sum_j \frac{\mathrm{d}x_i}{\mathrm{d}t} \frac{\mathrm{d}x_j}{\mathrm{d}t} \left( \frac{\partial \mathbf{r}}{\partial x_i} \frac{\partial \mathbf{r}}{\partial x_j} \right)} - \sqrt{\frac{\mathrm{d}x_i}{\mathrm{d}t} \frac{\mathrm{d}x_j}{\mathrm{d}t} \left( \frac{\partial \mathbf{r}}{\partial x_i} \frac{\partial \mathbf{r}}{\partial x_j} \right)}
$$

## Frenet-Serret formula

<MathBox title='Frenet vectors' boxType='definition'>
Let $C$ be a curve in $\R^3$ parametrized by the differentiable path $\alpha: I\to\R^3$. Then, the following vectors can be defined

- the unit tangent vector: $\mathbf{T}(t) = \frac{1}{\|\alpha'(t)\|}\alpha'(t)$ if $\alpha'(t) \neq \mathbf{0}$
- the principal normal vector: $\mathbf{N}(t) = \frac{1}{\|\mathbf{T}'\|}\mathbf{T}'(t)$ if $\mathbf{T}'(t) \neq \mathbf{0}$
- the binormal vector: $\mathbf{B}(t) = \mathbf{T}(t)\times\mathbf{N}(t)$

The vectors are called the Frenet vectors of $\alpha$.
</MathBox>

<MathBox title='Lemma' boxType='lemma'>
The derivative of the the binormal vector $\mathbf{B}'$ is always a scalar multiple of the principal normal $\mathbf{N}$, i.e. $\mathbf{B}'(t) = c(t)\mathbf{N}(t)$ where $c(t):\R\to\R$ is a scalar function.

<details>
<summary>Proof</summary>

If suffices to show that $\mathbf{B}'$ is orthogonal to (a) $\mathbf{B}$ and (b) $\mathbf{T}$. The only vectors orthogonal to both are precisely the scalar multiples of $\mathbf{N}$.

(a) Since $\mathbf{B}$ is a unit vector, $\| \mathbf{B} \|$ is constant. Hence $\mathbf{B}$ and its derivative $\mathbf{B}'$ are always orthogonal.

(b) By definition $\mathbf{B} = \mathbf{T}\times\mathbf{N}$, so using the cross product rule
$$
\begin{align*}
  \mathbf{B}' &= (\mathbf{T}\times\mathbf{N})' = \mathbf{T}'\times\mathbf{N} + \mathbf{T}\times\mathbf{N}' \\
  &= \left(\|\mathbf{T}'\|\mathbf{N}\right)\times\mathbf{N} + \mathbf{T}\times\mathbf{N}' = \\
  &= \mathbf{T}\times\mathbf{N}'
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Curvature and torsion' boxType='definition'>
Let $C$ be a curve in $\R^3$ parametrized by the differentiable path $\alpha: I\to\R^3$. Then, the curvature $\kappa$ of $C$ is defined as

$$
    \kappa(t) := \frac{\| \mathbf{T}(t) \|}{v(t)} = \frac{\| \mathbf{T}(t) \|}{\| \alpha'(t) \|}
$$

Given that $\mathbf{B}'(t) = c(t)\N$, the torsion $\tau$ of $C$ is defined as

$$
    \tau(t) := -\frac{c(t)}{v(t)}
$$
</MathBox>

<MathBox title='Frenet-Serret formulas' boxType='proposition'>
Let $\alpha:I\to\R^3$ be a differentiable path with speed $v$, Frenet vectors $(\mathbf{T}, \mathbf{N}, \mathbf{B})$, curvature $\kappa$ and torsion $\tau$. Assume that $v\neq 0$ and $\mathbf{T}' \neq\mathbf{0}$ so that the Frenet vectors are defined for all $t$. Then

$$
\begin{align*}
  \mathbf{T}' &= \kappa v\mathbf{N} \\
  \mathbf{N}' &= -\kappa v \mathbf{T} + \tau v\mathbf{B} \\
  \mathbf{B}' &= -\tau v\mathbf{N}
\end{align*}
$$

or in matrix form

$$
  \begin{bmatrix} \mathbf{T}' \\ \mathbf{N}' \\ \mathbf{B}' \end{bmatrix} = \begin{bmatrix} 
    0 & \kappa & 0 \\ 
    -\kappa & 0 & \tau \\
    0 & -\tau & 0
  \end{bmatrix} \cdot \begin{bmatrix} \mathbf{T} \\ \mathbf{N} \\ \mathbf{B} \end{bmatrix}
$$

<details>
<summary>Proof</summary>

The first equation follows from the definitions $\mathbf{N} = \frac{1}{\|\mathbf{T}\|}\mathbf{T}'$ and $\kappa = \frac{\|\mathbf{T}'\|}{v}$, giving

$$
  \mathbf{T}' = \|\mathbf{T}'\|\mathbf{N} = \kappa v\mathbf{N}
$$

For the third equation, we know that $\tau = -\frac{c}{v}$ such that

$$
  \mathbf{B}' = c\mathbf{N} = -\tau v\mathbf{N}
$$

The second equation follows from the cross product of $\mathbf{N} = \mathbf{B}\times\mathbf{N}$ and the first and third equations

$$
\begin{align*}
  \mathbf{N}' &= (\mathbf{B}\times\mathbf{T})' = \mathbf{B}'\times\mathbf{T} + \mathbf{B}\times\mathbf{T}' \\
  &= -\tau v\mathbf{N}\times\mathbf{T} + \mathbf{B}\times(\kappa v\mathbf{N}) = -\tau v(\underbrace{\mathbf{N}\times\mathbf{T}}_{=-\mathbf{B}}) + \kappa v(\underbrace{\mathbf{B} + \mathbf{N}}_{-\mathbf{T}}) \\
  &= -\kappa v \mathbf{T} + \tau v\mathbf{B}
\end{align*}
$$
</details>
</MathBox>

<MathBox title='$v\kappa\tau$ theorem' boxType='theorem'>
Let $\alpha,\beta :I\to\R^3$ be differentiable paths with nonzero $v$ and $\mathbf{T}'$ so that the Frenet vectors are defined for all $t$. Then either path can be translated and rotated so that it fits exactly on top of the other if and only if they have the same speed $v$, curvature $\kappa$ and torsion $\tau$. In this case, the two paths are said to be congruent to each other. 

<details>
<summary>Proof</summary>

**Translations and rotations preserve $v, \kappa, \tau$**<br/>
Suppose a path $\alpha:I\to\R^3$ is translated by a constant $\mathbf{c}\in\R^3$ to obtain a new path $\beta(t) = \alpha(t) + \mathbb{c}$. Then $\beta'(t) = \alpha'(t)$, so $\alpha$ and $\beta$ have the same velocity and therefore the same speed. It follows that $\beta$ and $\alpha$ have the same Frenet vectors and thus the same curvature and torsion. Hence, speed, curvature and torsion are preserved under translations.

Suppose $\alpha$ is rotated in $\R^3$ to obtain a new path $\beta(t) = R(\alpha(t) + \mathbf{c})$ where $R$ is a rotation matrix such that $RR^T = I$. Then intuitively, the velocities $\alpha'$ and $\beta'$ are related by the same rotation, and so are the Frenet vectors. The matrix $Q$ whose rows are the Frenet vectors also transforms as $Q\mapsto QR$ such that

$$
  \frac{\mathrm{d}(QR)}{\mathrm{d}t}(QM)^T = \frac{\mathrm{d}Q}{\mathrm{d}t}RR^T Q^T = \frac{\mathrm{d}Q}{\mathrm{d}t} Q^T
$$

Hence the entries $\kappa$ and $\tau$ of $\frac{\mathrm{d}Q}{\mathrm{d}t}Q^T$ (Frenet-Serret formulas) are preserved under rotations. 

**Paths with same $v, \kappa, \tau$ are congruent**<br/>
Suppose the paths $\alpha, \beta$ have the same speed, curvature and tension. The two paths are congruent if one can be rigidly moved to the other through translation and rotation. Consider a point $a\in I$. We first translate $\alpha$ by a constant vector $\mathbf{c} = \beta(a) - \alpha(a)$ to get a new path $\gamma(t) = \alpha(t) + \mathbf{d}$ such that $\gamma(a) = \beta(a) =: \mathbf{x}_0$. As a translate, the $v,\kappa,\tau$ of $\gamma$ are preserved. 

The unit tangents  to $\gamma$ and $\beta$ at $\mathbf{x}_0$ may not be equal, but $\gamma$ can be rotated until they are. Next, $\gamma$ can be rotated again about this tangent direction until the principal normals align at $\mathbf{x}_0$. The binormals are automatically the same, since they are the cross products of the unit tangents and the principal normal. Under these transformations, the final path $\tilde{\alpha}$ satisfies the following three conditions
- $\tilde{\alpha}(a) = \beta(a) = \mathbf{x}_0$
- $\tilde{\alpha}$ and $\beta$ have the same Frenet vectors at $\mathbf{x}_0$
- $\tilde{\alpha}$ and $\beta$ have the same speed, curvature and torsion for all $t$

To complete the proof, it remains to show that $\tilde{\alpha} = \beta$ using the Frenet-Serret formulas. We denote the Frenet vectors of $\tilde{\alpha}$ by $\left(\tilde{\mathbf{T}}, \tilde{\mathbf{N}}, \tilde{\mathbf{B}}\right)$ and those of $\beta$ by $\left(\mathbf{T}, \mathbf{N}, \mathbf{B}\right)$. Using similar notation, we further have $\tilde{v} = v$, $\tilde{\kappa}= \kappa$ and $\tilde{\tau} = \tau$ by construction. Let $\theta$ be the angle between $\tilde{\mathbf{T}}$ and $\mathbf{T}$. Then 

$$
\begin{gather*}
  \tilde{\mathbf{T}}(t)\cdot\mathbf{T}(t) = \|\tilde{\mathbf{T}}(t)\| \cdot \|\mathbf{T}(t)\|\cos\theta \\
  \cos\theta \begin{cases} = 1, \quad \theta = 0 \iff \tilde{\mathbf{T}}(t) = \mathbf{T}(t) \\ \leq 1, \quad \textrm{otherwise} \end{cases}
\end{gather*}
$$

Noting the same for $\tilde{\mathbf{N}}(t)\cdot\mathbf{N}(t)$ and $\tilde{\mathbf{B}}(t)\cdot\mathbf{B}(t)$, we may define $f:I\to\R$ by

$$
\begin{align*}
  f(t) &= \tilde{\mathbf{T}}(t)\cdot\mathbf{T}(t) + \tilde{\mathbf{N}}(t)\cdot\mathbf{N}(t) + \tilde{\mathbf{B}}(t)\cdot\mathbf{B}(t) \\
  &\begin{cases} 
    = 3 \quad \iff \tilde{\mathbf{T}}(t) = \mathbf{T}(t), \tilde{\mathbf{N}}(t) = \mathbf{N}(t), \tilde{\mathbf{B}}(t) = \mathbf{B}(t) \\
    \leq 3 \quad \textrm{otherwise}
  \end{cases}
\end{align*}
$$

Note that by construction $f(a) = 3$. Differentiating $f$ and using the Frenet-Serret formulas gives

$$
\begin{align*}
  f' =& \left(\tilde{\mathbf{T}}'\cdot\mathbf{T} + \tilde{\mathbf{T}}\cdot\mathbf{T}' \right) + \left(\tilde{\mathbf{N}}'\cdot\mathbf{N} + \tilde{\mathbf{N}}\cdot\mathbf{N}' \right) + \left(\tilde{\mathbf{B}}'\cdot\mathbf{B} + \tilde{\mathbf{B}}\cdot\mathbf{B}' \right) \\
  =& \left(\kappa v\tilde{\mathbf{N}}\cdot\mathbf{T} + \tilde{\mathbf{T}}\cdot\kappa v \mathbf{N} \right) \\
   &+ \left[\left(-\kappa v \tilde{\mathbf{T}} + \tau v\tilde{\mathbf{B}}\right)\cdot\mathbf{N} + \tilde{\mathbf{N}}\cdot\left(-\kappa v\mathbf{T} + \tau\mathbf{B}\right)\right] \\
  &+ \left[\left(-\tau v\tilde{\mathbf{N}}\right)\cdot\mathbf{B} + \tilde{\mathbf{B}}\cdot\left(-\tau v \mathbf{N}\right)\right] \\
  =& \kappa v \left[\left(\tilde{\mathbf{N}}\cdot\mathbf{T} - \tilde{\mathbf{N}}\cdot\mathbf{T} \right) + \left(\tilde{\mathbf{T}}\cdot\mathbf{N} - \tilde{\mathbf{T}}\cdot\mathbf{N}\right)\right] \\
   &+ \tau v \left[\left(\tilde{\mathbf{B}}\cdot\mathbf{N}\right) - \left(\tilde{\mathbf{B}}\cdot\mathbf{N}\right) + \left(\tilde{\mathbf{N}}\cdot\mathbf{B} - \tilde{\mathbf{N}}\cdot\mathbf{B}\right) \right] \\
  =& 0
\end{align*}
$$

Hence $f$ is a constant function. Since $f(a) = 3$ it follows that $f(t) = 3$ for all $t$. This implies that $\tilde{\mathbf{T}}(t) = \mathbf{T}(t)$ for all $t$. By the definition of the unit tangent, this gives

$$
  \frac{\tilde{\alpha}'(t)}{v(t)} = \frac{\beta'(t)}{v(t)} \iff \tilde{\alpha}'(t) = \beta'(t) \; \forall t
$$

Consequently $\tilde{a}$ and $\beta$ differ by a constant vector. Since $\tilde{a}(a) = \beta(a) = \mathbf{x}_0$ this constant is zero, giving $\tilde{\alpha}(t) = \beta(t)$ for all $t$.
</details>
</MathBox>

# Real-valued functions (scalar fields)

## Graphs and level sets

<MathBox title='Graph' boxType='definition'>
Let $f:A\subset \R^n \to\R$ be a scalar function. The graph of $f$ is the set

$$
  \Set{ \left(x_i \right)_{i=0}^{n+1} \in\R^{n+1} | \left(x_i \right)_{i=1}^n \in A \textrm{ and } x_{n+1} = f(x_1,x_2,\dots,x_n) }
$$
</MathBox>

<MathBox title='Level set (contour)' boxType='definition'>
Let $f:A\subset \R^n \to\R$ be a scalar function. For a given $c \in f[A] \subseteq\R$, its corresponding level set is the set of all points in the domain at which $f$ has a value of $c$, i.e. $\Set{\mathbf{x}\in A : f(\mathbf{x}) = c }$. For functions of two variables, level sets are also called level curves and, for functions of three variables, level surfaces.
</MathBox>

## Continuity

<MathBox title='Continuous real-valued function' boxType='definition'>
Let $f: U\subseteq\R^n \to\R$ be a real-valued function and consider a point $\mathbf{a}\in U$. Then, $f$ is called continuous at $\mathbf{a}$ if, given any open ball $B_\epsilon (f(\mathbf{a}))$ about $f(\mathbf{a})$, there exists an open ball $B_\delta (\mathbf{a})$ about $\mathbf{a}$ such that

$$
  f(B_\delta (\mathbf{a})) \subset B_\epsilon (f(\mathbf{a}))
$$

Equivalently, writing out the definition of the open ball, $f$ is continuous at $\mathbf{a}$ if, given any $\epsilon > 0$, there is a $\delta > 0$ such that

$$
  \|\mathbf{x} - \mathbf{a}\| < \delta : |f(\mathbf{x}) - f(\mathbf{a}) | < \epsilon
$$

The function $f$ is simply called continuous if it is continuous at every point of its domain $U$.
</MathBox>

<MathBox title='Properties of continuous real-valued functions' boxType='proposition'>
Let $f, g: U\to\R$ be real-valued functions defined on an open set $U\subseteq\R^n$. If $f$ and $g$ are continuous at a point $\mathbf{a}\in U$, then so are
1. the sum $f + g$
2. $cf$ for any scalar $c\in\R$
3. the product $fg$
4. the quotient $f/g$, assuming $g(\mathbf{a})\neq 0$

<details>
<summary>Proof</summary>

**(1):** Given $\epsilon > 0$, we need to find $\delta > 0$ such that 

$$
  \mathbf{x}\inB_\delta (\mathbf{a}) \implies \left|\left[f(\mathbf{x}) + g(\mathbf{x})\right] - \left[f(\mathbf{a}) + g(\mathbf{a})\right]\right| < \epsilon 
$$

The absolute values of the $f$ and $g$ contributions can be separated by applying the triangle inequality

$$
\begin{align*}
  \left|\left[f(\mathbf{x}) + g(\mathbf{x})\right] - \left[f(\mathbf{a}) + g(\mathbf{a})\right]\right| &= \left|\left[f(\mathbf{x}) - f(\mathbf{a})\right] + \left[g(\mathbf{x}) - g(\mathbf{a})\right]\right| \\
  &\overset{\triangle}{\leq} \left|f(\mathbf{x}) - f(\mathbf{a})\right| + \left|g(\mathbf{x}) - g(\mathbf{a})\right|
\end{align*}
$$

Since $f$ and $g$ are continuous at $\mathbf{a}$, then given any $\epsilon_f = \epsilon_g = \frac{\epsilon}{2}$, there are $\delta_f > 0$ and $\delta_g > 0$ such that 

$$
\begin{align*}
  \left|f(\mathbf{x}) - f(\mathbf{a})\right| < \epsilon_f \land \mathbf{x}\inB_{\delta_f}(\mathbf{a}) \\
  \left|g(\mathbf{x}) - g(\mathbf{a})\right| < \epsilon_g \land \mathbf{x}\inB_{\delta_g}(\mathbf{a})
\begin{align*}
$$

Taking $\delta = \min\Set{\delta_f, \delta_g}$, we get that

$$
  \mathbf{x}\inB_\delta (\mathbf{a}) \implies \left|\left[f(\mathbf{x}) + g(\mathbf{x})\right] - \left[f(\mathbf{a}) + g(\mathbf{a})\right]\right| < \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon
$$
</details>
</MathBox>

<MathBox title='Composition preserves continuity' boxType='proposition'>
Suppose $U\subseteq\R^n$ and $V\subseteq\R$ are open sets, and let $f:U\to\R$ and $g:V\to\R$ be functions such that $f(\mathbf{x})\in V$ for all $\mathbf{x}\in U$. This guarantees that the composition $g\circ f:U\to\R$ is defined. If $f$ and $g$ are continuous, so is the composition $f\circ g$.

<details>
<summary>Proof</summary>

Consider a point $\mathbf{a}\in U$ and let $\epsilon > 0$ be given. Since $g$ is continuous at the point $f(\mathbf{a})$, there is a $\delta' > 0$ such that

$$
  g\left(B_{\delta'}\left[f(\mathbf{a})\right]\right) \subset B_\epsilon \left(g[f(\mathbf{a})]\right)
$$

Since $f$ is continuous at $\mathbf{a}$, treating $\delta'$ as the given input, there is a $\delta > 0$ such that

$$
  f\left[B_\delta (\mathbf{a}) \right] \subset B_{\delta'} \left[ f(\mathbf{a})\right]
$$

Linking these two steps gives

$$
  (g\circ f)\left[B_\delta (\mathbf{a}) \right] = g\left(f\left[B_\delta (\mathbf{a})\right]\right) \subset g\left(B_{\delta'}\left[f(\mathbf{a})\right]\right) \subset B_\epsilon \left(g[f(\mathbf{a})]\right)
$$
</details>
</MathBox>

## Limits

<MathBox title='Limit' boxType='definition'>
Consider a point $\mathbf{a}\in U \subseteq\R^n$ where $U$ is an open set. If $f:\R^n \to\R$ is a real-valued function that is defined on $U$, except possibly at $\mathbf{a}$, then the limit of $f$ at $\mathbf{a}$, denoted $\lim_{\mathbf{x}\to\mathbf{a}}$, exists if there is a number $L$, such that the function $\tilde{f}:U\to\R$ defined by

$$
  \tilde{f} = \begin{cases}
    f(\mathbf{x}), \quad& \mathbf{x}\neq\mathbf{a} \\
    L, \quad& \mathbf{x}=\mathbf{a}
  \end{cases}
$$

is continuous at $\mathbf{a}$, in which case we write $\lim_{\mathbf{x}\to\mathbf{a}} f(\mathbf{x}) = L$.

Alternatively, using the definition of continuity to $f$ then $\lim_{\mathbf{x}\to\mathbf{a}} f(\mathbf{x}) = L$ if for any $\epsilon > 0$, there exists a $\delta > 0$ such that $|f(\mathbf{x}) - L| < \epsilon$ whenever $\|\mathbf{x} - \mathbf{a}\| < \delta$, except possibly when $\mathbf{x} = \mathbf{a}$.
</MathBox>

<MathBox title='Properties of limits' boxType='proposition'>
Suppose that $\lim_{\mathbf{x}\to\mathbf{a}}f(\mathbf{x})$ and $\lim_{\mathbf{x}\to\mathbf{a}}f(\mathbf{x})$ both exist for real-valued functions $f,g:\R^n \to\R$.
1. $\lim_{\mathbf{x}\to\mathbf{a}}[f(\mathbf{x}) + g(\mathbf{x})] = \lim_{\mathbf{x}\to\mathbf{a}}f(\mathbf{x}) + \lim_{\mathbf{x}\to\mathbf{a}}g(\mathbf{x})$
2. $\lim_{\mathbf{x}\to\mathbf{a}}cf(\mathbf{x}) = c \lim_{\mathbf{x}\to\mathbf{a}}f(\mathbf{x})$ for any scalar $c\in\R$
3. $\lim_{\mathbf{x}\to\mathbf{a}}f(\mathbf{x} g(\mathbf{x})) = \lim_{\mathbf{x}\to\mathbf{a}}f(\mathbf{x}) \lim_{\mathbf{x}\to\mathbf{a}}g(\mathbf{x})$
4. $\lim_{\mathbf{x}\to\mathbf{a}}\frac{f(\mathbf{x})}{g(\mathbf{x})} = \frac{\lim_{\mathbf{x}\to\mathbf{a}}f(\mathbf{x})}{\lim_{\mathbf{x}\to\mathbf{a}}g(\mathbf{x})}$ if $\lim_{\mathbf{x}\to\mathbf{a}}g(\mathbf{x}) \neq 0$

<details>
<summary>Proof</summary>

**(1):** Let $L = \lim_{\mathbf{x}\to\mathbf{a}} f(\mathbf{x})$ and $M = \lim_{\mathbf{x}\to\mathbf{x}}$ and consider the functions

$$
  \tilde{f}(\mathbf{x}) = \begin{cases}
    f(\mathbf{x}),\quad& \mathbf{x}\neq\mathbf{a} \\
    L,\quad& \mathbf{x}=\mathbf{a}
  \end{cases}
$$

and

$$
  \tilde{g}(\mathbf{x}) = \begin{cases}
    g(\mathbf{x}),\quad& \mathbf{x}\neq\mathbf{a} \\
    M,\quad& \mathbf{x}=\mathbf{a}
  \end{cases}
$$

By definition of limit $\tilde{f}$ and $\tilde{g}$ are continuous at $\mathbf{a}$, hence so is the their sum. In other words, the function given by

$$
  \tilde{f}(\mathbf{x}) + \tilde{g}(\mathbf{x}) = \begin{cases}
    f(\mathbf{x}) + g(\mathbf{x}),\quad& \mathbf{x} \neq\mathbf{a} \\
    L + M,\quad& \mathbf{x} = \mathbf{a}
  \end{cases}
$$

is continuous at $\mathbf{a}$, such that $\lim_{\mathbf{x}\to\mathbf{a}} (f(\mathbf{x}) + g(\mathbf{x})) = L + M$.
</details>
</MathBox>

<MathBox title='Composition preserves limits' boxType='proposition'>
Suppose that $f:U\subseteq\R^n \to\R$ is defined on an open set $U$ except possibly at $\mathbf{a}\in U$, and that $\lim_{\mathbf{x}\to\mathbf{a}} f(\mathbf{x}) = L$. Let $\alpha: I\subseteq\R \to U$ be a continuous path in $U$ passing through $\mathbf{a}$, i.e. $\alpha(t_0) = \mathbf{a}$ for some $t_0 \in I$. Consider that function $g:I\to\R$ given by

$$
  g(t) = \begin{cases}
    f(\alpha(t)),\quad& \alpha(t) \neq \mathbf{a} \\
    L,\quad& \alpha(t) = \mathbf{a}
  \end{cases}
$$

Then $\lim_{t\to t_0} g(t) = L$.

<details>
<summary>Proof</summary>

Because the limit $\lim_{\mathbf{x}\to\mathbf{a}} f(\mathbf{x}) = L$ exists, the function $\tilde{f}:U\to\R$ defined by

$$
  \tilde{f}(\mathbf{x}) = \begin{cases}
    f(\mathbf{x}),\quad& \mathbf{x}\neq\mathbf{a} \\
    L,\quad& \mathbf{x} = \mathbf{a}
  \end{cases} 
$$

is continuous at $\mathbf{a}$. Since $\alpha$ is continuous and $\lim_{t-t_0} \alpha(t) = \mathbf{a}$, the composition $g = \tilde{f}\circ\alpha: I\to\R$ given by

$$
  g(t) = \tilde{f}(\alpha(t)) = \begin{cases}
    f(\alpha(t)),\quad& \alpha(t) \neq \mathbf{a} \\
    L,\quad& \alpha(t) = \mathbf{a}
  \end{cases}
$$

is continuous at $t_0$. Hence

$$
\begin{align*}
  \lim_{t\to t_0} g(t) =& \lim_{\alpha(t)\to \alpha(t_0)} f(\alpha(t)) \\
  =& \lim_{\mathbf{x}\to \mathbf{a}} f(\mathbf{x}) = L
\end{align*}
$$
</details>
</MathBox>

## Differential calculus

### Partial derivative

In the one-variable case, a real-valued function $f:I\subseteq\R\to\R$ can be linearly approximated at $a\in I$ by a secant (affine linear function)

$$
  \ell(t) = \frac{f(x)-f(a)}{x - a} (t - a) + f(a) 
$$

In the limit $x\to a$, the secant converges to the tangent of $f$ at $a$. If this limit exists, we say that $f$ is differentiable at $a$ with derivative equal to the slope of $\ell$

$$
  f'(a) := \lim_{x\to a} \frac{f(x) - f(a)}{x - a} \\
$$

This can be rewritten as

$$
\begin{align*}
  0 =& \lim_{x\to a} \left( \frac{f(x) - f(a)}{x - a} - f'(a) \right) \\
  =& \lim_{x\to a} \frac{f(x) - f(a) - f'(a)(x-a)}{x - a} \\
  =& \lim_{x\to a} \frac{f(x) - \ell(x)}{x - a} 
\end{align*}
$$

Generalizing to the multivariable case, we expect a function $f:U\subseteq\R^n \to\R$ to be differentiable at a point $\mathbf{a}\in U$ if there is a linear affine function $\ell:\R^n \to\R$ given by $\ell(\mathbf{x}) = T(\mathbf{x}) + b$ such that

$$
  \lim_{\mathbf{x}\to\mathbf{a}} \frac{f(\mathbf{x}) - \ell(\mathbf{x})}{\| \mathbf{x} - \mathbf{a}\|} = 0
$$

<MathBox title='Affine function' boxType='definition'>
A function $\ell:\R^n \to R^m$ is called an *affine function* if it has the form $\ell(\mathbf{x}) = T(\mathbf{x}) + \mathbf{b}$, where $T:\R^n \to\R^m$ is a linear transformation and $\mathbf{b}\in\R^m$.
</MathBox>

The affine function $\ell$ represents the linear approximation of $f$ at $\mathbf{a}$ such that $f(\mathbf{a}) = \ell(\mathbf{a}) = T(\mathbf{a}) + b$. Thus, $b = f(\mathbf{a}) - T(\mathbf{a})$ giving

$$
\begin{align*}
  \ell(\mathbf{x}) =& T(\mathbf{x}) + f(\mathbf{a}) - T(\mathbf{a}) \\
  =& f(\mathbf{a}) + T(\mathbf{x} - \mathbf{a})
\end{align*}
$$

The difference quotient can thus be rewritten as

$$
  \lim_{\mathbf{x}\to\mathbf{a}} \frac{f(\mathbf{x}) - f(\mathbf{a}) - T(\mathbf{x} - \mathbf{a})}{\| \mathbf{x} - \mathbf{a}\|} = 0
$$

Suppose that $\mathbf{x}$ approaches $\mathbf{a}$ in the $x_i$-direction, i.e. $\mathbf{x} = \mathbf{a} + h\mathbf{e}_i$. Then $\mathbf{x} - \mathbf{a} = h\mathbf{e}_1$ and $\| \mathbf{x} - \mathbf{a} \| = |h|$. For the difference quotient to hold true then we require in both limits $h\downarrow 0$ and $h\uparrow 0$ that

$$
\begin{align*}
  0 =&\lim_{h\to 0} \frac{f(\mathbf{a} + h\mathbf{e}_i) - f(\mathbf{a}) - T(h\mathbf{e}_i)}{h} \\
  =& \lim_{h\to 0} \frac{f(\mathbf{a} + h\mathbf{e}_i) - f(\mathbf{a}) - hT(\mathbf{e}_i)}{h} 
\end{align*}
$$

where $T(h\mathbf{e}_i) = hT(\mathbf{e}_i)$ follows from the linearity of $T$. Solving for for $T(\mathbf{e}_i)$ gives

$$
\begin{align*}
  T(\mathbf{e}_i) =& \lim_{h\to 0} \frac{f(\mathbf{a} + h\mathbf{e}_i) - f(\mathbf{a})}{h} \\
  =& \lim_{h\to 0} \frac{f(a_i,\dots,a_i + h,\dots,a_n) - f(a_1,\dots,a_i,\dots,a_n)}{h}
\end{align*}
$$

This limit of the difference quotient is analogous to the one-variable derivative where only $x_i$ varies while $x_j = a_j$ for $j\neq i$ are held fixed. This limit is called the partial derivative of $f$ at $\mathbf{a}$ with respect to $x_i$, denoted $\frac{\partial}{\partial x_i}(\mathbf{a})$.

The linear transformation $T:\R^n \to\R$ takes the form of a $1\times n$ matrix

$$
  \mathrm{D}f(\mathbf{a}) = \begin{bmatrix} \frac{\partial f}{\partial x^1}(\mathbf{a}) & \cdots & \frac{\partial f}{\partial x_n}(\mathbf{a}) \end{bmatrix}
$$

which is called the *total derivative* of $f$ at $\mathbf{a}$. The total derivative is also known as the Jacobian matrix.

<MathBox title='Partial derivative' boxType='definition'>
Let $f:U\to\R$ be a real-valued function defined on an open set $U\in\R^n$ and consider a point $\mathbf{a}\in U$. The *partial derivative* of $f$ at $\mathbf{a}$ with respect to $x_i$ is defined by

$$
\begin{align*}
  \frac{\partial f}{\partial x_i}(\mathbf{a}) =& \lim_{h\to 0} \frac{f(\mathbf{a} + h\mathbf{e}_i) - f(\mathbf{a})}{h} \\
  =& \lim_{h\to 0} \frac{f(a_i,\dots,a_i + h,\dots,a_n) - f(a_1,\dots,a_i,\dots,a_n)}{h}
\end{align*}
$$
</MathBox>

<MathBox title='Differentiability' boxType='definition'>
A function $f:U\subseteq\R^n \to\R$ is *differentiable* at a point $\mathbf{a}\in U$ if

$$
  \lim_{\mathbf{x}\to\mathbf{a}}\frac{f(\mathbf{x}) - f(\mathbf{a}) - \mathrm{D}f(\mathbf{a})(\mathbf{x} - \mathbf{a})}{\| \mathbf{x} - \mathbf{a}\|} = 0
$$

If this limit exists, the matrix $\mathrm{D}f(\mathbf{a})$ is called the *derivative* of $f$ at $\mathbf{a}$. It is also known as the *Jacobian matrix*. The affine function $\ell(\mathbf{x}) = f(\mathbf{a}) + \mathrm{D}f(\mathbf{a})(\mathbf{x}-\mathbf{a})$ is called the *first-order affine transformation* of $f$ at $\mathbf{a}$.
</MathBox>

<MathBox title='Differentiability implies continuity' boxType='proposition'>
If $f:\R^n \to\R$ is differentiable at $\mathbf{a}\in\R^n$, then $f$ is continuous at $\mathbf{a}$.

<details>
<summary>Proof</summary>

Define a function $Q:\R^n \to\R$ by

$$
  Q(\mathbf{x}) = \begin{cases}
    \frac{f(\mathbf{x}) - f(\mathbf{a}) - \nabla f(\mathbf{a})\cdot(\mathbf{x} - \mathbf{a})}{\| \mathbf{x} - \mathbf{a} \|}, \quad& \mathbf{x}\neq\mathbf{a} \\
    0 \quad& \mathbf{x} = \mathbf{a} 
  \end{cases}
$$

If $f$ is differentiable at $\mathbf{a}$, then

$$
  \lim_{\mathbf{x}\to\mathbf{a}}\frac{f(\mathbf{x}) - f(\mathbf{a}) - \nabla f(\mathbf{a})\cdot(\mathbf{x} - \mathbf{a})}{\| \mathbf{x} - \mathbf{a}\|} = 0
$$

Thus

$$
\begin{gather*}
  \lim_{\mathbf{x}\to\mathbf{a}} |Q(\mathbf{x}) - Q(\mathbf{a})| = \lim_{\mathbf{x}\to\mathbf{a}} |Q(\mathbf{x})| = 0 \\
  \iff \lim_{\mathbf{x}\to\mathbf{a}} Q(\mathbf{x}) = Q(\mathbf{a})
\end{gather*}
$$

showing that $Q$ is continuous at $\mathbf{a}$. Using the triangle and Cauchy-Schwarz inequalities gives

$$
\begin{align*}
  |f(\mathbf{x}) - f(\mathbf{a})| =& |Q(\mathbf{x})\| \mathbf{x} - \mathbf{a} \| + \nabla f(\mathbf{a})\cdot (\mathbf{x} - \mathbf{a})| \\
  \overset{\triangle}{\leq}& |Q(\mathbf{x})\| \mathbf{x} - \mathbf{a} \|| + |\nabla f(\mathbf{a})\cdot (\mathbf{x} - \mathbf{a})| \\
  \leq& |Q(\mathbf{x})|\cdot\| \mathbf{x} - \mathbf{a} \| + \| \nabla f(\mathbf{a})\| \cdot \|\mathbf{x} - \mathbf{a}\|
\end{align*}
$$

Taking the limit $\mathbf{x}\to\mathbf{a}$ gives

$$
\begin{align*}
  \lim_{\mathbf{x}\to\mathbf{a}} |f(\mathbf{x}) - f(\mathbf{a})| \leq&
  \lim_{\mathbf{x}\to\mathbf{a}}|Q(\mathbf{x})|\cdot\| \mathbf{x} - \mathbf{a} \| \\
  &+ \lim_{\mathbf{x}\to\mathbf{a}} \| \nabla f(\mathbf{a})\| \cdot \|(\mathbf{x} - \mathbf{a})\| \\
  =& 0
\end{align*}
$$

Hence $\lim_{\mathbf{x}\to\mathbf{a}} f(\mathbf{x}) = f(\mathbf{a})$, showing that $f$ is continuous at $\mathbf{a}$.
</details>
</MathBox>

### Mean value theorem

<MathBox title='Mean value theorem' boxType='theorem'>
Let $B = B_r(\mathbf{a})$ be an open ball centered at $\mathbf{a}$. Let $f:B\to\R$ be a differentiable function. For any $\mathbf{b}\in B$, there is a point $\mathbf{c}\in\R$ on the line segment connecting $\mathbf{a}$ and $\mathbf{b}$ such that

$$
  f(\mathbf{b}) - f(\mathbf{a}) = \nabla f(\mathbf{c})\cdot (\mathbf{b} - \mathbf{a})
$$

<details>
<summary>Proof</summary>

The line segment between $\mathbf{a}$ and $\mathbf{b}$ can be parametrized by the curve $\alpha:[0,1]\to\R^n$ given by $\alpha(t) = \mathbf{a} + t(\mathbf{b} - \mathbf{a})$. Then the composition $f\circ \alpha:[0,1] \to\R$ is real-valued function that is differentiable on the open interval $(0,1)$. By the ordinary mean value theorem, there exists a point $t \in (0,1)$ with $\alpha(t) = \mathbf{c}$ such that

$$
\begin{align*}
  (f\circ\alpha)(1) - (f\circ\alpha)(0) =& (f \circ \alpha)'(t)(1 - 0) \\
  =& (f \circ \alpha)'(t)
\end{align*}
$$

Applying the little chain rule on $(f \circ \alpha)'(t)$, noting that $\alpha'(t) = \mathbf{b} - \mathbf{a} = \alpha(1) - \alpha(0)$, gives

$$
\begin{align*}
  (f \circ \alpha)'(t) =& \nabla f(\alpha(t))\cdot\alpha'(t) \\
  =& \nabla f(\alpha(t))\cdot (\alpha(1) - \alpha(0))
\end{align*}
$$

Inserting back gives

$$
\begin{align*}
  f(\alpha(1)) - f(\alpha(0)) =& \nabla f(\alpha(t))\cdot (\alpha(1) - \alpha(0)) \\
  f(\mathbf{b}) - f(\mathbf{a}) =& \nabla f(\mathbf{c})\cdot(\mathbf{b} - \mathbf{a})
\end{align*}
$$
</details>
</MathBox>

### $C^1$ test

<MathBox title='Continuously differentiable function' boxType='definition'>
A scalar function $f:U\subseteq\R^n \to\R$ whose partial derivatives are continuous on $U$ is *continuously differentiable* on $U$, denoted $f\in C^1(U)$.
</MathBox>

<MathBox title='$C^1$ test' boxType='theorem'>
Let $f:U\subseteq\R^n \to\R$ be a scalar function. If $f\in C^1(U)$, then $f$ is differentiable at every point of $U$.

<details>
<summary>Proof</summary>

Let $\mathbf{a}$ be an arbitrary point in $U$. For any $\mathbf{x}\in U$, there is a point $\mathbf{c}$ on the line segment between $\mathbf{x}$ and $\mathbf{a}$ such that by the mean value theorem

$$
  f(\mathbf{x}) - f(\mathbf{a}) = \nabla f(\mathbf{c})\cdot (\mathbf{b} - \mathbf{a})
$$

Also

$$
  \mathrm{D}f(\mathbf{a})\cdot (\mathbf{x} - \mathbf{a}) = \nabla f(\mathbf{a})\cdot (\mathbf{x} - \mathbf{a})
$$

Thus, the first-order approximation of $f$ at $\mathbf{a}$ takes the form

$$
\begin{align*}
  f(\mathbf{x}) - f(\mathbf{a}) - \mathrm{D}f(\mathbf{a})(\mathbf{x} - \mathbf{a}) =& \nabla f(\mathbf{c})\cdot (\mathbf{x} - \mathbf{a}) - \nabla f(\mathbf{a})\cdot (\mathbf{x} - \mathbf{a}) \\
  =& (\nabla f(\mathbf{c}) - \nabla f(\mathbf{a}))\cdot (\mathbf{x} - \mathbf{a})
\end{align*}
$$

Then

$$
\begin{align*}
  \frac{|f(\mathbf{x}) - f(\mathbf{a}) - \mathrm{D}f(\mathbf{a})(\mathbf{x} - \mathbf{a})|}{\| \mathbf{x} - \mathbf{a} \|} =& \frac{|(\nabla f(\mathbf{c}) - \nabla f(\mathbf{a}))\cdot (\mathbf{x} - \mathbf{a})|}{\| \mathbf{x} - \mathbf{a} \|} \\
  \leq& \| \nabla f(\mathbf{c}) - \nabla f(\mathbf{a}) \| \frac{\|\mathbf{x} - \mathbf{a}\|}{\| \mathbf{x} - \mathbf{a} \|} \\
  =& \| \nabla f(\mathbf{c}) - \nabla f(\mathbf{a}) \|
\end{align*}
$$

Since $\| \mathbf{c} - \mathbf{a} \| \leq \| \mathbf{x} - \mathbf{a} \|$, it follows that $\mathbf{c}\xrightarrow{\mathbf{x}\to \mathbf{a}} \mathbf{a}$. By continuity of partial derivatives, this mean that $\nabla f(\mathbf{c}) \xrightarrow{\mathbf{x}\to\mathbf{a}} \nabla f(\mathbf{a})$. Hence

$$
\begin{align*}
  &\lim_{\mathbf{x}\to\mathbf{a}} \frac{|f(\mathbf{x}) - f(\mathbf{a}) - \mathrm{D}f(\mathbf{a})(\mathbf{x} - \mathbf{a})|}{\| \mathbf{x} - \mathbf{a} \|} \\
  \leq& \lim_{\mathbf{x}\to\mathbf{a}} \| \nabla f(\mathbf{c}) - \nabla f(\mathbf{a}) \| = 0
\end{align*}
$$

showing that $f$ is differentiable at $\mathbf{a}$.
</details>
</MathBox>

### Gradient

If a scalar function $f:U\subseteq\R^n \to\R$ is differentiable at $\mathbf{a}\in U$, its total derivative at this point is the linear transformation $\mathrm{D}f(\mathbf{a}):\R^n \to\R$ in the form of the $1\times n$ Jacobian matrix

$$
  \mathrm{D}f(\mathbf{a}) = \begin{bmatrix} \frac{\partial f}{\partial x^1}(\mathbf{a}) & \cdots & \frac{\partial f}{\partial x^n}(\mathbf{a}) \end{bmatrix}
$$

Transposing $\mathrm{D}f(\mathbf{a})$ gives a vector ($n\times 1$ matrix) called the *gradient* of $f$ at $\mathbf{a}$, denoted

$$
\begin{align*}
  \nabla f(\mathbf{a}) =& \mathrm{D}f(\mathbf{a})^T \\
  =& \begin{bmatrix} \frac{\partial f}{\partial x^1}(\mathbf{a}) & \cdots & \frac{\partial f}{\partial x^n}(\mathbf{a}) \end{bmatrix}^T \\
  =& \begin{bmatrix} \frac{\partial f}{\partial x_i}(\mathbf{a}) \\ \vdots \\ \frac{\partial f}{\partial x^n}(\mathbf{a}) \end{bmatrix}
\end{align*}
$$

On a sidenote, since $\mathrm{D}f(\mathbf{a})$ maps vectors in $\R^n$ into $\R$ it is technically a differential 1-form, or a linear functional, identified with dual vector space $(\R^n)^*$, which is the set of all linear functionals. The total derivative $\mathrm{D}f(\mathbf{a})$ and the gradient $\nabla f(\mathbf{a})$ are thus dual to each other. 

<MathBox title='Gradient' boxType='definition'>
If a function $f:U\subseteq\R^n \to\R$ is totally differentiable $\mathbf{a}\R^n$, then the gradient of $f$ at $\mathbf{a}$ is defined as

$$
  \nabla f(\mathbf{a}) = \mathrm{grad}(f(\mathbf{a})) := \begin{bmatrix} \frac{\partial f}{\partial x_i}(\mathbf{a}) \\ \vdots \\ \frac{\partial f}{\partial x^n}(\mathbf{a}) \end{bmatrix}
$$

which is the transpose of the total derivative (Jacobian) $\mathrm{D}f(\mathbf{a})$. Since $\mathrm{D}f(\mathbf{a})(\mathbf{x} - \mathbf{a}) = \nabla f(\mathbf{a})\cdot (\mathbf{x}-\mathbf{a})$, the differentiability condition can be rewritten in terms of the gradient as

$$
  \lim_{\mathbf{x}\to\mathbf{a}}\frac{f(\mathbf{x}) - f(\mathbf{a}) - \nabla f(\mathbf{a})\cdot(\mathbf{x} - \mathbf{a})}{\| \mathbf{x} - \mathbf{a}\|} = 0
$$
</MathBox>

<MathBox title='Properties of the del operator' boxType='proposition'>
Let $f,g:\R^n \to\R$ be scalar functions and $\alpha\in\R$ a scalar. The del operator $\nabla$ has the following properties. 

1. $\nabla(\alpha f + \beta g) = \alpha\nabla f + \beta\nabla g$ **(linearity)**
2. $\nabla(fg) = g\nabla f + f\nabla g$ **(product rule):**
3. $\nabla\left(\frac{f}{g} \right) = \frac{g\nabla f - f\nabla g}{g^2}
$ **(quotient rule)**
</MathBox>

### Directional derivative

<MathBox title='Little chain rule' boxType='theorem'>
Let $U$ be an open set in $\R^n$, $\alpha:I\to U$ a path in $U$ defined on an open interval $I\in\R$, and $f:U\to\R$ a scalar function. If $\alpha$ is differentiable at $t$ and $f$ is differentiable at $\alpha(t)$, then $f\circ\alpha$ is differentiable at $t$ and

$$
  (f\circ\alpha)'(t) = \nabla f(\alpha(t))\cdot\alpha'(t)
$$

<details>
<summary>Proof</summary>

**Proof by componentwise chain rule**<br/>
Assume that $\alpha$ takes the form

$$
  \alpha(t) = (x_i(t))_{i=1}^n
$$

Applying the ordinary chain rule componentwise on the expression
  
$$
  (f\circ t)(t) = f(\alpha(t)) = f(x_1(t),\dots,x_n(t))
$$

we get

$$
\begin{align*}
  (f\circ t)'(t) =& \frac{\mathrm{d}}{\mathbf{d}t} f(\alpha(t)) = \frac{\mathrm{d}}{\mathbf{d}t} f(x_i(t))_{i=1}^n \\
  =& \sum_{i=1}^n \frac{\partial f}{\partial x_i}(\alpha(t))\frac{\partial x_i (t)}{\partial t} \\
  =& \sum_{i=1}^n \frac{\partial f}{\partial x_i}(\alpha(t))x'_i(t) \\
  =& \left(\frac{\partial f}{\partial x_i}(\alpha(t))\right)_{i=1}^n \cdot (x'_i(t))_{i=1}^n \\
  =& \nabla f(\alpha(t))\cdot \alpha'(t)
\end{align*}
$$

**Proof by differential limit**<br/>

Since $f: U\to\R$ is differentiable at $\mathbf{a}$, then

$$
  \lim_{\mathbf{x}\to\mathbf{a}}\frac{f(\mathbf{x}) - f(\mathbf{a}) - \nabla f(\mathbf{a})\cdot(\mathbf{x} - \mathbf{a})}{\| \mathbf{x} - \mathbf{a}\|} = 0
$$

Since $\alpha:I\subseteq\R\to U$ is differentiable at $t_0$, then

$$
  \alpha'(t_0) = \lim_{t\to t_0} \frac{\| \alpha(t) - \alpha(t_0) \|}{t - t_0} =  \lim_{t\to t_0} \frac{\| \mathbf{x} - \mathbf{a} \|}{t - t_0}
$$

Define a function $Q:U\subseteq\R^n \to\R$ by

$$
  Q(\mathbf{x}) = \begin{cases}
    \frac{f(\mathbf{x}) - f(\mathbf{a}) - \nabla f(\mathbf{a})\cdot (\mathbf{x} - \mathbf{a})}{\| \mathbf{x} - \mathbf{a} \|}, \quad& \mathbf{x}\neq\mathbf{a} \\
    0 \quad& \mathbf{x} = \mathbf{a} 
  \end{cases}
$$

Since $\alpha$ is differentiable at $t_0$ it is also continuous at this point with $\alpha(t_0) = \mathbf{a}$. Similarly, since $Q$ is differentiable at $\mathbf{a}$ is also continuous at this point. Thus, the composition $Q \circ\alpha$ is also continuous at $t_0$ such that the following limit exists

$$
\begin{align*}
  0 =& \lim_{\mathbf{x}\to\mathbf{a}} |Q(\mathbf{x})| = \lim_{t \to t_0} |Q(\alpha(t))| \\
  =& \lim_{t \to t_0} \frac{|f(\alpha(t)) - f(\alpha(t_0)) - \nabla f(\alpha(t_0))\cdot(\alpha(t) - \alpha(t_0))|}{\| \alpha(t) - \alpha(t_0) \|}
\end{align*}
$$

Multiplying the limit with $\alpha'(t_0)$ gives

$$
  0 = \lim_{t \to t_0} \left|\frac{f(\alpha(t)) - f(\alpha(t_0)) - \nabla f(\alpha(t_0))\cdot (\alpha(t) - \alpha(t_0))}{t - t_0}\right|
$$

which holds if and only if

$$
\begin{align*}
  \lim_{t \to t_0} \frac{f(\alpha(t)) - f(\alpha(t_0))}{t - t_0} =& \lim_{t \to t_0}\nabla f(\alpha(t_0)) \cdot \frac{\alpha(t) - \alpha(t_0)}{t - t_0} \\
  (f\circ \alpha)' (t_0) =& \nabla f(\alpha(t_0))\cdot\alpha'(t_0)
\end{align*}
$$

Substituting $t = t_0$, gives the desired result.
</details>
</MathBox>

<MathBox title='Directional derivative' boxType='definition'>
The directional derivative of a function $f:U\subseteq\R^n \to\R$  along a vector $\mathbf{v}\in\R^n$ at $\mathbf{a}\in U$ is defined as

$$
  \mathrm{D}_\mathbf{v}f = \nabla_\mathbf{v} f := \lim_{h \to 0} \frac{f(\mathbf{x} + h \mathbf{v}) - f(\mathbf{x})}{h}
$$
</MathBox>

<MathBox title='' boxType='proposition'>
Let $f:U\subseteq\R^n \to\R$ be totally differentiable at $\mathbf{a}\in U$. If $\alpha:I\subseteq\R\to U$ given by $\alpha(t) = \mathbf{a} + t\mathbf{v}$ is a path passig through $\mathbf{a}$ along $\mathbf{v}$, i.e. $\alpha(0) = \mathbf{a}$, then

$$
\begin{align*}
  \mathrm{D}_\mathbf{v}f =& \left.\frac{\mathrm{d}}{\mathrm{d}t}f(\mathbf{a} + t\mathbf{v})\right|_{t=0} = (f\circ\alpha)'(0) \\
  =& \nabla f(\alpha(0))\cdot \alpha'(0) \\
  =& \nabla f(\mathbf{a}) \cdot \mathbf{v} \\
\end{align*}
$$
</MathBox>

<MathBox title='Gradients are the directions of fastest ascent' boxType='proposition'>
Let $f:U\subseteq\R^n \to\R$ be totally differentiable at $\mathbf{a}\in U$. The gradient of $f$ at $\mathbf{a}$ gives the direction in which $f$ increases fastest.

<details>
<summary>Proof</summary>

The directional derivative of $f$ at $\mathbf{a}$ along a vector $\mathbf{v}\in\R^n$ is

$$
  \mathrm{D}_\mathbf{v} f(\mathbf{a}) = \nabla f(\mathbf{a}) \cdot \mathbf{v}
$$

If $\nabla f(\mathbf{a})$ and $\mathbf{v}$ are separated by an angle $\theta$, then

$$
  \nabla f(\mathbf{a}) \cdot \mathbf{v} = \| \nabla f(\mathbf{a})\| \cdot \| \mathbf{u} \| \cos\theta 
$$

showing that $\mathrm{D}_\mathbf{v}f(\mathbf{a})$ is maximum when $\theta = 0$, i.e. when $\nabla f(\mathbf{a})$ and $\mathbf{v}$ are aligned. This implies that the gradient of $f$ at $\mathbf{a}$ gives the direction of the fastest ascent.
</details>
</MathBox>

<MathBox title='Gradients are normal vectors to level sets' boxType='proposition'>
Let $f:U\subseteq\R^n \to\R$ be totally differentiable at $\mathbf{a}\in U$, and let $S\subseteq U$ be the level set corresponding to $f = c$ and containing $\mathbf{a}$. Then $\nabla f(\mathbf{a})$, is a normal vector to $S$ at $\mathbf{a}$.

<details>
<summary>Proof</summary>

Suppose that $\mathbf{v}$ is a vector tangent to $S$ at $\mathbf{a}$ and that $\alpha:I\subseteq \R \to S$ is path that lies in $S$ passing through $\mathbf{a}$ with veloctiy $\mathbf{v}$. That is, $\alpha(t_0) = \mathbf{a}$ for some $t_0 \in I$ and $\alpha'(t_0) = \mathbf{v}$. Then $f(\alpha(t)) = c$ for all $t\in I$, so $(f\circ a)'(t) = c' = 0$. By the little chain rule

$$
  0 = (f\circ a)'(t) = \nabla f(\alpha(t))\circ \alpha'(t) = \nabla f(\mathbf{a})\cdot\mathbf{v} = \| \nabla f(\mathbf{a}) \| \cdot \| \mathbf{v} \| \cos\theta
$$

where $\theta$ is the angle between $\nabla f(\mathbf{a})$ and $\mathbf{v}$. It follows that $\theta = 0$ and thus $\nabla f(\mathbf{a})$ is orthogonal (perpendicular) to $\mathbf{v}$ and hence normal to $S$ at $\mathbf{a}$.
</details>
</MathBox>

### Higher order partial derivatives

<MathBox title='Equality of mixed partials' boxType='theorem'>
If $C^2 \ni f: U\subseteq\R^n \to\R$ is a scalar function, whose second-order partial derivatives are continuous on $U$, then all its mixed partial derivatives are equal, i.e.

$$
  \frac{\partial^2}{\partial x_i \partial x_j} = \frac{\partial^2}{\partial x_i \partial x_j}
$$

at all points poinst of $U$ and for all $i,j=1,\dots,n$.

<details>
<summary>Proof</summary>

Without loss of generality, we can assume that $f: U\subseteq\R^2 \to\R$, since the second-order mixed partials treat all but two variables as fixed. Suppose that $f_{xy} = \frac{\partial f^2}{\partial x \partial y}$ and $f_{yx} \frac{\partial f^2}{\partial x \partial y}$ exists in a neighbourhood of $\mathbf{a} = (a,b) \in U$ and are continuous at $\mathbf{a}$.

Let $R$ be the rectangle whose vertices are
- $\mathbf{a} = (a,b)$
- $\mathbf{u} = (a+h, b)$
- $\mathbf{v} = (a + h, b + k)$
- $\mathbf{w} = (a, b + k)$

Define

$$
\begin{align*}
  \Delta(h,k) = f(a+h, b+k) - f(a, b+k) - f(a+h, b) + f(a, b)
\end{align*}
$$

First, we show that $\lim_{(h,k)\to(0,0)} \frac{\Delta(h,k)}{hk} = \frac{\partial^2 f}{\partial y \partial x}(a,b)$. Applying the mean value theorem on $g(x) = f(x, b+k) - f(x,b)$ we get for $c\in (a,a+h)$

$$
\begin{align*}
  g(a+h) - g(a) =& g'(c)\cdot h  \\
  \Delta(h,k) =& \left[\frac{\partial f}{\partial x} (c, b+k) - \frac{\partial f}{\partial x}(c,b) \right]h
\end{align*}
$$

Applying the mean value theorem on $G(y) = \frac{\partial f}{\partial x}(c,y)$ we get for $d\in(b, b+k)$

$$
\begin{align*}
  G(b + k) - G(b) =& G'(d) \cdot k \\
  \frac{\partial f}{\partial x}(c,b+k) - \frac{\partial f}{\partial x}(c,b) =& \frac{\partial^2 f}{\partial y \partial x}(c,d)\cdot k
\end{align*}
$$

Combining the expressions we get

$$
  \Delta(h,k) = \frac{\partial^2 f}{\partial y\partial x}(c,d)\cdot hk
$$

Since $f_{yx}$ is continuous at $\mathbf{a}$, it follows that $f_{yx}(c,d) \xrightarrow{(h,k)\to(0,0)} f_{yx}(a,b)$, thus

$$
  \lim_{(h,k)\to(0,0)} \frac{\Delta(h,k)}{hk} = \frac{\partial^2 f}{\partial y \partial x}(a, b)
$$

Next, we show that $\lim_{(h,k)\to(0,0)} \frac{\Delta(h,k)}{hk} = \frac{\partial^2 f}{\partial x \partial y}(a,b)$. Applying the mean value theorem on $\gamma(y) = f(a+h, y) - f(a,y)$ we get for $e\in (b,b+k)$

$$
\begin{align*}
  \gamma(b+k) - \gamma(b) =& \gamma'(e)\cdot k \\
  \Delta(h,k) =& \left[\frac{\partial f}{\partial y}(a+h, e) -  \frac{\partial f}{\partial y}(a, e)\right]k
\end{align*}
$$

Applying the mean value theorem on $\Gamma(x) = \frac{\partial f}{\partial y}(x,e)$ we get for $f\in (a, a+h)$

$$
\begin{align*}
  \Gamma(a + h) - \Gamma(a) =& \Gamma'(f)\cdot h \\
  \frac{\partial f}{\partial y}(a+h,e) - \frac{\partial f}{\partial y}(a,e) =& \frac{\partial^2 f}{\partial y \partial x}(f,e)\cdot h
\end{align*}
$$

Combining the expressions, we get

$$
  \Delta(h,k) = \frac{\partial^2 f}{\partial x \partial y} (f,e)\cdot hk
$$

Since $f_{xy}$ is continuous at $\mathbf{a}$, it follows that $f_{xy}(e,f) \xrightarrow{(h,k)\to(0,0)} f_{xy}(a,b)$, thus

$$
  \lim_{(h,k)\to(0,0)} \frac{\Delta(h,k)}{hk} = \frac{\partial^2 f}{\partial x \partial y}(a, b)
$$
</details>
</MathBox>

### Extremal values

<MathBox title='Extrema' boxType='definition'>
Let $U\subseteq\R^n$ be an open sut. A function $f:U \to\R$ has a *local maximum* at a point $\mathbf{a}\in U$ if there is an open ball $B_r(\mathbf{a})$ centered at $\mathbf{a}$ such that $f(\mathbf{a}) \geq f(\mathbf{x})$ for all $x\in B_r(\mathbf{a})$. It has a *local minimum* at $\mathbf{a}$ if instead $f(\mathbf{a}) \leq f(\mathbf{x})$ for all $\mathbf{x}\in B_r(\mathbf{a})$. It has a *global maximum/minimum* at $\mathbf{a}$ if the associated inequalities are true for all $\mathbf{x}\in U$.
</MathBox>

<MathBox title='' boxType='proposition'>
Suppose that $f:U\subseteq\R^n \to\R$ has a local extrema at $\mathbf{a}\in U$. If $f$ is differentiable at $\mathbf{a}$, then $\nabla f(\mathbf{a}) = \mathbf{0}$, i.e. $\frac{\partial f}{\partial x_i}(\mathbf{a}) = 0$ for all $i$.

<details>
<summary>Proof</summary>

Suppose that $f$ has a local extrema at $\mathbf{a} = (a_i)_{i=1}^n$. Define a function $g:\R \to\R$ by

$$
  g(x^i) = f(a_1,\dots,x_i,\dots,a_n)
$$

in which all coordinates except $x_i$ of $f$ are fixed. Then $g$ has a local extrema at $x_i = a_i$ such that $g'(a_i) = \frac{\partial f}{\partial x_i}(\mathbf{a}) = 0$. Repeating the process for all $i=1,\dots,n$ shows that $\nabla f(\mathbf{a}) = \mathbf{0}$.
</details>
</MathBox>

<MathBox title='Hessian matrix' boxType='definition'>
Suppose that $f:U\subseteq\R^n \to\R$ is second-order differentiable at $\mathbf{a}\in U$. The *Hessian matrix* of $f$ at $\mathbf{a}$ is an $n\times n$ matrix of all the second-order partial derivatives of $f$ at $\mathbf{a}$

$$
  H_f(\mathbf{a}) = \left[ \frac{\partial^2 f}{\partial x^i \partial x^j} \right]_{i,j=1}^n
$$

Since all the mixed partial are equal, the Hessian matrix is symmetric and thus have real eigenvalues.
</MathBox>

<MathBox title='Second-order approximation' boxType='proposition'>
Suppose that $f:U\subseteq\R^n \to\R$ has continuous second-order partial derivatives in an open ball $B_r(\mathbf{a})$ centered at $\mathbf{a}\in U$. 

1. For any $\mathbf{y}\in U$ with $\|\mathbf{y}\|$ there is $c\in(0,1)$ such that

$$
  f(\mathbf{a} + \mathbf{y}) = f(\mathbf{a}) + \nabla f(\mathbf{a}) \cdot \mathbf{y} + \frac{1}{2}(H_f(\mathbf{a} + c\mathbf{y})\mathbf{y})\cdot\mathbf{y}
$$

2. There is a function $\epsilon: U\to\R$ such that $\lim_{\mathbf{y}\to\mathbf{0}} \varepsilon(\mathbf{y}) = 0$ and

$$
  f(\mathbf{a} + \mathbf{y}) = f(\mathbf{a}) + \nabla f(\mathbf{a})\cdot\mathbf{y} + \frac{1}{2}(H_f(\mathbf{a})\mathbf{y})\cdot\mathbf{y} + \epsilon(\mathbf{y})|\mathbf{y}|^2
$$

for all sufficiently small $\mathbf{y}\in U$.

<details>
<summary>Proof</summary>

**(1):** Define a function $g:\R\to\R$ by $g(t) = f(\mathbf{a} + t\mathbf{y})$. Applying the chain rule twice we get

$$
\begin{align*}
  g'(t) =& \sum_{i=1}^n \frac{\partial f}{\partial x^i}(\mathbf{a} + t\mathbf{y})y_i = \nabla f(\mathbf{a} + t\mathbf{y})\cdot\mathbf{y} \\
  g''(t) =& \sum_{i=1}^n \sum_{j=1}^n \frac{\partial^2 f}{\partial x_j \partial x_i}(\mathbf{a} + t\mathbf{v})y_i y_j = H_f(\mathbf{a} + t\mathbf{y})\cdot\mathbf{y}
\end{align*}
$$

By Taylor's theorem there is $c\in(0,1)$ such that $g(1) = g(0) + g'(0) + \frac{1}{2}g''(c)$. Substituting $g(t) = f(\mathbf{a} + t\mathbf{v})$ we get

$$
  f(\mathbf{a} + \mathbf{y}) = f(\mathbf{a}) + \nabla f(\mathbf{a}) \cdot \mathbf{y} + \frac{1}{2}(H_f(\mathbf{a} + c\mathbf{y})\mathbf{y})\cdot\mathbf{y}
$$

**(2):** Adding and subtracting $\frac{1}{2}(H_f(\mathbf{a})\mathbf{y}\cdot\mathbf{y}$ on the right side of the second-order approximation, we get

$$
\begin{align*}
  f(\mathbf{a} + \mathbf{y}) =& f(\mathbf{a}) + \nabla f(\mathbf{a})\cdot\mathbf{y} + \frac{1}{2}(H_f(\mathbf{a})\mathbf{y})\cdot\mathbf{y} \\
  &+ \frac{1}{2}[(H_f(\mathbf{a} + c\mathbf{y}) - H_f(\mathbf{a}))\cdot\mathbf{y}]\mathbf{y}
\end{align*}
$$

For ease of notation, we set $A(\mathbf{y}) = \frac{1}{2}(H_f(\mathbf{a} + c\mathbf{y}) - H_f(\mathbf{a}))$ such that

$$
  f(\mathbf{a} + \mathbf{y}) = f(\mathbf{a}) + \nabla f(\mathbf{a})\cdot\mathbf{y} + \frac{1}{2}(H_f(\mathbf{a})\mathbf{y})\cdot\mathbf{y} + (A(\mathbf{y})\mathbf{y})\cdot\mathbf{y}
$$

Comparing with the expression from **(1)** we see that $\epsilon$ must take the form

$$
  \epsilon(\mathbf{y}) = \frac{(A(\mathbf{y})\mathbf{y})\cdot\mathbf{y}}{\|\mathbf{y}\|^2 } = \left( A(\mathbf{y})\frac{\mathbf{y}}{\|\mathbf{y}\|} \right)\cdot\frac{\mathbf{y}}{\|\mathbf{y}\|}
$$

It remains to show that $\lim_{\mathbf{y}\to\mathbf{0}} |\epsilon(\mathbf{y})| = 0$

$$
\begin{align*}
  \lim_{\mathbf{y}\to\mathbf{0}} |\epsilon(\mathbf{y})| =& \lim_{\mathbf{y}\to\mathbf{0}} \left| \sum_{i,j}^m a_{ij}(\mathbf{y}) \frac{y_i}{\|\mathbf{y}\|}\cdot\frac{y_j}{\|\mathbf{y}\|} \right| \\
  \leq& \lim_{\mathbf{y}\to\mathbf{0}} |a_{ij}(\mathbf{y})| = 0
\end{align*}
$$

where we have used that $\frac{y_i}{\|\mathbf{y}\|}, \frac{y_j}{\|\mathbf{y}\|}\leq 1$ and that $\lim_{\mathbf{y}\to\mathbf{0}} a_{ij}(\mathbf{y}) = 0$.
</details>
</MathBox>

<MathBox title='Second derivative test' boxType='theorem'>
Suppose that $f:U\subseteq\R^n \to\R$ has a critical point at $\mathbf{a}\in U$ and that the second-order partial derivatives of $f$ are continuous in a neighbourhood of $\mathbf{a}$.
1. If all the eigenvalues $H_f(\mathbf{a})$ are positive eigenvalues, i.e. $H_f(\mathbf{a})$ is positive definite, then $f(\mathbf{a})$ is a local minimum.
2. If all the eigenvalues of $H_f(\mathbf{a})$ are (strictly) negative, i.e. $H_f(\mathbf{a})$ is negative definite, then $f(\mathbf{a})$ is a local maximum.
3. If $H_f(\mathbf{a})$ has both negative and positive eigenvalues, i.e. $H_f(\mathbf{a})$ is indefinite, then $f(\mathbf{a}$ is a saddle point.

The test is inconclusive if some of the eigenvalues of $H_f(\mathbf{a})$ are zero, while the remaining have the same sign.

<details>
<summary>Proof</summary>

Since $\mathbf{a}$ is a critical point, $\nabla f(\mathbf{a}) = \mathbf{0}$ such that the second-order approximation takes the form

$$
\begin{align*}
  f(\mathbf{a} + \mathbf{y}) =& \nabla f(\mathbf{a})\cdot\mathbf{y} + \frac{1}{2}(H_f(\mathbf{a})\mathbf{y})\cdot\mathbf{y} + \epsilon(\mathbf{y})\|\mathbf{y}\|^2 \\
  =& \frac{1}{2}(H_f(\mathbf{a})\mathbf{y})\cdot\mathbf{y} + \epsilon(\mathbf{y})\|\mathbf{y}\|^2
\end{align*}
$$

Note that $\lim_{\mathbf{y}\to\mathbf{0}} \epsilon(\mathbf{y}) = 0$.

**(1):** Let $\lambda_1 > 0$ be the minimal eigenvalue of $H_f(\mathbf{a})$. Then $(H_f(\mathbf{a})\mathbf{y})\cdot\mathbf{y} \geq \lambda_1 \|\mathbf{y}\|^2$ such that

$$
  f(\mathbf{a} + \mathbf{y}) \geq \left(\frac{1}{2}\lambda_1 + \epsilon(\mathbf{y}) \right) \|\mathbf{y}\|^2 \geq 0
$$

showing that $f(\mathbf{a})$ is a local minimum.

**(2):** Let $\lambda_1 < 0$ be the maximal eigenvalue of $H_f(\mathbf{a})$. Then $(H_f(\mathbf{a})\mathbf{y})\cdot\mathbf{y} \leq \lambda_1 \|\mathbf{y}\|^2$ such that

$$
  f(\mathbf{a} + \mathbf{y}) \leq \left(\frac{1}{2}\lambda_1 + \epsilon(\mathbf{y}) \right) \|\mathbf{y}\|^2 \leq 0
$$

showing that $f(\mathbf{a})$ is a local maximum.

**(3):** Let $\mathbf{y}$ be an eigenvector of $H_f(\mathbf{a})$ with a positive eigenvalue $\lambda$. In this case

$$
  f(\mathbf{a} + \mathbf{y}) = \left(\frac{1}{2}\lambda_1 + \epsilon(\mathbf{y}) \right) \|\mathbf{y}\|^2 \leq 0
$$

If $\mathbf{y}$ is sufficiently small, then $\frac{1}{2}\lambda_1 + \epsilon(\mathbf{y}) > 0$ such that $f(\mathbf{a}+\mathbf{y} > f(\mathbf{a})$. Thus, $f(\mathbf{a})$ is not a local maximum.

Let $\mathbf{y}$ be an eigenvector of $H_f(\mathbf{a})$ with a negative eigenvalue $\lambda$. Then $\frac{1}{2}\lambda_1 + \epsilon(\mathbf{y}) < 0$ such that $f(\mathbf{a}+\mathbf{y} < f(\mathbf{a})$. Thus, $f(\mathbf{b})$ is not a local minimum.
</details>
</MathBox>

<MathBox title='Second derivative test for two variables' boxType='corollary'>
Suppose that $f:U\subseteq\R^2 \to\R$ has a critical point at $\mathbf{a}\in U$ and that the second-order partial derivatives of $f$ are continuous in a neighbourhood of $\mathbf{a}$. 

- If $\det(H_f(\mathbf{a})) > 0$ and:
    1. if $\frac{\partial^2 f}{\partial x^2}(\mathbf{a}) > 0$, then $f$ has a local minimum at $\mathbf{a}$
    2. if $\frac{\partial^2 f}{\partial x^2}(\mathbf{a}) < 0$, then $f$ has a local maximum at $\mathbf{a}$
3. If $\det(H_f(\mathbf{a})) < 0$, then $f$ has a saddle point at $\mathbf{a}$

Generally, if $\det(H_f(\mathbf{a})) \neq 0$, then $\mathbf{a}$ is a *nondegenerate* critical point. If $\det(H_f(\mathbf{a})) = 0$, the test is inconclusive.

<details>
<summary>Proof</summary>

The Hessian matrix of $f$ at $\mathbf{a}$ has determinant

$$
\begin{align*}
  \det(H_f(\mathbf{a})) &= \begin{vmatrix} 
    \frac{\partial^2 f}{\partial x^2}(\mathbf{a}) & \frac{\partial^2 f}{\partial y \partial x}(\mathbf{a}) \\ 
    \frac{\partial^2 f}{\partial x \partial y}(\mathbf{a}) & \frac{\partial^2 f}{\partial y^2}(\mathbf{a}
  \end{vmatrix} \\
  =& \left(\frac{\partial^2 f}{\partial x^2}(\mathbf{a})\right)\left(\frac{\partial^2 f}{\partial y^2}(\mathbf{a})\right) - \left(\frac{\partial^2 f}{\partial x \partial y}(\mathbf{a})\right)\left(\frac{\partial^2 f}{\partial y \partial x}(\mathbf{a})\right) \\
  =& f_{xx}f_{yy} - f_{xy}^2
\end{align*}
$$

Let $\lambda_1$ and $\lambda_2$ be the two eigenvalues of $H_f(\mathbf{a})$. Then $\det(H_f(\mathbf{a})) = \lambda_1 \lambda_2$.

**(1):** If $\det(H_f(\mathbf{a})) > 0$, then $\lambda_1$ and $\lambda_2$ share the same sign. By the second derivative test, $f$ has either a local minimum or maximum at $\mathbf{a}$ in this case. Since $\frac{\partial^2 f}{\partial x^2}(\mathbf{a}) > 0$, then $f$ has a local minimum in $\mathbf{a}$ as a function of $x$ alone. Thus, $\mathbf{a}$ must be a local minimum.

**(2):** Since $\frac{\partial^2 f}{\partial x^2}(\mathbf{a}) < 0$, then $f$ has a local maximum in $\mathbf{a}$ as a function of $x$ alone. Thus, $\mathbf{a}$ must be a local maximum.

**(3):** If $\det(H_f(\mathbf{a})) < 0$, then $\lambda_1$ and $\lambda_2$ must have opposite signs. By the the general second derivative test, $\mathbf{a}$ is a saddle point.
</details>
</MathBox>

### Lagrange multipliers

<MathBox title='Lagrange multiplier method' boxType='theorem'>
Suppose that $f, g_1, \dots, g_k:U\subseteq\R^n \to\R$ for $k\in\N_+$ are continunously differentiable functions on an open subset $\R^n$. If $\mathbf{a}$ is a local extrema for $f$ on the subset

$$
  A = \Set{\mathbf{x}\in U | g_i(\mathbf{x}) = b_i \in\R, i=1,\dots,k}
$$

then either
1. $\nabla g_1(\mathbf{a}),\dots, \nabla g_k(\mathbf{a})$ are linearly dependent
2. there are constants $\lambda_1,\dots,\lambda_k$ such that
$$
  \nabla f(\mathbf{a}) = \sum_{i=1}^k \lambda_i \nabla g_i(\mathbf{a}) 
$$

<details>
<summary>Proof</summary>

Assume that $1 \geq k < n$. It suffices to show that if the gradients $(\nabla g_i(\mathbf{a}))_{i=1}^k$ are linearly independent for $\mathbf{a} = (a_i)_{i=1}^n$, there are constants $(\lambda_i)_{i=1}^k$ such that 

$$
  \nabla f(\mathbf{a}) = \sum_{i=1}^k \lambda_i \nabla g_i(\mathbf{a}) 
$$

For any constraint $g_i(\mathbf{x}) = b_i$ for $i=1,\dots,k$ we may define a function $\tilde{g}_i (\mathbf{x}) = g_i(\mathbf{x} - b$, giving the constraint $\tilde{g}_i(\mathbf{x}) = 0$. We can therefore without loss of generality assume that $b_i = 0$.

Defining $\mathbf{G}:\R^n \to\R^k$ by $\mathbf{G}(\mathbf{x}) = (g_i(\mathbf{x}))_{i=1}^k$, we can concisely write the constraint equations as $\mathbf{G}(\mathbf{x}) = \mathbf{0}$. Differentiating $\mathbf{G}$ we get the Jacobian $k\times n$ matrix $\mathbf{G}'(\mathbf{x}) = (\nabla g_i(\mathbf{x}))_{i=1}^k$. Since the gradients are linearly independent at $\mathbf{a}$, it follows by the rank theorem that $\mathbf{G}'(\mathbf{a})$ has $k$ independent columns. By reindexing if necessary, we may assume that the $k$ last columns are linearly independent. 

For ease of notation define

$$
\begin{align*}
  \mathbf{z} =& (x_1,\dots,x_m),\; m = n - k \\
  \mathbf{y} =& (x_{m+1},\dots,x_n)
\end{align*}
$$

In this notation, the Jacobian can be written as the block matrix

$$
\mathbf{G}'(\mathbf{a}) = \left[\begin{array}{c|c}
  \frac{\partial\mathbf{G}}{\partial\mathbf{z}}(\mathbf{a}) & \frac{\partial\mathbf{G}}{\partial\mathbf{y}}(\mathbf{a})
\end{array}\right]
$$

Because $\frac{\partial\mathbf{G}}{\partial\mathbf{y}}(\mathbf{a})$ consists of linearly independent columns, it follows that it is invertible. By the implicit function theorem, there is a differentiable function $\boldsymbol{\Phi}$ defined in a neighbourhood of $\tilde{\mathbf{z}} = (a_i)_{i=1}^m$ such that

$$
  \mathbf{G}(\tilde{\mathbf{z}}, \boldsymbol{\Phi}(\tilde{\mathbf{z}})) = \mathbf{0}
$$

and $\boldsymbol{\Phi}(\tilde{\mathbf{z}}) = \tilde{\mathbf{y}} = (a_j)_{j=m+1}^n$. We also know that

$$
  \boldsymbol{\Phi}'(\tilde{\mathbf{z}}) = -\left( \frac{\partial\mathbf{G}}{\partial\mathbf{y}}(\tilde{\mathbf{y}}) \right)^{-1} \frac{\partial\mathbf{G}}{\partial\mathbf{z}}(\tilde{\mathbf{x}})
$$

From the construction above and the assumptions of the theorem, it follows that the function $h:\R^m \to\R$

$$
  h(\mathbf{z}) = f(\mathbf{z}, \boldsymbol{\Phi}(\mathbf{z}))
$$

has a local extrema at $\tilde{\mathbf{z}}$ with $\nabla h(\tilde{\mathbf{z}}) = \mathbf{0}$. In order to apply the chain rule to find $\nabla h$, we define a chain function $\boldsymbol{\Psi}:\R^m \to\R^k$

$$
  \boldsymbol{\Psi}(\mathbf{z}) = \begin{bmatrix} \mathbf{z} \\ \boldsymbol{\Phi}(\mathbf{z}) \end{bmatrix}
$$

with Jacobian matrix 

$$
  \boldsymbol{\Psi}'(\mathbf{z}) = \begin{bmatrix} \mathbf{I}_{n} \\ \boldsymbol{\Phi}'(\mathbf{z}) \end{bmatrix}
$$

Applying the chain rule we get

$$
  \nabla h(\mathbf{z}) = \nabla f(\mathbf{z}, \boldsymbol{\Phi}(\mathbf{z}))\boldsymbol{\Psi}'(\mathbf{z})
$$

Rewrite $\nabla f(\mathbf{z},\mathbf{y})$ as

$$
\begin{align*}
  \nabla_\mathbf{z} f(\mathbf{z},\mathbf{y}) =& \left( \frac{\partial f}{\partial z_1}(\mathbf{z}, \mathbf{y}),\dots, \frac{\partial f}{\partial z_m}(\mathbf{z}, \mathbf{y}), \frac{\partial f}{\partial y_1}(\mathbf{z}, \mathbf{y}),\dots,\frac{\partial f}{\partial y_k}(\mathbf{z}, \mathbf{y}) \right) \\
  =& (\nabla_\mathbf{z}f(\mathbf{z}, \mathbf{y}), \nabla_\mathbf{y}f(\mathbf{z}, \mathbf{y}))
\end{align*}
$$

where $\nabla_\mathbf{z}f(\mathbf{z}, \mathbf{y}) = \left( \frac{\partial f}{\partial z_i}(\mathbf{z}, \mathbf{y}) \right)_{i=1}^m$ is the gradient of $f$ as a function of $\mathbf{z}$ and  $\nabla_\mathbf{y}f(\mathbf{z}, \mathbf{y}) = \left( \frac{\partial f}{\partial y_j}(\mathbf{z}, \mathbf{y}) \right)_{j=1}^k$ is the gradient of $f$ as a function of $\mathbf{y}$. This allows us to write $\nabla h (\mathbf{z})$ as

$$
  \nabla h(\mathbf{z}) = \nabla_\mathbf{z} f(\mathbf{z}, \boldsymbol{\Phi(\mathbf{z})})\mathbf{I}_m + \nabla_\mathbf{y} f(\mathbf{z}, \boldsymbol{\Phi}(\mathbf{z}))\boldsymbol{\Psi}'(\mathbf{z})
$$

Since $\nabla h(\tilde{\mathbf{z}}) = \mathbf{0}$ and noting that $\mathbf{a} = (\tilde{\mathbf{z}}, \boldsymbol{\Phi}(\tilde{\mathbf{z}}))$

$$
\begin{align*}
  \mathbf{0} =& \nabla_\mathbf{z} f(\tilde{\mathbf{z}}, \boldsymbol{\Phi(\tilde{\mathbf{z}})}) + \nabla_\mathbf{y} f(\tilde{\mathbf{z}}, \boldsymbol{\Phi}(\tilde{\mathbf{z}}))\boldsymbol{\Psi}'(\tilde{\mathbf{z}}) \\
  =& \nabla_\mathbf{z} f(\mathbf{a}) + \nabla_\mathbf{y} f(\mathbf{a})\left( \frac{\partial\mathbf{G}}{\partial\mathbf{y}}(\mathbf{a}) \right)^{-1} \frac{\partial\mathbf{G}}{\partial\mathbf{z}}(\mathbf{a})
\end{align*}
$$

Assuming that $\nabla_\mathbf{y} f(\mathbf{a})\left( \frac{\partial\mathbf{G}}{\partial\mathbf{y}}(\mathbf{a}) \right)^{-1} = \boldsymbol{\Lambda} = (\lambda_i)_{i=1}^k$, the equation takes the form

$$
\begin{align*}
  \nabla_\mathbf{z} f(\mathbf{a}) =& \boldsymbol{\Lambda} \frac{\partial\mathbf{G}}{\partial\mathbf{z}}(\mathbf{a}) \\
  =& \sum_{i=1}^k \lambda_i \nabla_\mathbf{z} g_i(\mathbf{a})
\end{align*}
$$

It remains to show that $\nabla_\mathbf{y}f(\mathbf{a}) \sum_{i=1}^k \lambda_i \nabla_\mathbf{y} g_i(\mathbf{a})$, which is equivalent to

$$
  \nabla_\mathbf{y} f(\mathbf{a}) = \boldsymbol{\Lambda} \frac{\partial\mathbf{G}}{\partial\mathbf{y}}(\mathbf{a}) 
$$

Since $\boldsymbol{\Lambda} = \nabla_\mathbf{y} f(\mathbf{a}) \left( \frac{\partial\mathbf{G}}{\partial\mathbf{y}}(\mathbf{a}) \right)^{-1}$ we get

$$
  \boldsymbol{\Lambda} \frac{\partial\mathbf{G}}{\partial\mathbf{y}}(\mathbf{a}) =  \nabla_\mathbf{y} f(\mathbf{a}) \left( \frac{\partial\mathbf{G}}{\partial\mathbf{y}}(\mathbf{a}) \right)^{-1} \frac{\partial\mathbf{G}}{\partial\mathbf{y}}(\mathbf{a}) = \nabla_\mathbf{y} f(\mathbf{a}) 
$$

</details>
</MathBox>


<MathBox title='Lagrange multiplier method with single constraint' boxType='corollary'>


<details>
<summary>Proof</summary>


</details>
</MathBox>


### Gradient descent

The gradient of a scalar function points in the direction of its steepest ascent. This can be exploited to find local minima of the function by iteratively travelling in the opposite direction of the gradient.

Assume that we want to find a local minimal point of a function $f:U\subseteq\R^n \to\R$ starting from $\mathbf{x}_0\in U$. Since the gradient $\nabla f(\mathbf{x}_0)$ gives the direction of the steepest ascent, we want to travel in the opposite direction along the line

$$
  \mathbf{r}(t) = \mathbf{x}_0 - \nabla f(\mathbf{x}_0)t
$$

Applying the chain rule to the composition $g = f \circ \mathbf{r}$ gives

$$
\begin{align*}
  g'(t) =& \nabla f(\mathbf{r}(t))\cdot \mathbf{r}'(t) \\
  \nabla f(\mathbf{x}_0 - \nabla f(\mathbf{x}_0)t)\cdot\nabla f(\mathbf{x}_0)
\end{align*}
$$

We are looking for a $t_0 > 0$ such that $g(t_0) = 0$, or

$$
\nabla f(\mathbf{x}_0 - \nabla f(\mathbf{x}_0)t_0)\cdot\nabla f(\mathbf{x}_0) = 0
$$

Iterating the process, we get the recursive relation

$$
  \mathbf{x}_{n+1} = \mathbf{x}_n - \nabla f(\mathbf{x}_n)t_n
$$

## Integral calculus

### Gradient theorem

The gradient theorem is also known as the fundamental theorem of calculus for line integrals

$$
  \int_{C[a, b]} \nabla f \cdot \mathrm{d}\mathbf{r} = f(b) - f(a)
$$

### Partial integration

$$
\begin{gather*}
\begin{aligned}
  \nabla \cdot \left(f \mathbf{A} \right) &= f\left( \nabla \cdot \mathbf{A} \right) + \mathbf{A} \cdot \left( \nabla f \right) \\
  \int \nabla \cdot \left(f \mathbf{A} \right) \mathrm{d}\tau &= \int f\left( \nabla \cdot \mathbf{A} \right) \mathrm{d}\tau + \int \mathbf{A} \cdot \left( \nabla f \right) \mathrm{d}\tau = \oint f\mathbf{A} \cdot \mathrm{d}\mathbf{a}
\end{aligned} \\
  \int_\mathcal{V} \nabla \cdot \left(f \mathbf{A} \right) \mathrm{d}\tau = -\int_\mathcal{V} \mathbf{A} \cdot \left( \nabla f \right) \mathrm{d}\tau + \oint_\mathcal{S} f\mathbf{A} \cdot \mathrm{d}\mathbf{a}
\end{gather*}
$$

# Vector fields

## Differential calculus

### Divergence

$$
  \nabla\cdot\mathbf{v} = \frac{\partial v^i}{\partial x_i} \equiv \partial_i v^i
$$

The divergence of the curl of any vector field $\mathbf{A}$ always vanishes.

$$
\begin{align*}
  \nabla\cdot \left( \nabla \times \mathbf{A} \right) = \varepsilon_{ijk} \partial_i \partial_j A_k = -\varepsilon_{ikj} \partial_i \partial_j A_k = -\varepsilon_{ikj} \partial_j \partial_i A_k = -\varepsilon_{ijk} \partial_j \partial_i A_k = 0
\end{align*}
$$

Product rule

$$
  \nabla\cdot \left(\psi \mathbf{f}  \right) = \nabla\psi \cdot \mathbf{f} + \psi \nabla\cdot\mathbf{f}
$$

### Curl

$$
\begin{gather*}
  \nabla \times \mathbf{v} = \begin{vmatrix} \hat{\mathbf{x}} & \hat{\mathbf{y}} & \hat{\mathbf{z}} \\
  \frac{\partial}{\partial x} & \frac{\partial}{\partial y} & \frac{\partial}{\partial z} \\
  v^x & v^y & v^z \end{vmatrix} = \varepsilon^{ij}_{k}\mathbf{e}_i \frac{\partial v^k}{\partial x_j} \equiv \varepsilon^{ij}_{k}\mathbf{e}_i\partial_j v^k
\end{gather*}
$$

The curl of the gradient of any scalar field $\phi$ is always the zero vector field

$$
  \left[\nabla \times (\nabla \phi)\right]^i = \varepsilon_{ijk}\partial_j \partial_k \phi = -\varepsilon_{ikj}\partial_j \partial_k \phi = -\varepsilon_{ikj}\partial_k \partial_j \phi = -\varepsilon_{ijk}\partial_j \partial_k \phi = 0
$$

Curl of curl

$$
\begin{align*}
  \left[ \nabla \times \left( \nabla \times \mathbf{v} \right) \right]^i &= \varepsilon_{ijk}\partial_j \varepsilon_{kmn}\partial_m v^n \\
  &= \varepsilon_{kij}\varepsilon_{kmn}\partial_i \partial_m v^n \\
  &= \left( \delta_{im}\delta_{jn} - \delta_{in}\delta_{jm} \right)\partial_j \partial_m v^n \\
  &= \partial_j \partial_i v^j - \partial_j \partial_j v^i \\
  &= \partial_i \partial_j v^j - \partial_j \partial_j v^i \\
  &= \left[ \nabla \left(\nabla \cdot \mathbf{v}\right) - \nabla^2 \mathbf{v} \right]^i
\end{align*}
$$

### Integral calculus

### Green's theorem

$$
  \oint_C (P \mathrm{d}x + Q \mathrm{d}y) = \iint_D \left( \frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} \right)\mathrm{d}x \mathrm{d}y  
$$

### Stokes' theorem (curl)

Let $\mathbf{F} = \left[P(x, y, z), Q(x, y, z), R(x, y, z) \right]$ be a continuously differentiable vector field defined in a region $\Sigma \in \R^3$ with boundary $\partial\Sigma$, then

$$
\begin{align*}
  \iint_\Sigma \left(\nabla \times \mathbf{F}\right) \cdot \mathrm{d}\mathbf{f} &= \oint_{\partial\Sigma} \mathbf{F} \cdot \mathrm{d}\mathbf{l} \\
  \iint_\Sigma \left[ \left( \frac{\partial R}{\partial y} - \frac{\partial Q}{\partial z} \right) \mathrm{d}y\mathrm{d}z + \left( \frac{\partial P}{\partial z} - \frac{\partial R}{\partial x} \right) \mathrm{d}z\mathrm{d}x + \left( \frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} \right) \mathrm{d}x\mathrm{d}y \right] &= \oint_{\partial\Sigma} \left( P \mathrm{d}x + Q \mathrm{d}y + R P \mathrm{d}z \right)
\end{align*}
$$

### Gauss' theorem (flux)

Let $\mathbf{F}$ be a continuously differentiable vector field defined on a compact subset $V \in \R^n$ with the smooth boundary $\S$, then

$$
  \iiint_V \left(\nabla \cdot \mathbf{F} \right) \mathrm{d}V = \oiint_S \left( \mathbf{F} \cdot \mathbf{\hat{n}} \right) \mathrm{d}S
$$


#### Divergence of inverse-square functions

The divergence of an inverse square function vanishes
$$
  \nabla \cdot \left( \frac{\hat{\mathbf{r}}}{r^2} \right) = \frac{1}{r^2}\frac{\partial}{\partial r} \left(r^2 \frac{1}{r^2} \right) = 0
$$

The surface integral over a sphere of radius $R$ is however

$$
  \oint \left( \frac{\hat{\mathbf{r}}}{R^2} \right) \cdot \left( R^2 \sin\theta \;\mathrm{d}\theta \;\mathrm{d}\phi \,\hat{\mathbf{r}} \right) = \left( \int_0^\pi \sin\theta \;\mathrm{d}\theta \right) \left( \int_0^{2\pi} \mathrm{d}\phi \right) = 4\pi
$$

By the divergence theorem

$$
    \int_V \nabla \cdot \left( \frac{\hat{\mathbf{r}}}{r^2} \right) \mathrm{d}V = \oint \left( \frac{\hat{\mathbf{r}}}{R^2} \right) \cdot \mathrm{d}\mathbf{a} = 4\pi \\
    \implies \nabla \cdot \left( \frac{\hat{\mathbf{r}}}{r^2} \right) = 4\pi \delta^3 \left( \mathbf{r} \right)
$$

### Green's first identity

Green's first identity is derived from the divergence theorem applied to the vector field $\mathbf{F} = \psi \nabla \phi$. Using the divergence product rule gives

$$
  \nabla\cdot\left(\psi\nabla\phi \right) = \nabla\psi \cdot \nabla \phi + \psi \nabla \cdot \left(\nabla \phi\right) = \nabla\psi \cdot \nabla \phi + \psi \nabla^2 \phi
$$

Applying the divergence theorem gives

$$
\begin{align*}
  \int_\mathcal{V} \nabla \cdot \left( \psi \nabla \phi \right)\mathrm{d}V &= \int_\mathcal{V} \left( \nabla\psi \cdot \nabla \phi + \psi \nabla^2 \phi \right) \mathrm{d}V \\
  &= \oint_\mathcal{S} \psi \left(\nabla\phi \cdot \mathbf{n} \right) \mathrm{d}S = \oint_\mathcal{S} \psi \nabla \phi \cdot \mathrm{d}\mathbf{S}
\end{align*}
$$

### Green's second identity

If $\phi$ and $\psi$ are both twice continuously differentiable on $U \in \R^3$, and $\epsilon$ is once continuously differentiable, one may choose the vector field $\mathbf{F} = \psi \epsilon\nabla \phi - \phi\epsilon\nabla\psi$ to obtain

$$
  \int_U \left[ \psi \nabla \cdot \left( \epsilon\nabla\phi \right) - \phi \nabla \cdot \left( \epsilon\nabla\psi \right) \right] \mathrm{d}V = \oint_{\partial U} \epsilon \left( \psi \frac{\partial\phi}{\partial\mathbf{n}} - \phi \frac{\partial\psi}{\partial\mathbf{n}} \right) \mathrm{d}S
$$

In the special case of $\epsilon = 1$, the identity reduces to

$$
  \int_U \left( \psi \nabla^2 \phi - \phi \nabla^2 \psi \right)\mathrm{d}V = \oint_{\partial U} \left( \psi \frac{\partial\phi}{\partial\mathbf{n}} - \phi \frac{\partial\psi}{\partial\mathbf{n}} \right) \mathrm{d}S = \oint_{\partial U} \left( \psi \nabla \phi - \phi\nabla\psi \right) \mathrm{d}\mathbf{S}
$$

# Vector-valued functions

## Jacobian matrix (basis transform)

The Jacobian of a vector-valued function $\mathbf{f} : \mathbf{R}^n \mapsto \mathbf{R}^m$, is the matrix of all its first-order partial derivatives with entries $J_{ij} = \frac{\partial f_i}{\partial x_j}$, or explicitly

$$
  J = \begin{bmatrix} \frac{\partial \mathbf{f}}{\partial x_i} & \dots & \frac{\partial \mathbf{f}}{\partial x_n} \end{bmatrix} = \begin{bmatrix} \nabla^T f_1 \\ \dots \\ \nabla^T f_m \end{bmatrix} = \begin{bmatrix} \frac{\partial f_1}{\partial x_1} & \dots & \frac{\partial f_1}{\partial x_n} \\ \vdots & \ddots & \vdots \\ \frac{\partial f_m}{\partial x_1} & \dots & \frac{\partial f_m}{\partial x_n} \end{bmatrix}
$$

### Polar-Cartesian transformation

The transformation from polar coordinates $(r, \theta)$ to Cartesian coordinates $(x, y)$ is given by the function $\mathbf{f} : \R \times [0, 2\pi) \mapsto \R^2$ with components

$$
  \mathbf{f}(r, \theta) = \left[ r \cos{\theta}, r \sin{\theta} \right] \equiv (x, y)
$$

The Jacobian becomes

$$
  J = \begin{bmatrix} \frac{\partial x}{\partial r} & \frac{\partial y}{\partial r} \\ \frac{\partial x}{\partial \theta} & \frac{\partial y}{\partial \theta} \end{bmatrix} = \begin{bmatrix} \cos{\theta} & \sin{\theta} \\ - r \sin{\theta} & r \cos{\theta} \end{bmatrix}
$$

representing a linear transform of the Cartesian coordinate basis vectors, $J\mathbf{e}_{xy} = \mathbf{e}_{r\theta}$, written out

$$
\begin{align*}
  \begin{bmatrix} \mathbf{e}_r \\ \mathbf{e}_\theta \end{bmatrix} \equiv \begin{bmatrix} \frac{\partial \mathbf{R}}{\partial r} \\ \frac{\partial \mathbf{R}}{\partial \theta} \end{bmatrix} &= \begin{bmatrix} \frac{\partial x}{\partial r} & \frac{\partial y}{\partial r} \\ \frac{\partial x}{\partial \theta} & \frac{\partial y}{\partial \theta} \end{bmatrix} \cdot \begin{bmatrix} \frac{\partial \mathbf{R}}{\partial x} \\ \frac{\partial \mathbf{R}}{\partial y} \end{bmatrix} = \begin{bmatrix} \frac{\partial x}{\partial r} & \frac{\partial y}{\partial r} \\ \frac{\partial x}{\partial \theta} & \frac{\partial y}{\partial \theta} \end{bmatrix} \cdot \begin{bmatrix} \mathbf{e}_x \\ \mathbf{e}_y \end{bmatrix} \\
  &= \begin{bmatrix} \frac{\partial x}{\partial r}\frac{\partial \mathbf{R}}{\partial x} + \frac{\partial y}{\partial r}\frac{\partial \mathbf{R}}{\partial y} \\ \frac{\partial x}{\partial \theta}\frac{\partial \mathbf{R}}{\partial x} + \frac{\partial y}{\partial \theta}\frac{\partial \mathbf{R}}{\partial y} \end{bmatrix} \\
  &\equiv \begin{bmatrix} \frac{\partial x}{\partial r}\mathbf{e}_x + \frac{\partial y}{\partial r}\mathbf{e}_y \\ \frac{\partial x}{\partial \theta}\mathbf{e}_x + \frac{\partial y}{\partial \theta} \mathbf{e}_y \end{bmatrix}
\end{align*}
$$

### Cartesian-polar transformation

The transformation from Cartesian coordinates $(x, y)$ to polar coordinates $(r, \theta)$ is given by the function $\mathbf{f} : \R^2 \mapsto \R^+ \times [0, 2\pi)$ with components

$$
  \mathbf{f}(x, y) = \left[\sqrt{x^2 + y^2}, \arctan{\left( \frac{y}{x} \right)} \right] \equiv (r, \theta)
$$

The Jacobian becomes

$$
  J = \begin{bmatrix} \frac{\partial r}{\partial x} & \frac{\partial r}{\partial y} \\ \frac{\partial \theta}{\partial x} & \frac{\partial \theta}{\partial y} \end{bmatrix} = \begin{bmatrix} \frac{x}{\sqrt{x^2 + y^2}} & -\frac{y}{x^2 + y^2} \\ \frac{y}{\sqrt{x^2 + y^2}} & \frac{x}{x^2 + y^2} \end{bmatrix} = \begin{bmatrix} \frac{x}{r} & -\frac{y}{r^2} \\ \frac{y}{r} & \frac{x}{r^2} \end{bmatrix} 
$$

representing a linear transform of the polar coordinate basis vectors

$$
\begin{align*}
  \begin{bmatrix} \mathbf{e}_x \\ \mathbf{e}_y \end{bmatrix} \equiv \begin{bmatrix} \frac{\partial \mathbf{R}}{\partial x} \\ \frac{\partial \mathbf{R}}{\partial y} \end{bmatrix} &= \begin{bmatrix} \frac{\partial r}{\partial x} & \frac{\partial r}{\partial y} \\ \frac{\partial \theta}{\partial x} & \frac{\partial \theta}{\partial y} \end{bmatrix} \cdot \begin{bmatrix} \frac{\partial \mathbf{R}}{\partial r} \\ \frac{\partial \mathbf{R}}{\partial \theta} \end{bmatrix} = \begin{bmatrix} \frac{\partial r}{\partial x} & \frac{\partial r}{\partial y} \\ \frac{\partial \theta}{\partial x} & \frac{\partial \theta}{\partial y} \end{bmatrix} \cdot \begin{bmatrix} \mathbf{e}_r \\ \mathbf{e}_\theta \end{bmatrix} \\
  &= \begin{bmatrix} \frac{\partial r}{\partial x}\frac{\partial \mathbf{R}}{\partial r} + \frac{\partial \theta}{\partial x}\frac{\partial \mathbf{R}}{\partial \theta} \\ \frac{\partial r}{\partial y}\frac{\partial \mathbf{R}}{\partial r} + \frac{\partial \theta}{\partial y}\frac{\partial \mathbf{R}}{\partial \theta} \end{bmatrix} \\
  &\equiv \begin{bmatrix} \frac{\partial r}{\partial x}\mathbf{e}_r + \frac{\partial \theta}{\partial x}\mathbf{e}_\theta \\ \frac{\partial r}{\partial y}\mathbf{e}_r + \frac{\partial \theta}{\partial y} \mathbf{e}_\theta \end{bmatrix}
\end{align*}
$$

### Differential form (covector field)

A differential form $\mathrm{d}f$ maps a scalar field $f$ to a covector field (level sets). The covector field is a dual space (linear functionals) equipped with addition and scalar multiplication

$$
\begin{gather*}
  \mathrm{d}f(\mathbf{u} + \mathbf{v}) = \mathrm{d}f(\mathbf{u}) + \mathrm{d}f(\mathbf{v}) \\
  \mathrm{d}f(c\mathbf{u}) = c \left[ \mathrm{d}f(\mathbf{u}) \right]
\end{gather*}
$$

The linear functional $\mathrm{d}f(\mathbf{v})$ can geometrically be interpreted as a directional derivative.

$$
    \mathrm{d}f(\mathbf{v}) \equiv \nabla_\mathbf{v} = \mathbf{v} \cdot \nabla f
$$

The dual basis covectors $\epsilon^i \equiv \mathrm{d}x^i$ are formed by the bi-orthogonality property

$$
  \epsilon^i \left( \mathbf{e}_j \right) \equiv \mathrm{d}x^i \left( \frac{\partial}{\partial x^j} \right) = \frac{\partial x^i}{\partial x^j} = \delta_j^i
$$

The dual basis covectors transform covariantly

$$
  \mathrm{d}\tilde{x}^i = \frac{\partial \tilde{x}^i}{\partial x} \mathrm{d}x^j = \left( J^{-1} \right)_i^j \mathrm{d}x^j
$$

A differential form can be expressed as a linear combination of the basis covectors $\mathrm{d}x^i$. The derivative of a curve tangent vector is given by

$$
\begin{align*}
  \mathrm{d} f \left( \frac{\mathrm{d}}{\mathrm{d}\lambda} \right) &= \frac{\mathrm{d}f}{\mathrm{d}\lambda} \\
  &= \frac{\partial f}{\partial x^i} \frac{\mathrm{d} x^i}{\mathrm{d} \lambda} \\
  &= \frac{\partial f}{\partial x^i} \mathrm{d}x^i \left( \frac{\mathrm{d}}{\mathrm{d} \lambda} \right) \\
\end{align*}
$$

giving 

$$
  \mathrm{d}f = \frac{\partial f}{\partial x^i} \mathrm{d}x^i
$$

The differential form components transform covariantly using the chain rule

$$
  \frac{\partial f}{\partial \tilde{x}^i} = \frac{\partial x^j}{\partial \tilde{x}^i} \frac{\partial f}{\partial x^i} = J_i^j \frac{\partial f}{\partial x^i}
$$

# Curvilinear coordinates

| System | $q_1$ | $q_2$ | $q_3$ | $h_1$ | $h_2$ | $h_3$ |
|---|:-:|:-:|:-:|:-:|:-:|:-:|
| Cartesian | $x$ | $y$ | $z$ | $1$ | $1$ | $1$ | 
| Spherical | $r$ | $\theta$ | $\phi$ | $1$ | $r$ | $r\sin\theta$ |
| Cylindrical | $s$ | $\phi$ | $z$ | $1$ | $s$ | $1$ |

Consider the 3-dimensional Euclidean space. In Cartesian coordinates the position of a point $P$ is given by

$$
  \mathbf{r} = x_1 \mathbf{e}_1 + x_2 \mathbf{e}_2 + x_3 \mathbf{e}_3
$$

where $\mathbf{e}_i := \frac{\partial\mathbf{r}}{\partial x_i}$ are the standard basis vectors. The point $P$ can also be defined by curviliner coordinates $q_i$ that are related to the Cartesian coordinates by the functions

$$
\begin{split}
  x_1 = f_1(q_1, q_2, q_3) 
\end{split}\quad
\begin{split}
  x_2 = f_2(q_1, q_2, q_3) 
\end{split}\quad
\begin{split}
    x_3 = f_3(q_1, q_2, q_3) 
\end{split}
$$

Assuming that the curvilinear coordinate system is orthogonal, its basis vectors $\mathbf{h}_i := \frac{\partial\mathbf{r}}{\partial q_i}$ are mutually perpendicular such that

$$
  \mathbf{h_i}\cdot\mathbf{h_j} = \delta_{ij} = \begin{cases}
    1 \quad i = j \\ 
    0 \quad i \neq j
  \end{cases}
$$

The norm of the basis vectors are given by $\left| \mathbf{h}_i \right| = \sqrt{\frac{\partial\mathbf{r}}{\partial q_i} \cdot \frac{\partial\mathbf{r}}{\partial q_i}}$. An orthonormal basis $\Set{\mathbf{b}_i }_{i=1}^3$ can thus be formed by setting

$$
  \mathbf{b}_i = \frac{\mathbf{h_i}}{h_i} \quad h_j := \sqrt{\frac{\partial\mathbf{r}}{\partial q_i} \cdot \frac{\partial\mathbf{r}}{\partial q_i}}
$$

The total differential change in $\mathbf{r}$ is then given by 

$$
  \mathrm{d}\mathbf{r} = \sum_{i=1}^3 \left(\frac{\partial\mathbf{r}}{\partial q_i}\right)\mathrm{d}q_i = \sum_{i=1}^3 h_i \mathrm{d}q_i \mathbf{b}_i
$$

From the total differential change we can form the differential volume element by

$$
  \mathrm{d}V = \prod_{i=1}^3 \left(\mathrm{d}\mathbf{r} \right)_i = h_1 h_2 h_3 \;\mathrm{d}q_1\;\mathrm{d}q_2\;\mathrm{d}q_3
$$

and differential surface elements by

$$
  \mathrm{d}\mathbf{a}_i = (\mathrm{d}\mathbf{r})_j \cdot (\mathrm{d}\mathbf{r})_k \mathbf{b}_i = h_j h_k \;\mathrm{d}q_j\;\mathrm{d}q_k \mathbf{b}_i
$$

## Gradient

Consider a scalar function $f(q_i) : \R^3 \to\R$. The total derivative of $f$ is given by

$$
  \mathrm{d}f := \nabla f \cdot\mathrm{d}\mathbf{r} = \sum_{i=1}^3 \frac{\partial f}{\partial q_i}\mathrm{d}q_i
$$

Evaluating the inner product $\nabla f\cdot \mathrm{d}\mathbf{r}$ we get

$$
  \mathrm{d}f = \nabla f \cdot\mathrm{d}\mathbf{r} = \nabla f \cdot \left( \sum_{i=1}^3 h_i \mathrm{d}q_i \mathbf{b}_i \right) = \sum_{i=1}^3 h_i \left( \nabla f \cdot\mathbf{b}_i \right)\mathrm{d}q_i
$$

Equating the terms of $\mathrm{d}f$ we see that

$$
  \nabla f \cdot \mathbf{b}_i = \frac{1}{h_i}\frac{\partial f}{\partial q_i}
$$

and hence the gradient of $f$ wrt. to the curvilinear coordinates is given by

$$
  \nabla f = \sum_{i=1}^3 \left(\nabla f \cdot \mathbf{b}_i \right)\mathbf{b}_i = \sum_{i=1}^3 \frac{1}{h_i}\frac{\partial f}{\partial q_i}\mathbf{b}_i
$$

## Divergence

Suppose $\mathbf{A}: \R^3 \to\R^3$ is a vector field with components $\mathbf{A} = \sum_{i=1}^3 A_i\mathbf{b}_i$. The divergence of $\mathbf{A}$ can be derived by writing out

$$
\begin{align*}
  \nabla\cdot\mathbf{A} &= \nabla\cdot\left( A_1 \mathbf{b}_1 + A_2 \mathbf{b}_2 + A_3 \mathbf{b}_3 \right) \\
  &= \nabla\cdot\left( h_2 h_3 A_1 \frac{\mathbf{b}_1}{h_2 h_3} + h_1 h_3 A_2 \frac{\mathbf{b}_2}{h_1 h_3} + h_1 h_2 A_3 \frac{\mathbf{b}_3}{h_1 h_2} \right) \\
  &= \nabla\cdot\left( h_2 h_3 A_1 \frac{\mathbf{b}_1}{h_2 h_3} \right) + \nabla\cdot\left( h_1 h_3 A_2 \frac{\mathbf{b}_2}{h_1 h_3} \right) + \nabla\cdot\left( h_1 h_2 A_3 \frac{\mathbf{b}_3}{h_1 h_2} \right)
\end{align*}
$$

Setting $f:= h_j h_k A_i$ and $\mathbf{v}:= \frac{\mathbf{b}_i}{h_j h_k}$, each divergence term takes the form $\nabla\cdot(f\mathbf{v})$ allowing us to apply the identity

$$
  \nabla\cdot(f\mathbf{v}) = \mathbf{v}\cdot\nabla f + f\nabla\cdot\mathbf{v}
$$

It can be further shown that $\nabla\cdot\mathbf{v} = \nabla\cdot\left( \frac{\mathbf{b}_i}{h_j h_k} \right) = 0$, by recalling that $\nabla q_i = \frac{\mathbf{b}_i}{h_i}$ and $\mathbf{b}_i \times \mathbf{b}_j = \sum_{k=1}^3 \varepsilon_{ijk}\mathbf{b}_k$ where $\varepsilon_{ijk}$ is the Levi-Civita symbol, so that

$$
  \nabla q_j \times\nabla q_k = \frac{\mathbf{b}_j \times\mathbf{b}_k}{h_j h_k} = \frac{\mathbf{b}_i}{h_j h_k}
$$

Taking the divergence of the left hand side, we may use the identity

$$
  \nabla\cdot(\mathbf{u}\times\mathbf{v}) = \mathbf{v}\cdot(\nabla\times\mathbf{u}) - \mathbf{u}\cdot(\nabla\times\mathbf{v})
$$

with $\mathbf{u}:= \nabla q_j$ and $\mathbf{v}:= q_k$. However, because gradients are curl free, i.e. $\nabla\times(\nabla f) = \mathbf{0}$ we conclude that

$$
\begin{align*}
  \nabla\cdot\left( \frac{\mathbf{b}_i}{h_j h_k} \right) &= \nabla\cdot\left(\nabla q_j \times\nabla q_k\right) \\
  &= \nabla q_k \left(\nabla \times \nabla q_j \right) - \nabla q_j \cdot\left( \nabla\times \nabla q_k \right) \\
  &= 0
\end{align*}
$$

It therefore follows that

$$
  \nabla\left( h_j h_k A_i \frac{\mathbf{b}_i}{h_j h_k} \right) = \frac{\mathbf{b}_i}{h_j h_k}\cdot \nabla(h_j h_k A_i) = \frac{1}{h_i h_j h_k}\frac{\partial}{\partial q_i}\left( h_j h_k A_i \right)
$$

where we have used that $\mathbf{q}_i \cdot \nabla f = \frac{1}{h_i}\frac{\partial f}{\partial q_i}$. In conclusion, the divergence of $\mathbf{A}$ wrt. curvilinear coordinates is given by

$$
  \nabla\cdot\mathbf{A} := \frac{1}{h_1 h_2 h_3}\left[\frac{\partial}{\partial q_1}\left(h_2 h_3 A_1\right) + \frac{\partial}{\partial q_2}\left(h_1 h_3 A_2\right) + \frac{\partial}{\partial q_3}\left(h_1 h_2 A_3\right)\right]
$$

## Curl

The curl of $\mathbf{A}$ can be derived by writing out

$$
\begin{align*}
  \nabla\times\mathbf{A} &= \nabla\times\left(\sum_{i=1}^3 A_i \mathbf{b}_i \right) \\
  &= \nabla\times\left(\sum_{i=1}^3 h_i A_i \frac{\mathbf{b}_i}{h_i} \right) \\
  &= \sum_{i=1}^3 \nabla\times\left(h_i A_i \frac{\mathbf{b}_i}{h_i} \right)
\end{align*}
$$

Setting $f:= h_i A_i$ and $\mathbf{v} = \frac{\mathbf{b}_i}{h_i}$, each curl term takes the form $\nabla\times (f\mathbf{v})$, allowing us to apply the identity

$$
  \nabla\times(f\mathbf{v}) = -\mathbf{v}\times\nabla f + f\nabla\times\mathbf{v}
$$

Since $\nabla\times\left(\frac{\mathbf{b}_i}{h_i}\right) = \nabla\times\left(\nabla q_i \right) = 0$, we are left with

$$
  \nabla\times\mathbf{A} = \sum_{i=1}^3 -\frac{\mathbf{b}_i}{h_i}\times\nabla\left(h_i A_i\right) = \sum_{i=1}^3 -\frac{\mathbf{b}_i}{h_i}\times\left[\sum_{j=1}^3 \frac{1}{h_j}\frac{\partial}{\partial q_j}\left( h_i A_i \right)\mathbf{b}_j \right] 
$$

Applyting the relation $\mathbf{b}_i \times \mathbf{b}_j = \sum_{k=1}^3 \varepsilon_{ijk}\mathbf{b}_k$, we conclude that

$$
  \nabla\times\mathbf{A} = \frac{\mathbf{q}_1}{h_2 h_3}\left[\frac{\partial}{\partial q_2}\left(h_3 A_3 \right) - \frac{\partial}{\partial q_3}\left(h_2 A_2 \right)\right] + \frac{\mathbf{q}_2}{h_1 h_3}\left[\frac{\partial}{\partial q_3}\left(h_1 A_1 \right) - \frac{\partial}{\partial q_1}\left(h_3 A_3 \right)\right] + \frac{\mathbf{q}_3}{h_1 h_2}\left[\frac{\partial}{\partial q_1}\left(h_2 A_2 \right) - \frac{\partial}{\partial q_2}\left(h_1 A_1 \right)\right]
$$

## Laplacian

The Laplacian of a scalar function $f:\R^3 \to\R$ is defined as the divergence of $\nabla f$. Using the results for the gradient and dirvergence in curvilinear coordinates, we find that

$$
\begin{align*}
  \nabla^2 f &:= \nabla\cdot\left(\nabla f \right) = \nabla\cdot\left(\sum_{i=1}^3 \frac{1}{h_1}\frac{\partial f}{\partial q_i}\mathbf{b}_i \right) \\
  &= \nabla\cdot\left( \frac{h_2 h_3}{h_1} \frac{\partial f}{\partial q_1} \frac{\mathbf{b}_1}{h_2 h_3} + \frac{h_1 h_3}{h_2} \frac{\partial f}{\partial q_2} \frac{\mathbf{b}_2}{h_1 h_3} + \frac{h_1 h_2}{h_3} \frac{\partial f}{\partial q_3} \frac{\mathbf{b}_3}{h_1 h_2} \right) \\
  &= \nabla\cdot\left( \frac{h_2 h_3}{h_1} \frac{\partial f}{\partial q_1} \frac{\mathbf{b}_1}{h_2 h_3}\right)  + \nabla\cdot\left(\frac{h_1 h_3}{h_2} \frac{\partial f}{\partial q_2} \frac{\mathbf{b}_2}{h_1 h_3}\right) + \nabla\cdot\left( \frac{h_1 h_2}{h_3} \frac{\partial f}{\partial q_3} \frac{\mathbf{b}_3}{h_1 h_2} \right) \\
  &= \frac{\mathbf{b_1}}{h_2 h3}\cdot\left(\frac{h_2 h_3}{h_1}\frac{\partial f}{\partial q_1} \right) + \frac{\mathbf{b_2}}{h_1 h3}\cdot\left(\frac{h_1 h_3}{h_2} \frac{\partial f}{\partial q_2} \right) + \frac{\mathbf{b_3}}{h_1 h2}\cdot\left(\frac{h_1 h_2}{h_3} \frac{\partial f}{\partial q_3} \right) \\
  &= \frac{1}{h_1 h_2 h_3}\left[ \frac{\partial}{\partial q_1}\left( \frac{h_2 h_3}{h_1}\frac{\partial f}{\partial q_1} \right) + \frac{\partial}{\partial q_2}\left( \frac{h_1 h_3}{h_2}\frac{\partial f}{\partial q_2} \right) + \frac{\partial}{\partial q_3}\left( \frac{h_1 h_2}{h_3}\frac{\partial f}{\partial q_3} \right) \right]
\end{align*}
$$