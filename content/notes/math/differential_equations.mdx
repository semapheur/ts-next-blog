---
title: 'Differential Equations'
subject: 'Mathematics'
showToc: true
---

# Ordinary differential equations (ODE)

An ordinary differential equation (ODE) is a differential equation with functions of one independent variable. A general form of an $n^{\textrm{th}}$ order ODE is

$$
  G\left(x, f(x), \frac{\d f}{\d x}, \dots, \frac{\d^n f}{\d x^n} \right) = 0
$$

This is an implicit form of the ODE. In an explicit form, the highest order derivative is given as a function of the lower derivatives

$$
  \frac{\d^n f}{\d x^n} = F\left(x, f(x), \frac{\d f}{\d x}, \dots, \frac{\d^{n-1} f}{\d x^{n-1}} \right)
$$

## Linear ODE

The general form of linear ODEs of order $n$ is

$$
\begin{align}
  \sum_{k=0}^n \alpha_k(x) \frac{\d^k y}{\d x^k} &= \alpha_n(x) \frac{\d^n y}{\d x^n} + \alpha_{n-1}(x) \frac{\d^{n-1} y}{\d x^{n-1}} + \dots + \alpha_1 (x) \frac{\d y}{\d x} + \alpha_0 (x) y \\
  &= f(x)
\end{align}
$$

The ODE is called homogenous if $f(x) = 0$ and inhomogenous otherwise.

### Linear operators

The differential operator of order $n$ is defined as $\mathcal{D}^n[f] := \frac{\d^n}{\d x^n}f(x)$. It is a linear operator because it is closed under addition and scalar multiplication

$$
  \mathcal{D}^n \left[\lambda_1 f_1 + \lambda_2 f_2 \right] = \lambda_1 \mathcal{D}^n \left[f_1 \right] + \lambda_2 \mathcal{D}^n \left[ f_2 \right]
$$

Linear ODEs are associated with a linear operator defined using the differential operators

$$
  \mathcal{L}[y] := \sum_{k=0}^n \alpha_k (x) \mathcal{D}^k [y]
$$

A linear can thus be compactly expressed as $\mathcal{L}[y] = f(x)$. The solutions of the $n^{\textrm{th}}$ order homogenous linear ODE $\mathcal{L}[y] = 0$ form a vector space of dimension $n$. Thus, the general solution of a linear homogenous ODE can be written as

$$
  y\left(x; c_1, \dots, c_n \right) = \sum_{k=1}^n c_k y_k
$$

where $\Set{ y_k(x) }_{k=1}^n$ is a set of linearly independent solutions forming a basis for the solution vector space.

#### Linear independence

A set of vectors $\Set{ y_k(x) }_{k=1}^n$ is linearly independent if the Wronskian, which is the determinant of the Wronskian matrix $W$, is non-zero

$$
  0 \neq W\left[ \Set{ y_k(x) }_{k=1}^n \right] = \det W = \begin{vmatrix} 
    y_1 (x) & y_2 (x) & \dots & y_n (x) \\ 
    \frac{\d y_1}{\d x} (x) & \frac{\d y_2}{\d x} (x) & \dots & \frac{\d y_n}{\d x} (x) \\
    \vdots & \vdots & & \vdots \\
    \frac{\d^{n-1} y_1}{\d x^{n-1}}(x) & \frac{\d^{n-1} y_2}{\d x^{n-1}}(x) & \dots & \frac{\d^{n-1} y_n}{\d x^{n-1}}(x)
  \end{vmatrix}
$$

### Non-homogeneous linear ODE

The general solution of the non-homogeneous linear ODE $\mathcal{L}[y] = f(x)$ can be found by splitting the problem into two steps.

1. Solve the corresponding homogeneous linear ODE $\mathcal{L}[y] = 0$. The general solution, called the complementary function, is given by the independent linear combination

$$
  y_\mathrm{CF} \left(x; c_1, \dots, c_n \right) = \sum_{k=1}^n c_k y_k
$$

2. Obtain any particular solution for the non-homogenous ODE, called the particular integral $y_\mathrm{PI}$. 

The general solution of the non-homogenous linear ODE is then given by the sum of the complementary funcion and a particular integral: 

$$
  \mathcal{L}\left[y \left(x; c_1, \dots, c_n \right) \right] = \mathcal{L}\left[y_\mathrm{CF} + y_\mathrm{PI} \right] = \mathcal{L}\left[ y_\mathrm{CF} \right] + \mathcal{L}\left[ y_\mathrm{PI} \right] = f(x)
$$

#### Constant coefficients

The general form of an $n^\textrm{th}$ order linear ODE with constant coefficient is

$$
  \mathcal{L}[y] = \sum_{k=0}^n \alpha_k \mathcal{D}^k [y] = f(x)
$$

The general solution of the corresponding homogenous equation $\mathcal{L}[y] = 0$ is obtained with the ansatz $y = e^{\lambda x}$ giving

$$
  \mathcal{L}[e^{\lambda x}] = e^{\lambda x} \sum_{k=0}^n \alpha_k \lambda^k = 0 \implies \sum_{k=0}^n \alpha_k \lambda^k = 0
$$

The resulting characteristic polynomial has $n$ roots. If all roots are distinct the solutions $\Set{e^{\lambda_k x} }_{k=1}^n$ can be shown to be linear using the Wronskian

$$
\begin{align*}
  W\left[ \Set{e_k^{\lambda_k x} }_{k=1}^n \right] &= e^{\sum_{k=1}^n \lambda_k x} \begin{vmatrix} 1 & 1 & \dots & 1 \\ \lambda_1 & \lambda_2 & \dots & \lambda_n \\ \vdots & \vdots & & \vdots \\ \lambda_1^{n-1} & \lambda_2^{n-1} & \dots & \lambda_n^{n-1} \end{vmatrix} \\
  &= e^{\sum_{k=1}^n \lambda_k x} \prod_{1 \leq k < l \leq n} \left( \lambda_k - \lambda_l \right) \neq 0
\end{align*}
$$

which is known as the Vandermonde determinant.

### First order linear ODE

First order linear ODEs have the general form

$$
  \frac{\d y}{\d x} + p(x)y = q(x)
$$

This is solved by finding an integrating factor. We look for $I(x)$ such that

$$
  I(x) \left[ \frac{\d y}{\d x} + p(x)y \right] = \frac{\d\left[ I(x)y \right]}{\d x}
$$

We then have

$$
\begin{align*}
  \frac{\d\left[ I(x)y \right]}{\d x} &= I(x)q(x) \\
  \int \d\left[ I(x)y \right] &= \int q(x)I(x)\d x + c_1 \\
  y(x) &= \frac{1}{I(x)}\left[\int q(x)I(x)\d x + c_1 \right]
\end{align*}
$$

Integrating factors must fulfill

$$
\begin{align*}
  \frac{\d(Iy)}{\d x} &= I\frac{\d y}{\d x} + Ipy \\
  I\frac{\d y}{\d x} + y\frac{\d I}{\d x} &= I\frac{\d y}{\d x} + Ipy \\
  \int \frac{\d I}{I} &= \int p(x)\d x + c' \\
  \ln{I(x)} &= \int p(x)\d x + A \\
  I(x) &= Ae^{\int p(x)\d x}
\end{align*}
$$

The general solution becomes

$$
  y(x) = e^{-\int p(x)\d x} \left[ \int e^{-\int p(x)\d x} q(x) \mathrm{x} + c \right] \, , \quad c = \frac{c_1}{A}
$$

#### Constant coefficient

The general form of first order ODEs with constant coefficient is

$$
\begin{gather*}
  \mathcal{L}[y] = \alpha_1 \frac{\d y}{\d x} + \alpha_0 y = f(x) \\
  \frac{\d y}{\d x} + \frac{\alpha_0}{\alpha_1} y = \frac{f(x)}{\alpha_1}
\end{gather*}
$$


The general solution is obtained by applying the integrating factor $I = e^{\frac{\alpha_0}{\alpha_1}x}$ resulting in

$$
  y = c_1 + e^{\frac{\alpha_0}{\alpha_1}x} \int e^{\frac{\alpha_0}{\alpha_1}x}
$$

### Second order linear ODE

The homogenous second order linear ODE with constant coefficient is of the form

$$
  \mathcal{L}[y] = \alpha_2 \frac{\d^2y}{\d x^2} + \alpha_1 \frac{\d y}{\d x} + \alpha_0 y
$$

To find two linearly independent solutions we apply the ansatz $y = e^{\lambda x}$

$$
\begin{gather*}
  \mathcal{L}[y] = \alpha_2 \lambda^2 e^{\lambda x} + \alpha_1 \lambda e^{\lambda x} + \alpha_0 e^{\lambda x} = 0 \\
  \alpha_2 \lambda^2 + \alpha_1 \lambda + \alpha_0 = 0
\end{gather*}
$$

This quadratic equation is called the characteristic equation of the ODE, with roots given by

$$
  \lambda_\pm = \frac{-\alpha_1 \pm \sqrt{\alpha_1^2 - 4\alpha_0 \alpha_2}}{2\alpha_2}
$$

Two candidate solutions are $y_\pm (x)= e^{\lambda_\pm x}$. For these solutions to form a basis for the solution, the Wronskian must be non-zero

$$
  W[y_\pm] = \begin{vmatrix} e^{\lambda_+ x} & e^{\lambda_- x} \\ \lambda_+  e^{\lambda_+ x} & \lambda_-  e^{\lambda_- x} \end{vmatrix} = \left( \lambda_- - \lambda_+ \right) e^{\left(\lambda_+ + \lambda_- \right)x}
$$

If the roots are distinct $\lambda_+ \neq \lambda_-$ then $W[y_\pm] \neq 0$ and the solutions form a linearly independent set

$$
    y(x) = c_+ e^{\lambda_+ x} + c_- e^{\lambda_- x}
$$

#### Repeated roots

For the case of a repeated root $\lambda_+ = \lambda_- = -\frac{\alpha_1}{2\alpha_2}$, first solution becomes $y_1 = e^{\lambda x}$. The second solution can be found with the ansatz $y_2 = A(x) y_1(x) = A(x)e^{\lambda x}$. Plugging the ansatz into the ODE gives

$$
\begin{gather*}
  \alpha_0 A y_1 + \alpha_1 \left( \frac{\d A}{\d x}y_1 + A  \frac{\d y_1}{\d x} \right) + \alpha_2 \left( \frac{\d^2 A}{\d x^2}y_1 + 2 \frac{\d A}{\d x} \frac{\d y_1}{\d x} + A \frac{\d^2 y}{\d x^2} \right) = 0 \\
  \alpha_0A e^{\lambda x} + \alpha_1 e^{\lambda x} \left( \frac{\d A}{\d x} + \lambda A \right) + \alpha_2 e^{\lambda x} \left(\frac{\d^2 A}{\d x^2} + 2\lambda \frac{\d A}{\d x} + A\lambda^2 \right) = 0 \\
  \frac{\alpha_1^2}{4 \alpha_2}A + \alpha_1 \left( \frac{\d A}{\d x} - \frac{\alpha_1}{2\alpha_2} A \right) + \alpha_2  \left(\frac{\d^2 A}{\d x^2} - \frac{\alpha_1}{\alpha_2} \frac{\d A}{\d x} + \frac{\alpha_1^2}{4\alpha_2^2} A \right) = 0 \\
  \frac{\d^2 A}{\d x^2} = 0
\end{gather*}
$$

The solution is $A(x) = B_1x + B_2 \implies y_2 = \left(B_1 x + B_2 \right)e^{\lambda x}$. We are free to choose constants so that we can set $y_2 = xe^{\lambda x}$. To test the linear independence, we evaluate the Wronskian

$$
  W[y(x)] = \begin{vmatrix} 
    e^{\lambda x} & x e^{\lambda x} \\ 
    \lambda  e^{\lambda x} & e^{\lambda x} + \lambda x e^{\lambda x} 
  \end{vmatrix} = e^{2\lambda x} \neq 0
$$

which shows that $y_1$ and $y_2$ form a basis for the solution space. The general solution is thus

$$
  y(x) = c_1 e^{\lambda x} + c_2 x e^{\lambda x}
$$

#### Complex roots

For the case of a negative discrimant $\alpha_1^2 - 4\alpha_0\alpha_2 < 0$ in the characteristic equation, the roots becomes complex

$$
  \lambda_\pm = -\frac{\alpha_1}{2\alpha_2} \pm i\omega \quad \omega^2 = \left| \frac{\alpha_1^2 - 4\alpha_0 \alpha_2}{4\alpha_2^2}\right| 
$$

giving the general solution

$$
  y(x) = e^{-\frac{\alpha_1}{2\alpha_2}x} \left[c_+ e^{i\omega x} + c_- e^{-i\omega x} \right] = e^{-\frac{\alpha_1}{2\alpha_2}x} \left[ (c_+ + c_-)\cos\omega x + i(c_+ - c_-)\sin \omega x \right]
$$

Choosing the constants as complex conjugate we obtain $c'_+ = c_+ + c_-$ and $c'_- = i\left(c_- - c_+ \right)$, so that the general solution becomes

$$
  y(x) = e^{-\frac{\alpha_1}{2\alpha_2}x} \left( c'_+ \cos\omega x + c'_- \sin \omega x \right) = e^{-\frac{\alpha_1}{2\alpha_2}x} A \cos (\omega x - \phi)
$$

with the change of constants $c'_+ = A\cos\phi$ and $c'_- = A \sin\phi$.

## System of ODEs

A system of ODEs has a vector functions $\mathbf{y}(x):\R\to\R^n$ as unknown. Its general form is

$$
\begin{gather*}
  G_1 \left(x, \mathbf{y}, \frac{\d\mathbf{y}}{\d x}, \dots \frac{\d^n\mathbf{y}}{\d x^n} \right) = 0 \\
  \vdots \\
  G_n \left(x, \mathbf{y}, \frac{\d\mathbf{y}}{\d x}, \dots \frac{\d^n\mathbf{y}}{\d x^n} \right) = 0
\end{gather*}
$$

Systems of ODEs can be rewritten in terms of systems of first order ODEs with explicit form

$$
\begin{gather*}
  \frac{\d y_1}{\d x} = F_1\left(x, y_1, \dots, y_n \right) \\
  \vdots \\
  \frac{\d y_n}{\d x} = F_n\left(x, y_1, \dots, y_n \right)
\end{gather*}
$$

Systems of linear 1st order ODEs with constant coefficients have the general form

$$
\begin{gather*}
  \frac{\d y_1}{\d x} = \sum_{k=1}^n \alpha_{1k}y_k + g_1(t) \\
  \vdots \\
  \frac{\d y_n}{\d x} = \sum_{k=1}^n \alpha_{nk}y_k + g_n(t)
\end{gather*}
$$

The system can also be expressed in matrix form

$$
\begin{gather*}
  \begin{bmatrix} 
    \frac{\d y_1}{\d x} \\ 
    \frac{\d y_2}{\d x} \\ 
    \vdots \\ 
    \frac{\d y_n}{\d x} 
  \end{bmatrix} = \begin{bmatrix} 
    \alpha_{11} & \alpha_{12} & \dots & \alpha_{1n} \\ 
    \alpha_{21} & \alpha_{22} & \dots & \alpha_{2n} \\ 
    \vdots & \vdots & \ddots & \vdots \\ 
    \alpha_{n1} & \alpha_{n2} & \dots & \alpha_{nn} 
  \end{bmatrix} \cdot \begin{bmatrix} 
    y_1 \\ 
    y_2 \\ 
    \vdots \\ 
    y_n 
  \end{bmatrix} + \begin{bmatrix} 
    g_1 (t) \\ 
    g_2 (t) \\ 
    \vdots \\ 
    g_n (t) 
  \end{bmatrix} \\
  \frac{\d\mathbf{y}}{\d x} = A\mathbf{y} + \mathbf{g}(t)
\end{gather*}
$$

By defining the linear operator $\mathcal{L}[\mathbf{y}] = \left[\frac{\d}{\d t} - A \right]\mathbf{y}$, we can write the system of linear first order ODEs as

$$
  \mathcal{L}[\mathbf{y}] = \mathbf{g}(t)
$$

The general solution is obtained in a similar manner as for a single linear ODE, by
1. Solving for the corresponding homogenous system of ODEs
2. Finding any particular integral that satisfies the full non-homogenous systems of ODEs

The corresponding homogenous system of ODEs takes the form

$$
  \mathcal{L}\left[ \mathbf{y}_H \right] = \mathbf{0} \implies \frac{\d\mathbf{y}_H}{\d t} = A \mathbf{y}_H
$$

In the case where $A$ is diagonalizable with $n$ distinct roots, there exists a matrix $V$ such that

$$
  V^{-1}AV = \Lambda = \lambda_i \delta_{ij}
$$

where the $i$th column of $V$ is the eigenvector of $A$ corresponding to the eigenvalue $\lambda_i$

$$
  A\mathbf{v}_i = \lambda_i \mathbf{v}_i
$$

Applying $V$ on the homogenous system gives

$$
  V^{-1}\frac{\d\mathbf{y}}{\d t} = V^{-1} AVV^{-1} \mathbf{y}
$$

Letting $\mathbf{z} = V^{-1}\mathbf{y}$, we can write the system of ODEs as

$$
  \frac{\d\mathbf{z}}{\d t} = \Lambda \mathbf{z}
$$

The $i$th row of this equation reads $\frac{\d z_i}{\d t} = \lambda_i z_i$, giving $z_i = c_i e^{\lambda_1 t}$. Transforming back gives the homogenous solution

$$
\begin{gather*}
  Z = \begin{bmatrix} 
    c_1 e^{\lambda_1 t} \\ 
    c_2 e^{\lambda_2 t} \\ 
    \vdots \\ 
    c_n e^{\lambda_n t} 
  \end{bmatrix} \\ 
  \implies \mathbf{y}_H = VZ = \sum_{i=1}^n c_i e^{\lambda_i t}\mathbf{v}_i
\end{gather*}
$$

In the case where $A$ is non-diagonalizable with a single repeated eigenvalue $\lambda$, we can use the Jordan normal form $J$ to obtain a solution. We look for a similarity transformation $W$ to transform $A$ to $J$

$$
  W^{-1}AW = J = \begin{bmatrix} 
    \lambda & 1 & 0 & 0 & 0 \\ 
    0 & \lambda & 1 & 0 & 0 \\ 
    0 & 0 & \ddots & 1 & 0 \\ 
    0 & 0 & 0 & \lambda & 1 \\ 
    0 & 0 & 0 & 0 & \lambda 
  \end{bmatrix}
$$

Letting $\mathbf{z} = W^{-1}\mathbf{z}$, we obtain $\frac{\d\mathbf{z}}{\d t} = J\mathbf{z}$ so that

$$
\begin{align*}
  \frac{\d z_n}{\d t} = \lambda z_n &\implies z_n = c_n e^{\lambda t} \\
  \frac{\d z_{n-1}}{\d t} = \lambda z_{n-1} + z_n &\implies z_{n-1} = c_{n-1}e^{\lambda t} + c_n t e^{\lambda t} \\
  &\vdots \\
  \frac{\d z_1}{\d t} = \lambda z_1 + z_2 &\implies z_1 = c_1 e^{\lambda t} + \dots + c_n \frac{t^{n-1}}{(n-1)!} e^{\lambda t}
\end{align*}
$$

Transforming back gives the homogenous solution $\mathbf{y}_H = W\mathbf{z}$.

## Sturm-Liouville theorem

A Sturm-Liouville problem a is a second-order linear ODE of the form

$$
  \frac{\d}{\d x}\left[ p(x) \frac{\d y}{\d x} \right] + q(x)y = -\lambda w(x) y
$$

with separated boundary conditions of the form

$$
\begin{align*}
  \alpha_1 y(a) + \alpha_2 y'(a) &= 0 \\
  \beta_1 y(b) + \beta_2 y'(b) &= 0
\end{align*}
$$

Non-trivial solutions $y_n$ to the S-L problem are called eigenfunctions, each corresponding to an eigenvalue $\lambda_n$.

The normalized eigenfunctions form an orthonormal basis under the $w$-weighted inner product in the Hilbert space $L^2\left([a,b], w(x) \d x\right)$, such that

$$
  \langle y_n, y_m \rangle = \int_a^b y_n(x) y_m(x) w(x) \d x = \delta_{mn}
$$

## Stability analysis

### Phase space

The general solution of systems of ODEs is given by the familiy of parametric curves, specified by the initial condition

$$
  \mathbf{t}\left( t; c_1,\dots,c_n \right) \in \R^n
$$

These solutions represent trajectories in $\R^n$ for a system of n dimensional ODEs. The solution $\mathbf{y}(t)$ corresponds to a trajectory of a point moving in the phase space with velocity $\dot{\mathbf{y}}(t)$. For a system of first order ODEs of the form $\dot{\mathbf{y}}(t) = F(\mathbf{y})$, the velocity is a vector field tangent to the trajectory.

Solutions of ODEs are uniquely defined by initial conditions except at fixed or singular points in the phase space. Trajectories can therefore not cross except at fixed or singular points where trajectories start or end.

For linear systems $\dot{\mathbf{y}} = A\mathbf{y}$ the direction of the trajectories are given by the eigenvectors $A\mathbf{v} = \lambda\mathbf{v}$. The line defined by $\mathbf{v}$ is invariant. 

Let $\mathbf{y}(0) = \alpha\mathbf{v}$ with $\alpha \in \R$, we have

$$
  A\mathbf{y}(0) = \alpha A \mathbf{v} = \alpha\lambda\mathbf{v} = \dot{\mathbf{v}}(0)
$$

If $\lambda > 0$, then $\mathbf{y}(t)$ grows along $\mathbf{v}$ and if $\lambda < 0$, then $\mathbf{y}(t)$ decays along $\mathbf{v}$.

### Fixed point

A point $\mathbf{y}(t_0) = \mathbf{y}^*$ of a system of first order ODEs is called a fixed point if it satisfies

$$
  \left.\frac{\d\mathbf{y}}{\d t}\right|_{\mathbf{y} = \mathbf{y}^*} = \mathbf{0}
$$

for $t > t_0$. A fixed point is stable if whenever the initial state is near that point, the state remains near it. Formally, there are two types of stability.

<MathBox title='Lyapunov stability' boxType='definition'>
A fixed point $\mathbf{y}^*$ is said to be Lyapunov stable, if for every $\epsilon > 0$, there exists a $\delta > 0$ such that, if $\norm{ \mathbf{y}(0) - \mathbf{y}^*} < \delta$, then $\forall t \geq 0$ we have $\norm{ \mathbf{y}(t) - \mathbf{y}^*} < \epsilon$. Intuitively, it means the solution does not blow up, but does not necessarily approach to the fixed point.
</MathBox>

<MathBox title='Asymptotic stability' boxType='definition'>
A fixed point $\mathbf{y}^*$ is said to be asymptotically stable, if it is Lyapunov stable and there exists a $\delta > 0$ such that if $\norm{ \mathbf{y}(0) - \mathbf{y}^*} < \delta$, then

$$
  \lim_{t\to\infty} \norm{ \mathbf{y}(t) - \mathbf{y}^*} = 0
$$
</MathBox>

### 2D systems of linear ODEs

<LatexFigure width={90} src='/fig/2d_linear_ode_phase_portrait.svg' alt=''
  caption='Phase portaits of 2D systems of linear ODEs.'
>
```latex
\documentclass[tikz]{standalone}
\usepackage{amsmath}
\usepackage{tikz}

\tikzset{
  every pin/.style = {pin edge = {<-}},
  > = stealth,
  flow/.style = {
    decoration = {markings, mark=at position #1 with {\arrow{>}}},
    postaction = {decorate}
  },
  flow/.default = 0.5,
  main/.style = {line width=1pt}
}

\newcommand\newtemplate[4][0.18]{
  \newsavebox#2
  \savebox#2{
    \begin{tabular}{@{}c@{}}
      \begin{tikzpicture}[scale=#1]
        #4
      \end{tikzpicture}\\[-1ex]
      \templatecaption{#3}\\[-1ex]
    \end{tabular}
  }
}

\newcommand\template[1]{\usebox{#1}}
\newcommand\templatecaption[1]{{\sffamily\scriptsize#1}}
%\newcommand\tr{\mathop{\mathrm{\tr}}}
\DeclareMathOperator{\tr}{tr}

\newtemplate\sink{sink}{
  \foreach \sx in {+,-}{ % right/left half
    \draw[flow] (\sx4, 0) -- (0, 0); % draw half of horizontal axis
    \draw[flow] (0, \sx4) -- (0, 0); % draw half of vertical axis
    \foreach \sy in {+,-} % upper/lower quadrant
      \foreach \a/\b in {2/1,3/0.44} % draw to half-parabolas
        \draw[flow, domain=\sx\a:0] plot (\x, {\sy\b * \x * \x});
  }
}
\newtemplate\source{source}{
  \foreach \sx in {+,-}{ % right/left half
    \draw[flow] (0, 0) -- (\sx4, 0); % draw half of horizontal axis
    \draw[flow] (0, 0) -- (0, \sx4); % draw half of vertical axis
    \foreach \sy in {+,-} % upper/lower quadrant
      \foreach \a/\b in {2/1,3/0.44} % draw to half-parabolas
        \draw[flow, domain=\sx\a:0] plot (\x, {\sy\b * \x * \x});
  }
}
\newtemplate\stablefp{line of stable fixed points}{
  \draw (-4, 0) -- (4, 0); % horizontal axis
  \foreach \sy in {+,-}{ % upper/lower half
    \draw[flow] (0, \sy4) -- (0, 0);
    \foreach \x in {-3, -2, -1, 1, 2, 3} % draw six vertical half-lines
      \draw[flow] (\x, \sy3) -- (\x, 0);
  }
}
\newtemplate\unstablefp{line of unstable fixed points}{
  \draw (-4, 0) -- (4, 0); % horizontal axis
  \foreach \sy in {+,-}{ % upper/lower half
    \draw[flow] (0, 0) -- (0, \sy4);
    \foreach \x in {-3, -2, -1, 1, 2, 3} % draw six vertical half-lines
      \draw[flow] (\x, 0) -- (\x, \sy3);
  }
}
\newtemplate\spiralsink{spiral sink}{
  \draw (-4, 0) -- (4, 0); % draw horizontal axis
  \draw (0, -4) -- (0, 4); % draw vertical axis
  \draw [samples=100, smooth, domain=27:7] % draw spiral
    plot ({\x r}: {0.005 * \x * \x});
  \def\x{26}
  \draw[->] ({\x r}:{0.005 * \x * \x}) -- +(0.01, -0.01);
}
\newtemplate\spiralsource{spiral source}{
  \draw (-4, 0) -- (4, 0); % draw horizontal axis
  \draw (0, -4) -- (0, 4); % draw vertical axis
  \draw [samples=100, smooth, domain=10:28] % draw spiral
    plot ({-\x r}: {0.005 * \x * \x});
  \def\x{27.5}
  \draw[<-] ({-\x r}:{0.005 * \x * \x}) -- +(0.01, -0.008);
}
\newtemplate[0.15]\centre{center}{
  \draw (-4, 0) -- (4, 0); % draw horizontal axis
  \draw (0, -4) -- (0, 4); % draw vertical axis
  \foreach \r in {1,2,3} % draw three circles
    \draw[flow=0.63] (\r, 0) arc (0: -360: \r cm);
}
\newtemplate\saddle{saddle}{
  \foreach \sx in {+,-}{ % left/right half
    \draw[flow] (\sx4, 0) -- (0, 0); % draw half of horizontal axis
    \draw[flow] (0,0) -- (0, \sx4); % draw half of vertical axis
    \foreach \sy in {+,-} % upper/lower quadrant
      \foreach \a/\b/\c/\d in {2.8/0.3/0.7/0.6, 3.9/0.4/1.3/1.1}
        \draw[flow] (\sx\a, \sy\b) % draw two bent lines
          .. controls (\sx\c, \sy\d) and (\sx\d, \sy\c)
          .. (\sx\b, \sy\a);
  }
}
\newtemplate\degensink{degenerate sink}{
  \draw (0, -4) -- (0, 4); % draw vertical axis
  \foreach \s in {+,-}{ % upper/lower half
    \draw[flow] (\s4, 0) -- (0,0); % draw half of horizontal axis
      \foreach \a/\b/\c/\d in {3.5/4/1.5/1, 2.5/2/1/0.8}
        \draw[flow] (\s-3.5, \s\a) % draw to bent lines
          .. controls (\s\b, \s\c) and (\s\b, \s\d)
          .. (0, 0); 
  }
}
\newtemplate\degensource{degenerate source}{
  \draw (0, -4) -- (0, 4); % draw vertical axis
  \foreach \s in {+,-}{ % upper/lower half
    \draw[flow] (0, 0) -- (\s4, 0); % draw half of horizontal axis
      \foreach \a/\b/\c/\d in {3.5/4/1.5/1, 2.5/2/1/0.8}
        \draw[flow] (0, 0) % draw to bent lines
          .. controls (\s\b, \s\c) and (\s\b, \s\d)
          .. (\s-3.5, \s\a); 
  }
}

\begin{document}
\begin{tikzpicture}[scale=2, transform shape, line cap=round, line join=round]
  % Main diagram
  \draw[main, ->] (0,-0.3) -- (0, 7) % vertical axis
    node [label={[above]\scriptsize$\det A$}] {};
  \draw[main, ->] (-7, 0) -- (7, 0) % horizontal axis
    node [label={[below right]\scriptsize$\tr A$}] {};
  \draw[main, samples=50, smooth, domain=-5:5] plot (\x, {0.25 * \x * \x}); % discriminant
  \node at (-5, 6.3) [pin={[above]\scriptsize$\Delta=0$}] {};
  \node at (5, 6.3) [pin={[above, align=left]
    {\scriptsize$\Delta=0:\;\det A = \frac{1}{4}(\tr A)^2$}}] {};

  % Phase potraits
  \node at (0, -1.4) {\template\saddle};
  \node at (-4, 1) {\template\sink};
  \node at (4, 1) {\template\source};
  \node at (-1.8, 5) {\template\spiralsink};
  \node at (1.8, 5) {\template\spiralsource};

  \node at (0, 2) [pin={[draw, right, xshift=0.5cm]
    \template\centre}] {};
  \node at (-3, 0) [pin={[draw, below, yshift=-1cm]
    \template\stablefp}] {};
  \node at (3, 0) [pin={[draw, below, yshift=-1cm]
    \template\unstablefp}] {};
  \node at (-3.5, {0.25*3.5*3.5}) [pin={[draw, left, xshift=-1.15cm, yshift=-0.3cm]
    \template\degensink}] {};
  \node at (3.5, {0.25*3.5*3.5}) [pin={[draw, right, xshift=0.9cm, yshift=-0.3cm]
    \template\degensource}] {};
  \node at (0, 0) [pin={[draw, above left, align=center, xshift=-0.3cm]
    \templatecaption{uniform}\\[-1ex]\templatecaption{motion}}] {};
  \end{tikzpicture}
\end{document}
```
</LatexFigure>

A general two-dimensional system of linear ODEs is given by

$$
  \frac{\d\mathbf{y}}{\d t} = A\mathbf{y} \, ,\quad A = \begin{bmatrix} a & b \\ c & d \end{bmatrix}
$$

The general solution of this system is a linear combination of the eigenvectors to $A$. The eigenvalues can be obtained from the characteristic equation

$$
\begin{gather*}
  \lambda^2 - \tau \lambda + \Delta = 0 \\
  \lambda = \frac{\tau \pm \sqrt{\tau^2 - 4\Delta}}{2}
\end{gather*}
$$

where $\tau = \mathrm{trace}(A)$ and $\Delta = \det(A)$. The general solution is

$$
  \mathbf{y}(t) = c_1 e^{\lambda_1 t}\mathbf{v}_1 + c_2 e^{\lambda_2 t}\mathbf{v}_2 
$$

The qualitative behaviour of this system depends on $\tau$ and $\Delta$.

**Saddle-point (hyperbolic profile): $\Delta < 0$**

In this case $\lambda_1 > 0$ and $\lambda_2 < 0$ since $\Delta < 0 \implies \tau^2 - 4\Delta > \tau^2 > 0$. This gives

$$
    \lim_{t\to\infty} \mathbf{y}(t) = c_1 e^{\lambda_1}\mathbf{v}_1
$$

which grows exponentially. However, if we start on the line characterized by $\mathbf{v}_2$ direction, the solution goes to zero.

**Repelling or unstable point: $0 < \Delta < \frac{\tau^2}{4}\, , \quad \tau > 0$**

**Attracting or stable point: $0 < \Delta < \frac{\tau^2}{4}\, , \quad \tau < 0$**

**Centre or elliptic profile: $\Delta > \frac{\tau^2}{4}\, , \quad \tau = 0$**

**Repelling or unstable spiral: $\Delta > \frac{\tau^2}{4}\, , \quad \tau > 0$**

**Attracting or stable spiral: $\Delta > \frac{\tau^2}{4}\, , \quad \tau < 0$**

**Line of repelling or unstable fixed points: $\Delta = 0\, , \quad \tau > 0$**

**Line of attracting or stable fixed points: $\Delta = 0\, , \quad \tau < 0$**

**Repelling and attracting star node: $\tau^2 - 4\Delta = 0$**

**Unstable and stable improper or degenerate node: $\tau^2 - 4\Delta = 0$**

## Lie symmetry analysis

Sophus Lie discovered that the constant of any indefinite integral is in fact an element of a continuous symmetry group, the invariance group that maps solutions of a differential equation into other solutions. Lie exploited this observation to develop an algorithm for determining when a differential equation has an invariance group. If such a group exists, then a first order ODE can be integrated by quadratures, or the order of a higher order ODEs can be reduced.

### First order ODEs
The simplest ODE is of the form $y'(x) = g(x)$ with solution

$$
  y(x) = \int g(x) \d x + c = G(x) + c
$$

where $c$ is an arbitrary constant. Rewriting the solution as $y(x) + c - G(x) = 0$ suggests that the arbitrary constant is a one-parameter invariant group of translations represented by the Taylor series displacement operator $e^{c\frac{\partial}{\partial y}}$ such that

$$
  e^{c\frac{\partial}{\partial y}} [y - G(x)] = y + c - G(x) = 0
$$

Introducing the coordinate $p \equiv y'(x)$, the ODE can be written as 

$$
  F(x, y, p) = p - g(x) = 0
$$

The relations among the three variables $(x, y, p)$ are governed by the surface equation $F(x, y, p) = 0$ and the constraint equation $p = y'(x)$ when $F(x, y, p) = 0$. If the surface equation is not of the form $p - g(x)$ so that $\frac{\partial}{\partial y}F(x, y, p) \neq 0$, then we can attempt to find

1. a one-parameter group that leaves $F(x, y, p) = 0$ invariant.
2. a new canonical coordinate system $(R, S, T)$ where $R(x, y)$ is the independent variable, $S(x, y)$ is the dependent variable and $T(x, y, p)$ is the new constraint variable. In this canonical form, the surface equation is not a function of the new independent variable, ie. $F(R, T) = 0$.

In this new coordinate system, the source term for the constraint equation is also independent of the dependent variable, ie. $\frac{\d S}{\d R} = f(R, T)$.

The one-parameter group of transformations leaving the surface equation invariant is found by changing variables in the $(x, y)$-place according to

$$
\begin{align*}
  x \mapsto \bar{x}(\epsilon) &= x + \epsilon\xi(x, y) + O(\epsilon^2) \quad& \bar{x}(\epsilon = 0) = x \\
  y \mapsto \bar{y}(\epsilon) &= y + \epsilon\eta(x, y) + O(\epsilon^2) \quad& \bar{y}(\epsilon = 0) = y \\
  p \mapsto \bar{p}(\epsilon) &= p + \epsilon\zeta(x, y, p) + O(\epsilon^2) \quad& \bar{p}(\epsilon = 0) = p
\end{align*}
$$

Where $\zeta$ is related to $\xi$ and $\eta$ by the first prolongation formula

$$
  \bar{p} = \frac{\d\bar{y}}{\d\bar{x}} = \frac{\d\bar{y}/\d x}{\d\bar{x}/\d x} = \frac{p + \epsilon\left(\frac{\partial \eta}{\partial x} + p\frac{\partial \eta}{\partial y} \right)}{1 + \epsilon\left(\frac{\partial \xi}{\partial x} + p\frac{\partial\xi}{\partial y} \right)} = p + \epsilon\left[\eta_x + \left(\eta_y - \eta_x \right)p - \xi_y p^2 \right] + O(\epsilon^2)
$$

as a result

$$
  \zeta(x, y, p) = \eta'(x, y, y') = \eta_x + \left(\eta_y - \eta_x \right)p - \xi_y p^2
$$

The surface equation must remain invariant under the one-group of transformations so that

$$
  F(\bar{x}, \bar{y}, \bar{p}) = e^{\epsilon X} F(x, y, p) = F(x, y, p) + \epsilon XF(x, y, p) + O(\epsilon^2) = 0
$$

where $X = \xi\frac{\partial}{\partial x} + \eta\frac{\partial}{\partial y} + \zeta\frac{\partial}{\partial p}$ is the infinitesimal generator. The two leading terms of the Taylor series expansion of the exponential map define the determining equations for $X$, ie.

$$
  F(x, y, p) = 0 \quad XF(x, y, p) = 0
$$

If an infinitesimal generator $X$ can be constructed from the determining equations, then it is possible to find canonical coordinates from the equations

$$
  X(x, y, p) \begin{bmatrix} 
    R(x, y) \\ 
    S(x, y) \\ 
    T(x, y, p) 
  \end{bmatrix} = \begin{bmatrix} 0 \\ k \\ 0 \end{bmatrix}
$$

for some arbitrary constant $k \neq 0$ (usually set to $k=1$). The dependent coordinate $S$ is determined from the equation

$$
  XS = \left[ \xi(x, y) \frac{\partial}{\partial x} + \eta(x, y) \frac{\partial}{\partial y} \right]S(x, y) = k
$$

The coordinates $R$ and $T$ are invariant under the one-parameter transformation group. These functions obey

$$
\begin{align*}
  XR = \left[ \xi(x, y) \frac{\partial}{\partial x} + \eta(x, y) \frac{\partial}{\partial y} \right]R(x, y) &= 0 \\
  XT = \left[ \xi(x, y) \frac{\partial}{\partial x} + \eta(x, y) \frac{\partial}{\partial y} + \zeta(x, y, p)\frac{\partial}{\partial p} \right]R(x, y) &= 0
\end{align*}
$$

The solutions obey the differential relations

$$
  \frac{\d x}{\xi(x, y)} = \frac{\d y}{\eta(x, y)} = \frac{\d p}{\zeta(x, y, p)}
$$

In the new coordinate system there is a constraint equation

$$
  \frac{\d S}{\d R} = \frac{\d S/\d x}{\d R\d x} = \frac{S_x + S_y p}{R_x + R_y p}
$$

This derivative is independent of the group parameter $\epsilon$. Thus it is independent of $S$ and depends only on the invariant coordinates $R$ and $T$. In the canonical coordinates, the surface equations is $F(R, T) = 0$ and the constraint equation is $\frac{\d S}{\d R} = f(R, T)$.

The solution of the transformed differential equation is found by first solving the surface equation for $T$ as a function for $R$. Then, $S$ can be determined by integrating the constraint equation

$$
  S = \int f\left[R, T(R) \right] \d R + c
$$

The constant $c$ is the image of the group parameter $\epsilon$. The inverse relations $x = x(R, S)$ and $y = y(R, S)$ are used to express the transformed solution in terms of the original coordinates.

### Higher order ODEs

Lie symmetry analysis can be generalized to an $n$th order ODE of the form $F\left(x, y, \dots, y^{(n)}\right) = 0$. The generalized infinitesimal generator becomes

$$
  X = \xi \frac{\partial}{\partial x} + \sum_{i=0}^n \eta^{(i)}\frac{\partial}{\partial y^{(i)}}
$$

The prolongations formulas are recursively related

$$
\begin{align*}
  \eta(x, y) &= \eta(x, y) \\
  \eta^{(1)}\left(x, y, y^{(1)} \right) &= D^{(0)}\eta^{(0)} - y^{(1)}D^{(0)}\xi \\
  \eta^{(2)}\left(x, y, y^{(1)}, y^{(2)} \right) &= D^{(1)}\eta^{(1)} - y^{(2)}D^{(0)}\xi \\
  &\vdots \\
  \eta^{(n)}\left(x, y, \dots, y^{(n)} \right) &= D^{(n-1)}\eta^{(n-1)} - y^{(n)}D^{(0)}\xi
\end{align*}
$$

where

$$
  D^{(n)} = \frac{\partial}{\partial x} + \sum_{i=0}^n \frac{\d y^{(n)}}{\d x}\frac{\partial}{\partial y^{(n)}}
$$

The canonical coordinates contains the independent coordinate $R(x, y)$, the dependent coordinate $S(x, y)$ and constraint coordinates $T^{(i)}\left(x, y, \dots, y^{(i+1)} \right)$. The dependent coordinate is determined from the equation $XS = 1$. The remaining invariant coordinates are obtained from

$$
  \frac{\d x}{\xi} = \frac{\d y}{\eta} = \frac{\d y^{(1)}}{\eta^{(1)}} = \dots = \frac{\d y^{(n)}}{\eta^{(n)}}
$$

In fact, only the first two invariant coordinates $R(x, y)$ and $T(x, y, y')$ need to be computed. The remaining invariant coordinates are given by $\frac{\d T^{(i)}}{\d R^{(i)}}$ for $i = 1, 2, \dots, n-1$. Each of these is of first degree in $y^{(i+1)}$. As a result, the existence of a Lie symmetry can be used to reduced an $n$th order equation to an $(n-1)$th order equation.

## Frobenius method

A second-order linear ODE of the form

$$
  y'' + p(x)y' + q(x)y = r(x)
$$

has a power series solution $y = \sum_{n=0}^\infty a_n (x - x_0)^n$ if the coefficient functions $p$, $q$ and $r$ have valid Taylor series expansion about $x_0$.

The Frobenius method allows power series solutions $y = \sum_{n=0}^\infty a_n (x - x_0)^{n + r}$ for second-order ODEs of the form

$$
  y'' + \frac{p(x)}{x}y' + \frac{q(x)}{x^2}y = 0
$$

if $x_0$ is a regular singularity. $x_0$ is a regular singular point of the ODE if both $(x - x_0)p(x)$ and $(x - x_0)^2 q(x)$ have valid Taylor expansions about $x_0$.

Differentiating the power series gives

$$
\begin{align*}
  y' &= \sum_{n=0}^\infty (n + r)a_n x^{n + r - 1}\\
  y'' &= \sum_{n=0}^\infty (n + r - 1)(n + r)a_n x^{n + r - 2}
\end{align*}
$$

Substituting into the ODE gives

$$
\begin{align*}
  0 &= x^2 \sum_{n=0}^\infty (n + r - 1)(n + r)a_n x^{n + r - 2} + xp(x) \sum_{n=0}^\infty (n + r)a_n x^{n + r - 1} + q(x) \sum_{n=0}^\infty a_n x^{n + r} \\
  &= \sum_{n=0}^\infty (n + r - 1)(n + r)a_n x^{n + r} + p(x) \sum_{n=0}^\infty (n + r)a_n x^{n + r} + q(x) \sum_{n=0}^\infty a_n x^{n + r} \\
  &= \sum_{n=0}^\infty \left[ (n + r - 1)(n + r) + p(x) (n + r) + q(x)\right] a_n x^{n + r} \\
  &= \left[ r(r - 1) + p(x)r + q(x) \right] a_0 x^r + \sum_{n=1}^\infty \left[ (n + r - 1)(n + r) + p(x)(n + r) + q(x) \right]a_n x^{n + r}
\end{align*}
$$

The indicial equation $r(r - 1) + p(x_0)r + q(x_0)$ determines the recursive relations for the solution. For distinct roots $r_1$ and $r_2$ that differ by an integer, the general solution is given by

$$
  y_2 = Cy_1 \ln x + (x - x_0)^{r^2} \sum_{n=0}^\infty A_n (x - x_0)^n
$$

where $y_1$ is the solution corrensponding to $r_1$.

## Damped harmonic oscillator

$$
  m \frac{\d^2 x}{\d t^2} + b \frac{\d x}{\d t} + kx = F_0 \cos{\omega_0 t}
$$

### Universal oscillator equation

The DHO equation can be reduced to a simpler form through nondimensionalization with following transforms of the variables

$$
  \tilde{x} = \frac{x - x_r}{x_s} \qquad \tilde{t} = \frac{t - t_r}{t_s}
$$

Setting the reference values $x_r$ and $t_r$ equal to zero, and substituting back to the equation

$$
\begin{align*}
  m \frac{\d}{\d (t_s \tilde{t})} \left[ \frac{\d (x_s\tilde{x})}{\d (t_s \tilde{t})} \right] + b \frac{\d (x_s\tilde{x})}{\d (t_s \tilde{t})} + k(x_s \tilde{x}) &= F_0 \cos {\omega_0 t_s \tilde{t}} \\
  \frac{mx_s}{t_s^2} \frac{\d^2\tilde{x}}{\d \tilde{t}^2} + \frac{bx_s}{t_s} \frac{\d\tilde{x}}{\d \tilde{t}} + kx_s \tilde{x} &= F_0 \cos {\omega_0 t_s \tilde{t}}
\end{align*}
$$

Divide by the coefficient of the highest order derivative

$$
  \frac{\d^2\tilde{x}}{\d \tilde{t}^2} + \frac{bt_s}{m} \frac{\d\tilde{x}}{\d\tilde{t}} + \frac{kt_s^2}{m} \tilde{x} = \frac{F_0 t_s^2}{m x_s} \cos{\omega_0 t_s \tilde{t}}
$$

Setting the coefficients containing $x_s$ and $t_s$ equal to $1$

$$
\begin{align*}
  \frac{kt_s^2}{m} &= 1 \qquad t_s = \sqrt{\frac{m}{k}} \\
  \frac{F_0 t_s^2}{mx_s} &= 1 \qquad \frac{F_0 m}{mkx_s} = \frac{F_0 }{kx_s} = 1 \qquad x_s = \frac{F_0}{k}
\end{align*}
$$

Substituting back into the equation

$$
\begin{align*}
  \frac{\d^2\tilde{x}}{\d \tilde{t}^2} + \frac{b}{\sqrt{mk}} \frac{\d\tilde{x}}{\d \tilde{t}} + \tilde{x} &= \cos{\left( \omega_0 \sqrt{\frac{m}{k}} \tilde{t} \right)} \\
  \frac{\d^2\tilde{x}}{\d \tilde{t}^2} + \alpha \frac{\d\tilde{x}}{\d \tilde{t}} + \tilde{x} &= \cos{\beta \tilde{t}}
\end{align*}
$$

## Legendre's equation

$$
\begin{gather*}
\begin{aligned}
  (1 - x^2) \frac{\d^2y}{\d x^2} - 2x \frac{\d y}{\d x} + k(k + 1)y &= 0 \\
  \frac{\d^2y}{\d x^2} - \frac{2x}{(1 - x^2)} \frac{\d y}{\d x} + \frac{k(k + 1)}{(1 - x^2)}y &= 0
\end{aligned} \\
  \frac{\d}{\d x} \left[\left(1 - x^2 \right) \frac{\d y}{\d x} \right] + k(k + 1)y = 0
\end{gather*}
$$

The equation can be solved with the power series method about $x_0 = 0$. The following substitutes are used

$$
\begin{align*}
  y(x) &= \sum_{n=0}^\infty a_n x^n \\
  \frac{\d y}{\d x} &= \sum_{n=0}^\infty na_n x^{n - 1} \\
  \frac{\d^2 y}{\d x^2} &= \sum_{n=0}^\infty n(n - 1)a_n x^{n - 2}
\end{align*}
$$

Inserting into the ODE

$$
\begin{gather*}
  (1 - x^2)\sum_{n=0}^\infty n(n - 1)a_n x^{n - 2} - 2x \sum_{n=0}^\infty n a_n x^{n - 1} + k(k + 1) \sum_{n=0}^\infty a_n x^n = 0 \\
  \sum_{n=0}^\infty n(n - 1)a_n x^{n - 2} - \sum_{n=0}^\infty n(n - 1)a_n x^n - \sum_{n=0}^\infty 2n a_n x^n + \sum_{n=0}^\infty k(k - 1)a_n x^n = 0 \\
  \sum_{n=0}^\infty (n + 2)(n + 1)a_{n + 2} x^n - \sum_{n=0}^\infty n(n - 1)a_n x^n - \sum_{n=0}^\infty 2n a_n x^n + \sum_{n=0}^\infty k(k - 1)a_n x^n = 0 \\
  \sum_{n=0}^\infty \left[ (n + 2)(n + 1)a_{n + 2} - n(n - 1)a_n - 2na_n + k(k + 1)a_n \right] x^n = 0 \\
  (n + 2)(n + 1)a_{n + 2} - n(n - 1)a_n - 2na_n + k(k + 1)a_n = 0
\end{gather*}
$$

This gives the recursion relation

$$
\begin{align*}
  a_{n + 2} &= \frac{a_n \left[ n(n - 1) + 2n - k(k + 1) \right]}{(n + 2)(n + 1)} \\
  &= \frac{a_n \left[ (n - k)(n + k) + (n - k) \right]}{(n + 2)(n + 1)} \\
  &= \frac{a_n (n - k)(n + k + 1)}{(n + 2)(n + 1)}
\end{align*}
$$

For even $k$ the even series terminates, and for odd $k$ the odd series terminates. The terminating series form the Legendre polynomials as given by the Rodrigues' formula

$$
  P_k (x) = \frac{1}{2^k k!} \frac{\d^k}{\d x^k} \left[ (x^2 - 1)^k \right]
$$

The derivative can be evaluated with binomial expansion, $(x + y)^n = \sum_{i=0}^n \binom{n}{i} x^{n-i} y^i = \sum_{i=0}^n \frac{n!}{i!(n-i)!} x^{n-i} y^i$, giving

$$
\begin{align*}
  P_k (x) &= \frac{1}{2^k k!} \frac{\d^k}{\d x^k} \sum_{i=0}^k \frac{k!}{i!(k-i)!}\left( x^2 \right)^{k-i} (-1)^i \\
  &= \frac{1}{2^k k!} \sum_{i=0}^k \frac{k!}{i!(k-i)!} (-1)^i \frac{\d^k}{\d x^k} \left[ \left( x^2 \right)^{k-i} \right] \\
  &= \frac{1}{2^k k!} \sum_{i=0}^k \frac{k!}{i!(k-i)!} (-1)^i \frac{\d^k}{\d x^k} \left[ \left( x^2 \right)^{k - i} \right] 
\end{align*}
$$

We have $\frac{\d^n}{\d x^n} \left(x^r \right) = \frac{r!}{(r-n)!} x^{r - n}$

$$
\begin{align*}
  P_k (x) &= \frac{1}{2^k k!} \sum_{i=0}^{\frac{k}{2}|\frac{k-1}{2}} \frac{k!}{i!(k-i)!} (-1)^i \frac{(2k - 2i)!}{k - 2i} x^{k - 2i} \\
  &= \sum_{i=0}^{\frac{k}{2}|\frac{k-1}{2}} \frac{(-1)^i (2k - 2i)!}{2^k(i!)(k - i)! (k - 2i)!} x^{k - 2i}
\end{align*}
$$

## Bessel's equation

$$
  x^2 y'' + xy' + (x^2 - p^2)y = 0
$$

The equation can be solved with the Frobenius method, assuming a power series solution $y = \sum_{n=0}^\infty a_n x^{n + r}$

$$
\begin{align*}
  0 &= \sum_{n=0}^\infty (n + r - 1)(k + r)a_n x^{n + r - 2} + \frac{1}{x} \sum_{n=0}^\infty (n + r)a_n x^{n + r - 1} + \left(1 - \frac{p^2}{x^2} \right) \sum_{n=0}^\infty a_n x^{n + r} \\
  &= \sum_{n=0}^\infty (n + r - 1)(k + r)a_n x^{n + r - 2} + \sum_{n=0}^\infty (n + r)a_n x^{n + r - 2} +  \sum_{n=0}^\infty a_n x^{n + r} - p^2 \sum_{n=0}^\infty a_n x^{n + r - 2} \\
  &= \sum_{n=0}^\infty (n + r - 1)(n + r)a_n x^{n + r - 2} + \sum_{n=0}^\infty (n + r)a_n x^{n + r - 2} +  \sum_{n=2}^\infty a_{n-2} x^{n + r - 2} - p^2 \sum_{n=0}^\infty a_n x^{n + r - 2} \\
  &= r(r - 1)a_0 x^{r - 2} + r(r + 1)a_1 x^{r - 1} + ra_0 x^{r - 2} + (r + 1)a_1 x^{r - 1} - p^2 a_0 x^{r - 2} - p^2 a_1 x^{r - 1} \\
  &\quad + \sum_{n=2}^\infty \left(\left[(n + r - 1)(n + r) + (n + r) - p \right]a_n + a_{n - 2} \right) x^{n + r - 2}
\end{align*}
$$

For $x^{r - 2}$ indicial equation becomes 

$$
\begin{gather*}
  \left[ r(r - 1) + r - p^2 \right] a_0 = 0, \quad a_0 \neq 0 \\
  r(r - 1) + r - p^2 = r^2 - p^2 = (r + p)(r - p) = 0, \quad r = \pm p 
\end{gather*}
$$

For $x^{r - 1}$ the indicial equation gives

$$
  \left[ r(r + 1) + (r + 1) - p^2 \right] a_1 = 0, \quad \Rightarrow a_1 = 0
$$

The recursion relation becomes

$$
\begin{gather*}
  \left[(n + r - 1)(n + r) + (n + r) - p \right]a_n + a_{n - 2} = 0 \\
  a_n = \frac{-a_{n-2}}{(n + r)^2 - p^2}, \quad n = 2k
\end{gather*}
$$

This gives the solution for integer $p$

$$
  y_1 = a_0 \sum_{n=0}^\infty \frac{(-1)^n}{m!\Gamma(n + p + 1)}\left(\frac{x}{p}\right)^{2n + p} = a_0 J_p (x)
$$

where $J_p (x)$ is the Bessel function of the first kind

### Half-integer order ($p = \frac{1}{2}$)

If the roots of the indicial equation are half-integers $r = \pm \frac{1}{2}$, the solutions are of the form

$$
\begin{align*}
  y_1 &= J_{\frac{1}{2}}(x) = \sqrt{x} \sum_{k=0}^\infty \frac{(-1)^k}{2^{2k + \frac{1}{2}}} \frac{1}{k! \Gamma \left(k + \frac{1}{2} + 1 \right)} x^{2k} \\
  y_2 &= C \ln (x) y_1 + x^{-\frac{1}{2}} \sum_{n=0}^\infty A_n x^n \\
  &= C \ln (x) + \sqrt{x} \sum_{k=0}^\infty \frac{(-1)^k}{2^{2k + \frac{1}{2}}} \frac{1}{k! \Gamma \left(k + \frac{1}{2} + 1 \right)} x^{2k} + x^{-\frac{1}{2}} \sum_{n=0}^\infty A_n x^n \\
  &= C \ln (x) \sum_{k=0}^\infty \alpha_k x^{2k + \frac{1}{2}} + \sum_{n=0}^\infty A_n x^{n - \frac{1}{2}}
\end{align*}
$$

The coefficients $C$ and $A_n$ can be determined by plugging $y_2$ into Bessel's equation. The derivatives $y_2$ are

$$
\begin{align*}
  y_2' &= C \ln(x) \sum_{k=0}^\infty \alpha_k \left(2k + \frac{1}{2} \right) x^{2k - \frac{1}{2}} + \frac{C}{x} \sum_{k=0}^\infty \alpha_k x^{2k + \frac{1}{2}} + \sum_{n=0}^\infty A_n \left(n - \frac{1}{2} \right) x^{n - \frac{3}{2}} \\
  &= C \ln(x) \sum_{k=0}^\infty \alpha_k \left(2k + \frac{1}{2} \right) x^{2k - \frac{1}{2}} + C \sum_{k=0}^\infty \alpha_k x^{2k - \frac{1}{2}} + \sum_{n=0}^\infty A_n \left(n - \frac{1}{2} \right) x^{n - \frac{3}{2}} \\ \\
  y_2'' &=  C \ln(x) \sum_{k=0}^\infty \alpha_k \left(2k + \frac{1}{2} \right) \left( 2k - \frac{1}{2} \right) x^{2k - \frac{3}{2}} + \frac{C}{x} \sum_{k=0}^\infty \alpha_k \left(2k + \frac{1}{2} \right) x^{2k - \frac{1}{2}} \\ 
  &\quad +  C \ln(x) \alpha_k \left(2k - \frac{1}{2} \right) x^{2k - \frac{3}{2}} + \sum_{n=0}^\infty A_n \left(n - \frac{1}{2} \right) \left(n - \frac{3}{2} \right) x^{n - \frac{5}{2}} \\
  &= C \ln (x) \sum_{k=0}^\infty \alpha_k \left( 4k^2 - \frac{1}{4} \right) x^{2k - \frac{3}{2}} + C \sum_{k=0}^\infty \alpha_k (4k) x^{2k -\frac{3}{2}} + \sum_{n=0}^\infty A_n \left(n - \frac{1}{2} \right) \left(n - \frac{3}{2} \right) x^{n - \frac{5}{2}}
\end{align*}
$$

Substituting into Bessel's equation $x^2 y'' + xy' + \left( x^2 - \frac{1}{4} \right)y = 0$ gives

$$
\begin{align*}
  0 &= C x^2 \ln (x) \sum_{k=0}^\infty \alpha_k \left( 4k^2 - \frac{1}{4} \right) x^{2k - \frac{3}{2}} + C x^2 \sum_{k=0}^\infty \alpha_k (4k) x^{2k -\frac{3}{2}} \\
  &\quad + x^2 \sum_{n=0}^\infty A_n \left(n - \frac{1}{2} \right) \left(n - \frac{3}{2} \right) x^{n - \frac{5}{2}} + Cx \ln(x) \sum_{k=0}^\infty \alpha_k \left(2k + \frac{1}{2} \right) x^{2k - \frac{1}{2}} \\
  &\quad + Cx \sum_{k=0}^\infty \alpha_k x^{2k - \frac{1}{2}} + x \sum_{n=0}^\infty A_n \left(n - \frac{1}{2} \right) x^{n - \frac{3}{2}} + C \left( x^2 - \frac{1}{4} \right) \ln (x) \sum_{k=0}^\infty \alpha_k x^{2k + \frac{1}{2}} \\
  &\quad + \left( x^2 - \frac{1}{4} \right) \sum_{n=0}^\infty A_n x^{n - \frac{1}{2}}
\end{align*}
$$

Since we have

$$
\begin{align*}
  y_1' &= \sum_{k=0}^\infty \alpha_k \left( 2k + \frac{1}{2} \right) x^{2k - \frac{1}{2}} \\
  y_1'' &= \sum_{k=0}^\infty \alpha_k \left( 4k^2 - \frac{1}{4} \right) x^{2k - \frac{3}{2}}
\end{align*}
$$

and that $y_1$ itself is a solution to the Bessel equation, the terms giving $C \ln(x) \left[ x^2 y_1'' + xy_1' + \left( x^2 - \frac{1}{4} \right)y_1 \right] = 0$ cancel out, leaving

$$
\begin{align*}
  0 &= C x^2 \sum_{k=0}^\infty \alpha_k (4k) x^{2k -\frac{3}{2}} + x^2 \sum_{n=0}^\infty A_n \left(n - \frac{1}{2} \right) \left(n - \frac{3}{2} \right) x^{n - \frac{5}{2}} + Cx \sum_{k=0}^\infty \alpha_k x^{2k - \frac{1}{2}} \\ 
  &\quad + x \sum_{n=0}^\infty A_n \left(n - \frac{1}{2} \right) x^{n - \frac{3}{2}} + \left( x^2 - \frac{1}{4} \right) \sum_{n=0}^\infty A_n x^{n - \frac{1}{2}} \\
  &= C \sum_{k=0}^\infty \alpha_k (4k) x^{2k + \frac{1}{2}} + \sum_{n=0}^\infty A_n \left(n - \frac{1}{2} \right) \left(n - \frac{3}{2} \right) x^{n - \frac{1}{2}} + Cx \sum_{k=0}^\infty \alpha_k x^{2k - \frac{1}{2}} \\ 
  &\quad + \sum_{n=0}^\infty A_n \left(n - \frac{1}{2} \right) x^{n - \frac{1}{2}} + \sum_{n=0}^\infty A_n x^{n + \frac{3}{2}}  - \frac{1}{4} \sum_{n=0}^\infty A_n x^{n - \frac{1}{2}} \\
  &= \sum_{n=2}^\infty A_n n(n - 1) x^{n - \frac{1}{2}} + C \sum_{k=0}^\infty \alpha (4k + 1) x^{2k + \frac{1}{2}} + \sum_{n=0}^\infty A_n x^{n + \frac{3}{2}} \\
  &= \sum_{n=2}^\infty \left[ A_n n(n - 1) + A_{n - 2} \right] x^{n - \frac{1}{2}} + C \sum_{k=0}^\infty \alpha (4k + 1) x^{2k + \frac{1}{2}}
\end{align*}
$$

A power-by-power comparison gives

$$
  0 \cdot \sqrt{x} = -C \alpha_0 \sqrt{x} \quad \therefore C = 0  
$$

resulting in the recursion relation

$$
\begin{gather*}
  n(n - 1) A_n + A_{n - 2} = 0 \\
  A_n = \frac{- A_{n - 2}}{n(n - 1)}
\end{gather*}
$$

The even and odd indexed coefficients take the forms

$$
\begin{align*}
  A_{2j} &= \frac{(-1)^j A_0}{(2j)!} \\
  A_{2j + 1} &= \frac{(-1)^j A_1}{(2j + 1)!}
\end{align*}
$$

giving the solution 

$$
  y_2 = \frac{A_0}{\sqrt{x}} \sum_{j=0}^\infty \frac{(-1)^j}{(2j)!}x^{2j} + \frac{A_1}{\sqrt{x}} \sum_{j=0}^\infty \frac{(-1)^j}{(2j + 1)!}x^{2j + 1} = \frac{A_0}{\sqrt{x}} \cos x + \frac{A_1}{\sqrt{x}} \sin x
$$

Rearranging the solution $y_1$

$$
  y_1 = J_{\frac{1}{2}}(x) = \sqrt{\frac{x}{2}} \sum_{k=0}^\infty \frac{(-1)^k}{2^{2k}} \frac{1}{k! \Gamma \left( k + \frac{3}{2} \right)}
$$

Rewriting the denominator (omitting the summation symbol)

$$
\begin{align*}
  2^{2k} k! \Gamma \left( k + \frac{3}{2} \right) &= 2^{2k} k! \frac{1}{2} \Gamma\left( \frac{1}{2} \right)\prod_{j=1}^k \left( j + \frac{1}{2}\right) \\
  &= 2^k k! \frac{1}{2} \Gamma\left( \frac{1}{2} \right) \prod_{j=1}^k \left( 2j + 1\right) \\
  &= (2k + 1)! \frac{1}{2} \Gamma\left( \frac{1}{2} \right) \\
  &= (2k + 1)! \frac{1}{2} \sqrt{\pi}
\end{align*}
$$

The solution becomes

$$
\begin{align*}
  y_1 &= \sqrt{\frac{2}{\pi x}} \sum_{k=0}^\infty \frac{(-1)^k}{(2k + 1)!} x^{2k + 1} \\
  &= \sqrt{\frac{2}{\pi x}} \sin (x)
\end{align*}
$$

Since Bessel's equation is linear, any linear combination of $y_1$ and $y_2$ is also a solution. Therefore, $y_2$ can be transformed to give 

$$
  y_2 = \sqrt{\frac{2}{\pi x}} \cos (x)
$$

## Riccati equation

$$
  \frac{\d y}{\d x} = a(x) + b(x)y + c(t)y^2
$$

### Reduction to second order linear equation

The quadratic term can be eliminated with the following transform

$$
\begin{align*}
  y &= -\frac{1}{cu} \frac{\d u}{\d x} = -\frac{1}{c} \frac{\d}{\d x} (\ln{u}) \\
  \frac{\d y}{\d x} &= -\frac{1}{cu} \frac{\d^2u}{\d x^2} - \frac{1}{u}\frac{\d u}{\d x}\frac{\d}{\d x}\left( \frac{1}{c} \right) - \frac{1}{c} \frac{\d u}{\d t}\frac{\d}{\d t}\left( \frac{1}{u} \right) \\
  &= -\frac{1}{cu} \frac{\d^2u}{\d x^2} + \frac{1}{u}\frac{\d u}{\d x}\frac{1}{c^2}\frac{\d c}{\d x} + \frac{1}{c}\frac{\d u}{\d x}\frac{1}{u^2}\frac{\d u}{\d x} \\
  &= -\frac{1}{cu} \frac{\d^2u}{\d x^2} + \frac{1}{uc^2}\frac{\d c}{\d x}\frac{\d u}{\d x} + \frac{1}{cu^2}\left(\frac{\d u}{\d x}\right)^2
\end{align*}
$$

Substituting into the Riccati equation

$$
\begin{gather*}
  -\frac{1}{cu} \frac{\d^2u}{\d x^2} + \frac{1}{uc^2}\frac{\d c}{\d x}\frac{\d u}{\d x} + \frac{1}{cu^2}\left(\frac{\d u}{\d x}\right)^2 = a - b \frac{1}{cu} \frac{\d u}{\d x} + c \left(-\frac{1}{cu} \frac{\d u}{\d x} \right)^2 \\
  -\frac{\d^2u}{\d x^2} + \frac{1}{c}\frac{\d c}{\d x}\frac{\d u}{\d x} = acu - b\frac{\d u}{\d x} \\
  \frac{\d^2u}{\d x^2} - \left( \frac{1}{c}\frac{\d c}{\d x} + b \right)\frac{\d u}{\d x} + acu = 0
\end{gather*}
$$

#### Solution with constant coefficients

With constant coefficients the transformed Riccati equation simplifies to the homogenous equation

$$
  \frac{\d^2u}{\d x^2} - b \frac{\d u}{\d x} + acu = 0
$$

The characteristic quadratic equation becomes

$$
\begin{gather*}
  r^2 - br + ac = 0 \\
  r = \frac{b \pm \sqrt{b^2 - 4ac}}{2} = \frac{b \pm R}{2} 
\end{gather*}
$$

which gives the solution

$$
\begin{align*}
  u &= C_1 e^{\frac{b + R}{2}x} + C_2 e^{\frac{b - R}{2}x} \\
  \frac{\d u}{\d x} &= C_1 \frac{b + R}{2} e^{\frac{b + R}{2}x} + C_2 \frac{b - R}{2} e^{\frac{b - R}{2}x}
\end{align*}
$$

Retransforming back to $y$

$$
\begin{align*}
  y(x) &= -\frac{1}{cu} \frac{\d u}{\d x} \\
  &= -\frac{1}{2c} \frac{C_1(b + R) e^{\frac{b + R}{2}x} + C_2 (b - R) e^{\frac{b - R}{2}x}}{C_1 e^{\frac{b + R}{2}x} + C_2 e^{\frac{b - R}{2}x}} \\
  &= -\frac{1}{2c} \frac{C_1(b + R) e^{\frac{R}{2}x} + C_2 (b - R) e^{-\frac{R}{2}x}}{C_1 e^{\frac{R}{2}x} + C_2 e^{-\frac{R}{2}x}} \\
  &= -\frac{1}{2c} \frac{C_1(b + R) e^{Rx} + C_2 (b - R)}{C_1 e^{Rx} + C_2} \\
  &= -\frac{1}{2c} \frac{D(b + R) e^{Rx} + (b - R)}{D e^{Rx} + 1} \qquad D = \frac{C_1}{C_2}
\end{align*}
$$

For the initial condition $y(0) = 0$, the coefficient $D$ becomes

$$
\begin{align*}
  0 &= -\frac{1}{2c} \frac{D(b + R) + (b - R)}{D + 1} \\
  &= D(b + R) + (b - R) \\
  D &= -\frac{b - R}{b + R}
\end{align*}
$$

giving the solution

$$
\begin{align*}
  y(x) &= -\frac{1}{2c} \frac{-(b - R) e^{Rx} + (b - R)}{-\frac{b - R}{b + R} e^{Rx} + 1} \\
  &= \frac{1}{2c} \frac{(b - R) \left(e^{Rx} - 1 \right)}{-\frac{b - R}{b + R} e^{Rx} + 1} \\
  &= \frac{1}{2c} \frac{(b^2 - R^2) \left(e^{Rx} - 1 \right)}{-(b - R) e^{Rx} + (b + R)} \qquad R^2 = b^2 - 4ac \\
  &= \frac{2a \left(e^{Rx} - 1 \right)}{(R - b) e^{Rx} + (R + b) - (R - b) + (R - b)} \\
  &= \frac{2a \left(e^{Rx} - 1 \right)}{(R - b) \left(e^{Rx} - 1\right) + 2M}
\end{align*}
$$

## Numerical methods

### Euler method

A first-order ODE of the form

$$
  y'(x) = f\left[x, y(x) \right]
$$

gives the slope of the tangent line to the integral curve $y$ at any point $x$. Euler's method approximates the curve $y$ over the interval $[x_i, x_{i + 1}]$ with the curve's tangent line at point $(x_i, y(x_i))$. The equation of the tangent line at $(x_i, y(x_i))$ is

$$
  y' = y(x_i) + f\left[x_i, y(x_i)\right] (x - x_i)
$$

Introducing a step $h$ so that $x_{i + 1} = x_i + h$ yields the iterative equation

$$
  y_{i + 1} = y_n + hf(x_i, y_i)
$$

which start from the initial value $y(x_0) = y_0$.

### Runge-Kutta methods

The Runge-Kutta methods is a family of numerical procedures for approximating solutions of ODE that extends on the Euler method. The iterative equation for an explicit $s$ order Runge-Kutta method with step $h$ is given by

$$
  y_{n + 1} = y_n + h\sum_{i=1}^s b_i k_i
$$

where

$$
\begin{align*}
  k_1 &= f(t_n, y_n) \\
  k_2 &= f\left[t_n + c_2 h, y_n + h(a_{21}k_1)\right] \\
  k_3 &= f\left[t_n + c_3 h, y_n + h(a_{31}k_1 + a_{32}k_2)\right] \\
  &\vdots \\
  k_s &= f\left[t_n + c_s h, y_n + h\left(a_{s1} k_1 + a_{s2}k_2 + \dots + a_{s,s-1}k_{s-1} \right)\right]
\end{align*}
$$

The matrix $[a_{ij}]$ is called the Runge-Kutta matrix, while $b_i$ and $c_i$ are known as the weights and nodes. 

The most widely known member of the Runge-Kutta family is the fourth order method, which is generally referred to as "RK4". For an initial value problem $\dot{y}(t) = f[t, y(t)], \, y(t_0) = y_0$, the RK4 iterative equation is given by

$$
  y_{n + 1} = y_n + \frac{1}{6}h \left( k_1 + 2k_2 + 2k_3 + k_4 \right)
$$

where

- $k1$ is the slope the beginning of the interval $[t_n, t_n + h]$ (Euler's method)
$$
  k_1 = f(t_n, y_n)
$$
- $k_2$ is the slope at the midpoint of the interval using $y$ and $k_1$
$$
  k_2 = f\left( t_n + \frac{h}{2}, y_n + h\frac{k_1}{2} \right)
$$
- $k_3$ is again the slope at the midpoint of the interval, but now using $y$ and $k_2$
$$
  k_3 = f\left( t_n + \frac{h}{2}, y_n + h\frac{k_2}{2} \right)
$$
- $k_4$ is the slope at the end of the interval using $y$ and $k_3$
$$
  k_4 = f\left( t_n + h, y_n + hk_3 \right)
$$

#### Lua implementation

```lua

-- Differential equation y'(t) = f(t, y)
function f(t, y)
  return y * math.cos(t + math.sqrt(1 + y))
end

-- Runge-Kutta 4th order iteration
function rk4(h, t, y)
  k1 = h * f(t, y)
  k2 = h * f(t + h/2, y + k1/2)
  k3 = h * f(t + h/2, y + k2/2)
  k4 = h * f(t + h, y + k3)
  return {t + h, y + (k1 + 2*k2 + 2*k3 + k4)/6}
end

function print_coords(tMax, nPoints, option)

  -- Initial conditions
  local t0 = 0.
  local y0 = 1.
    
  -- Step size
  local h = (tMax - t0) / (nPoints - 1)
    
  if option~=[[]] then
    tex.sprint("\\addplot["..option.."] coordinates{")
  else
    tex.sprint("\\addplot coordinates{")
  end
    
  tex.sprint("("..t0..","..y0..")") -- Print first coordinates
    
  for i=1, nPoints do
    m = rk4(h, t, y)
    tex.sprint("("..m[1]..","..y..")")
  end
    
  tex.sprint("}")  
end
```