---
title: 'Statistics'
subject: 'Mathematics'
showToc: true
---

# Statistical model

A statistical model describes the process of generating sample data from a population, usually in the form of mathematical relationship between one or more random variables and deterministic parameters.

<MathBox title='Statistical model' boxType='definition'>
A statistical model can formally be defined by a pair $(S,\mathcal{P})$ where $S$ is the sample space and $\mathcal{P}$ is a set of probability distributions on $S$. The set $\mathcal{P}$ is almost always parametrized

$$
  \mathcal{P} = \Set{ P_\theta : \theta\in\Theta }
$$

where $\Theta$ is the parameter space of the model. The observed outcome of a statistical experiment has the form $\boldsymbol{x} = (x_i)_{i=1}^n \in S$, where $x_i$ is the vector of measurements for the $i$-th object. 

</MathBox> 

<MathBox title='Empirical distribution' boxType='definition'>
Suppose $\boldsymbol{x} = (x_i)_{i=1}^{n\in\N}\in S$ is a sample. The empirical distribution associated with $\boldsymbol{X}$ assigns the probability $\frac{1}{n}$ at each $x_i$. If the sample values are distinct, the empirical distribution is the discrete uniform distribution. Generally, if $x$ occurs $k$ times in the sample data, the empirical distribution assigns probability $\frac{k}{n}$ to $x$. Thus, every finite data set defines a probability distribution.
</MathBox> 

<MathBox title='Statistics' boxType='definition'>
Suppose $\boldsymbol{x} = (x_i)_{i=1}^{n\in\N}\in S$ is a sample. A statistic $w = w(\boldsymbol{x}):S\to \Theta$ is an observable function of the sample $\boldsymbol{x}$ where $\Theta$ is the parameter space.  
</MathBox> 

<MathBox title='Equivalence of statistics' boxType='proposition'>
Suppose $\boldsymbol{x} = (x_i)_{i=1}^{n\in\N}\in S$ is a sample. Statistics $u$ and $v$ on $\boldsymbol{x}$ are equivalent if and only if for any $\boldsymbol{x},\boldsymbol{y}\in S$, $u(\boldsymbol{x}) = u(\boldsymbol{y})$ if and only if $v(\boldsymbol{x}) = v(\boldsymbol{y})$. This defines a equivalence relation on the collection of statistics for a given statistical model, which satisfies for arbitrary statistics $u, v$ and $w$

1. $u$ is equivalent to $u$ (reflexivity)
2. if $u$ is equivalent to $v$, then $v$ is equivalent to $u$ (symmetry)
3. if $u$ is equivalent to $v$ and $v$ is equivalent to $w$, then $u$ is equivalent to $w$ (transitivity)
</MathBox>

## Statistical inference

There are two broad branches of statistics
- descriptive statistics
- inferential statistics

Descriptive statistics refers to methods for summarizing and displaying sample data. The methods usually involve computing various statistics. In the context of descriptive statistics, the term parameter refers to a characteristic of the entire population.

Inferential statistics describes a statistical experiment as a random process with a probability measure $\mathbb{P}$ on an underlying sample space. The sample $\boldsymbol{x}$ of the experiment is an observed value of a random variable $\boldsymbol{X}$ with unknown distribution defined on this probability space. The goal of statical inference is to draw inferences about the distribution of $\boldsymbol{X}$ from the observed value $\boldsymbol{x}$. In inferential statistics, a statistic is itself a random variable, while a parameter refers to a characteristic of the distribution of $\boldsymbol{X}$.

<MathBox title='Random sample' boxType='definition'>
Suppose $\boldsymbol{X} = (X_i)_{i=1}^{n\in\N}$ is an observable random variable for a statistical experiment. The most common and important special case of the inferential statistical model occurs when $\boldsymbol{X}$ is a sequence of independent and identically distributed (i.i.d.) random variables. In this case $\boldsymbol{X}$ represent independent copies of an underlying measurement vector $X$, and $\boldsymbol{X}$ is called a random sample of size $n$ from the distribution of $X$.
</MathBox>

<MathBox title='Parameter' boxType='definition'>
Suppose $\boldsymbol{X} = (X_i)_{i=1}^{n\in\N}$ is an observable random variable for a statistical experiments. A parameter $\boldsymbol{\theta}$ is a function of distribution of $\boldsymbol{X}$ taking values in a parameter space $\Theta$.
</MathBox>

Typically, the distribution of an obervable random variable $\boldsymbol{X}$ will have $k\in\N^+$ real parameters of interest, so that $\boldsymbol{\theta} = (\theta_i)_{i=1}^{k\in\N}\in T\subseteq\R^k$

# Random samples

<MathBox title='Law of large numbers' boxType='theorem'>
Let $(X_i)_{i=1}^{n\in\N}$ be a sequence of independent and identically distributed integrable random variables with expected value $\mathbb{E}(X_i) = \mu$. Suppose $\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i$.

**Weak law of large numbers (Kinchin's law):** The sample average $\bar{X}_i$ converges in probability to $\mu$, i.e. $\bar{X}_n \xrightarrow[\textrm{i.p.}]{n\to\infty} \mu$, or for every $\varepsilon > 0$

$$
\begin{gather*}
  \lim_{n\to\infty} \mathbb{P}\left(|\bar{X}_n - \mu| < \varepsilon \right) = 1 \\
  \iff \lim_{n\to\infty} \mathbb{P}\left(|\bar{X}_n - \mu| > \varepsilon \right) = 0
\end{gather*}
$$

**Strong law of large numbers (Kolmogorov's law):** The sample average $\bar{X}_i$ converges almost surely to $\mu$, i.e. $\bar{X}_n \xrightarrow[\textrm{a.s.}]{n\to\infty}_ \mu$, or

$$
  \mathbb{P}\left(\lim_{n\to\infty} \bar{X}_n = \mu \right) = 1
$$

<details>
<summary>Proof</summary>

The weak law follows from Chebyshev's inequality

$$
  \mathbb{P}\left(|\bar{X}_n - \mu| > \varepsilon \right)\leq\frac{\mathrm{var}(\bar{X}_n)}{\varepsilon^2} = \frac{\sigma^2}{n\varepsilon^2} \xrightarrow{n\to\infty} 0
$$

The strong law is proved in three major steps. The first step is to show that with probability $1$ that $\bar{X}_{n^2} \xrightarrow{n\to\infty}\mu$. From Chebyshev's inequality,

$$
  \mathbb{P}\left(|\bar{X}_{n^2} - \mu| > \varepsilon \right)\leq\frac{\mathrm{var}(\bar{X}_n)}{\varepsilon^2} = \left(\frac{\sigma}{n\varepsilon}\right)^2 \xrightarrow{n\to\infty} 0
$$

Since $\sum_{n\N} \left(\frac{\sigma}{n\varepsilon}\right)^2 < \infty$, it follows from the first Borelli-Cantelli lemma that for every $\varepsilon > 0$

$$
  \mathbb{P}\left(|\bar{X}_{n^2} - \mu| > \varepsilon \textrm{ for infinitely many }n\in\N_+ \right) = 0
$$

From Boole's inequality it follows that for some rational $\varepsilon > 0$

$$
  \mathbb{P}\left(|\bar{X}_{n^2} - \mu| > \varepsilon \textrm{ for infinitely many }n\in\N^+ \right) = 0
$$

showing that $\bar{X}_{n^2}$ converges almost surely to $\mu$.

In the second step, we show that if the underlying sampling variable is nonnegative, so that $\mathbb{P}(X\geq 0) = 1$, then $\bar{X}_n\xrightarrow[\textrm{a.s.}]{n\to\infty}\mu$. Let $Y_n = \sum_{i=1}^n X_i$ so that $\bar{X}_n = Y_n / n$. Note first that $Y_n$ is almost surely increasing in $n$. For $n\in\N$ be the unique positive integer such that $k_n^2 \leq n < (k_n + 1)^2$. From the increasing propery it follows that (almost surely)

$$
  \frac{Y_{k_n^2}}{(k_n + 1)^2} \leq \frac{Y_n}{n} \leq \frac{Y_{(k_n + 1)^2}}{k_n^2}
$$

From the first step

$$
  \frac{Y_{k_n^2}}{(k_n + 1)^2} = \frac{Y_{k_n^2}}{k_n^2}\frac{k_n^2}{(k_n + 1)^2} \xrightarrow[\textrm{a.s.}]{n\to\infty}\mu
$$

Similarly,

$$
  \frac{Y_{(k_n + 1)^2}}{k_n^2} = \frac{Y_{(k_n + 1)^2}}{(k_n + 1)^2}\frac{(k_n + 1)^2}{k_n^2} \xrightarrow[\textrm{a.s.}]{n\to\infty}\mu
$$

By the squeeze theorem for limits it follows that $\bar{X}_n = Y_{n}/n \xrightarrow[\textrm{a.s.}]{n\to\infty}\mu$.

Finally, we relax the condition that the underlying sampling variable is $X$ is nonnegative. From step two, it follows that

$$
\begin{align*}
  \frac{1}{n}\sum_{i=1}^n X_i^+ &\xrightarrow[\textrm{a.s.}]{n\to\infty} \mathbb{E}(X^+) \\
  \frac{1}{n}\sum_{i=1}^n X_i^- &\xrightarrow[\textrm{a.s.}]{n\to\infty} \mathbb{E}(X^-)
\end{align*}
$$

From the linearity of expected value

$$
\begin{align*}
  \frac{1}{n}\sum_{i=1}^n X_i &= \frac{1}{n}\sum_{i=1}^n \left(X_i^+ - X_i^+\right) \\
  &= \frac{1}{n}\sum_{i=1}^n X_i^+ - \frac{1}{n}\sum_{i=1}^n X_i^- \\
  &\xrightarrow[\textrm{a.s.}]{n\to\infty} \mathbb{E}(X^+) - \mathbb{E}(X^+) = \mathbb{E}(X^+ - X^-) = \mathbb{E}(X) = \mu
\end{align*}
$$
</details>
</MathBox>

## Partial sum process

<MathBox title='Partial sum process' boxType='definition'>
Suppose $\boldsymbol{X} = (X_n)_{n\in\N}$ is a sequence of independent, identically distributed random variables with common probability density function $f$, mean $\mu$ and standard deviation $\sigma\in(0,\infty)$. Let $Y_n = \sum_{i=1}^n X_i$ with $Y_0 = 0$. The random process $\boldsymbol{Y} = (Y_n)_{n\in\N^+}$ is called the partial sum process associated with $\boldsymbol{X}$.
</MathBox>

<MathBox title='Properties of partial sum processes' boxType='proposition'>
Let $\boldsymbol{Y} = (Y_n)_{n\in\N^+}$ be a partial sum process for a sample variable $X$ with mean $\mu$, variance $\sigma^2$ and probability density function $f$. If $m, n\in\N$ with $m\leq n$ then

1. $Y_n - Y_m$ has the same distribution as $Y_{n-m}$, implying that $\boldsymbol{Y}$ has stationary increments
2. if $(n_i)_{i\in\N}$ is an increasing sequence then $(Y_{n_i} - Y_{n_{i-i}})_{i\in\N}$ is a sequence of independent random variables, implying that $\mathbb{Y}$ has independent increments
3. $\mathbb{E}(Y_n) = n\mu$
4. $\mathrm{var}(Y_n) = n\sigma^2$
5. $\mathrm{cov}(Y_m, Y_n) = m\sigma^2$
6. $\mathrm{cor}(Y_m, Y_n) = \sqrt{\frac{m}{n}}$
7. $\mathbb{E}(Y_m Y_n) = m\sigma^2 + mn\mu^2$
8. the probability density function of $Y_n$ is the convolution power of $f$ of order $n$, i.e. $f^{*n}$
  a. if $(n_i)_{i=1}^{k\in\N}$ is a strictly increasing sequence then $(Y_{n_i})_{i=1}^{k\in\N}$ has joint probability density function for $\boldsymbol{y} = (y_i)_{i=1}^{k\in\N} \in\R^k$

$$
\begin{align*}
  f_{n_1, n_2,\dots,n_k} (\boldsymbol{y}) &= f^{*n_1}(y_1)\prod_{i=2}^k f^{*(n_i - n_{i-1})}(y_i - y_{i-1})
\end{align*}
$$

8. if $\mathbf{X}$ has moment generating function $G$, then $Y_n$ has moment generating function $G^n$

<details>
<summary>Proof</summary>

1. Note that $Y_n - Y_m = sum_{i=m+1}^{n} X_i$, which is the sum of $n-m$ independent variables, each with the common distribution. Conversely, $Y_{n-m}$ is also the sum of $n-m$ independent variables, each with the common distribution.

2. The terms in the sequence of increments $(Y_{n_i} - Y_{n_{i-i}})$ are sums over disjoint collections of terms in the sequence $\boldsymbol{X}$. Since the sequence $\boldsymbol{X}$ is independent, so is the sequence of increments 

3. This follows from the linear property of expected value 

$$
  \mathbb{E}(Y_n) = \mathbb{E}\left(\sum_{i=i}^n X_i \right) = \sum_{i=1}^n \mathbb{E}(X_i) = n\mu
$$

4. By independence 

$$
  \mathrm{var}(Y_n) = \mathrm{var}\left(\sum_{i=i}^n X_i \right) = \sum_{i=1}^n \mathrm{var}(X_i) = n\sigma^2
$$

5. Note that $Y_n = Y_m + (Y_n - Y_m)$ giving

$$
\begin{align*}
  \mathrm{cor}(Y_m, Y_n) &= \mathrm{cor}(Y_m, Y_m) + \mathrm{cor}(Y_m, Y_n - Y_m) \\
  &= \mathrm{var}(Y_m) + 0 \\
  &= m\sigma^2
\end{align*}
$$

6.
$$
\begin{align*}
  \mathrm{cor}(Y_m, Y_n) &= \frac{\mathrm{cov}(Y_m, Y_n)}{\mathrm{sd}(Y_m) \mathrm{sd}(Y_m)} \\
  &= \frac{m\sigma^2}{\sqrt{m\sigma^2}\sqrt{n\sigma^2}} = \sqrt{\frac{m}{n}}
\end{align*}
$$

7.
$$
  \mathbb{E}(Y_m Y_n) = \mathrm{cov}(Y_m, Y_n) + \mathbb{E}(Y_m)\mathbb{E}(Y_n) = m\sigma^2 + m\mu n\mu
$$

8. The probability density function (PDF) of a sum of independent variables is the convolution of the PDFs of the terms.

9. This follows from the fact that the generating function of a sum of independent variables is the product of the generating functions of the terms.
</details>
</MathBox>

## Central limit theorem

<MathBox title='Central limit theorem' boxType='theorem'>
Let $\boldsymbol{Y}$ be a partial sum process of a sample variable $X$ with mean $\mu$, variance $\sigma^2$ and characteristic function

$$
  \chi_n (t) = \mathbb{E}\left[\exp\left(it\frac{X - \mu}{\sigma} \right)\right],\quad t\in\R
$$

Define the common standard score 

$$
  Z_n = \frac{Y_n - n\mu}{\sqrt{n}\sigma} = \frac{\bar{X}_n - \mu}{\sigma/\sqrt{n}}
$$

with characteristic function

$$
  \chi_n (t) = \mathbb{E}(e^{itZ_n}),\quad t\in\R
$$

In the limit $n\to\infty$, the distribution of $Z_n$ converges to the standard normal distribution, i.e.

$$
  \lim_{n\to\infty} \chi_n (t) = e^{-t^2/2}
$$

<details>
<summary>Proof</summary>

Noting that

$$
  Z_n = \frac{\bar{X}_n - \mu}{\sigma/\sqrt{n}} = \sum_{i=1}^n \frac{X_i - \mu}{\sqrt{n}\sigma}
$$

we may rewrite

$$
\begin{align*}
  \chi_n (t) &= \mathbb{E}\left[\exp\left(itZ_n\right)\right] = \mathbb{E}\left[\exp\left(i\frac{t}{\sqrt{n}}\sum_{i=1}^n \frac{X_i - \mu}{\sigma} \right)\right] \\
  &= \mathbb{E}\left[\prod_{i=1}^n\exp\left(i\frac{t}{\sqrt{n}} \frac{X_i - \mu}{\sigma} \right)\right] = \prod_{i=1}^n \mathbb{E}\left[\exp\left(i\frac{t}{\sqrt{n}} \frac{X_i - \mu}{\sigma} \right)\right] \\
  &= \chi^n \left(\frac{t}{\sqrt{n}}\right)
\end{align*}
$$

Noting that $\chi(0)=1$, $\chi'(0)=0$, $\chi''(0) = -1$, the second order Taylor expansion of $chi\left(\frac{t}{\sqrt{n}}\right)$ about $t=0$ is

$$
  \chi\left(\frac{t}{\sqrt{n}}\right) = 1 - \frac{t^2}{2n} + o\left(\frac{t^2}{n} \right)
$$

In the limit $n\to\infty$, the higher order terms $o\left(\frac{t^2}{n}\right)$ vanish and noting that $e^x = \lim_{n\to\infty} \left(1 + \frac{x_n}{n} \right)^n$, we get

$$
  \lim_{n\to\infty} \chi^n \left(\frac{t}{\sqrt{n}}\right) = e^{-\frac{t^2}{2}}
$$
</details>
</MathBox>

## Sample moments

### Sample mean

<MathBox title='Sample mean' boxType='definition'>
Suppose $\boldsymbol{x}\in S\subseteq\R^n$ is a sample of size $n\in\N^+$ from a real-valued variable. The *sample mean* is simply the arithmetic average of the sample values

$$
  m \equiv \bar{x} := \frac{1}{n}\sum_{i=1}^n x_i
$$
</MathBox>

<MathBox title='Properties of sample mean' boxType='proposition'>
Suppose $\boldsymbol{x}, \boldsymbol{y}\in S\subseteq\R^n$ are real-valued samples of size $n\in\N^+$, and that $a, b\in\R$ are constants. The sample mean has the following properties

1. Linearity: $m(a\boldsymbol{x} + b\boldsymbol{y}) = am(\boldsymbol{x}) + bm(\boldsymbol{y})$
2. Positivity: if $x_i \geq 0$ for each $i$ then $m(\boldsymbol{x})\geq 0$. The inequality becomes strict if $x_j > 0$ for some $j$.
3. Ordering: if $x_i \leq y_i$ for each $i$ then $m(\boldsymbol{x})\leq m(\boldsymbol{y})$. The inequality becomes strict if $x_j < y_j$ for some $j$.
4. If $\boldsymbol{c}$ is a sample of a constant $c\in\R$ then $m(\boldsymbol{c}) = c$.

<details>
<summary>Proof</summary>

1. 
$$
  m(a\boldsymbol{x} + b\boldsymbol{y}) &= \frac{1}{n}\sum_{i=1}^n (a x_i + by_i) \\
  &= \frac{a}{n}\sum_{i=1}^n x_i + \frac{b}{n}\sum_{i=1}^n y_i \\
  &= am(\boldsymbol{x}) + bm(\boldsymbol{y})
$$

2. This follows immediately from the definition.

3. Note that $x_i \leq y_i \iff y_i - x_i \geq 0$ for each $i$, such that $0\leq m(\boldsymbol{y}-\boldsymbol{x}) = m(\boldsymbol{y}) - m(\boldsymbol{x})$ and hence $m(\boldsymbol{x})\leq m(\boldsymbol{y})$.
</details>

4. Note that

$$
  m(\boldsymbol{c}) = \frac{1}{n}\sum_{i=1}^n c_i = \frac{nc}{n} = c
$$
</MathBox>

#### Empirical statistics

<MathBox title='Relative frequency' boxType='definition'>
Suppose $\boldsymbol{x}\in S$ is a sample of size $n\in\N^+$. For $A\subseteq S$, the frequency of $A$ corresponding $\boldsymbol{x}$ is the number of data values that are in $A$

$$
  n(A) = \sum_{i=1}^n \mathbf{1}(x_i \in A)
$$

The relative frequency of $A$ corresponding to $\boldsymbol{x}$ is the proportion of sample values that are in $A$

$$
  p(A) = \frac{n(A)}{n} = \frac{1}{n}\sum_{i=1}^n \mathbf{1}(x_i \in A)
$$
</MathBox>

<MathBox title='Empirical probability distribution' boxType='proposition'>
The relative frequency function $p:S\to[0,\infty)$ is a probability measure on $S$ which satisfies

1. $p(A)\geq 0$ for every $A\subseteq S$
2. $p(S) = 1$
3. if $\Set{A_j}_{j\in J\subseteq\N}$ is a countable collection of pairwise disjoint subsets of $S$, then

$$
  p\left(\bigcup_{j\in J} A_j \right) = \sum_{j\in J} p(A_j)
$$

The empirical probability distribution is a discrete distribution that assigns probability $\frac{1}{n}$ at each sample value. If the sample values are distinct, the empirical distribution is the discrete uniform distribution.

<details>
<summary>Proof</summary>

The 1st and 2nd properties are obvious. For the third property, note that since the sets are disjoint

$$
\begin{align*}
  p\left(\bigcup_{i\in I} A_i \right) &= \frac{1}{n}\sum_{i=1}^n \mathbf{1}\left(x_i \in \bigcup_{j\in J} \right) \\
  &= \frac{1}{n}\sum_{i=1}^n \sum_{j\in J}\mathbf{1}(x_i \in A_j) \\
  &= \sum_{j\in J}\frac{1}{n}\sum_{i=1}^n \mathbf{1}(x_i\in A_j) \\
  &= \sum_{j\in J} p(A_j)
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Empirical density' boxType='definition'>
Suppose $\boldsymbol{x}\in S\subseteq\R^n$ is a sample of size $n\in\N^+$.
For $A\subseteq S\subseteq\R^d$ with standard measure $\lambda_n (A) = \int_A 1\;\mathrm{d}x > 0$, the *empirical density* of $A$ corresponding to $\boldsymbol{x}$ is

$$
  D(A) = \frac{p(A)}{\lambda_n (A)} = \frac{1}{n\lambda_d (A)} \sum_{i=1}^n \mathbf{1}(x_i \in A)
$$
</MathBox>

<MathBox title='Empirical distribution function' boxType='definition'>
Suppose $\boldsymbol{x}\in S\subseteq\R^n$ is a sample of size $n\in\N^+$. For $x\in\R$, let $F(x)$ denote the relative frequency of $(-\infty, x]$ corresponding to $\boldsymbol{x}$. The function $F(x)$ is the sample mean of the data $\Set{ \mathbf{1}(x_i \leq x)}_{i=1}^n$

$$
  F(x) = p((-\infty, x]) = \frac{1}{n}\sum_{i=1}^n \mathbf{1}(x_i\leq x)
$$

and defines a distribution function that satisfies
1. $F$ increases from $0$ to $1$
2. $F$ is a step function with jumps at the distinct sample values
</MathBox>

<MathBox title='Empirical discrete density function' boxType='definition'>
Suppose $\boldsymbol{x}\in S\subseteq\R^n$ is a sample of size $n\in\N^+$.
For $x\in S$, let $f(x)$ be the relative frequency of $x$ corresponding the sample $\boldsymbol{x}$. The function $f$ is a sample mean for the data $\Set{\mathbf{1}(x_i = x)}_{i=1}^n$

$$
  f(x) = p(\Set{x}) = \frac{1}{n}\sum_{i=1}^n \mathbf{1}(x_i = x)
$$

and defines a discrete probability density function satisfying
1. $f(x)\geq 0$
2. $\sum_{x\in S} f(x) = 1$

If the underlying population variable is real-valued, then the sample mean is the expected value computed relative to the empirical density function

$$
  \frac{1}{n}\sum_{i=1}^n x_i = \sum_{x\in S} xf(x)
$$

<details>
<summary>Proof</summary>

Note that 

$$
\begin{align*}
  \sum_{x\in S} xf(x) &= \sum_{x\in S} x\frac{1}{n}\sum_{i=1}^n \mathbf{1}(x_i = x) \\
  &= \frac{1}{n}\sum_{i=1}^n \sum_{x\in S} x\mathbf{1}(x_i = x) \\
  &= \frac{1}{n}\sum_{i=1}^n x_i
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Empirical continuous density function' boxType='definition'>
Suppose $\boldsymbol{x}\in S\subseteq\R^n$ is a sample of size $n\in\N^+$. Let $\mathcal{A} = \Set{A_j}_{j\in J}$ be a partition of $S$ into a countable number of subsets, each of positive, finite measure. Let $f$ be the function on $S$ defined by the rule that $f(x)$ is empirical density of $A_j$, corresponding to the data set $\mathbf{x}$, for each $x\in A_j$

$$
  f(x) = D(A_j) = \frac{p(A_j)}{\lambda_n (A_j)} = \frac{1}{n\lambda_d (A_j)}\sum_{i=1}^n \mathbf{1}(x_i\in A_j)
$$

The function $f$ is a continuous probability density function satisfying
1. $f(x)\geq 0$
2. $\int_S f(x)\;\mathrm{d}x = 1$

<details>
<summary>Proof</summary>

Note that 

$$
\begin{align*}
  \int_S f(x)\;\mathrm{d}x &= \sum_{j\in J}\int_{A_j} f(x)\;\mathrm{d}x \\
  &= \sum_{j\in J} \lambda_k (A_j) \frac{p(A_j)}{\lambda_k(A_j)} \\
  &= \sum_{j\in J}p(A_j) = 1
\end{align*}
$$
</details>
</MathBox>

### Sample variance

<MathBox title='Sample variance' boxType='definition'>
Suppose $\boldsymbol{x}\in S\subseteq\R^n$ is a sample of size $n\in\N^+$ from a real-valued variable. The *sample variance* is defined as the mean square deviation

$$
\begin{align*}
  s^2 &= \frac{1}{n-1}\sum_{i=1}^n (x_i - m)^2 \\
  &= \frac{1}{n-1}\sum_{i=1}^n x_i^2 - \frac{n}{n-1}m^2 = \frac{n}{n-1}\left[m(\boldsymbol{x^2}) - m^2(\boldsymbol{x})\right] \\
  &= \frac{1}{2n(n - 1)}\sum_{i=1}^n \sum_{j=1}^n (x_i - x_j)^2
\end{align*}
$$

where $\mathbf{x}^2 = (x_i^2)_{i=1}^n$.

<details>
<summary>Details</summary>

The first alternate form of the sample variance is found from

$$
\begin{align*}
  \sum_{i=1}^n (x_i - m)^2 &= \sum_{i=1}^n (x_i^2 - 2mx_i + m^2) \\
  &= \sum_{i=1}^n x_i^2 - 2m\sum_{i=1}^n x_i - \sum_{i=1}^n m^2 \\
  &= \sum_{i=1}^n x_i^2 2nm^2 + nm^2 \\
  &= \sum_{i=1}^n x_i^2 - nm^2
\end{align*}
$$

The second alternate form of the sample variance is found from

$$
\begin{align*}
  \frac{1}{2n}\sum_{i=1}^n \sum_{j=1}^n (x_i - x_j)^2 &= \frac{1}{2n}\sum_{i=1}^n \sum_{j=1}^n (x_i - m + m - x_j)^2 \\
  &= \frac{1}{2n}\sum_{i=1}^n\sum_{j=1}^n \left[(x_i - m)^2 + 2(x_i - m)(m - x_j) + (m - x_j)^2\right] \\
  &= \frac{1}{2n}\sum_{i=1}^n \sum_{j=1}^n (x_i - m)^2 + \frac{1}{n} \sum_{i=1}^n \sum_{j=1}^n (x_i - m)(m - x_j) + \frac{1}{2n}\sum_{i=1}^n \sum_{j=1}^n (m - x_j)^2 \\
  &= \frac{1}{2}\sum_{i=1}^n (x_i - m)^2 + 0 + \frac{1}{2}\sum_{j=1}^n (m - x_j)^2 \\
  &= \sum_{i=1}^n (x_i - m)^2
\end{align*}
$$
</details>
</MathBox>

The reason for by $n - 1$ rather than $n$ is that there are only $n-1$ degrees of freedom in the set of deviations as the sum of all deviations vanishes

$$
\begin{align*}
  \sum_{i=1}^n (x_i - m) &= \sum_{i=1}^n x_i - \sum_{i=1}^n m \\
  &= nm - nm = 0
\end{align*}
$$

<MathBox title='Properties of the sample variance' boxType='proposition'>
Suppose $\boldsymbol{x}\in S\subseteq\R^n$ is a sample of size $n\in\N^+$ from a real-valued variable. The sample variance $s^2$ has the following properties

1. $s^2 \geq 0$ and $s^2 = 0$ if and only if $x_i = x_j$ for each $i,j$ (positive definiteness)
2. if $c$ is a constant then $s^2(c\boldsymbol{x}) = c^2 s^2(\boldsymbol)$
  a. $s(c\boldsymbol{x}) = |c|s(\boldsymbol{x})$
3. if $\boldsymbol{c}$ is a sample of size $n$ from a constant $c$ then $s^2(\boldsymbol{x} + \boldsymbol{c}) = s^2(\boldsymbol{x})$
<details>
<summary>Details</summary>

2. Recall that $m(c\boldsymbol{x}) = cm(\boldsymbol{x})$ so that

$$
\begin{align*}
  s^2 (c\boldsymbol{x}) &= \frac{1}{n-1}\sum_{i=1}^n \left[ cx_i - cm(\boldsymbol{x})\right]^2 \\
  &= \frac{1}{n - 1}\sum_{i=1}^n c^2\left[x_i - m(\boldsymbol{x})\right]^2 \\
  &= c^2 s^2 (\boldsymbol{x})
\end{align*}
$$

3. Recall that $m(\boldsymbol{x} + \boldsymbol{c}) = m(\boldsymbol{x}) + c$ so that

$$
\begin{align*}
  s^2(\boldsymbol{x} + \boldsymbol{c}) &= \frac{1}{n-1}\sum_{i=1}^n \left( (x_i + c) - (m(\boldsymbol{x}) + c) \right)^2 \\
  &= \frac{1}{n-1}\sum_{i=1}^n \left[ x_i - m(\boldsymbol{x}) \right]^2 = s^2(\boldsymbol{x})
\end{align*}
$$
</details>
</MathBox>

#### Loss function

<MathBox title='Mean square error function' boxType='definition'>
Suppose $\boldsymbol{x}\in S\subseteq\R^n$ is a sample of size $n\in\N^+$ from a real-valued variable. For $a\in\R$, the means square error function is defined by

$$
  \mathrm{mse}(a) = \frac{1}{n-1}\sum_{i=1}^n (x_i - a)^2
$$

The graph of $\mathrm{mse}$ is a convex parabola with minimum value $s^2$ at $a = m$, the sample mean.

<details>
<summary>Details</summary>

Taking the derivative of $\mathrm{mse}$ gives

$$
\begin{align*}
  \frac{\mathrm{d}}{\mathrm{d}a}\mathrm{mse}(a) = -\frac{2}{n-1}\sum_{i=1}^n (x_i - a) \\
  &= -\frac{2}{n-1}(nm - na)
\end{align*}
$$

which shows that $\mathrm{mse}$ is minimized when $a = m$
</details>
</MathBox>

<MathBox title='Mean absolute error function' boxType='definition'>
Suppose $\boldsymbol{x}\in S\subseteq\R^n$ is a sample of size $n\in\N^+$ from a real-valued variable. For $a\in\R$, the means absolute error function is defined by

$$
  \mathrm{mae}(a) = \frac{1}{n-1}\sum_{i=1}^n |x_i - a|
$$
</MathBox>

<MathBox title='Sample standard score' boxType='definition'>
Suppose $\boldsymbol{x}\in S\subseteq\R^n$ is a sample of size $n\in\N^+$ from a real-valued variable. The standar score associated with $x_i$ is defined as

$$
  z_i = \frac{(x_i - m)}{s}
$$

The sample of standard scores $\boldsymbol(z) = \frac{\boldsymbol{x} - \boldsymbol{m}}{s}$ has mean $0$ and variance $1$.
</MathBox>

# Point estimation

<MathBox title='Estimator' boxType='definition'>
Let $\theta\in \Theta\subseteq\R$ be an unknown real parameter. A real-valued statistic $U = u(\mathbf{X})$ that is used to estimate $\theta$ is called an estimator of $\theta$. The estimator is a random variable whose moments generally depend on $\theta$.

If $U$ is an estimator for $\theta$, the following can be defined
1. $U-\theta$ is the error
2. $\mathrm{bias}(U) = \mathbb{E}(U - \theta) = \mathbb{E}(U) - \theta$ is the bias of $U$
3. $\mathrm{mse}(U) = \mathbb{E}\left[(U - \theta)^2\right]$ is the mean square error of $U$

Depending on the sign of $\mathrm{bias}(U)$ we say that
1. $U$ is *unbiased* if $\mathrm{bias}(U) = 0$, or equivalently $\mathbb{E}(U) = \theta$
2. $U$ is negatively biased if $\mathrm{bias}(U)\leq 0$, or equivalently $\mathbb{E}(U)\leq\theta$
3. $U$ is positively biased if $\mathrm{bias}(U)\geq 0$, or equivalently $\mathbb{E}(U)\geq\theta$
</MathBox>

<MathBox title='Estimator bias' boxType='definition'>
Let $\theta\in\Theta\subseteq\R$ be an unknown real parameter. A real-valued statistic $U = u(\mathbf{X})$ that is used to estimate $\theta$ is called an estimator of $\theta$. The estimator is a random variable whose moments generally depend on $\theta$.

If $U$ is an estimator for $\theta$, the following can be defined
1. $U-\theta$ is the error
2. $\mathrm{bias}(U) = \mathbb{E}(U - \theta) = \mathbb{E}(U) - \theta$ is the bias of $U$
3. $\mathrm{mse}(U) = \mathbb{E}\left[(U - \theta)^2\right] = \mathrm{var}(U) + \mathrm{bias}^2(U)$ is the mean square error of $U$.

<details>
<summary>Details</summary>

$$
\begin{align*}
  \mathbb{E}\left[(U - \theta)^2\right] &= \mathrm{var}(U-\theta) + \left[\mathbb{E}(U-\theta)\right]^2 \\
  &= \mathrm{var}(U) + \mathrm{bias}^2(U) 
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Estimator efficiency' boxType='definition'>
Let $U$ and $V$ be unbiased estimators of a parameter $\theta\in \Theta\subseteq\R$. Then
1. $U$ is *more efficient than* $V$ if $\mathrm{var}(U)\leq\mathrm{var}(V)$
2. the *relative efficiency** of $U$ with respect to $V$ is

$$
  \mathrm{eff}(U, V) = \frac{\mathrm{var}(V)}{\mathrm{var}(U)}
$$
</MathBox>

## Asymptotic properties

<MathBox title='Asymptotically unbiased' boxType='definition'>
The sequence of estimators $\boldsymbol{U} = (U_i)_{i=1}^{n\in\N}$ is *asymptotically unbiased* if $\mathrm{bias}(U_n)\xrightarrow{n\to\infty} 0$ for every $\theta\in T$, or equivalently $\mathbb{E}(U_n)\xrightarrow{n\to\infty}\theta$. 
</MathBox>

<MathBox title='Asymptotic relative efficiency' boxType='definition'>
Suppose $\boldsymbol{U} = (U_i)_{i=1}^{n\in\N}$ and $\boldsymbol{V} = (V_i)_{i=1}^{n\in\N}$ are two sequences of estimators that are asymptotically unbiased. The *asymptotic relative efficiency* of $\boldsymbol{U}$ to $\boldsymbol{V}$ is

$$
  \lim_{n\to\infty}\mathrm{eff}(U_n, V_n) = \lim_{n\to\infty}\frac{\mathrm{var}(V_n)}{\mathrm{var}(U_n)}
$$

assuming that the limit exists.
</MathBox>

<MathBox title='Consistency' boxType='definition'>
Suppose $\boldsymbol{U} = (U_i)_{i=1}^{n\in\N}$ is a sequence of estimators for $\theta\in \Theta\subseteq\R$. Then
1. $\boldsymbol{U}$ is *consistent* if $U_n\xrightarrow{n\to\infty}\theta$ for each $\theta\in T$, i.e. $\mathbb{P}\left(|U_n - \theta| > \varepsilon \right) \xrightarrow{n\to\infty} 0$ for every $\varepsilon > 0$.
2. $\boldsymbol{U}$ is *mean-square consistent* if $\mathrm{var}(U_n) = \mathbb{E}\left[(U_n - \theta)^2 \right]$  
</MathBox>

<MathBox title='Consistency relation' boxType='proposition'>
Suppose $\boldsymbol{U} = (U_i)_{i=1}^{n\in\N}$ is a sequence of estimators for $\theta\in \Theta\subseteq\R$. If $\mathbb{U}$ is mean-square consistent then $\mathbb{U}$ is consistent and asymptotically unbiased.

<details>
<summary>Proof</summary>

From Markov's inequality

$$
\begin{align*}
  \mathbb{P}\left(|U_n - \theta| > \varepsilon \right) &= \mathbb{P}\left[(U_n - \theta)^2 > \varepsilon \right] \\
  &\leq \frac{\mathbb{E}\left[ (U_n  - \theta)^2 \right]}{\varepsilon^2} \xrightarrow{n\to\infty} 0
\end{align*}
$$
</details>
</MathBox>

## The method of moments

Consider a statistical experiment with an observable real random value $X$. The distribution of $X$ has $k$ unknown real parameters $\boldsymbol{\theta} = (\theta_i)_{i=1}^k\in\Theta\subseteq\R^k$. Repeating the experiment $n$ times generates a random sample of size $n$ from the distribution of $X$ in the form $\boldsymbol{X} = (X_i)_{i=1}^n$. Thus, $\boldsymbol{X}$ is a sequence of independent random variables, each with the distribution of $X$. 

The method of moments is a technique fro constructing estimators of the parameters that is based on matching the sample moments with the corresponding distribution moments. The $j$-th moment of $X$ about $0$ is a function of $\boldsymbol{\theta}$

$$
  \mu^{(j)}(\boldsymbol{\theta}) = \mathbb{E}(X^j),\quad j\in\N^+
$$

where $\mu^{(1)}(\boldsymbol{\theta})$ is just the mean of $X$. The $j$-th sample moment about $0$ takes the form

$$
  M^{(j)}(\boldsymbol{X}) = \frac{1}{n}\sum_{i=1}^n X_i^j
$$

where $M^{(1)}(\boldsymbol{X})$ is the sample mean $\bar{X}_n$.

To construct the method of moments estimators $\boldsymbol{W} = (W_i)_{i=1}^k$ for the parameters $\boldsymbol{\theta} = (\theta_i)_{i=1}^k$ respectively, we consider the equations

$$
  \mu^{(j)}(\boldsymbol{W}) = M^{(j)}(\boldsymbol{X})
$$

consecutively for $j$ until we are able to solve for $\boldsymbol{W}$ in terms of $M^{(j)}$.

## Maximum likelihood estimator

<MathBox title='Likelihood function' boxType='definition'>
Suppose $\boldsymbol{X}\in S$ is an observable random variable for a statistical experiment, depending on unknown parameters $\boldsymbol{\theta} = (\theta_i)_{i=1}^{k\in\N}\in\Theta\subseteq\R^k$. Let $f_{\boldsymbol{\theta}}$ denote the probability density function of $\boldsymbol{X}$ for $\boldsymbol{\theta}$. The likelihood function at $\boldsymbol{x}\in S$ is the function $L_{\boldsymbol{x}}:\Theta\to[0,\infty)$ given by

$$
  L_{\boldsymbol{x}} (\boldsymbol{\theta}) = f_{\boldsymbol{\theta}}(\boldsymbol{x})
$$

The logarithm of the likelihood function, called log-likelihood function, at $\boldsymbol{x}\in S$ is the function $\ln L_{\boldsymbol{x}}$ given by

$$
  \ln L_{\boldsymbol{x}}(\boldsymbol{\theta}) = \ln f_{\boldsymbol{\theta}}(\boldsymbol{x})
$$

The likelihood function is the function obtained by reversing the roles of $\boldsymbol{x}$ and $\theta$ in the probability density function, i.e. we view $\theta$ as the variable and $\boldsymbol{x}$ as the given information.
</MathBox>

<MathBox title='Maximum likelihood estimator' boxType='definition'>
Suppose the maximum value of $L_{\boldsymbol{x}}$ or $\ln L_{\boldsymbol{x}}$ occurs at $u(\boldsymbol{x})\in\Theta$ for each $\boldsymbol{x}\in S$. Then the statistic $u(\boldsymbol{X})$ is a *maximum likelihood estimator* of $\boldsymbol{\theta}$. If $L_{\boldsymbol{x}}$ is differentiable, we can find this point by solving

$$
  \frac{\partial}{\partial\theta_i}L_{\boldsymbol{x}}(\boldsymbol{\theta}) = 0
$$

or equivalently

$$
  \frac{\partial}{\partial\theta_i}\ln L_{\boldsymbol{x}}(\boldsymbol{\theta}) = 0
$$

The most important special case is when the sample is a random sample of an observable real random variable $X$. Suppose $\boldsymbol{X} = (X_i)_{i=1}^n \in S\subseteq\R^n$ is random sample of size $n$ from the distribution of $X$ with probability density function $f_{\boldsymbol{\theta}}$. The likelihood and log-likelihood functions for $\boldsymbol{x}\in S$ are

$$
\begin{align*}
  L_{\boldsymbol{x}} (\boldsymbol{\theta}) &= \prod_{i=1}^n f_{\boldsymbol{\theta}} (x_i) \\
  \ln L_{\boldsymbol{x}} (\boldsymbol{\theta}) &= \sum_{i=1}^n \ln f_{\boldsymbol{\theta}} (x_i)
\end{align*}
$$
</MathBox>

<MathBox title='Reparametrized likelihood function' boxType='definition'>
Suppose $h:\Theta\to\Lambda$ and let $\boldsymbol{\lambda} = h(\boldsymbol{\theta})$ denote the new parameter. Define the likelihood function for $\lambda$ at $\boldsymbol{x}\in S$ by

$$
  \hat{L}_{\boldsymbol{x}}(\boldsymbol{\lambda}) = \max\Set{ L_{\boldsymbol{x}}(\boldsymbol{\theta}) | \boldsymbol{\theta}\in h^{-1}(\boldsymbol{\lambda}) },\quad \boldsymbol{\lambda}\in\Lambda
$$

If $v(\boldsymbol{x})\in\Lambda$ maximized $\hat{L}_{\boldsymbol{x}}$ for each $\boldsymbol{x}\in S$ then $V = v(\boldsymbol{X})$ is a maximum likelihood estimator of $\lambda$. Conversely, if $U = u(\boldsymbol{x})\in\Theta$ is a maximum likelihood estimator for $\boldsymbol{\theta}$, then $V = h(U)$ is a maximum likelihood estimator for $\boldsymbol{\lambda}$. This is known as the invariance property.
</MathBox>

## Bayesian estimator

Consider a statistical experiment with an observable random variable $\boldsymbol{X}\in S$, whose distribution depends on parameters $\boldsymbol{\theta}\in T$. *Bayesian analysis* models the parameters $\boldsymbol{\theta}$ with a random variable $\boldsymbol{\Theta}$ that has a specified distribution on the parameter space $T$. This distribution is called the *prior distribution* of $\boldsymbol{\Theta}$ and reflects knowledge of the parameters $\boldsymbol{\theta}$ before sampling data. After observing $\boldsymbol{X} = \boldsymbol{x}\in S$, Bayes' theorem is used to compute the conditional distribution of $\boldsymbol{\Theta}$ given $\boldsymbol{X} = \boldsymbol{x}$. This distribution is called the *posterior distribution* of $\boldsymbol{\Theta}$ and reflects updated inferences of $\boldsymbol{\theta}$ given new information.

<MathBox title='Posterior distribution' boxType='definition'>
Let $\boldsymbol{X}\in S$ be an observable random variable with probability density function $f$. Suppose the *prior distrubution* of $\boldsymbol{\Theta}$ on $T$ has probability density function $h$, and that given $\boldsymbol{\Theta} = \boldsymbol{\theta}\in T$, the conditional probability density function of $\boldsymbol{X}$ on $S$ is $f(\cdot|\boldsymbol{\theta})$. Then the probability density function of the posterior distribution of $\boldsymbol{\Theta}$ given $\boldsymbol{X} = \boldsymbol{x}\in S$ is

$$
  h(\boldsymbol{\theta}|\boldsymbol{x}) = \frac{h(\boldsymbol{\theta})f(\boldsymbol{x}|\boldsymbol{\theta})}{f(\boldsymbol{x})}
$$

where the unconditional probability function $f$ is defined as follows, in the discrete and continuous cases, respectively

$$
\begin{align*}
  f(\boldsymbol{x}) &= \sum_{\boldsymbol{\theta}\in T} h(\boldsymbol{\theta})f(\boldsymbol{x}|\boldsymbol{\theta}) \\
  f(\boldsymbol{x}) &= \int_T h(\boldsymbol{\theta})f(\boldsymbol{x}|\boldsymbol{\theta})\;\mathrm{d}\boldsymbol{\theta}
\end{align*}
$$

The most import and common special case occurs when $\boldsymbol{X} = (X_i)_{i=1}^{n\in\N}$ is a random sample of size $n$ from the distribution of an obervable random variable $X$. If $X$ is real-valued and has probability density function $g(\cdot|\boldsymbol{\theta})$ for a given $\boldsymbol{\theta}\in T$. In this case, $S=\R^n$ and the probability function $f(\cdot|\boldsymbol{\theta})$ of $\boldsymbol{X}$ given $\boldsymbol{\theta}$ is

$$
  f(\boldsymbol{x}|\boldsymbol{\theta}) = \prod_{i=1}^n g(x_i\mid \boldsymbol{\theta})
$$

<details>
<summary>Details</summary>

Note that the joint probability density function of $(\boldsymbol{X},\boldsymbol{\Theta})$ is a mapping on $f: S\times T\to [0, \infty)$ given by

$$
  (\boldsymbol{x}, \boldsymbol{\theta})\mapsto h(\boldsymbol{\theta})f(\boldsymbol{x}|\boldsymbol{\theta})
$$

The function $f(\boldsymbol{x})$ is the marginal probability density function of $\boldsymbol{X}$. 
</details>
</MathBox>

If the parameter space has finite measure $c$, then one possible prior distribution is the uniform distribution on $T$. If $\theta\in T\subseteq\R$ the probability density function is given by $h(\theta) = \frac{1}{c}$. This distribution reflects no prior knowledge about the parameter, and is called the *non-informative* prior distribution.

<MathBox title='Bayesian estimator' boxType='definition'>
Suppose $\theta\in T\subseteq\R$ is a real-valued parameter for an observable random variable $\boldsymbol{X}$. The conditional expected value $\mathbb{E}(\theta|\boldsymbol{X})$ is the Bayesian estimator for $\theta$, which takes the form, in the discrete and continuous cases, respectively

$$
\begin{align*}
  \mathbb{E}(\theta|\boldsymbol{X} &= \boldsymbol{x}) &= \sum_{\theta\in T} \theta h(\theta|\boldsymbol{x}) \\
  \mathbb{E}(\theta|\boldsymbol{X} &= \int_T \theta h(\theta|\boldsymbol{x})\;\mathrm{d}\boldsymbol{\theta}
\end{align*}
$$

The definitions of bias and mean square get conditioned on $\Theta = \theta\in T$. If $U$ is a Bayesian estimator for $\theta$ then

1. The *bias* of $\mathrm{bias}(U\mid \theta) = \mathbb{E}(U-\theta|\Theta = \theta)$
2. The *mean square error* of $U$ is $\mathrm{mse}(U\mid \theta) = \mathbb{E}\left[(U-\theta)^2 | \Theta = \theta \right] = \mathrm{var}(U\mid \theta) + \mathrm{bias}^2(U\mid \theta)$
</MathBox>

<MathBox title='Properties of Bayesian estimators' boxType='proposition'>
Let $\boldsymbol{U} = (U_n)_{n\in\N^+}$ be a sequence of Baysian estimators of $\theta\in T\subseteq\R$. Then
1. $\boldsymbol{U}$ is *asymptotically unbiased* if $\mathrm{bias}(U_n\mid \theta)\xrightarrow{n\to\infty} 0$
2. $\boldsymbol{U}$ is *mean-square consistent* if $\mathrm{mse}(U_n\mid \theta)\xrightarrow{n\to\infty} 0$
</MathBox>

# Hyptheses testing

<MathBox title='Statistical hypothesis' boxType='definition'>
A statistical hypothesis is a statement about the distribution of a sample $\mathbf{X}$. Equivalently, a statistical hypothesis specifies a set of possible distributions of $\mathbf{X}$: the set of distributions for which the statement is true. A hypothesis that specifies a single distribution for $\mathbf{X}$ is called simple; a hypothesis that specifies more than one distribution for $\mathbf{X}$ is called composite.
</MathBox>

In hypothesis testing, the goal is to see if there is sufficient statistical evidence to reject a presumed *null hypothesis* in favour of a conjectured *alternative hypothesis*. The null hypothesis is usually denoted $H_0$, while the alternative hypothesis is usually denoted $H_1$.

| State/decision | Fail to reject $H_0$ | Reject $H_0$ |
| ::: | ::: | ::: |
| $H_0$ true | Correct | Type 1 error |
| $H_1$ true | Type 2 error | Correct |

An hypothesis test is a statistical decision; the conclusion will either be to reject the null hypothesis in favour of the alternative, or to fail to reject the null hypothesis. The decision is based on the observed value $\mathbf{x}$ of the sample $\mathbf{X}$. Thus, we will find an appropriate subset $\mathbf{R}$ of the sample space $S$ and reject $H_0$ if and only if $\mathbf{x}\in R$. The set $R$ is known as the rejection/critical region.

An hypothesis test is in a sense a statistical analogy to proof by contradiction. Suppose $H_1$ is a logical statement and that $H_0$ is its negation. One way that we can prove $H_1$ is to assume $H_0$ and work our way logically to a contradiction. Similarly, in a hypothesis test we assume $H_0$ and then see if the data $\mathbf{x}$ are sufficiently at odds with that assumption that we feel justified in rejecting $H_0$ in favor of $H_1$.

Often, the critical region is defined in terms of a statistic $w(\mathbf{X})$, known as a *test statistic*, where $w:S\to T$ is a function from the sample space $S$ into another set $T$. We find an appropriate rejection region $R_T\subseteq T$ and reject $H_0$ when the observed value $w(\mathbf{x})\in R_T$. Thus, the rejection region in $S$ is $R = w^{-1}(R_T) = \Set{\mathbf{x}\in S | w(\mathbf{x})\in R_T}$. The use of a statistic often allows significant data reduction when the dimension of the test statistic is much smaller than the dimension of the data vector.

<MathBox title='Test errors' boxType='definition'>
1. A *type 1 error* is rejecting the null hypothesis $H_0$ when $H_0$ is true.
2. A *type 2 error* is failing to reject the null hypothesis $H_0$ when the alternative hypothesis $H_1$ is true.
</MathBox>

If $H_0$ is true, then $\mathbb{P}(\mathbf{X}\in R)$ is the probability of a type 1 error for this distribution. If $H_0$ is composite, then $H_0$ specifies a variety of different distributions for $\mathbf{X}$ and thus there is a set of type 1 error probabilities.

<MathBox title='Significance level' boxType='definition'>
The maximum probability of a type 1 error, over the set of distributions specified by $H_0$, is the *significance level* of the test or the $size$ of the critical region.
</MathBox>

The significance level is often denoted by $\alpha$. Usually, the rejection region is constructed so that the significance level is a prescribed, small value (typically $0.1, 0.05, 0.01$).

If $H_1$ is true, then $\mathbf{P}(\mathbf{X}\notin R)$ is the probability of a type 2 error for this distribution. If $H_1$ is composite, then $H_1$ specifies a variety of different distributions for $\mathbf{X}$, and thus there will be a set of type 2 error probabilities. Generally, there is a tradeoff between the type 1 and type 2 error probabilities. If we reduce the probability of a type 1 error, by making the rejection region $R$ smaller, we necessarily increase the probability of a type 2 error because the complementary region $S\setminus R$ is larger.

The extreme case can give us some insight. First, consider the decision rule in which we never reject $H_0$, regardless of the evidence $\mathbf{x}$. This corresponds to the rejection region $R=\emptyset$. A type 1 error is impossible, so the significance level is $0$. On the other hand, the probability for a type 2 error is $1$ for any distribution defined by $H_1$. At the other extreme, consider the decision rule in which we always reject $H_0$, regardless of the evidence $\mathbf{x}$. This corresponds to the rejection region $R = S$. A type 2 error is impossible, but now the probability of a type 1 error is $1$ for any distribution defined by $H_0$.

## Power

<MathBox title='Power' boxType='definition'>
If $H_1$ is true, so that the distribution of $\mathbf{X}$ is specified by $H_1$, then $\mathbb{P}(\mathbf{X}\in R)$, the probability of rejecting $H_0$ is the *power* of the test for that distribution.
</MathBox>

The power of the test for a distribution specified by $H_1$ is the probability of making the correct decision.

<MathBox title='Uniformly powerful test' boxType='definition'>
Suppose we have two tests, corresponding to rejection regions $R_1$ and $R_2$, respectively, each having significance level $\alpha$. The test with $R_1$ is *uniformly more powerful* than the test with region $R_2$ if for every distribution $\mathbf{X}$ specified by $H_1$

$$
  \mathbb{P}(\mathbf{X}\in R_1)\geq\mathbb{P}(\mathbf{X}\in R_2)
$$

If a test has significance level $\alpha$ and is uniformly more powerful than any other test with same significance level $\alpha$, then the test is the *uniformly most powerful test* at level $\alpha$.
</MathBox>

## $P$-value

<MathBox title='P-value' boxType='definition'>
The $P$-value of the observed value $\mathbf{x}$ of $\mathbf{X}$, denoted $P(\mathbf{x})$, is defined to be the smallest $\alpha$ for which $\mathbf{x}\in R_\alpha$, i.e. the smallest significance level for which $H_0$ is rejected given $\mathbf{X} = \mathbf{x}$. Note that $P(\mathbf{x})$ is a statistic.
</MathBox>

Knowing $P(\mathbf{x})$ allows us to test $H_0$ at any significance level for the given data $\mathbf{x}$. If $P(\mathbf{x})\leq\alpha$ then we would reject $H_0$ at significance level $\alpha$; if $P(\mathbf{x})> \alpha$ then we fail to reject $H_0$ at significance level $\alpha$.

## Test of an unknown parameter

An important special class of hypothesis testing occurs when the distribution of the sample $\mathbf{X}$ depends on a parameter $\theta$ taking values in a parameter space $\Theta$. In this case the hypotheses take the form

$$
  H_0:\theta\in\Theta_0 \textrm{ versus } H_1:\theta\notin\Theta_0
$$

where $\Theta_0$ is a prescribed subset of the parameter space $\Theta$. In this setting, the probabilities of making an error or a correct decision depend on the true value of $\theta$. If $R$ is the rejection region, then the power function $Q$ is given by

$$
  Q(\theta) = \mathbb{P}_\theta (\mathbf{X}\in R),\quad \theta\in\Theta
$$

<MathBox title='Properties of power functions' boxType='proposition'>
The power function $Q:\Theta\to[0,1]$ satisfies the following properties:
1. $Q(\theta)$ is the probability of a type 1 error when $\theta\in\Theta_0$.
2. $\max\Set{Q(\theta) : \theta\in\Theta_0 }$ is the significance level of the test.
3. $1-Q(\theta)$ is the probability of a type 2 error when $\theta\notin\Theta_0$.
4. $Q(\theta)$ is the power of the test when $\theta\notin\Theta_0$
</MathBox>

<MathBox title='Hypotheses tests for real parameters' boxType='definition'>
Suppose $\theta\in\R$ is a real parameter and $\theta_0\in\Theta$ a specified value. Hypotheses tests for $\theta$ fall into three categories
1. Two-sided test: $H_0:\theta = \theta_0\textrm{ versus }H_1:\theta\neq\theta_0$
2. Left-sided test: $H_0:\theta \geq \theta_0\textrm{ versus }H_1:\theta < \theta_0$
3. Right-sided test: $H_0:\theta \leq \theta_0\textrm{ versus }H_1:\theta > \theta_0$ 
</MathBox>

## Equivalence with confidence sets

<MathBox title='' boxType='proposition'>
Suppose $C(\mathbf{x})$ is a $1 - \alpha$ level confidence set for $\theta$. Consider the hypothesis

$$
  H_0:\theta = \theta_0 \textrm{ versus }H_1:\theta\neq\theta_0
$$ 

The following test has significance level $\alpha$ for the hypothesis: Reject $H_0$ if and only if $\theta_0\notin C(\mathbf{x})$.

<details>
<summary>Proof</summary>

By definition, $\mathbb{P}[\theta\in C(\mathbf{X})] = 1 - \alpha$. Thus, if $H_0$ is true so that $\theta = \theta_0$, then the probability of a type 1 error is $\mathbb{P}[\theta\notin C(\mathbf{X})] = \alpha$.
</details>
</MathBox>

In each case below, the confidence interval has confidence level $1 - \alpha$ and the test has significance level $\alpha$.
1. Suppose $L(\mathbf{X})$ is a confidence lower bound for $\theta$. Reject $H_0 : \theta\leq\theta_0$ versus $H_1: \theta > \theta_0$ if and only if $\theta_0 < L(\mathbf{X})$.
2. Suppose $U(\mathbf{X})$ is a confidence upper bound for $\theta$. Reject $H_0 : \theta\geq\theta_0$ versus $H_1: \theta < \theta_0$ if and only if $\theta_0 > U(\mathbf{X})$.
3. Suppose $[L(\mathbf{X}), U(\mathbf{X})]$ is a two-sided confidence interval for $\theta$. Reject $H_0 : \theta = \theta_0$ versus $H_1: \theta \neq \theta_0$ if and only if $\theta_0 < L(\mathbf{X})$ or $\theta_0 > U(\mathbf{X})$.

# Time series analysis

Let $X_t \overset{\Delta}{=} \Set{X_t}_{t\in T}$ be a stochastic process on an index set $T\subseteq\R$ and let $F_X \left(x_{t_i}\right)_{i=1}^{n\in\N} = P\left( X_{t_i}\leq x_{t_i} \right)_{i=1}^{n\in\N}$ denote the cumulative joint distribution function of $X_t$.

## Autocovariance

<MathBox title='Autocovariance and autocorrelation' boxType='definition'>
The autocovariance of a stochastic process $X_t$ with mean $\mu_t = E[X_t]$ is defined as

$$
  \gamma_{XX}(t_1, t_2) := \mathrm{cov}\left[X_{t_1}, X_{t_2}\right] = E\left[\left(X_{t_1} - \mu_{t_1}\right)\left(X_{t_2} - \mu_{t_2}\right)\right] = E\left[X_{t_1} X_{t_2} \right] - \mu_{t_1}\mu_{t_2}
$$

The normalized autocorrelation of $X_t$ is defined as

$$
  \rho_{XX}(t_1, t_2) = \frac{\gamma_{XX}(t_1, t_2)}{\sigma_{t_1}\sigma_{t_2}}
$$
</MathBox>

## Stationary process

<MathBox title='Stationarity' boxType='definition'>
**Strict stationarity**: A stochastic process $X_t$ is called strictly/strongly stationary if 

$$
  \left( X_{t_i} \right)_{i=1}^{n\in\N} \overset{d}{=} (X_{t_i + \tau})_{i=1}^{n\in\N}\quad \tau, (t_i)_{i=1}^{n\in\N} \in T
$$

i.e. the finite dimensional distributions of $X_t$ are invariant under time shifts. In terms of the cumulative joint distribution function, this can be reformulated as

$$
  F_X \left(x_{t_i}\right)_{i=1}^{n\in\N} = F_X\left(x_{t_i + \tau}\right)_{i=1}^{n\in\N}
$$

**Weak stationarity**: The stochastic process $X_t$ is called weakly stationary if for all $t\in T$
- the means are time invariant, i.e. $E[X_t] = E[X_{t + \tau}]$
- the second moments are finite, i.e. $E\left[|X_t|^2\right] < \infty$
- the autocovariance is time invariant, $\gamma_{X} (t, t + \tau) = \gamma_{X}(\tau, 0) \overset{\Delta}{=} \gamma(\tau)$
</MathBox>

## Autoregressive models (AR)

An autoregressive model of order $p$, denoted $\mathrm{AR}(p)$ is defined as 

$$
  X_t = c + \sum_{i=1}^p \varphi_i X_{t-i} + \epsilon_t
$$

where
- $\phi_i$ re the parameters of the model
- $c$ is a constant
- $\epsilon_t$ is white noise

This can be equivalently written in terms of the lag (backshift) operator $L$ ($B$) as

$$
\begin{gather*}
  X_t = c + \sum_{i=1}^p \varphi_i L^i X_t + \epsilon_t \\
  \varphi(L)X_t \equiv \left(1 - \sum_{i=1}^p \varphi_i L^i \right) X_t = \epsilon_t
\end{gather*}
$$

### Moving average model (ARMA)

An autoregressive-moving average (ARMA) model describes a stationary stochastic process in terms of two polynomials, one for the autoregression (AR) and the second for the moving average (MA). An ARMA-model with $p$ autoregressive terms and $q$ moving-average terms, denoted $\mathrm{ARMA}(p, q)$ is defined as

$$
\begin{gather*}
  X_t = c + \epsilon_t + \sum_{i=1}^p \varphi_i X_{t-i} + \sum_{i=1}^q \theta_i \epsilon_{t-i} \\
  \left(1 - \sum_{i=1}^p \varphi_i L^i \right) X_t = \left(1 + \sum_{i=1}^p \theta_i L^i \right)\epsilon_t \\
  \varphi(L) X_t = \theta(L) \epsilon_t
\end{gather*}
$$

### Integrated (ARIMA)

Assume the AR-polynomial $\alpha(L) = 1 - \sum_{i=1}^{p'} \alpha_i L^i$ has a unit root $(1 - L)$ of multiplicity $d$. Then it can be rewritten as

$$
  1 - \sum_{i=1}^{p'} \alpha_i B^i = \left(1 - \sum_{i=1}^{p' - d} \varphi_i B^i \right) (1 - L)^d
$$

An $\mathrm{ARIMA}(p, d, q)$ process expresses this polynomial factorisation property with $p = p' - d$ given by

$$
  \left(1 - \sum_{i=1}^{p' - d} \varphi_i B^i \right) (1 - L)^d X_t = \delta + \left(1 + \sum_{i=1}^p \theta_i L^i \right)\epsilon_t
$$

with drift $\frac{\delta}{1 - \sum_{i=1}^p \varphi_i}$.

### Fractal integrated model (ARFIMA)

The ARFIMA process allows a fractional value for the mulitiplicity $d$. For integer $d \in \N$, the differencing operator $(1 - L)^d$ can be expressed as a binomial expansion

$$
  (1 - L)^d = \sum_{i=0}^d \binom{d}{i} (-1)^i L^{d - i}
$$

To allow non-integer $d$, the binomial coefficients can be expressed in terms of the Gamma function

$$
  \binom{d}{i} = \frac{d!}{i!(d - i)!} = \frac{\Gamma(d + 1)}{\Gamma(i + 1)\Gamma(d - i + 1)} 
$$

The self-similar properties of ARFIMA depend on the values of $d$
- $d = 0$ reduces to an $\mathrm{ARMA}(p, q)$ process, i.e. a short-memory process
- For $d \in \left(0, \frac{1}{2}\right)$, the autocorrelations are all positive. The resulting process is asymptotically second order stationary exhibiting long memory. The corresponding Hurst exponent is $H = \frac{1}{2} + d$.
- For $d \in \left( -\frac{1}{2}, 0 \right)$ the autocorrelations are all negative, except $\rho(0) = 1$. The resulting process is anti-persistent exhibiting intermediate memory.
- For $d \geq \frac{1}{2}$, the process is no longer covariance stationary, and have infinite variance. By taking appropriate differences, a non-stationary process with $d > \frac{1}{2}$ can be reduced to a stationary process with $-\frac{1}{2} < d < \frac{1}{2}$


## Autoregressive conditional heteroskedasticity models (ARCH)

Autoregressive conditional heteroskedasticity (ARCH) models the variance of a mean process $\mu_t$ of the form

$$
  r_t = \mu_t + \epsilon_t
$$

with the return residuals $\epsilon_t = \sigma_t z_t$ where $z_t$ is a white noise process and

$$
  \sigma_t^2 = \alpha_0 + \sum_{i=1}^q \alpha_i \epsilon_{t-i}^2
$$

### Generalized autoregressive conditional heteroskedasticity (GARCH)

$$
  \sigma_t^2 = \omega + \sum_{i=1}^q \alpha_i \epsilon_{t-i}^2 + \sum_{i=1}^p \beta_i \sigma_{t-i}^2
$$

### FIGARCH

## Detrended fluctuation analysis

Detrended fluctuation analysis (DFA) is a method for determining the self-similarity of a time series by measuring the scaling-behaviours of its second moment-fluctuations, described by a fluctiation function $F:\N\to[0,2)$. The fluctiation function of a discrete process $\mathbf{X} = \Set{X_t }_{t=1}^{N\in\N}$ of length $N$ is computed through following steps

1. Calculate the cumulative (integrated) sum of $X_t$, which defines a new process

$$
  Y_t = \sum_{i=1}^t \left[X_i - \bar{X} \right]
$$

2. Partition the cumulative sum process $Y_t$ into $N_n = \lfloor N/n \rfloor$ into non-overlapping time intervals of length $n$. Since the length of the process is not necessarily a multiple of $n$, some data at the end of $Y_t$. To avoid neglecting the excluded data points, the same procedure can be repeated starting from the end of $Y_t$, and the total number of time intervals will amount to $2N_n$.

3. Each partition of $Y_t$ are fitted with a polynomial $\hat{Y}_\nu$ giving the local trend at interval $\nu$. In general, a polynomial fit of order $i$ removes trends of order $i - 1$. In particular, a linear fit, i.e. $i = 1$, removes constant trends in $X_t$, which corresponds to a rescaled range analysis for determining Hurst exponents.

4. For each interval $\nu$, the variance of the polynimal fit is given by

$$
\begin{align*}
  F_2 (n, \nu) = \frac{1}{n}\sum_{i=1}^n \left( Y[(\nu - 1)n + i] - \hat{Y}_\nu(t) right)^2,\quad \nu\in\Set{1,\dots,N_n} \\
  F_2 (n, \nu) = \frac{1}{n}\sum_{i=1}^n \left( Y[N_n - (\nu - N_n)n + i] - \hat{Y}_\nu(t) right)^2,\quad \nu\in\Set{1,\dots,N_n},\quad \nu\in\Set{N_n + 1,\dots, 2N_n}
\end{align*}
$$

The fluctiation function is defined as the root mean square error deviation from the trend

$$
  F_2(n) = \left( \frac{1}{2N_n} \sum_[\nu=1]^{2N_n} [F_2 (n, \nu)]^2 \right)^{1/2}
$$

The fluctiation function is expected to scale as $F(n) \propto n^h$. The scaling exponent $h$ is given by the slope of a straight line fit to the log-log graph of $n$ against $F(n)$ using least squares. The scaling

- $h \in \left[0, \frac{1}{2}\right)$: anti-correlated (anti-persistency)
- $h = \frac{1}{2}$: uncorrelated (white noise)
- $h \in \left(\frac{1}{2}, 1\right)$: correlated (persistency)
- $h = 1$: $1/f$-noise (pink/fractal noise)
- $h \in \left(1,\frac{3}{2}\right)$: non-stationary, unbounded
- $h = \frac{3}{2}$: Brownian noise

### Multifractal generalization

The fluctuation function can be generalized to moments of arbitrary real-valued order $q\in\R$

$$
  F_q(n) = \left( \frac{1}{2N_n} \sum_[\nu=1]^{2N_n} [F_2 (n, \nu)]^{q/2} \right)^{1/q}
$$

For $q = 0$, the fluctiation function is divergent and can be replaced by an exponential of a logarithmic sum

$$
  F_0 (n) = \exp\left( \frac{1}{4N_n} \sum_{\nu = 1}^{2N_n} \ln[F_2 (n, \nu)] \right) \propto n^{h(0)}
$$

Multifractal systems scale as $F_q (n) \propto n^{h(q)}$. In general, the exponent $h$ may depend on $q$. For stationary time series, $h(2)$ is identical to the Hurst exponent. Thus, the function $h(q)$ is usually called the generalized Hurst exponent.

For monofractal time series with compact suppoert, $h$ is independent of $q$, since the scaling variance $F_2(n,\nu)$ is identical for all intervals $\nu$. For multifractal time series, $h$ depends on $q$

- For $q > 0$, the intervals with large variance $F_2(n,\nu)$ dominate. Thus $h(q)$ describes the scaling behaviour of the intervals with large fluctuations. These are usually characterized by smaller $h(q)$.
- For $q < 0$, the intervals with small variance $F_2(n,\nu)$ dominate. Thus $h(q)$ describes the scaling behaviour of the segments with small fluctiations. These are usually characterized by larger $\alpha(q)$.

#### Relation to standard multifractal analysis

If the process $\mathbf{X}$ is a stationary and Gaussian sequence, the detrending procedure is not required, since no trend is present. In this case, the variance for each interval $\nu$ simplifies to

$$
  F_2 (n,\nu) = |Y(\nu n) - Y((\nu - 1)s)|^2
$$

Averaging over all intervals, gives the simplified fluctiation function

$$
  F_q (n) = \left( \frac{1}{2N_n} \sum_{\nu=1}^{2N_n} \left| Y(\nu n) - Y[(\nu - 1)n]\right|^q \right)^{1/q} \propto n^{h(q)}
$$

Assuming that the length $N$ of $\mathbf{X}$ is an integer multiple of the scale $n$, i.e. $N_n = \frac{N}{n}$, we obtain

$$
  \sum_{\nu=1}^{N/n} |Y(\nu n) - Y[(\nu - 1)n]|^q \propto n^{qh(q) - 1}
$$

Noting that the summand $Y(\nu n) - Y[(\nu - 1)n]$ gives the sum of numbers $X_t$ within each interval $\nu$ of size $n$. This sum is known as the box probability defined as

$$
  p_n(\nu) = \sum_{t = (\nu - 1)n + 1}^{\nu n} X_t = Y(\nu n) - Y[(\nu - 1)n]
$$

The mass component $\tau(q)$ in standard fluctuation analysis is usually defined via the partition function 

$$
  Z_q (n) = \sum_{\nu = 1}^{N/n} |p_n (\nu)|^q \propto n^{\tau(q)}
$$

Which is identical to the simplified fluctiation function, thus giving the relation

$$
  \tau(q) = qh(q) - 1
$$

Multifractal processes can also be characterized by a singularity spectrum $f$ that is related to $\tau$ via a Legendre transform

$$
  \alpha = \tau'(q) \quad f(\alpha) = q\alpha - \tau(q)
$$

where $\alpha$ is the Hlder exponent describing the singularity strength, while $f(\alpha)$ denotes the dimension of the subset of the process that is characterized by $\alpha$. Eliminating $\tau(q)$, we get

$$
  \alpha = h(q) + qh'(q) \quad f(\alpha) = q[\alpha - h(q)] + 1
$$

## Markov-switching model

A Markov-switching model is constructed by combining two or more dynamic models via a Markov switching mechanism.

An $n$-state Markov switching model can be expressed as

$$
  y_t = B \mathbf{s}_t \mathbf{x}_t + \boldsymbol{\sigma} \mathbf{s}_t \varepsilon_t,\quad \epsilon_t \sim \mathcal{N}(0, 1)
$$

where
- $B = (\boldsymbol{\beta}_i)_{i=1}^n$ is an $n\times k$ matrix and $\boldsymbol{\beta}_i$ is a $k\times 1$ parameter vector
- $\mathbf{x}_t$ is a $k\times 1$ vector of exogeneous regressors
- $\boldssymbol{\sigma} = (\sigma_i)_{i=1}^n$ is a $m\times 1$ vector of error standard deviations
- $\mathbf{s}_t (s_{i,t})_{i=1}^n$ is an $n\times 1$ vector of binary state indicators, such that $s_{i,t} = 1$ and $s_{j,t} = 0$ for $j\neq i$ if the process is in state $i$ at time $t$.

The state vector $\mathbf{s}_t$ is assumed to an ergodic Markov chain with transition probabilities

$$
  P = \left[p_{ij} = \mathrm{Pr}(s_{j,t} = 1)|s_{i, t-1} = 1)\right]_{n\times n}
$$

## Markov-switching multifractal (MSM)

Let $P_t$ be the price of a financial asseet, and let $r_t = \ln\left(\frac{P_t}{P_{t-1}} \right)$ denote the return. In the Markov-switching multifractal model, returns are defined as

$$
  r_t = \mu + \sigma_t \epsilon_t
$$

where $\epsilon_t$ is white noise, and $\sigma_t$ is the instantaneous volatility generated by the product of $k$ volatility components

$$
  \sigma_t^2 = \sigma^2 \prod_{i=1}^k m_{t, i} = \sigma^2 M_t
$$

The hidden volatility states $M_t$ are assumed to follow a first-order Markov chain (equivalently that $M_t$ are independent in each level of cascade)  

$$
\begin{align*}
  P\left[ M_t | M_{t-1}, M_{t-2},\dots \right] &= P\left[ M_t | M_{t-1} \right] \\
  &= P\left[ \prod_{i=1}^k m_{t, i} \middle| \prod_{i=1}^k m_{t-1, i}  \right] \\
  &= \prod_{i=1}^k P\left[ m_{t,i} | m_{t-1, i} \right]
\end{align*}
$$

Each volatility component $m_{t, i}$ is updated at time $t$ with transition probability $\gamma_i$ given by

$$
  \gamma_i = 1 - \left( 1 - \gamma_k \right)^{b^{i - k}} \quad \gamma_k \in [0, 1] \quad b \in (1, \infty)
$$

In this context, $i$ refers to the frequency of volatility components. At low frequencies, the sequence $\gamma_i$ is approximately geometric $\gamma_i = \gamma_1 b^{k - 1}$.

The transition probability $\gamma_i$ grows with increasing $i$. Therefore, the persistency of a component $m_{t,i}$ i reduced as $i \to k$. In other words, the expected duration of each volatility component decreases on average as the number of components $k$ increases.

In the discrete case, the volatility components are modelled with a multinomial distribution, while in the contiuous case the lognormal distribution is most commonly used.

### Trinomial MSM

Assuming that $m_{t, i}$ follows a trinomial distribution, it takes one of three values

$$
  m_{t, k} \in \Set{ m_0, m_1, m_2 }
$$

which are constrained $m_0 + m_1 + m_2 = 3$ so that the normalization condition $\mathrm{E}[M_t] = 1$ is satisfied. Setting $b = 3$ and $\gamma_i = 1/3$, the transition probability becomes

$$
  \gamma_i = 1 - \left( 1 - \frac{1}{3} \right)^{3^{i - k}}
$$

The unconditional moments of the hidden states are given by

$$
\begin{align*}
  \mathrm{E}\left[ M_t^q \right] &= \mathrm{E}\left[ \left( \prod_{i=1}^k m_{t, k} \right)^q \right] \\
  &= \left( \frac{1}{3}m_0^q + \frac{1}{3}m_1^q + \frac{1}{3}m_2^q \right)^k
\end{align*}
$$

The first and second moments are simply

$$
\begin{align*}
  \mathrm{E}\left[ M_t \right] = \left( \frac{1}{3}m_0 + \frac{1}{3}m_1 + \frac{1}{3}m_2 \right)^k = 1 \\
  \mathrm{E}\left[ M_t^2 \right] = \left( \frac{1}{3}m_0^2 + \frac{1}{3}m_1^2 + \frac{1}{3}m_2^2 \right)^k
\end{align*}
$$

The variance of the hidden states is

$$
\begin{align*}
  \mathrm{Var}[M_t] &= \mathrm{E}\left[ M_t^2 \right] - \mathrm{E}\left[ M_t \right]^2 \\
  &= \left( \frac{1}{3}m_0^2 + \frac{1}{3}m_1^2 + \frac{1}{3}m_2^2 \right)^k - 1
\end{align*}
$$

The autocovariance at time interval $\Delta t$ is

$$
\begin{align*}
  \mathrm{E}\left[M_{t+\Delta} M_t \right] = \prod_{i=1}^k &\Set\ \frac{2}{3}\left[ 1 - \left(1 - \gamma_k \right)^{\Delta t} \right] \left( \frac{1}{3}m_0 m_1 + \frac{1}{3}m_1 m_2 + \frac{1}{3}m_2 m_0 \right) \right. \\
  &\left. + \left( \left( 1 - \gamma_k \right)^{\Delta t} + \frac{1}{3}\left[ 1 - \left( 1 - \gamma_k \right)^{\Delta t} \right] \right) \left( \frac{1}{3}m_0^2 + \frac{1}{3}m_1^2 + \frac{1}{3}m_2^2 \right) \right}
\end{align*}
$$

The corresponding autocorrelation function is

$$
  \rho_{\Delta t} = \frac{\mathrm{E}\left[M_{t+\Delta} M_t \right] - \mathrm{E}\left[ M_t \right]^2}{\mathrm{Var}[M_t]}
$$

#### Moments of compound process

The moments of absolute returns as a process $\mathrm{E}\left[ \left| x_t \right|^q \right] = \mathrm{E}\left[ \left| \sigma_t \right|^q \right] \mathrm{E}\left[ \left| \epsilon_t \right|^q \right]$ are given by for $q = 1, \dots, 4$

$$
\begin{align*}
  \mathrm{E}\left[ \left| x_t \right| \right] &= \frac{2\sigma}{\sqrt{2\pi}}\left( \frac{1}{3}m_0^{1/2} + \frac{1}{3}m_1^{1/2} + \frac{1}{3}m_2^{1/2} \right)^2 \\
  \mathrm{E}\left[ \left| x_t \right|^2 \right] &= \sigma^2 \\
  \mathrm{E}\left[ \left| x_t \right|^3 \right] &= \frac{4\sigma^3}{\sqrt{2\pi}}\left( \frac{1}{3}m_0^{3/2} + \frac{1}{3}m_1^{3/2} + \frac{1}{3}m_2^{3/2} \right)^2 \\
  \mathrm{E}\left[ \left| x_t \right|^3 \right] &= 3\sigma^4 \left( \frac{1}{3}m_0^2 + \frac{1}{3}m_1^2 + \frac{1}{3}m_2^2 \right)^2
\end{align*}
$$

The variance of the compound process is

$$
\begin{align*}
  \mathrm{var}\left[ \left| x_t \right| \right] &= \mathrm{E}\left[ \left| x_t \right|^2 \right] - \left( \mathrm{E}\left[ \left| x_t \right| \right] \right)^2 \\
  &= \sigma^2 \left[ 1 - \frac{2}{\pi}\left( \frac{1}{3}m_0^{1/2} + \frac{1}{3}m_1^{1/2} + \frac{1}{3}m_2^{1/2} \right)^{2k} \right]
\end{align*}
$$

#### Log-transformed volatility

Considering logarithmic increments

$$
\begin{align*}
  \eta_{t, \Delta t} &= \ln M_t - \ln M_{t-\Delta t} \\
  &= \sum_{i=1}^k \ln m_{t, i} + \sum_{i=1}^k \ln m_{t - \Delta t, i} \\
  &= \sum_{i=1}^k \epsilon_{t, i} + \sum_{i=1}^k \epsilon_{t - \Delta t, i}
\end{align*}
$$

The second unconditional moment of the log-transformed volatility process

$$
\begin{align*}
  \mathrm{E}\left[ \eta_{t+\Delta t, \Delta t}^2 \right] &= \frac{4}{3}\left[ \ln^2\left( m_0 \right) + \ln^2\left( m_1 \right) + \ln^2\left( m_0 \right) \right. \\
  &\quad \left. - \ln\left( m_0 \right)\ln\left( m_1 \right) - \ln\left( m_1 \right)\ln\left( m_2 \right) - \ln\left( m_2 \right)\left( m_0 \right) \right] \sum_{i=1}^k \left[ \frac{1}{3} \left( 1 - \left( 1 - \gamma_k \right)^{\Delta t} \right) \right]
\end{align*}
$$

The autocovariance of the log-transformed volatility process

$$
\begin{align*}
  \mathrm{E}\left[\eta_{t + \Delta t, \Delta t}, \eta_{t, \Delta t} \right] &= 2 \left[\ln \left(m_0\right) \ln \left(m_1\right) + \ln \left(m_1\right) \ln \left(m_2\right) + \ln \left(m_2\right)\ln \left(m_0\right) \right. \\
  &\quad \left. - \ln^2 \left(m_0\right) - \ln^2 \left(m_1\right) - \ln^2 \left(m_2\right) \right] \sum_{i=1}^k \left[ \frac{1}{3}\left( 1 - \left( 1 - \gamma_k \right)^{\Delta t} \right) \right]^2
\end{align*}
$$

The squared autocovariance of the log-transformed volatility process

$$
\begin{align*}
  \mathrm{E}\left[\eta_{t + \Delta t, \Delta t}^2, \eta_{t, \Delta t}^2 \right] =& 2 \sum_{i=1}^k \Set{ \frac{1}{9} \left[ 1 - \left( 1 - \gamma_k \right)^{\Delta t} \right]^2 }\xi \\
  &+ \frac{16}{9} \sum_{i=1}^k \Set{ \frac{1}{3}\left[ 1 - \left( 1 - \gamma_k \right)^{\Delta t} \right] \sum_{i'=1, i'\neq i}^k \frac{1}{3}\left[ 1 - \left( 1 - \gamma_k \right)^{\Delta t} \right] } \xi \\
  &+ 8 \sum_{i=1}^k \Set{ \frac{1}{9}\left[ 1 - \left( 1 - \gamma_k \right)^{\Delta t} \right]^2 \sum_{i'=1, i'\neq i}^k \frac{1}{9}\left[ 1 - \left( 1 - \gamma_k \right)^{\Delta t} \right]^2 }\xi
\end{align*}
$$

with 

$$
  \xi = \left[ \ln\left( m_0 \right) \ln\left( m_1 \right) + \ln\left( m_1 \right) \ln\left( m_2 \right) + \ln\left( m_2 \right) \ln\left( m_0 \right) - \ln^2\left( m_0 \right) - \ln^2\left( m_1 \right) - \ln^2\left( m_2 \right) \right]
$$

### GMM estimation

An MSM model can be estimated using GMM with the following moment conditions

$$
  \mathrm{E}\left[ \xi_{t+\Delta t, \Delta t}, \xi_{t, \Delta t} \right] \\
  \mathrm{E}\left[ \xi_{t+\Delta t, \Delta t}^2, \xi_{t, \Delta t}^2 \right]
$$

where $\xi_{t, \Delta t}$ represents either the log difference of absolute returns or the squared log difference

$$
  \xi_{t, \Delta t} = \ln\left| x_t \right| - \ln\left| x_{t - \Delta t} \right|
$$

or

$$
  \xi_{t, \Delta t} = \ln x_t^2 - \ln x_{t - \Delta t}^2
$$

For an $n$-nomial MSM, the parameter vector is given by

$$
    \theta = \begin{bmatrix} m_0, & m_1, & \dots, & m_{n-1}, & \sigma \end{bmatrix}
$$

Since the moment conditions of the multinomial weights $m_i$ are independent of $\sigma$, the covariance matrix of the parameters should be block-diagonal and estimated values of $\sigma$ should be identical to the sample standard deviation.

#### Log difference of absolute returns

Due to the assumption of independence between the Markov switching and white noise processes, we rewrite the log difference of absolute returns as

$$
\begin{align*}
  \xi_{t, \Delta t} &= \ln\left| x_t \right| - \ln\left| x_{t - \Delta t} \right| \\
  &= \ln\left[ \left| \sqrt{\prod_{i=1}^k} m_{t, i} \sigma \epsilon_t \right| \right] - \ln\left[ \left| \sqrt{\prod_{i=1}^k} m_{t-\Delta t, i} \sigma \epsilon_{t-\Delta} \right| \right] \\
  &= \frac{1}{2}\sum_{i=1}^k \left( \ln m_{t,k} - \ln m_{t-\Delta,k} \right) + \ln\left| \epsilon_t \right| - \ln\left| \epsilon_{t-\Delta t} \right|
\end{align*}
$$

The first autovariance condition moment is

$$
  \mathrm{E}\left[ \xi_{t+\Delta t, \Delta t}, \xi_{t, \Delta t} \right] = \frac{1}{4}\mathrm{E}\left[ \eta_{t+\Delta t, \Delta t} \eta_{t, \Delta t}  \right] + \left( \mathrm{E}\left[ \ln\left| \epsilon_t \right| \right] \right)^2 - \mathrm{E}\left[ \left(\ln\left| \epsilon_t \right|\right)^2 \right] 
$$

The second autovariance condition moment is

$$
\begin{align*}
  \mathrm{E}\left[ \xi_{t+\Delta t, \Delta t}^2, \xi_{t, \Delta t}^2 \right] &= \frac{1}{16}\mathrm{E}\left[ \eta_{t+\Delta t, \Delta t}^2 \eta_{t, \Delta t}^2 \right] - \Set{ \mathrm{E}\left[ \eta_{t, \Delta t}^2 \right] - \mathrm{E}\left[ \eta_{t+\Delta t, \Delta t} \eta_{t, \Delta t} \right] } \cdot \\
  &\quad \Set{ \left( \mathrm{E}\left[ \ln\left| \epsilon_t \right| \right] \right)^2 - \mathrm{E}\left[ \left(\ln\left| \epsilon_t \right|\right)^2 \right] } + 3\mathrm{E}\left[ \left(\ln\left| \epsilon_t \right|\right)^2 \right]^2 \\
  &\quad -4\mathrm{E}\left[ \ln\left| \epsilon_t \right| \right] \mathrm{E}\left[ \left(\ln\left| \epsilon_t \right|\right)^3 \right] + \mathrm{E}\left[ \left(\ln\left| \epsilon_t \right|\right)^4 \right] 
\end{align*}
$$

#### Log difference of squared returns

Writing the squared log returns as 

$$
\begin{align*}
  \ln x_t^2 &= \ln \sigma_t^2 + \ln \epsilon_t^2 \\
  &= \ln \sigma^2 + \sum_{i=1}^k \ln m_{t, i} + \ln \epsilon_t^2
\end{align*}
$$

the log difference of squared return can be expressed as

$$
  \xi_{t,\Delta t} = \sum_{i=1}^k \left( \ln m_{t, i} - \ln m_{t-\Delta t, i} \right) + \ln \epsilon_t^2 - \ln \epsilon_{t-\Delta t}^2
$$

### Maximum likelihood estimation

The $n$-nomial MSM model can be rewritten in the quasi state space

$$
\begin{gather*}
  x_t = \psi_t \epsilon_t \\
  \psi_t = A \psi_{t-1}
\end{gather*}
$$

where $\psi_t = \left( \psi_{t, i} \right)_{i=1}^{n^k}$ is the volatility state vector and $A$ is the transition matrix of size $n^k \times n^k$. The number of coefficients in $A$ increases with $(d^2 - d)$ with $d = n^k$, making it inefficient to estimate $A$ directly. By assuming that all parameters of the model are known, the conditional probability distribution of the volatility state can be expressed as

$$
  \hat{\psi}_{t|t} \equiv E(\psi_{t}| I_t ) = \begin{bmatrix} P\left( \psi_{t, 1} | I_t \right) \\ P\left( \psi_{t, 2} | I_t \right) \\ \vdots \\ P\left( \psi_{t, d} | I_t \right) \end{bmatrix}
$$

where $I_t = \Set{ x_i }_{i=0}^t$ is the information set. By Bayes' law, the posterior probability $P\left( \psi_t | x_t, I_{t-1} \right)$ are given by

$$
  P\left( \psi_t | x_t, I_{t-1} \right) \equiv P\left( \psi_t | I_t \right) = \frac{f\left(x_t \mid  \psi_t, I_{t-1} \right) P\left( \psi_t | I_{t-1} \right)}{f\left( x_t | I_{t-1} \right)}
$$

with the prior probability

$$
  P\left( \psi_t | I_t \right) = \sum_{\psi_{t-1}} P\left( \psi_t | \psi_{t-1} \right) P\left( \psi_{t-1} | I_{t-1} \right)
$$

and the density

$$
  f\left( x_t | I_{t-1} \right) = \sum_{\psi_t} f\left( x_t, \psi_t | I_{t-1} \right) = \sum_{\psi_t} P\left( \psi_t | I_{t-1} \right) f\left( x_t | \psi_t, I_{t-1} \right)
$$

Let $f_t$ be the density vector of $x_t$ conditional on $\psi_t$ and $I_{t-1}$

$$
  f_t = \begin{bmatrix} f\left( x_t | \theta_1, I_{t-1} \right) \\ f\left( x_t | \theta_2, I_{t-1} \right) \\ \vdots \\ f\left( x_t | \theta_d, I_{t-1} \right) \end{bmatrix} = \begin{bmatrix} f\left( x_t | \theta_1, \psi_{t,1}, I_{t-1} \right) \\ f\left( x_t | \theta_2, \psi_{t,2} I_{t-1} \right) \\ \vdots \\ f\left( x_t | \theta_d, \psi_{t, d}, I_{t-1} \right) \end{bmatrix}
$$

The conditional density $f\left( x_t | I_{t-1} \right)$ is determined by

$$
  f\left( x_t | I_{t-1} \right) = f_t^T \hat{\psi}_{t|t-1} = \boldsymbol{1}_d^T \left(f_t \odot \hat{\psi}_{t|t-1} \right)
$$

where $\odot$ denotes element-wise matrix multiplication and $\boldsymbol{1}_d^T$ is a unit vector. The filter inference $\hat{\psi}_{t|t}$ is written matrix notation by

$$
  \hat{\psi}_{t|t} = \frac{f_t \odot \hat{\psi}_{t|t-1}}{\boldsymbol{1}_d^T f_t \odot \hat{\psi}_{t|t-1}}
$$

which describes the filtered regime probabilities as the updated estimate of $\hat{\psi}_{t|t}$ of $\hat{\psi}_{t|t-1}$ given new information $x_t$. Because the filtered inference updates as

$$
  \hat{\psi}_{t+1|t} = A\hat{\psi}_{t|t}
$$

the iterated filtered inference can be written

$$
  \hat{\psi}_{t+1|t} = \frac{\left[f_t \odot A \right] \hat{\psi}_{t|t-1}}{\boldsymbol{1}_d^T \left[f_t \odot A \right] \hat{\psi}_{t|t-1}}
$$

The log-likelihood function can be derived as a by-product of the filter

$$
  \ln L(\theta|X) = \sum_{t=1}^\tau \ln\hat{\psi}_{t|t-1} \left[ f_t \odot A \right]\boldsymbol{1}_d^T 
$$

Bayes' rule implies

$$
\begin{align*}
  \ln\hat{\psi}_{t|t-1} \left[ f_t \odot A \boldsymbol{1}_d^T \right] &= \sum_{i=1}^d \sum_{j=1}^d P\left(\psi_{t-1}=m_i, \psi_t = m_j | I_{t-1}\right) f\left(x_t\mid  \psi_{t-1} = m_i, \psi_t = m_j \right) \\
  &= \sum_{i=1}^d \sum_{j=1}^d \hat{\psi}_{t-1|t-1, i} a_{ij} f^{ij}(x_t)
\end{align*}
$$

where $f^{ij}(x_t) = f\left(x_t\mid  \psi_{t-1} = m_i, \psi_t = m_j \right)$.

#### Probability state vector

The probability notion represented by the estimate $\hat{\psi}_{t|\tau} = E\left[ \psi_t | I_\tau \right]$ depends on the bound of $\tau$.

- For $\tau = t$, a filtering probability $\hat{\psi}_{t|\tau}$ gives an estimate for $\psi_t$ based on the available information up to $t = \tau$.
- For $\tau > t$, a smoothing probability $\hat{\psi}_{t|\tau}$ gives inference inference about $\psi_t$ and $x_t$ based on all information in the sample. The smoothed joint probability based on full information is given by

$$
\begin{align*}
  P\left[\psi_t^{(i)}, \psi_{t+1}^{(i)} | I_\tau \right] &= \frac{P\left[ \psi_{t + 1}^{(i)} | I_\tau \right] P\left[ \psi_t^{(j)} | I_t \right] P\left[ \psi_{t+1}^{(i)} | \psi_t^{(i)} \right]}{P\left[ \psi_{t + 1}^{(i)} | I_t \right]} \\
  P\left[ \psi_t^{(j)} | I_\tau \right] &= \sum_{i=1}^d P\left[\psi_t^{(i)}, \psi_{t+1}^{(i)} | I_\tau \right]
\end{align*}
$$

- For $\tau < t$ a forecasting probability $\hat{\psi}_{t|\tau}$ gives inference probability over future periodes on the basis of estimates of state probabilities.

### Model selection test

#### Markov switching criterion (MSC)

The Markov switching criterio (MSC) is a kind of penalized likelihood criteria with the form

$$
  \mathrm{MSC} = -2 \ln L + \sum_{i=1}^N \frac{T_i \left(T_i + \lambda_i \kappa \right)}{\delta_i T_i - \lambda_i \kappa - 2}
$$

where $N$ is the number of states, $\kappa$ is the number of multi-fractal parameters, $T_i$ is the number of observations in state $i$, which is calculated as the sum of smoothing probabilities of each state $i$. 