---
title: 'Statistics'
subject: 'Mathematics'
showToc: true
---

# Statistical model

A statistical model describes the process of generating sample data from a population, usually in the form of mathematical relationship between one or more random variables and deterministic parameters.

<MathBox title='Statistical model' boxType='definition'>
A statistical model can formally be defined by a pair $(S,\mathcal{P})$ where $S$ is the sample space and $\mathcal{P}$ is a set of probability distributions on $S$. The set $\mathcal{P}$ is almost always parametrized

$$
  \mathcal{P} = \{ P_\theta : \theta\in\Theta \}
$$

where $\Theta$ is the set of parameters of the model. The observed outcome of a statistical experiment has the form $\mathbf{x} = (x_i)_{i=1}^n \in S$, where $x_i$ is the vector of measurements for the $i$-th object. 

</MathBox> 

<MathBox title='Empirical distribution' boxType='definition'>
Suppose that $\mathbf{x} = (x_i)_{i=1}^{n\in\mathbb{N}}\in S$ is a sample. The empirical distribution associated with $\mathbf{X}$ assigns the probability $\frac{1}{n}$ at each $x_i$. If the sample values are distinct, the empirical distribution is the discrete uniform distribution. Generally, if $x$ occurs $k$ times in the sample data, the empirical distribution assigns probability $\frac{k}{n}$ to $x$. Thus, every finite data set defines a probability distribution.
</MathBox> 

<MathBox title='Statistics' boxType='definition'>
Suppose that $\mathbf{x} = (x_i)_{i=1}^{n\in\mathbb{N}}\in S$ is a sample. A statistic $w = w(\mathbf{x}):S\to \Theta$ is an observable function of the sample $\mathbf{x}$ where $\Theta$ is the parameter space.  
</MathBox> 

<MathBox title='Equivalence of statistics' boxType='proposition'>
Suppose that $\mathbf{x} = (x_i)_{i=1}^{n\in\mathbb{N}}\in S$ is a sample. Statistics $u$ and $v$ on $\mathbf{x}$ are equivalent if and only if for any $\mathbf{x},\mathbf{y}\in S$, $u(\mathbf{x}) = u(\mathbf{y})$ if and only if $v(\mathbf{x}) = v(\mathbf{y})$. This defines a equivalence relation on the collection of statistics for a given statistical model, which satisfies for arbitrary statistics $u, v$ and $w$

1. $u$ is equivalent to $u$ (reflexivity)
2. if $u$ is equivalent to $v$, then $v$ is equivalent to $u$ (symmetry)
3. if $u$ is equivalent to $v$ and $v$ is equivalent to $w$, then $u$ is equivalent to $w$ (transitivity)
</MathBox>

## Statistical inference

There are two broad branches of statistics
- descriptive statistics
- inferential statistics

Descriptive statistics refers to methods for summarizing and displaying sample data. The methods usually involve computing various statistics. In the context of descriptive statistics, the term parameter refers to a characteristic of the entire population.

Inferential statistics describes a statistical experiment as a random process with a probability measure $\mathbb{P}$ on an underlying sample space. The sample $\mathbf{x}$ of the experiment is an observed value of a random variable $\mathbf{X}$ with unknown distribution defined on this probability space. The goal of statical inference is to draw inferences about the distribution of $\mathbf{X}$ from the observed value $\mathbf{x}$. In inferential statistics, a statistic is itself a random variable, while a parameter refers to a characteristic of the distribution of $\mathbf{X}$.

<MathBox title='Random sample' boxType='definition'>
Suppose that $\mathbf{X} = (X_i)_{i=1}^{n\in\mathbb{N}}$ is an observable random variable for a statistical experiment. The most common and important special case of the inferential statistical model occurs when $\mathbf{X}$ is a sequence of independent and identically distributed (i.i.d.) random variables. In this case $\mathbf{X}$ represent independent copies of an underlying measurement vector $X$, and $\mathbf{X}$ is called a random sample of size $n$ from the distribution of $X$.
</MathBox>

<MathBox title='Parameter' boxType='definition'>
Suppose that $\mathbf{X} = (X_i)_{i=1}^{n\in\mathbb{N}}$ is an observable random variable for a statistical experiments. A parameter $\boldsymbol{\theta}$ is a function of distribution of $\mathbf{X}$ taking values in a parameter space $\Theta$.
</MathBox>

Typically, the distribution of an obervable random variable $\mathbf{X}$ will have $k\in\mathbb{N}^+$ real parameters of interest, so that $\boldsymbol{\theta} = (\theta_i)_{i=1}^{k\in\mathbb{N}}\in T\subseteq\mathbb{R}^k$

# Random samples

<MathBox title='Law of large numbers' boxType='theorem'>
Let $(X_i)_{i=1}^{n\in\mathbb{N})$ be a sequence of independent and identically distributed integrable random variables with expected value $\mathbb{E}(X_i) = \mu$. Suppose that $\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i$.

**Weak law of large numbers (Kinchin's law):** The sample average $\bar{X}_i$ converges in probability to $\mu$, i.e. $\bar{X}_n \xrightarrow[\textrm{i.p.}]{n\to\infty} \mu$, or for every $\varepsilon > 0$

$$
\begin{gather*}
  \lim_{n\to\infty} \mathbb{P}\left(|\bar{X}_n - \mu| < \varepsilon \right) = 1 \\
  \iff \lim_{n\to\infty} \mathbb{P}\left(|\bar{X}_n - \mu| > \varepsilon \right) = 0
\end{gather*}
$$

**Strong law of large numbers (Kolmogorov's law):** The sample average $\bar{X}_i$ converges almost surely to $\mu$, i.e. $\bar{X}_n \xrightarrow[\textrm{a.s.}]{n\to\infty}_ \mu$, or

$$
  \mathbb{P}\left(\lim_{n\to\infty} \bar{X}_n = \mu \right) = 1
$$

<details>
<summary>Proof</summary>

The weak law follows from Chebyshev's inequality

$$
  \mathbb{P}\left(|\bar{X}_n - \mu| > \varepsilon \right)\leq\frac{\mathrm{var}(\bar{X}_n)}{\varepsilon^2} = \frac{\sigma^2}{n\varepsilon^2} \xrightarrow{n\to\infty} 0
$$

The strong law is proved in three major steps. The first step is to show that with probability $1$ that $\bar{X}_{n^2} \xrightarrow{n\to\infty}\mu$. From Chebyshev's inequality,

$$
  \mathbb{P}\left(|\bar{X}_{n^2} - \mu| > \varepsilon \right)\leq\frac{\mathrm{var}(\bar{X}_n)}{\varepsilon^2} = \left(\frac{\sigma}{n\varepsilon}\right)^2 \xrightarrow{n\to\infty} 0
$$

Since $\sum_{n\mathbb{N}} \left(\frac{\sigma}{n\varepsilon}\right)^2 < \infty$, it follows from the first Borelli-Cantelli lemma that for every $\varepsilon > 0$

$$
  \mathbb{P}\left(|\bar{X}_{n^2} - \mu| > \varepsilon \textrm{ for infinitely many }n\in\mathbb{N}_+ \right) = 0
$$

From Boole's inequality it follows that for some rational $\varepsilon > 0$

$$
  \mathbb{P}\left(|\bar{X}_{n^2} - \mu| > \varepsilon \textrm{ for infinitely many }n\in\mathbb{N}_+ \right \right) = 0
$$

showing that $\bar{X}_{n^2}$ converges almost surely to $\mu$.

In the second step, we show that if the underlying sampling variable is nonnegative, so that $\mathbb{P}(X\geq 0) = 1$, then $\bar{X}_n\xrightarrow[\textrm{a.s.}]{n\to\infty}\mu$. Let $Y_n = \sum_{i=1}^n X_i$ so that $\bar{X}_n = Y_n / n$. Note first that $Y_n$ is almost surely increasing in $n$. For $n\in\mathbb{N}$ be the unique positive integer such that $k_n^2 \leq n < (k_n + 1)^2$. From the increasing propery it follows that (almost surely)

$$
  \frac{Y_{k_n^2}}{(k_n + 1)^2} \leq \frac{Y_n}{n} \leq \frac{Y_{(k_n + 1)^2}}{k_n^2}
$$

From the first step

$$
  \frac{Y_{k_n^2}}{(k_n + 1)^2} = \frac{Y_{k_n^2}}{k_n^2}\frac{k_n^2}{(k_n + 1)^2} \xrightarrow[\textrm{a.s.}]{n\to\infty}\mu
$$

Similarly,

$$
  \frac{Y_{(k_n + 1)^2}}{k_n^2} = \frac{Y_{(k_n + 1)^2}}{(k_n + 1)^2}\frac{(k_n + 1)^2}{k_n^2} \xrightarrow[\textrm{a.s.}]{n\to\infty}\mu
$$

By the squeeze theorem for limits it follows that $\bar{X}_n = Y_{n}/n \xrightarrow[\textrm{a.s.}]{n\to\infty}\mu$.

Finally, we relax the condition that the underlying sampling variable is $X$ is nonnegative. From step two, it follows that

$$
\begin{align*}
  \frac{1}{n}\sum_{i=1}^n X_i^+ &\xrightarrow[\textrm{a.s.}]{n\to\infty} \mathbb{E}(X^+) \\
  \frac{1}{n}\sum_{i=1}^n X_i^- &\xrightarrow[\textrm{a.s.}]{n\to\infty} \mathbb{E}(X^-)
\end{align*}
$$

From the linearity of expected value

$$
\begin{align*}
  \frac{1}{n}\sum_{i=1}^n X_i &= \frac{1}{n}\sum_{i=1}^n \left(X_i^+ - X_i^+\right) \\
  &= \frac{1}{n}\sum_{i=1}^n X_i^+ - \frac{1}{n}\sum_{i=1}^n X_i^- \\
  &\xrightarrow[\textrm{a.s.}]{n\to\infty} \mathbb{E}(X^+) - \mathbb{E}(X^+) = \mathbb{E}(X^+ - X^-) = \mathbb{E}(X) = \mu
\end{align*}
$$
</details>
</MathBox>

## Partial sum process

<MathBox title='Partial sum process' boxType='definition'>
Suppose that $\mathbf{X} = (X_n)_{n\in\mathbb{N}}$ is a sequence of independent, identically distributed random variables with common probability density function $f$, mean $\mu$ and standard deviation $\sigma\in(0,\infty)$. Let $Y_n = \sum_{i=1}^n X_i$ with $Y_0 = 0$. The random process $\mathbf{Y} = (Y_n)_{n\in\mathbb{N}^+}$ is called the partial sum process associated with $\mathbf{X}$.
</MathBox>

<MathBox title='Properties of partial sum processes' boxType='proposition'>
Let $\mathbf{Y} = (Y_n)_{n\in\mathbb{N}^+}$ be a partial sum process for a sample variable $X$ with mean $\mu$, variance $\sigma^2$ and probability density function $f$. If $m, n\in\mathbb{N}$ with $m\leq n$ then

1. $Y_n - Y_m$ has the same distribution as $Y_{n-m}$, implying that $\mathbf{Y}$ has stationary increments
2. if $(n_i)_{i\in\mathbb{N}}$ is an increasing sequence then $(Y_{n_i} - Y_{n_{i-i}})_{i\in\mathbb{N}}$ is a sequence of independent random variables, implying that $\mathbb{Y}$ has independent increments
3. $\mathbb{E}(Y_n) = n\mu$
4. $\mathrm{var}(Y_n) = n\sigma^2$
5. $\mathrm{cov}(Y_m, Y_n) = m\sigma^2$
6. $\mathrm{cor}(Y_m, Y_n) = \sqrt{\frac{m}{n}}$
7. $\mathbb{E}(Y_m Y_n) = m\sigma^2 + mn\mu^2$
8. the probability density function of $Y_n$ is the convolution power of $f$ of order $n$, i.e. $f^{*n}$
  a. if $(n_i)_{i=1}^{k\in\mathbb{N}}$ is a strictly increasing sequence then $(Y_{n_i})_{i=1}^{k\in\mathbb{N}}$ has joint probability density function for $\mathbf{y} = (y_i)_{i=1}^{k\in\mathbb{N}} \in\mathbb{R}^k$

$$
\begin{align*}
  f_{n_1, n_2,\dots,n_k} (\mathbf{y}) &= f^{*n_1}(y_1)\prod_{i=2}^k f^{*(n_i - n_{i-1})}(y_i - y_{i-1})
\end{align*}
$$

8. if $\mathbb{X}$ has moment generating function $G$, then $Y_n$ has moment generating function $G^n$

<details>
<summary>Proof</summary>

1. Note that $Y_n - Y_m = sum_{i=m+1}^{n} X_i$, which is the sum of $n-m$ independent variables, each with the common distribution. Conversely, $Y_{n-m}$ is also the sum of $n-m$ independent variables, each with the common distribution.

2. The terms in the sequence of increments $(Y_{n_i} - Y_{n_{i-i}})$ are sums over disjoint collections of terms in the sequence $\mathbf{X}$. Since the sequence $\mathbf{X}$ is independent, so is the sequence of increments 

3. This follows from the linear property of expected value 

$$
  \mathbb{E}(Y_n) = \mathbb{E}\left(\sum_{i=i}^n X_i \right) = \sum_{i=1}^n \mathbb{E}(X_i) = n\mu
$$

4. By independence 

$$
  \mathrm{var}(Y_n) = \mathrm{var}\left(\sum_{i=i}^n X_i \right) = \sum_{i=1}^n \mathrm{var}(X_i) = n\sigma^2
$$

5. Note that $Y_n = Y_m + (Y_n - Y_m)$ giving

$$
\begin{align*}
  \mathrm{cor}(Y_m, Y_n) &= \mathrm{cor}(Y_m, Y_m) + \mathrm{cor}(Y_m, Y_n - Y_m) \\
  &= \mathrm{var}(Y_m) + 0 \\
  &= m\sigma^2
\end{align*}
$$

6.
$$
\begin{align*}
  \mathrm{cor}(Y_m, Y_n) &= \frac{\mathrm{cov}(Y_m, Y_n)}{\mathrm{sd}(Y_m) \mathrm{sd}(Y_m)} \\
  &= \frac{m\sigma^2}{\sqrt{m\sigma^2}\sqrt{n\sigma^2}} = \sqrt{\frac{m}{n}}
\end{align*}
$$

7.
$$
  \mathbb{E}(Y_m Y_n) = \mathrm{cov}(Y_m, Y_n) + \mathbb{E}(Y_m)\mathbb{E}(Y_n) = m\sigma^2 + m\mu n\mu
$$

8. The probability density function (PDF) of a sum of independent variables is the convolution of the PDFs of the terms.

9. This follows from the fact that the generating function of a sum of independent variables is the product of the generating functions of the terms.
</details>
</MathBox>

## Central limit theorem

<MathBox title='Central limit theorem' boxType='theorem'>
Let $\mathbf{Y}$ be a partial sum process of a sample variable $X$ with mean $\mu$, variance $\sigma^2$ and characteristic function

$$
  \chi_n (t) = \mathbb{E}\left[\exp\left(it\frac{X - \mu}{\sigma} \right)\right],\quad t\in\mathbb{R}
$$

Define the common standard score 

$$
  Z_n = \frac{Y_n - n\mu}{\sqrt{n}\sigma} = \frac{\bar{X}_n - \mu}{\sigma/\sqrt{n}}
$$

with characteristic function

$$
  \chi_n (t) = \mathbb{E}(e^{itZ_n}),\quad t\in\mathbb{R}
$$

In the limit $n\to\infty$, the distribution of $Z_n$ converges to the standard normal distribution, i.e.

$$
  \lim_{n\to\infty} \chi_n (t) = e^{-t^2/2}
$$

<details>
<summary>Proof</summary>

Noting that

$$
  Z_n &= \frac{\bar{X}_n - \mu}{\sigma/\sqrt{n}} = \sum_{i=1}^n \frac{X_i - \mu}{\sqrt{n}\sigma}
$$

we may rewrite

$$
\begin{align*}
  \chi_n (t) &= \mathbb{E}\left[\exp\left(itZ_n\right)\right] = \mathbb{E}\left[\exp\left(i\frac{t}{\sqrt{n}}\sum_{i=1}^n \frac{X_i - \mu}{\sigma} \right)\right] \\
  &= \mathbb{E}\left[\prod_{i=1}^n\exp\left(i\frac{t}{\sqrt{n}} \frac{X_i - \mu}{\sigma} \right)\right] = \prod_{i=1}^n \mathbb{E}\left[\exp\left(i\frac{t}{\sqrt{n}} \frac{X_i - \mu}{\sigma} \right)\right] \\
  &= \chi^n \left(\frac{t}{\sqrt{n}}\right)
\end{align*}
$$

Noting that $\chi(0)=1$, $\chi'(0)=0$, $\chi''(0) = -1$, the second order Taylor expansion of $chi\left(\frac{t}{\sqrt{n}}\right)$ about $t=0$ is

$$
  \chi\left(\frac{t}{\sqrt{n}}\right) = 1 - \frac{t^2}{2n} + o\left(\frac{t^2}{n} \right)
$$

In the limit $n\to\infty$, the higher order terms $o\left(\frac{t^2}{n}\right)$ vanish and noting that $e^x = \lim_{n\to\infty} \left(1 + \frac{x_n}{n} \right)^n$, we get

$$
  \lim_{n\to\infty} \chi^n \left(\frac{t}{\sqrt{n}}\right) = e^{-\frac{t^2}{2}}
$$
</details>
</MathBox>

## Sample moments

### Sample mean

<MathBox title='Sample mean' boxType='definition'>
Suppose that $\mathbf{x}\in S\subseteq\mathbb{R}^n$ is a sample of size $n\in\mathbb{N}^+$ from a real-valued variable. The *sample mean* is simply the arithmetic average of the sample values

$$
  m \equiv \bar{x} := \frac{1}{n}\sum_{i=1}^n x_i
$$
</MathBox>

### Sample variance

<MathBox title='Sample mean' boxType='definition'>
Suppose that $\mathbf{x}\in S\subseteq\mathbb{R}^n$ is a sample of size $n\in\mathbb{N}^+$ from a real-valued variable. The *sample variance* is defined as the mean square deviation

$$
  s^2 = \frac{1}{n-1}\sum_{i=1}^n (x_i - m)^2
$$
</MathBox>

# Point estimation

<MathBox title='Estimator' boxType='definition'>
Let $\theta\in \Theta\subseteq\mathbb{R}$ be an unknown real parameter. A real-valued statistic $U = u(\mathbb{X})$ that is used to estimate $\theta$ is called an estimator of $\theta$. The estimator is a random variable whose moments generally depend on $\theta$.

If $U$ is an estimator for $\theta$, the following can be defined
1. $U-\theta$ is the error
2. $\mathrm{bias}(U) = \mathbb{E}(U - \theta) = \mathbb{E}(U) - \theta$ is the bias of $U$
3. $\mathrm{mse}(U) = \mathbb{E}\left[(U - \theta)^2\right]$ is the mean square error of $U$

Depending on the sign of $\mathrm{bias}(U)$ we say that
1. $U$ is *unbiased* if $\mathrm{bias}(U) = 0$, or equivalently $\mathbb{E}(U) = \theta$
2. $U$ is negatively biased if $\mathrm{bias}(U)\leq 0$, or equivalently $\mathbb{E}(U)\leq\theta$
3. $U$ is positively biased if $\mathrm{bias}(U)\geq 0$, or equivalently $\mathbb{E}(U)\geq\theta$
</MathBox>

<MathBox title='Estimator bias' boxType='definition'>
Let $\theta\in\Theta\subseteq\mathbb{R}$ be an unknown real parameter. A real-valued statistic $U = u(\mathbb{X})$ that is used to estimate $\theta$ is called an estimator of $\theta$. The estimator is a random variable whose moments generally depend on $\theta$.

If $U$ is an estimator for $\theta$, the following can be defined
1. $U-\theta$ is the error
2. $\mathrm{bias}(U) = \mathbb{E}(U - \theta) = \mathbb{E}(U) - \theta$ is the bias of $U$
3. $\mathrm{mse}(U) = \mathbb{E}\left[(U - \theta)^2\right] = \mathrm{var}(U) + \mathrm{bias}^2(U)$ is the mean square error of $U$.

<details>
<summary>Details</summary>

$$
\begin{align*}
  \mathbb{E}\left[(U - \theta)^2\right] &= \mathrm{var}(U-\theta) + \left[\mathbb{E}(U-\theta)\right]^2 \\
  &= \mathrm{var}(U) + \mathrm{bias}^2(U) 
\end{align*}
$$
</details>
</MathBox>

<MathBox title='Estimator efficiency' boxType='definition'>
Let $U$ and $V$ be unbiased estimators of a parameter $\theta\in \Theta\subseteq\mathbb{R}$. Then
1. $U$ is *more efficient than* $V$ if $\mathrm{var}(U)\leq\mathrm{var}(V)$
2. the *relative efficiency** of $U$ with respect to $V$ is

$$
  \mathrm{eff}(U, V) = \frac{\mathrm{var}(V)}{\mathrm{var}(U)}
$$
</MathBox>

## Asymptotic properties

<MathBox title='Asymptotically unbiased' boxType='definition'>
The sequence of estimators $\mathbf{U} = (U_i)_{i=1}^{n\in\mathbb{N}}$ is *asymptotically unbiased* if $\mathrm{bias}(U_n)\xrightarrow{n\to\infty} 0$ for every $\theta\in T$, or equivalently $\mathbb{E}(U_n)\xrightarrow{n\to\infty}\theta$. 
</MathBox>

<MathBox title='Asymptotic relative efficiency' boxType='definition'>
Suppose that $\mathbf{U} = (U_i)_{i=1}^{n\in\mathbb{N}}$ and $\mathbf{V} = (V_i)_{i=1}^{n\in\mathbb{N}}$ are two sequences of estimators that are asymptotically unbiased. The *asymptotic relative efficiency* of $\mathbf{U}$ to $\mathbf{V}$ is

$$
  \lim_{n\to\infty}\mathrm{eff}(U_n, V_n) = \lim_{n\to\infty}\frac{\mathrm{var}(V_n)}{\mathrm{var}(U_n)}
$$

assuming that the limit exists.
</MathBox>

<MathBox title='Consistency' boxType='definition'>
Suppose that $\mathbf{U} = (U_i)_{i=1}^{n\in\mathbb{N}}$ is a sequence of estimators for $\theta\in \Theta\subseteq\mathbb{R}$. Then
1. $\mathbf{U}$ is *consistent* if $U_n\xrightarrow{n\to\infty}\theta$ for each $\theta\in T$, i.e. $\mathbb{P}\left(|U_n - \theta| > \varepsilon \right) \xrightarrow{n\to\infty} 0$ for every $\varepsilon > 0$.
2. $\mathbf{U}$ is *mean-square consistent* if $\mathrm{var}(U_n) = \mathbb{E}\left[(U_n - \theta)^2 \right]$  
</MathBox>

<MathBox title='Consistency relation' boxType='proposition'>
Suppose that $\mathbf{U} = (U_i)_{i=1}^{n\in\mathbb{N}}$ is a sequence of estimators for $\theta\in \Theta\subseteq\mathbb{R}$. If $\mathbb{U}$ is mean-square consistent then $\mathbb{U}$ is consistent and asymptotically unbiased.

<details>
<summary>Proof</summary>

From Markov's inequality

$$
\begin{align*}
  \mathbb{P}\left(|U_n - \theta| > \varepsilon \right) &= \mathbb{P}\left[(U_n - \theta)^2 > \varepsilon \right] \\
  \leq \frac{\mathbb{E}\left[ (U_n  - \theta)^2 \right]}{\varepsilon^2} \xrightarrow{n\to\infty} 0
\end{align*}
$$
</details>
</MathBox>

## The method of moments

Consider a statistical experiment with an observable real random value $X$. The distribution of $X$ has $k$ unknown real parameters $\boldsymbol{\theta} = (\theta_i)_{i=1}^k\in\Theta\subseteq\mathbb{R}^k$. Repeating the experiment $n$ times generates a random sample of size $n$ from the distribution of $X$ in the form $\mathbf{X} = (X_i)_{i=1}^n$. Thus, $\mathbf{X}$ is a sequence of independent random variables, each with the distribution of $X$. 

The method of moments is a technique fro constructing estimators of the parameters that is based on matching the sample moments with the corresponding distribution moments. The $j$-th moment of $X$ about $0$ is a function of $\boldsymbol{\theta}$

$$
  \mu^{(j)}(\boldsymbol{\theta}) = \mathbb{E}(X^j),\quad j\in\mathbb{N}^+
$$

where $\mu^{(1)}(\boldsymbol{\theta})$ is just the mean of $X$. The $j$-th sample moment about $0$ takes the form

$$
  M^{(j)}(\mathbf{X}) = \frac{1}{n}\sum_{i=1}^n X_i^j
$$

where $M^{(1)}(\mathbf{X})$ is the sample mean $\bar{X}_n$.

To construct the method of moments estimators $\mathbf{W} = (W_i)_{i=1}^k$ for the parameters $\boldsymbol{\theta} = (\theta_i)_{i=1}^k$ respectively, we consider the equations

$$
  \mu^{(j)}(\mathbf{W}) = M^{(j)}(\mathbf{X})
$$

consecutively for $j$ until we are able to solve for $\mathbf{W}$ in terms of $M^{(j)}$.

## Maximum likelihood estimator

<MathBox title='Likelihood function' boxType='definition'>
Suppose that $\mathbf{X}\in S$ is an observable random variable for a statistical experiment, depending on unknown parameters $\boldsymbol{\theta} = (\theta_i)_{i=1}^{k\in\mathbb{N}}\in\Theta\subseteq\mathbb{R}^k$. Let $f_\boldsymbol{\theta}$ denote the probability density function of $\mathbf{X}$ for $\boldsymbol{\theta}$. The likelihood function at $\mathbf{x}\in S$ is the function $L_\mathbf{x}:\Theta\to[0,\infty)$ given by

$$
  L_\mathbf{x} (\boldsymbol{\theta}) = f_\boldsymbol{\theta}(\mathbf{x})
$$

The logarithm of the likelihood function, called log-likelihood function, at $\mathbf{x}\in S$ is the function $\ln L_\mathbf{x}$ given by

$$
  \ln L_\mathbf{x}(\boldsymbol{\theta}) = \ln f_\boldsymbol{\theta}(\mathbf{x})
$$

The likelihood function is the function obtained by reversing the roles of $\mathbf{x}$ and $\theta$ in the probability density function, i.e. we view $\theta$ as the variable and $\mathbf{x}$ as the given information.
</MathBox>

<MathBox title='Maximum likelihood estimator' boxType='definition'>
Suppose that the maximum value of $L_\mathbf{x}$ or $\ln L_\mathbf{x}$ occurs at $u(\mathbf{x})\in\Theta$ for each $\mathbf{x}\in S$. Then the statistic $u(\mathbf{X})$ is a *maximum likelihood estimator* of $\boldsymbol{\theta}$. If $L_\mathbf{x}$ is differentiable, we can find this point by solving

$$
  \frac{\partial}{\partial\theta_i}L_\mathbf{x}(\boldsymbol{\theta}) = 0
$$

or equivalently

$$
  \frac{\partial}{\partial\theta_i}\ln L_\mathbf{x}(\boldsymbol{\theta}) = 0
$$

The most important special case is when the sample is a random sample of an observable real random variable $X$. Suppose that $\mathbf{X} = (X_i)_{i=1}^n \in S\subseteq\mathbb{R}^n$ is random sample of size $n$ from the distribution of $X$ with probability density function $f_\boldsymbol{\theta}$. The likelihood and log-likelihood functions for $\mathbf{x}\in S$ are

$$
\begin{align*}
  L_\mathbf{x} (\boldsymbol{\theta}) &= \prod_{i=1}^n f_\boldsymbol{\theta} (x_i) \\
  \ln L_\mathbf{x} (\boldsymbol{\theta}) &= \sum_{i=1}^n \ln f_\boldsymbol{\theta} (x_i)
\end{align*}
$$
</MathBox>

<MathBox title='Reparametrized likelihood function' boxType='definition'>
Suppose that $h:\Theta\to\Lambda$ and let $\boldsymbol{\lambda} = h(\boldsymbol{\theta})$ denote the new parameter. Define the likelihood function for $\lambda$ at $\mathbf{x}\in S$ by

$$
  \hat{L}_\mathbf{x}(\boldsymbol{\lambda}) = \max\left\{ L_\mathbf{x}(\boldsymbol{\theta}) \;|\; \boldsymbol{\theta}\in h^{-1}(\boldsymbol{\lambda}) \right\},\quad \boldsymbol{\lambda}\in\Lambda
$$

If $v(\mathbf{x})\in\Lambda$ maximized $\hat{L}_\mathbf{x}$ for each $\mathbf{x}\in S$ then $V = v(\mathbf{X})$ is a maximum likelihood estimator of $\lambda$. Conversely, if $U = u(\mathbf{x})\in\Theta$ is a maximum likelihood estimator for $\boldsymbol{\theta}$, then $V = h(U)$ is a maximum likelihood estimator for $\boldsymbol{\lambda}$. This is known as the invariance property.
</MathBox>

## Bayesian estimator

Consider a statistical experiment with an observable random variable $\mathbf{X}\in S$, whose distribution depends on parameters $\boldsymbol{\theta}\in T$. *Bayesian analysis* models the parameters $\boldsymbol{\theta}$ with a random variable $\boldsymbol{\Theta}$ that has a specified distribution on the parameter space $T$. This distribution is called the *prior distribution* of $\boldsymbol{\Theta}$ and reflects knowledge of the parameters $\boldsymbol{\theta}$ before sampling data. After observing $\mathbf{X} = \mathbf{x}\in S$, Bayes' theorem is used to compute the conditional distribution of $\boldsymbol{\Theta}$ given $\mathbf{X} = \mathbf{x}$. This distribution is called the *posterior distribution* of $\boldsymbol{\Theta}$ and reflects updated inferences of $\boldsymbol{\theta}$ given new information.

<MathBox title='Posterior distribution' boxType='definition'>
Let $\mathbf{X}\in S$ be an observable random variable with probability density function $f$. Suppose that the *prior distrubution* of $\boldsymbol{\Theta}$ on $T$ has probability density function $h$, and that given $\boldsymbol{\Theta} = \boldsymbol{\theta}\in T$, the conditional probability density function of $\mathbf{X}$ on $S$ is $f(\cdot|\boldsymbol{\theta})$. Then the probability density function of the posterior distribution of $\boldsymbol{\Theta}$ given $\mathbf{X} = \mathbf{x}\in S$ is

$$
  h(\boldsymbol{\theta}|\mathbf{x}) = \frac{h(\boldsymbol{\theta})f(\mathbf{x}|\boldsymbol{\theta})}{f(\mathbf{x})}
$$

where the unconditional probability function $f$ is defined as follows, in the discrete and continuous cases, respectively

$$
\begin{align*}
  f(\mathbf{x}) &= \sum_{\boldsymbol{\theta}\in T} h(\boldsymbol{\theta})f(\mathbf{x}|\boldsymbol{\theta}) \\
  f(\mathbf{x}) &= \int_T h(\boldsymbol{\theta})f(\mathbf{x}|\boldsymbol{\theta})\,\mathrm{d}\boldsymbol{\theta}
\end{align*}
$$

The most import and common special case occurs when $\mathbf{X} = (X_i)_{i=1}^{n\in\mathbb{N}}$ is a random sample of size $n$ from the distribution of an obervable random variable $X$. If $X$ is real-valued and has probability density function $g(\cdot|\boldsymbol{\theta})$ for a given $\boldsymbol{\theta}\in T$. In this case, $S=\mathbb{R}^n$ and the probability function $f(\cdot|\boldsymbol{\theta})$ of $\mathbf{X}$ given $\boldsymbol{\theta}$ is

$$
  f(\mathbf{x}|\boldsymbol{\theta}) = \prod_{i=1}^n g(x_i|\boldsymbol{\theta})
$$

<details>
<summary>Details</summary>

Note that the joint probability density function of $(\mathbf{X},\boldsymbol{\Theta})$ is a mapping on $f: S\times T\to [0, \infty)$ given by

$$
  (\mathbf{x}, \boldsymbol{\theta})\mapsto h(\boldsymbol{\theta})f(\mathbf{x}|\boldsymbol{\theta})
$$

The function $f(\mathbf{x})$ is the marginal probability density function of $\mathbf{X}$. 
</details>
</MathBox>

If the parameter space has finite measure $c$, then one possible prior distribution is the uniform distribution on $T$. If $\theta\in T\subseteq\mathbb{R}$ the probability density function is given by $h(\theta) = \frac{1}{c}$. This distribution reflects no prior knowledge about the parameter, and is called the *non-informative* prior distribution.

<MathBox title='Bayesian estimator' boxType='definition'>
Suppose that $\theta\in T\subseteq\mathbb{R}$ is a real-valued parameter for an observable random variable $\mathbf{X}$. The conditional expected value $\mathbb{E}(\theta|\mathbf{X})$ is the Bayesian estimator for $\theta$, which takes the form, in the discrete and continuous cases, respectively

$$
\begin{align*}
  \mathbb{E}(\theta|\mathbf{X} &= \mathbf{x}) &= \sum_{\theta\in T} \theta h(\theta|\mathbf{x}) \\
  \mathbb{E}(\theta|\mathbf{X} &= \int_T \theta h(\tehta|\mathbf{x})\,\mathrm{d}\boldsymbol{\theta}
\end{align*}
$$

The definitions of bias and mean square get conditioned on $\Theta = \theta\in T$. If $U$ is a Bayesian estimator for $\theta$ then

1. The *bias* of $\mathrm{bias}(U|\theta) = \mathbb{E}(U-\theta|\Theta = \theta)$
2. The *mean square error* of $U$ is $\mathrm{mse}(U|\theta) = \mathbb{E}\left[(U-\theta)^2 | \Theta = \theta \right] = \mathrm{var}(U|\theta) + \mathrm{bias}^2(U|\theta)$
</MathBox>

<MathBox title='Properties of Bayesian estimators' boxType='proposition'>
Let $\mathbf{U} = (U_n)_{n\in\mathbb{N}^+}$ be a sequence of Baysian estimators of $\theta\in T\subseteq\mathbb{R}$. Then
1. $\mathbf{U}$ is *asymptotically unbiased* if $\mathrm{bias}(U_n|\theta)\xrightarrow{n\to\infty} 0$
2. $\mathbf{U}$ is *mean-square consistent* if $\mathrm{mse}(U_n|\theta)\xrightarrow{n\to\infty} 0$
</MathBox>

# Time series analysis

Let $X_t \overset{\Delta}{=} \{X_t\}_{t\in T}$ be a stochastic process on an index set $T\subseteq\mathbb{R}$ and let $F_X \left(x_{t_i}\right)_{i=1}^{n\in\mathbb{N}} = P\left( X_{t_i}\leq x_{t_i} \right)_{i=1}^{n\in\mathbb{N}}$ denote the cumulative joint distribution function of $X_t$.

## Autocovariance

<MathBox title='Autocovariance and autocorrelation' boxType='definiton'>
The autocovariance of a stochastic process $X_t$ with mean $\mu_t = E[X_t]$ is defined as

$$
  \gamma_{XX}(t_1, t_2) := \mathrm{cov}\left[X_{t_1}, X_{t_2}\right] = E\left[\left(X_{t_1} - \mu_{t_1}\right)\left(X_{t_2} - \mu_{t_2}\right)\right] = E\left[X_{t_1} X_{t_2} \right] - \mu_{t_1}\mu_{t_2}
$$

The normalized autocorrelation of $X_t$ is defined as

$$
  \rho_{XX}(t_1, t_2) = \frac{\gamma_{XX}(t_1, t_2)}{\sigma_{t_1}\sigma_{t_2}}
$$
</MathBox>

## Stationary process

<MathBox title='Stationarity' boxType='definiton'>
**Strict stationarity**: A stochastic process $X_t$ is called strictly/strongly stationary if 

$$
  \left( X_{t_i} \right)_{i=1}^{n\in\mathbb{N}} \overset{d}{=} (X_{t_i + \tau})_{i=1}^{n\in\mathbb{N}}\quad \tau, (t_i)_{i=1}^{n\in\mathbb{N}} \in T
$$

i.e. the finite dimensional distributions of $X_t$ are invariant under time shifts. In terms of the cumulative joint distribution function, this can be reformulated as

$$
  F_X \left(x_{t_i}\right)_{i=1}^{n\in\mathbb{N}} = F_X\left(x_{t_i + \tau}\right)_{i=1}^{n\in\mathbb{N}}
$$

**Weak stationarity**: The stochastic process $X_t$ is called weakly stationary if for all $t\in T$
- the means are time invariant, i.e. $E[X_t] = E[X_{t + \tau}]$
- the second moments are finite, i.e. $E\left[|X_t|^2\right] < \infty$
- the autocovariance is time invariant, $\gamma_{X} (t, t + \tau) = \gamma_{X}(\tau, 0) \overset{\Delta}{=} \gamma(\tau)$
</MathBox>

## Autoregressive models (AR)

An autoregressive model of order $p$, denoted $\mathrm{AR}(p)$ is defined as 

$$
    X_t = c + \sum_{i=1}^p \varphi_i X_{t-i} + \epsilon_t
$$

where
- $\phi_i$ re the parameters of the model
- $c$ is a constant
- $\epsilon_t$ is white noise

This can be equivalently written in terms of the lag (backshift) operator $L$ ($B$) as

$$
\begin{gather*}
  X_t = c + \sum_{i=1}^p \varphi_i L^i X_t + \epsilon_t \\
  \varphi(L)X_t \equiv \left(1 - \sum_{i=1}^p \varphi_i L^i \right) X_t = \epsilon_t
\end{gather*}
$$

### Autoregressive moving average (ARMA)

An autoregressive-moving average (ARMA) model describes a stationary stochastic process in terms of two polynomials, one for the autoregression (AR) and the second for the moving average (MA). An ARMA-model with $p$ autoregressive terms and $q$ moving-average terms, denoted $\mathrm{ARMA}(p, q)$ is defined as

$$
\begin{gather*}
  X_t = c + \epsilon_t + \sum_{i=1}^p \varphi_i X_{t-i} + \sum_{i=1}^q \theta_i \epsilon_{t-i} \\
  \left(1 - \sum_{i=1}^p \varphi_i L^i \right) X_t = \left(1 + \sum_{i=1}^p \theta_i L^i \right)\epsilon_t \\
  \varphi(L) X_t = \theta(L) \epsilon_t
\end{gather*}
$$

### Autoregressive integrated moving average (ARIMA)

Assume that the AR-polynomial $\alpha(L) = 1 - \sum_{i=1}^{p'} \alpha_i L^i$ has a unit root $(1 - L)$ of multiplicity $d$. Then it can be rewritten as

$$
  1 - \sum_{i=1}^{p'} \alpha_i B^i = \left(1 - \sum_{i=1}^{p' - d} \varphi_i B^i \right) (1 - L)^d
$$

An $\mathrm{ARIMA}(p, d, q)$ process expresses this polynomial factorisation property with $p = p' - d$ given by

$$
  \left(1 - \sum_{i=1}^{p' - d} \varphi_i B^i \right) (1 - L)^d X_t = \delta + \left(1 + \sum_{i=1}^p \theta_i L^i \right)\epsilon_t
$$

with drift $\frac{\delta}{1 - \sum_{i=1}^p \varphi_i}$.

### Autoregressive fractal integrated moving average (ARFIMA)

The ARFIMA process allows a fractional value for the mulitiplicity $d$. For integer $d \in \mathbb{N}$, the differencing operator $(1 - L)^d$ can be expressed as a binomial expansion

$$
  (1 - L)^d = \sum_{i=0}^d \binom{d}{i} (-1)^i L^{d - i}
$$

To allow non-integer $d$, the binomial coefficients can be expressed in terms of the Gamma function

$$
  \binom{d}{i} = \frac{d!}{i!(d - i)!} = \frac{\Gamma(d + 1)}{\Gamma(i + 1)\Gamma(d - i + 1)} 
$$

The self-similar properties of ARFIMA depend on the values of $d$
- $d = 0$ reduces to an $\mathrm{ARMA}(p, q)$ process, i.e. a short-memory process
- For $d \in \left(0, \frac{1}{2}\right)$, the autocorrelations are all positive. The resulting process is asymptotically second order stationary exhibiting long memory. The corresponding Hurst exponent is $H = \frac{1}{2} + d$.
- For $d \in \left( -\frac{1}{2}, 0 \right)$ the autocorrelations are all negative, except $\rho(0) = 1$. The resulting process is anti-persistent exhibiting intermediate memory.
- For $d \geq \frac{1}{2}$, the process is no longer covariance stationary, and have infinite variance. By taking appropriate differences, a non-stationary process with $d > \frac{1}{2}$ can be reduced to a stationary process with $-\frac{1}{2} < d < \frac{1}{2}$


## Autoregressive conditional heteroskedasticity models (ARCH)

Autoregressive conditional heteroskedasticity (ARCH) models the variance of a mean process $\mu_t$ of the form

$$
  r_t = \mu_t + \epsilon_t
$$

with the return residuals $\epsilon_t = \sigma_t z_t$ where $z_t$ is a white noise process and

$$
  \sigma_t^2 = \alpha_0 + \sum_{i=1}^q \alpha_i \epsilon_{t-i}^2
$$

### Generalized autoregressive conditional heteroskedasticity (GARCH)

$$
  \sigma_t^2 = \omega + \sum_{i=1}^q \alpha_i \epsilon_{t-i}^2 + \sum_{i=1}^p \beta_i \sigma_{t-i}^2
$$

### FIGARCH

## Markov-switching multifractal (MSM)

Let $P_t$ be the price of a financial asseet, and let $r_t = \ln\left(\frac{P_t}{P_{t-1}} \right)$ denote the return. In the Markov-switching multifractal model, returns are defined as

$$
  r_t = \mu + \sigma_t \epsilon_t
$$

where $\epsilon_t$ is white noise, and $\sigma_t$ is the instantaneous volatility generated by the product of $k$ volatility components

$$
  \sigma_t^2 = \sigma^2 \prod_{i=1}^k m_{t, i} = \sigma^2 M_t
$$

The hidden volatility states $M_t$ are assumed to follow a first-order Markov chain (equivalently that $M_t$ are independent in each level of cascade)  

$$
\begin{align*}
  P\left[ M_t | M_{t-1}, M_{t-2},\dots \right] &= P\left[ M_t | M_{t-1} \right] \\
  &= P\left[ \prod_{i=1}^k m_{t, i} \middle| \prod_{i=1}^k m_{t-1, i}  \right] \\
  &= \prod_{i=1}^k P\left[ m_{t,i} | m_{t-1, i} \right]
\end{align*}
$$

Each volatility component $m_{t, i}$ is updated at time $t$ with transition probability $\gamma_i$ given by

$$
  \gamma_i = 1 - \left( 1 - \gamma_k \right)^{b^{i - k}} \quad \gamma_k \in [0, 1] \quad b \in (1, \infty)
$$

In this context, $i$ refers to the frequency of volatility components. At low frequencies, the sequence $\gamma_i$ is approximately geometric $\gamma_i = \gamma_1 b^{k - 1}$.

The transition probability $\gamma_i$ grows with increasing $i$. Therefore, the persistency of a component $m_{t,i}$ i reduced as $i \to k$. In other words, the expected duration of each volatility component decreases on average as the number of components $k$ increases.

In the discrete case, the volatility components are modelled with a multinomial distribution, while in the contiuous case the lognormal distribution is most commonly used.

### Trinomial MSM

Assuming that $m_{t, i}$ follows a trinomial distribution, it takes one of three values

$$
  m_{t, k} \in \{ m_0, m_1, m_2 \}
$$

which are constrained $m_0 + m_1 + m_2 = 3$ so that the normalization condition $\mathrm{E}[M_t] = 1$ is satisfied. Setting $b = 3$ and $\gamma_i = 1/3$, the transition probability becomes

$$
  \gamma_i = 1 - \left( 1 - \frac{1}{3} \right)^{3^{i - k}}
$$

The unconditional moments of the hidden states are given by

$$
\begin{align*}
  \mathrm{E}\left[ M_t^q \right] &= \mathrm{E}\left[ \left( \prod_{i=1}^k m_{t, k} \right)^q \right] \\
  &= \left( \frac{1}{3}m_0^q + \frac{1}{3}m_1^q + \frac{1}{3}m_2^q \right)^k
\end{align*}
$$

The first and second moments are simply

$$
\begin{align*}
  \mathrm{E}\left[ M_t \right] = \left( \frac{1}{3}m_0 + \frac{1}{3}m_1 + \frac{1}{3}m_2 \right)^k = 1 \\
  \mathrm{E}\left[ M_t^2 \right] = \left( \frac{1}{3}m_0^2 + \frac{1}{3}m_1^2 + \frac{1}{3}m_2^2 \right)^k
\end{align*}
$$

The variance of the hidden states is

$$
\begin{align*}
  \mathrm{Var}[M_t] &= \mathrm{E}\left[ M_t^2 \right] - \mathrm{E}\left[ M_t \right]^2 \\
  &= \left( \frac{1}{3}m_0^2 + \frac{1}{3}m_1^2 + \frac{1}{3}m_2^2 \right)^k - 1
\end{align*}
$$

The autocovariance at time interval $\Delta t$ is

$$
\begin{align*}
  \mathrm{E}\left[M_{t+\Delta} M_t \right] = \prod_{i=1}^k &\left\{ \frac{2}{3}\left[ 1 - \left(1 - \gamma_k \right)^{\Delta t} \right] \left( \frac{1}{3}m_0 m_1 + \frac{1}{3}m_1 m_2 + \frac{1}{3}m_2 m_0 \right) \right. \\
  &\left. + \left( \left( 1 - \gamma_k \right)^{\Delta t} + \frac{1}{3}\left[ 1 - \left( 1 - \gamma_k \right)^{\Delta t} \right] \right) \left( \frac{1}{3}m_0^2 + \frac{1}{3}m_1^2 + \frac{1}{3}m_2^2 \right) \right\}
\end{align*}
$$

The corresponding autocorrelation function is

$$
  \rho_{\Delta t} = \frac{\mathrm{E}\left[M_{t+\Delta} M_t \right] - \mathrm{E}\left[ M_t \right]^2}{\mathrm{Var}[M_t]}
$$

#### Moments of compound process

The moments of absolute returns as a process $\mathrm{E}\left[ \left| x_t \right|^q \right] = \mathrm{E}\left[ \left| \sigma_t \right|^q \right] \mathrm{E}\left[ \left| \epsilon_t \right|^q \right]$ are given by for $q = 1, \dots, 4$

$$
\begin{align*}
  \mathrm{E}\left[ \left| x_t \right| \right] &= \frac{2\sigma}{\sqrt{2\pi}}\left( \frac{1}{3}m_0^{1/2} + \frac{1}{3}m_1^{1/2} + \frac{1}{3}m_2^{1/2} \right)^2 \\
  \mathrm{E}\left[ \left| x_t \right|^2 \right] &= \sigma^2 \\
  \mathrm{E}\left[ \left| x_t \right|^3 \right] &= \frac{4\sigma^3}{\sqrt{2\pi}}\left( \frac{1}{3}m_0^{3/2} + \frac{1}{3}m_1^{3/2} + \frac{1}{3}m_2^{3/2} \right)^2 \\
  \mathrm{E}\left[ \left| x_t \right|^3 \right] &= 3\sigma^4 \left( \frac{1}{3}m_0^2 + \frac{1}{3}m_1^2 + \frac{1}{3}m_2^2 \right)^2
\end{align*}
$$

The variance of the compound process is

$$
\begin{align*}
  \mathrm{var}\left[ \left| x_t \right| \right] &= \mathrm{E}\left[ \left| x_t \right|^2 \right] - \left( \mathrm{E}\left[ \left| x_t \right| \right] \right)^2 \\
  &= \sigma^2 \left[ 1 - \frac{2}{\pi}\left( \frac{1}{3}m_0^{1/2} + \frac{1}{3}m_1^{1/2} + \frac{1}{3}m_2^{1/2} \right)^{2k} \right]
\end{align*}
$$

#### Log-transformed volatility

Considering logarithmic increments

$$
\begin{align*}
  \eta_{t, \Delta t} &= \ln M_t - \ln M_{t-\Delta t} \\
  &= \sum_{i=1}^k \ln m_{t, i} + \sum_{i=1}^k \ln m_{t - \Delta t, i} \\
  &= \sum_{i=1}^k \epsilon_{t, i} + \sum_{i=1}^k \epsilon_{t - \Delta t, i}
\end{align*}
$$

The second unconditional moment of the log-transformed volatility process

$$
\begin{align*}
  \mathrm{E}\left[ \eta_{t+\Delta t, \Delta t}^2 \right] &= \frac{4}{3}\left[ \ln^2\left( m_0 \right) + \ln^2\left( m_1 \right) + \ln^2\left( m_0 \right) \right. \\
  &\quad \left. - \ln\left( m_0 \right)\ln\left( m_1 \right) - \ln\left( m_1 \right)\ln\left( m_2 \right) - \ln\left( m_2 \right)\left( m_0 \right) \right] \sum_{i=1}^k \left[ \frac{1}{3} \left( 1 - \left( 1 - \gamma_k \right)^{\Delta t} \right) \right]
\end{align*}
$$

The autocovariance of the log-transformed volatility process

$$
\begin{align*}
  \mathrm{E}\left[\eta_{t + \Delta t, \Delta t}, \eta_{t, \Delta t} \right] &= 2 \left[\ln \left(m_0\right) \ln \left(m_1\right) + \ln \left(m_1\right) \ln \left(m_2\right) + \ln \left(m_2\right)\ln \left(m_0\right) \right. \\
  &\quad \left. - \ln^2 \left(m_0\right) - \ln^2 \left(m_1\right) - \ln^2 \left(m_2\right) \right] \sum_{i=1}^k \left[ \frac{1}{3}\left( 1 - \left( 1 - \gamma_k \right)^{\Delta t} \right) \right]^2
\end{align*}
$$

The squared autocovariance of the log-transformed volatility process

$$
\begin{align*}
  \mathrm{E}\left[\eta_{t + \Delta t, \Delta t}^2, \eta_{t, \Delta t}^2 \right] =& 2 \sum_{i=1}^k \left\{ \frac{1}{9} \left[ 1 - \left( 1 - \gamma_k \right)^{\Delta t} \right]^2 \right\}\xi \\
  &+ \frac{16}{9} \sum_{i=1}^k \left\{ \frac{1}{3}\left[ 1 - \left( 1 - \gamma_k \right)^{\Delta t} \right] \sum_{i'=1, i'\neq i}^k \frac{1}{3}\left[ 1 - \left( 1 - \gamma_k \right)^{\Delta t} \right] \right\} \xi \\
  &+ 8 \sum_{i=1}^k \left\{ \frac{1}{9}\left[ 1 - \left( 1 - \gamma_k \right)^{\Delta t} \right]^2 \sum_{i'=1, i'\neq i}^k \frac{1}{9}\left[ 1 - \left( 1 - \gamma_k \right)^{\Delta t} \right]^2 \right\}\xi
\end{align*}
$$

with 

$$
  \xi = \left[ \ln\left( m_0 \right) \ln\left( m_1 \right) + \ln\left( m_1 \right) \ln\left( m_2 \right) + \ln\left( m_2 \right) \ln\left( m_0 \right) - \ln^2\left( m_0 \right) - \ln^2\left( m_1 \right) - \ln^2\left( m_2 \right) \right]
$$

### GMM estimation

An MSM model can be estimated using GMM with the following moment conditions

$$
  \mathrm{E}\left[ \xi_{t+\Delta t, \Delta t}, \xi_{t, \Delta t} \right] \\
  \mathrm{E}\left[ \xi_{t+\Delta t, \Delta t}^2, \xi_{t, \Delta t}^2 \right]
$$

where $\xi_{t, \Delta t}$ represents either the log difference of absolute returns or the squared log difference

$$
  \xi_{t, \Delta t} = \ln\left| x_t \right| - \ln\left| x_{t - \Delta t} \right|
$$

or

$$
  \xi_{t, \Delta t} = \ln x_t^2 - \ln x_{t - \Delta t}^2
$$

For an $n$-nomial MSM, the parameter vector is given by

$$
    \theta = \begin{bmatrix} m_0, & m_1, & \dots, & m_{n-1}, & \sigma \end{bmatrix}
$$

Since the moment conditions of the multinomial weights $m_i$ are independent of $\sigma$, the covariance matrix of the parameters should be block-diagonal and estimated values of $\sigma$ should be identical to the sample standard deviation.

#### Log difference of absolute returns

Due to the assumption of independence between the Markov switching and white noise processes, we rewrite the log difference of absolute returns as

$$
\begin{align*}
  \xi_{t, \Delta t} &= \ln\left| x_t \right| - \ln\left| x_{t - \Delta t} \right| \\
  &= \ln\left[ \left| \sqrt{\prod_{i=1}^k} m_{t, i} \sigma \epsilon_t \right| \right] - \ln\left[ \left| \sqrt{\prod_{i=1}^k} m_{t-\Delta t, i} \sigma \epsilon_{t-\Delta} \right| \right] \\
  &= \frac{1}{2}\sum_{i=1}^k \left( \ln m_{t,k} - \ln m_{t-\Delta,k} \right) + \ln\left| \epsilon_t \right| - \ln\left| \epsilon_{t-\Delta t} \right|
\end{align*}
$$

The first autovariance condition moment is

$$
  \mathrm{E}\left[ \xi_{t+\Delta t, \Delta t}, \xi_{t, \Delta t} \right] = \frac{1}{4}\mathrm{E}\left[ \eta_{t+\Delta t, \Delta t} \eta_{t, \Delta t}  \right] + \left( \mathrm{E}\left[ \ln\left| \epsilon_t \right| \right] \right)^2 - \mathrm{E}\left[ \left(\ln\left| \epsilon_t \right|\right)^2 \right] 
$$

The second autovariance condition moment is

$$
\begin{align*}
  \mathrm{E}\left[ \xi_{t+\Delta t, \Delta t}^2, \xi_{t, \Delta t}^2 \right] &= \frac{1}{16}\mathrm{E}\left[ \eta_{t+\Delta t, \Delta t}^2 \eta_{t, \Delta t}^2 \right] - \left\{ \mathrm{E}\left[ \eta_{t, \Delta t}^2 \right] - \mathrm{E}\left[ \eta_{t+\Delta t, \Delta t} \eta_{t, \Delta t} \right] \right\} \cdot \\
  &\quad \left\{ \left( \mathrm{E}\left[ \ln\left| \epsilon_t \right| \right] \right)^2 - \mathrm{E}\left[ \left(\ln\left| \epsilon_t \right|\right)^2 \right] \right\} + 3\mathrm{E}\left[ \left(\ln\left| \epsilon_t \right|\right)^2 \right]^2 \\
  &\quad -4\mathrm{E}\left[ \ln\left| \epsilon_t \right| \right] \mathrm{E}\left[ \left(\ln\left| \epsilon_t \right|\right)^3 \right] + \mathrm{E}\left[ \left(\ln\left| \epsilon_t \right|\right)^4 \right] 
\end{align*}
$$

#### Log difference of squared returns

Writing the squared log returns as 

$$
\begin{align*}
  \ln x_t^2 &= \ln \sigma_t^2 + \ln \epsilon_t^2 \\
  &= \ln \sigma^2 + \sum_{i=1}^k \ln m_{t, i} + \ln \epsilon_t^2
\end{align*}
$$

the log difference of squared return can be expressed as

$$
  \xi_{t,\Delta t} = \sum_{i=1}^k \left( \ln m_{t, i} - \ln m_{t-\Delta t, i} \right) + \ln \epsilon_t^2 - \ln \epsilon_{t-\Delta t}^2
$$

### Maximum likelihood estimation

The $n$-nomial MSM model can be rewritten in the quasi state space

$$
\begin{gather*}
  x_t = \psi_t \epsilon_t \\
  \psi_t = A \psi_{t-1}
\end{gather*}
$$

where $\psi_t = \left( \psi_{t, i} \right)_{i=1}^{n^k}$ is the volatility state vector and $A$ is the transition matrix of size $n^k \times n^k$. The number of coefficients in $A$ increases with $(d^2 - d)$ with $d = n^k$, making it inefficient to estimate $A$ directly. By assuming that all parameters of the model are known, the conditional probability distribution of the volatility state can be expressed as

$$
  \hat{\psi}_{t|t} \equiv E(\psi_{t}| I_t ) = \begin{bmatrix} P\left( \psi_{t, 1} | I_t \right) \\ P\left( \psi_{t, 2} | I_t \right) \\ \vdots \\ P\left( \psi_{t, d} | I_t \right) \end{bmatrix}
$$

where $I_t = \{ x_i \}_{i=0}^t$ is the information set. By Bayes' law, the posterior probability $P\left( \psi_t | x_t, I_{t-1} \right)$ are given by

$$
  P\left( \psi_t | x_t, I_{t-1} \right) \equiv P\left( \psi_t | I_t \right) = \frac{f\left(x_t | \psi_t, I_{t-1} \right) P\left( \psi_t | I_{t-1} \right)}{f\left( x_t | I_{t-1} \right)}
$$

with the prior probability

$$
  P\left( \psi_t | I_t \right) = \sum_{\psi_{t-1}} P\left( \psi_t | \psi_{t-1} \right) P\left( \psi_{t-1} | I_{t-1} \right)
$$

and the density

$$
  f\left( x_t | I_{t-1} \right) = \sum_{\psi_t} f\left( x_t, \psi_t | I_{t-1} \right) = \sum_{\psi_t} P\left( \psi_t | I_{t-1} \right) f\left( x_t | \psi_t, I_{t-1} \right)
$$

Let $f_t$ be the density vector of $x_t$ conditional on $\psi_t$ and $I_{t-1}$

$$
  f_t = \begin{bmatrix} f\left( x_t | \theta_1, I_{t-1} \right) \\ f\left( x_t | \theta_2, I_{t-1} \right) \\ \vdots \\ f\left( x_t | \theta_d, I_{t-1} \right) \end{bmatrix} = \begin{bmatrix} f\left( x_t | \theta_1, \psi_{t,1}, I_{t-1} \right) \\ f\left( x_t | \theta_2, \psi_{t,2} I_{t-1} \right) \\ \vdots \\ f\left( x_t | \theta_d, \psi_{t, d}, I_{t-1} \right) \end{bmatrix}
$$

The conditional density $f\left( x_t | I_{t-1} \right)$ is determined by

$$
  f\left( x_t | I_{t-1} \right) = f_t^T \hat{\psi}_{t|t-1} = \mathbf{1}_d^T \left(f_t \odot \hat{\psi}_{t|t-1} \right)
$$

where $\odot$ denotes element-wise matrix multiplication and $\mathbf{1}_d^T$ is a unit vector. The filter inference $\hat{\psi}_{t|t}$ is written matrix notation by

$$
  \hat{\psi}_{t|t} = \frac{f_t \odot \hat{\psi}_{t|t-1}}{\mathbf{1}_d^T f_t \odot \hat{\psi}_{t|t-1}}
$$

which describes the filtered regime probabilities as the updated estimate of $\hat{\psi}_{t|t}$ of $\hat{\psi}_{t|t-1}$ given new information $x_t$. Because the filtered inference updates as

$$
  \hat{\psi}_{t+1|t} = A\hat{\psi}_{t|t}
$$

the iterated filtered inference can be written

$$
  \hat{\psi}_{t+1|t} = \frac{\left[f_t \odot A \right] \hat{\psi}_{t|t-1}}{\mathbf{1}_d^T \left[f_t \odot A \right] \hat{\psi}_{t|t-1}}
$$

The log-likelihood function can be derived as a by-product of the filter

$$
  \ln L(\theta|X) = \sum_{t=1}^\tau \ln\hat{\psi}_{t|t-1} \left[ f_t \odot A \right]\mathbf{1}_d^T 
$$

Bayes' rule implies

$$
\begin{align*}
  \ln\hat{\psi}_{t|t-1} \left[ f_t \odot A \mathbf{1}_d^T \right] &= \sum_{i=1}^d \sum_{j=1}^d P\left(\psi_{t-1}=m_i, \psi_t = m_j | I_{t-1}\right) f\left(x_t| \psi_{t-1} = m_i, \psi_t = m_j \right) \\
  &= \sum_{i=1}^d \sum_{j=1}^d \hat{\psi}_{t-1|t-1, i} a_{ij} f^{ij}(x_t)
\end{align*}
$$

where $f^{ij}(x_t) = f\left(x_t| \psi_{t-1} = m_i, \psi_t = m_j \right)$.

#### Probability state vector

The probability notion represented by the estimate $\hat{\psi}_{t|\tau} = E\left[ \psi_t | I_\tau \right]$ depends on the bound of $\tau$.

- For $\tau = t$, a filtering probability $\hat{\psi}_{t|\tau}$ gives an estimate for $\psi_t$ based on the available information up to $t = \tau$.
- For $\tau > t$, a smoothing probability $\hat{\psi}_{t|\tau}$ gives inference inference about $\psi_t$ and $x_t$ based on all information in the sample. The smoothed joint probability based on full information is given by

$$
\begin{align*}
  P\left[\psi_t^{(i)}, \psi_{t+1}^{(i)} | I_\tau \right] &= \frac{P\left[ \psi_{t + 1}^{(i)} | I_\tau \right] P\left[ \psi_t^{(j)} | I_t \right] P\left[ \psi_{t+1}^{(i)} | \psi_t^{(i)} \right]}{P\left[ \psi_{t + 1}^{(i)} | I_t \right]} \\
  P\left[ \psi_t^{(j)} | I_\tau \right] &= \sum_{i=1}^d P\left[\psi_t^{(i)}, \psi_{t+1}^{(i)} | I_\tau \right]
\end{align*}
$$

- For $\tau < t$ a forecasting probability $\hat{\psi}_{t|\tau}$ gives inference probability over future periodes on the basis of estimates of state probabilities.

### Model selection test

#### Markov switching criterion (MSC)

The Markov switching criterio (MSC) is a kind of penalized likelihood criteria with the form

$$
  \mathrm{MSC} = -2 \ln L + \sum_{i=1}^N \frac{T_i \left(T_i + \lambda_i \kappa \right)}{\delta_i T_i - \lambda_i \kappa - 2}
$$

where $N$ is the number of states, $\kappa$ is the number of multi-fractal parameters, $T_i$ is the number of observations in state $i$, which is calculated as the sum of smoothing probabilities of each state $i$. 