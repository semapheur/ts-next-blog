---
title: 'Statistics'
subject: 'Mathematics'
showToc: true
---

# Time series analysis

## Autoregressive models (AR)

An autoregressive model of order $p$, denoted $\mathrm{AR}(p)$ is defined as 

$$
    X_t = c + \sum_{i=1}^p \varphi_i X_{t-i} + \epsilon_t
$$

where
- $\phi_i$ re the parameters of the model
- $c$ is a constant
- $\epsilon_t$ is white noise

This can be equivalently written in terms of the lag (backshift) operator $L$ ($B$) as

$$
\begin{gather*}
  X_t = c + \sum_{i=1}^p \varphi_i L^i X_t + \epsilon_t \\
  \varphi(L)X_t \equiv \left(1 - \sum_{i=1}^p \varphi_i L^i \right) X_t = \epsilon_t
\end{gather*}
$$

### Autoregressive moving average (ARMA)

An autoregressive-moving average (ARMA) model describes a stationary stochastic process in terms of two polynomials, one for the autoregression (AR) and the second for the moving average (MA). An ARMA-model with $p$ autoregressive terms and $q$ moving-average terms, denoted $\mathrm{ARMA}(p, q)$ is defined as

$$
\begin{gather*}
  X_t = c + \epsilon_t + \sum_{i=1}^p \varphi_i X_{t-i} + \sum_{i=1}^q \theta_i \epsilon_{t-i} \\
  \left(1 - \sum_{i=1}^p \varphi_i L^i \right) X_t = \left(1 + \sum_{i=1}^p \theta_i L^i \right)\epsilon_t \\
  \varphi(L) X_t = \theta(L) \epsilon_t
\end{gather*}
$$

### Autoregressive integrated moving average (ARIMA)

Assume that the AR-polynomial $\alpha(L) = 1 - \sum_{i=1}^{p'} \alpha_i L^i$ has a unit root $(1 - L)$ of multiplicity $d$. Then it can be rewritten as

$$
  1 - \sum_{i=1}^{p'} \alpha_i B^i = \left(1 - \sum_{i=1}^{p' - d} \varphi_i B^i \right) (1 - L)^d
$$

An $\mathrm{ARIMA}(p, d, q)$ process expresses this polynomial factorisation property with $p = p' - d$ given by

$$
  \left(1 - \sum_{i=1}^{p' - d} \varphi_i B^i \right) (1 - L)^d X_t = \delta + \left(1 + \sum_{i=1}^p \theta_i L^i \right)\epsilon_t
$$

with drift $\frac{\delta}{1 - \sum_{i=1}^p \varphi_i}$.

### Autoregressive fractal integrated moving average (ARFIMA)

The ARFIMA process allows a fractional value for the mulitiplicity $d$. For integer $d \in \mathbb{N}$, the differencing operator $(1 - L)^d$ can be expressed as a binomial expansion

$$
  (1 - L)^d = \sum_{i=0}^d \binom{d}{i} (-1)^i L^{d - i}
$$

To allow non-integer $d$, the binomial coefficients can be expressed in terms of the Gamma function

$$
  \binom{d}{i} = \frac{d!}{i!(d - i)!} = \frac{\Gamma(d + 1)}{\Gamma(i + 1)\Gamma(d - i + 1)} 
$$

The self-similar properties of ARFIMA depend on the values of $d$
- $d = 0$ reduces to an $\mathrm{ARMA}(p, q)$ process, i.e. a short-memory process
- For $d \in \left(0, \frac{1}{2}\right)$, the autocorrelations are all positive. The resulting process is asymptotically second order stationary exhibiting long memory. The corresponding Hurst exponent is $H = \frac{1}{2} + d$.
- For $d \in \left( -\frac{1}{2}, 0 \right)$ the autocorrelations are all negative, except $\rho(0) = 1$. The resulting process is anti-persistent exhibiting intermediate memory.
- For $d \geq \frac{1}{2}$, the process is no longer covariance stationary, and have infinite variance. By taking appropriate differences, a non-stationary process with $d > \frac{1}{2}$ can be reduced to a stationary process with $-\frac{1}{2} < d < \frac{1}{2}$


## Autoregressive conditional heteroskedasticity models (ARCH)

Autoregressive conditional heteroskedasticity (ARCH) models the variance of a mean process $\mu_t$ of the form

$$
  r_t = \mu_t + \epsilon_t
$$

with the return residuals $\epsilon_t = \sigma_t z_t$ where $z_t$ is a white noise process and

$$
  \sigma_t^2 = \alpha_0 + \sum_{i=1}^q \alpha_i \epsilon_{t-i}^2
$$

### Generalized autoregressive conditional heteroskedasticity (GARCH)

$$
  \sigma_t^2 = \omega + \sum_{i=1}^q \alpha_i \epsilon_{t-i}^2 + \sum_{i=1}^p \beta_i \sigma_{t-i}^2
$$

### FIGARCH

## Markov-switching multifractal (MSM)

Let $P_t$ be the price of a financial asseet, and let $r_t = \ln\left(\frac{P_t}{P_{t-1}} \right)$ denote the return. In the Markov-switching multifractal model, returns are defined as

$$
  r_t = \mu + \sigma_t \epsilon_t
$$

where $\epsilon_t$ is white noise, and $\sigma_t$ is the instantaneous volatility generated by the product of $k$ volatility components

$$
  \sigma_t^2 = \sigma^2 \prod_{i=1}^k m_{t, i} = \sigma^2 M_t
$$

The hidden volatility states $M_t$ are assumed to follow a first-order Markov chain (equivalently that $M_t$ are independent in each level of cascade)  

$$
\begin{align*}
  P\left[ M_t | M_{t-1}, M_{t-2},\dots \right] &= P\left[ M_t | M_{t-1} \right] \\
  &= P\left[ \prod_{i=1}^k m_{t, i} \middle| \prod_{i=1}^k m_{t-1, i}  \right] \\
  &= \prod_{i=1}^k P\left[ m_{t,i} | m_{t-1, i} \right]
\end{align*}
$$

Each volatility component $m_{t, i}$ is updated at time $t$ with transition probability $\gamma_i$ given by

$$
  \gamma_i = 1 - \left( 1 - \gamma_k \right)^{b^{i - k}} \quad \gamma_k \in [0, 1] \quad b \in (1, \infty)
$$

In this context, $i$ refers to the frequency of volatility components. At low frequencies, the sequence $\gamma_i$ is approximately geometric $\gamma_i = \gamma_1 b^{k - 1}$.

The transition probability $\gamma_i$ grows with increasing $i$. Therefore, the persistency of a component $m_{t,i}$ i reduced as $i \to k$. In other words, the expected duration of each volatility component decreases on average as the number of components $k$ increases.

In the discrete case, the volatility components are modelled with a multinomial distribution, while in the contiuous case the lognormal distribution is most commonly used.

### Trinomial MSM

Assuming that $m_{t, i}$ follows a trinomial distribution, it takes one of three values

$$
  m_{t, k} \in \{ m_0, m_1, m_2 \}
$$

which are constrained $m_0 + m_1 + m_2 = 3$ so that the normalization condition $\mathrm{E}[M_t] = 1$ is satisfied. Setting $b = 3$ and $\gamma_i = 1/3$, the transition probability becomes

$$
  \gamma_i = 1 - \left( 1 - \frac{1}{3} \right)^{3^{i - k}}
$$

The unconditional moments of the hidden states are given by

$$
\begin{align*}
  \mathrm{E}\left[ M_t^q \right] &= \mathrm{E}\left[ \left( \prod_{i=1}^k m_{t, k} \right)^q \right] \\
  &= \left( \frac{1}{3}m_0^q + \frac{1}{3}m_1^q + \frac{1}{3}m_2^q \right)^k
\end{align*}
$$

The first and second moments are simply

$$
\begin{align*}
  \mathrm{E}\left[ M_t \right] = \left( \frac{1}{3}m_0 + \frac{1}{3}m_1 + \frac{1}{3}m_2 \right)^k = 1 \\
  \mathrm{E}\left[ M_t^2 \right] = \left( \frac{1}{3}m_0^2 + \frac{1}{3}m_1^2 + \frac{1}{3}m_2^2 \right)^k
\end{align*}
$$

The variance of the hidden states is

$$
\begin{align*}
  \mathrm{Var}[M_t] &= \mathrm{E}\left[ M_t^2 \right] - \mathrm{E}\left[ M_t \right]^2 \\
  &= \left( \frac{1}{3}m_0^2 + \frac{1}{3}m_1^2 + \frac{1}{3}m_2^2 \right)^k - 1
\end{align*}
$$

The autocovariance at time interval $\Delta t$ is

$$
\begin{align*}
  \mathrm{E}\left[M_{t+\Delta} M_t \right] = \prod_{i=1}^k &\left\{ \frac{2}{3}\left[ 1 - \left(1 - \gamma_k \right)^{\Delta t} \right] \left( \frac{1}{3}m_0 m_1 + \frac{1}{3}m_1 m_2 + \frac{1}{3}m_2 m_0 \right) \right. \\
  &\left. + \left( \left( 1 - \gamma_k \right)^{\Delta t} + \frac{1}{3}\left[ 1 - \left( 1 - \gamma_k \right)^{\Delta t} \right] \right) \left( \frac{1}{3}m_0^2 + \frac{1}{3}m_1^2 + \frac{1}{3}m_2^2 \right) \right\}
\end{align*}
$$

The corresponding autocorrelation function is

$$
  \rho_{\Delta t} = \frac{\mathrm{E}\left[M_{t+\Delta} M_t \right] - \mathrm{E}\left[ M_t \right]^2}{\mathrm{Var}[M_t]}
$$

#### Moments of compound process

The moments of absolute returns as a process $\mathrm{E}\left[ \left| x_t \right|^q \right] = \mathrm{E}\left[ \left| \sigma_t \right|^q \right] \mathrm{E}\left[ \left| \epsilon_t \right|^q \right]$ are given by for $q = 1, \dots, 4$

$$
\begin{align*}
  \mathrm{E}\left[ \left| x_t \right| \right] &= \frac{2\sigma}{\sqrt{2\pi}}\left( \frac{1}{3}m_0^{1/2} + \frac{1}{3}m_1^{1/2} + \frac{1}{3}m_2^{1/2} \right)^2 \\
  \mathrm{E}\left[ \left| x_t \right|^2 \right] &= \sigma^2 \\
  \mathrm{E}\left[ \left| x_t \right|^3 \right] &= \frac{4\sigma^3}{\sqrt{2\pi}}\left( \frac{1}{3}m_0^{3/2} + \frac{1}{3}m_1^{3/2} + \frac{1}{3}m_2^{3/2} \right)^2 \\
  \mathrm{E}\left[ \left| x_t \right|^3 \right] &= 3\sigma^4 \left( \frac{1}{3}m_0^2 + \frac{1}{3}m_1^2 + \frac{1}{3}m_2^2 \right)^2
\end{align*}
$$

The variance of the compound process is

$$
\begin{align*}
  \mathrm{var}\left[ \left| x_t \right| \right] &= \mathrm{E}\left[ \left| x_t \right|^2 \right] - \left( \mathrm{E}\left[ \left| x_t \right| \right] \right)^2 \\
  &= \sigma^2 \left[ 1 - \frac{2}{\pi}\left( \frac{1}{3}m_0^{1/2} + \frac{1}{3}m_1^{1/2} + \frac{1}{3}m_2^{1/2} \right)^{2k} \right]
\end{align*}
$$

#### Log-transformed volatility

Considering logarithmic increments

$$
\begin{align*}
  \eta_{t, \Delta t} &= \ln M_t - \ln M_{t-\Delta t} \\
  &= \sum_{i=1}^k \ln m_{t, i} + \sum_{i=1}^k \ln m_{t - \Delta t, i} \\
  &= \sum_{i=1}^k \epsilon_{t, i} + \sum_{i=1}^k \epsilon_{t - \Delta t, i}
\end{align*}
$$

The second unconditional moment of the log-transformed volatility process

$$
\begin{align*}
  \mathrm{E}\left[ \eta_{t+\Delta t, \Delta t}^2 \right] &= \frac{4}{3}\left[ \ln^2\left( m_0 \right) + \ln^2\left( m_1 \right) + \ln^2\left( m_0 \right) \right. \\
  &\quad \left. - \ln\left( m_0 \right)\ln\left( m_1 \right) - \ln\left( m_1 \right)\ln\left( m_2 \right) - \ln\left( m_2 \right)\left( m_0 \right) \right] \sum_{i=1}^k \left[ \frac{1}{3} \left( 1 - \left( 1 - \gamma_k \right)^{\Delta t} \right) \right]
\end{align*}
$$

The autocovariance of the log-transformed volatility process

$$
\begin{align*}
  \mathrm{E}\left[\eta_{t + \Delta t, \Delta t}, \eta_{t, \Delta t} \right] &= 2 \left[\ln \left(m_0\right) \ln \left(m_1\right) + \ln \left(m_1\right) \ln \left(m_2\right) + \ln \left(m_2\right)\ln \left(m_0\right) \right. \\
  &\quad \left. - \ln^2 \left(m_0\right) - \ln^2 \left(m_1\right) - \ln^2 \left(m_2\right) \right] \sum_{i=1}^k \left[ \frac{1}{3}\left( 1 - \left( 1 - \gamma_k \right)^{\Delta t} \right) \right]^2
\end{align*}
$$

The squared autocovariance of the log-transformed volatility process

$$
\begin{align*}
  \mathrm{E}\left[\eta_{t + \Delta t, \Delta t}^2, \eta_{t, \Delta t}^2 \right] =& 2 \sum_{i=1}^k \left\{ \frac{1}{9} \left[ 1 - \left( 1 - \gamma_k \right)^{\Delta t} \right]^2 \right\}\xi \\
  &+ \frac{16}{9} \sum_{i=1}^k \left\{ \frac{1}{3}\left[ 1 - \left( 1 - \gamma_k \right)^{\Delta t} \right] \sum_{i'=1, i'\neq i}^k \frac{1}{3}\left[ 1 - \left( 1 - \gamma_k \right)^{\Delta t} \right] \right\} \xi \\
  &+ 8 \sum_{i=1}^k \left\{ \frac{1}{9}\left[ 1 - \left( 1 - \gamma_k \right)^{\Delta t} \right]^2 \sum_{i'=1, i'\neq i}^k \frac{1}{9}\left[ 1 - \left( 1 - \gamma_k \right)^{\Delta t} \right]^2 \right\}\xi
\end{align*}
$$

with 

$$
  \xi = \left[ \ln\left( m_0 \right) \ln\left( m_1 \right) + \ln\left( m_1 \right) \ln\left( m_2 \right) + \ln\left( m_2 \right) \ln\left( m_0 \right) - \ln^2\left( m_0 \right) - \ln^2\left( m_1 \right) - \ln^2\left( m_2 \right) \right]
$$

### GMM estimation

An MSM model can be estimated using GMM with the following moment conditions

$$
  \mathrm{E}\left[ \xi_{t+\Delta t, \Delta t}, \xi_{t, \Delta t} \right] \\
  \mathrm{E}\left[ \xi_{t+\Delta t, \Delta t}^2, \xi_{t, \Delta t}^2 \right]
$$

where $\xi_{t, \Delta t}$ represents either the log difference of absolute returns or the squared log difference

$$
  \xi_{t, \Delta t} = \ln\left| x_t \right| - \ln\left| x_{t - \Delta t} \right|
$$

or

$$
  \xi_{t, \Delta t} = \ln x_t^2 - \ln x_{t - \Delta t}^2
$$

For an $n$-nomial MSM, the parameter vector is given by

$$
    \theta = \begin{bmatrix} m_0, & m_1, & \dots, & m_{n-1}, & \sigma \end{bmatrix}
$$

Since the moment conditions of the multinomial weights $m_i$ are independent of $\sigma$, the covariance matrix of the parameters should be block-diagonal and estimated values of $\sigma$ should be identical to the sample standard deviation.

#### Log difference of absolute returns

Due to the assumption of independence between the Markov switching and white noise processes, we rewrite the log difference of absolute returns as

$$
\begin{align*}
  \xi_{t, \Delta t} &= \ln\left| x_t \right| - \ln\left| x_{t - \Delta t} \right| \\
  &= \ln\left[ \left| \sqrt{\prod_{i=1}^k} m_{t, i} \sigma \epsilon_t \right| \right] - \ln\left[ \left| \sqrt{\prod_{i=1}^k} m_{t-\Delta t, i} \sigma \epsilon_{t-\Delta} \right| \right] \\
  &= \frac{1}{2}\sum_{i=1}^k \left( \ln m_{t,k} - \ln m_{t-\Delta,k} \right) + \ln\left| \epsilon_t \right| - \ln\left| \epsilon_{t-\Delta t} \right|
\end{align*}
$$

The first autovariance condition moment is

$$
  \mathrm{E}\left[ \xi_{t+\Delta t, \Delta t}, \xi_{t, \Delta t} \right] = \frac{1}{4}\mathrm{E}\left[ \eta_{t+\Delta t, \Delta t} \eta_{t, \Delta t}  \right] + \left( \mathrm{E}\left[ \ln\left| \epsilon_t \right| \right] \right)^2 - \mathrm{E}\left[ \left(\ln\left| \epsilon_t \right|\right)^2 \right] 
$$

The second autovariance condition moment is

$$
\begin{align*}
  \mathrm{E}\left[ \xi_{t+\Delta t, \Delta t}^2, \xi_{t, \Delta t}^2 \right] &= \frac{1}{16}\mathrm{E}\left[ \eta_{t+\Delta t, \Delta t}^2 \eta_{t, \Delta t}^2 \right] - \left\{ \mathrm{E}\left[ \eta_{t, \Delta t}^2 \right] - \mathrm{E}\left[ \eta_{t+\Delta t, \Delta t} \eta_{t, \Delta t} \right] \right\} \cdot \\
  &\quad \left\{ \left( \mathrm{E}\left[ \ln\left| \epsilon_t \right| \right] \right)^2 - \mathrm{E}\left[ \left(\ln\left| \epsilon_t \right|\right)^2 \right] \right\} + 3\mathrm{E}\left[ \left(\ln\left| \epsilon_t \right|\right)^2 \right]^2 \\
  &\quad -4\mathrm{E}\left[ \ln\left| \epsilon_t \right| \right] \mathrm{E}\left[ \left(\ln\left| \epsilon_t \right|\right)^3 \right] + \mathrm{E}\left[ \left(\ln\left| \epsilon_t \right|\right)^4 \right] 
\end{align*}
$$

#### Log difference of squared returns

Writing the squared log returns as 

$$
\begin{align*}
  \ln x_t^2 &= \ln \sigma_t^2 + \ln \epsilon_t^2 \\
  &= \ln \sigma^2 + \sum_{i=1}^k \ln m_{t, i} + \ln \epsilon_t^2
\end{align*}
$$

the log difference of squared return can be expressed as

$$
  \xi_{t,\Delta t} = \sum_{i=1}^k \left( \ln m_{t, i} - \ln m_{t-\Delta t, i} \right) + \ln \epsilon_t^2 - \ln \epsilon_{t-\Delta t}^2
$$

### Maximum likelihood estimation

The $n$-nomial MSM model can be rewritten in the quasi state space

$$
\begin{gather*}
  x_t = \psi_t \epsilon_t \\
  \psi_t = A \psi_{t-1}
\end{gather*}
$$

where $\psi_t = \left( \psi_{t, i} \right)_{i=1}^{n^k}$ is the volatility state vector and $A$ is the transition matrix of size $n^k \times n^k$. The number of coefficients in $A$ increases with $(d^2 - d)$ with $d = n^k$, making it inefficient to estimate $A$ directly. By assuming that all parameters of the model are known, the conditional probability distribution of the volatility state can be expressed as

$$
  \hat{\psi}_{t|t} \equiv E(\psi_{t}| I_t ) = \begin{bmatrix} P\left( \psi_{t, 1} | I_t \right) \\ P\left( \psi_{t, 2} | I_t \right) \\ \vdots \\ P\left( \psi_{t, d} | I_t \right) \end{bmatrix}
$$

where $I_t = \{ x_i \}_{i=0}^t$ is the information set. By Bayes' law, the posterior probability $P\left( \psi_t | x_t, I_{t-1} \right)$ are given by

$$
  P\left( \psi_t | x_t, I_{t-1} \right) \equiv P\left( \psi_t | I_t \right) = \frac{f\left(x_t | \psi_t, I_{t-1} \right) P\left( \psi_t | I_{t-1} \right)}{f\left( x_t | I_{t-1} \right)}
$$

with the prior probability

$$
  P\left( \psi_t | I_t \right) = \sum_{\psi_{t-1}} P\left( \psi_t | \psi_{t-1} \right) P\left( \psi_{t-1} | I_{t-1} \right)
$$

and the density

$$
  f\left( x_t | I_{t-1} \right) = \sum_{\psi_t} f\left( x_t, \psi_t | I_{t-1} \right) = \sum_{\psi_t} P\left( \psi_t | I_{t-1} \right) f\left( x_t | \psi_t, I_{t-1} \right)
$$

Let $f_t$ be the density vector of $x_t$ conditional on $\psi_t$ and $I_{t-1}$

$$
  f_t = \begin{bmatrix} f\left( x_t | \theta_1, I_{t-1} \right) \\ f\left( x_t | \theta_2, I_{t-1} \right) \\ \vdots \\ f\left( x_t | \theta_d, I_{t-1} \right) \end{bmatrix} = \begin{bmatrix} f\left( x_t | \theta_1, \psi_{t,1}, I_{t-1} \right) \\ f\left( x_t | \theta_2, \psi_{t,2} I_{t-1} \right) \\ \vdots \\ f\left( x_t | \theta_d, \psi_{t, d}, I_{t-1} \right) \end{bmatrix}
$$

The conditional density $f\left( x_t | I_{t-1} \right)$ is determined by

$$
  f\left( x_t | I_{t-1} \right) = f_t^T \hat{\psi}_{t|t-1} = \mathbf{1}_d^T \left(f_t \odot \hat{\psi}_{t|t-1} \right)
$$

where $\odot$ denotes element-wise matrix multiplication and $\mathbf{1}_d^T$ is a unit vector. The filter inference $\hat{\psi}_{t|t}$ is written matrix notation by

$$
  \hat{\psi}_{t|t} = \frac{f_t \odot \hat{\psi}_{t|t-1}}{\mathbf{1}_d^T f_t \odot \hat{\psi}_{t|t-1}}
$$

which describes the filtered regime probabilities as the updated estimate of $\hat{\psi}_{t|t}$ of $\hat{\psi}_{t|t-1}$ given new information $x_t$. Because the filtered inference updates as

$$
  \hat{\psi}_{t+1|t} = A\hat{\psi}_{t|t}
$$

the iterated filtered inference can be written

$$
  \hat{\psi}_{t+1|t} = \frac{\left[f_t \odot A \right] \hat{\psi}_{t|t-1}}{\mathbf{1}_d^T \left[f_t \odot A \right] \hat{\psi}_{t|t-1}}
$$

The log-likelihood function can be derived as a by-product of the filter

$$
  \ln L(\theta|X) = \sum_{t=1}^\tau \ln\hat{\psi}_{t|t-1} \left[ f_t \odot A \right]\mathbf{1}_d^T 
$$

Bayes' rule implies

$$
\begin{align*}
  \ln\hat{\psi}_{t|t-1} \left[ f_t \odot A \mathbf{1}_d^T \right] &= \sum_{i=1}^d \sum_{j=1}^d P\left(\psi_{t-1}=m_i, \psi_t = m_j | I_{t-1}\right) f\left(x_t| \psi_{t-1} = m_i, \psi_t = m_j \right) \\
  &= \sum_{i=1}^d \sum_{j=1}^d \hat{\psi}_{t-1|t-1, i} a_{ij} f^{ij}(x_t)
\end{align*}
$$

where $f^{ij}(x_t) = f\left(x_t| \psi_{t-1} = m_i, \psi_t = m_j \right)$.

#### Probability state vector

The probability notion represented by the estimate $\hat{\psi}_{t|\tau} = E\left[ \psi_t | I_\tau \right]$ depends on the bound of $\tau$.

- For $\tau = t$, a filtering probability $\hat{\psi}_{t|\tau}$ gives an estimate for $\psi_t$ based on the available information up to $t = \tau$.
- For $\tau > t$, a smoothing probability $\hat{\psi}_{t|\tau}$ gives inference inference about $\psi_t$ and $x_t$ based on all information in the sample. The smoothed joint probability based on full information is given by

$$
\begin{align*}
  P\left[\psi_t^{(i)}, \psi_{t+1}^{(i)} | I_\tau \right] &= \frac{P\left[ \psi_{t + 1}^{(i)} | I_\tau \right] P\left[ \psi_t^{(j)} | I_t \right] P\left[ \psi_{t+1}^{(i)} | \psi_t^{(i)} \right]}{P\left[ \psi_{t + 1}^{(i)} | I_t \right]} \\
  P\left[ \psi_t^{(j)} | I_\tau \right] &= \sum_{i=1}^d P\left[\psi_t^{(i)}, \psi_{t+1}^{(i)} | I_\tau \right]
\end{align*}
$$

- For $\tau < t$ a forecasting probability $\hat{\psi}_{t|\tau}$ gives inference probability over future periodes on the basis of estimates of state probabilities.

### Model selection test

#### Markov switching criterion (MSC)

The Markov switching criterio (MSC) is a kind of penalized likelihood criteria with the form

$$
  \mathrm{MSC} = -2 \ln L + \sum_{i=1}^N \frac{T_i \left(T_i + \lambda_i \kappa \right)}{\delta_i T_i - \lambda_i \kappa - 2}
$$

where $N$ is the number of states, $\kappa$ is the number of multi-fractal parameters, $T_i$ is the number of observations in state $i$, which is calculated as the sum of smoothing probabilities of each state $i$. 